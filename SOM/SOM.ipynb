{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapas Auto Organizativos\n",
    "\n",
    "En este ejemplo se implementa una Red de Kohonen o Mapa Auto Organizativo para agrupar los dígitos manuscritos del conjunto de datos Mixed National Institute of Standards and Technology (MNIST). Esta muestra de entrenamiento fue construida por Yann LeCun y otros para servir como benchmark para los algoritmos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener los datos de MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Leer los datos de MNIST de la ruta 'path'\"\"\"\n",
    "    labels_path = os.path.join(path,'%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path,'%s-images.idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    "    return images, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "    \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X_train[y_train == 7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('mnist_scaled.npz', X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_train', 'y_train', 'X_test', 'y_test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = np.load('mnist_scaled.npz')\n",
    "mnist.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [mnist[f] for f in mnist.files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de la red SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Data_Shape = np.array([60000, 784])\n",
    "SOM_Network_Shape = np.array([50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train/np.linalg.norm(X_train, axis=1).reshape(Raw_Data_Shape[0], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de los pesos y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Initial_Guess = np.random.uniform(0, 1, (SOM_Network_Shape[0]*SOM_Network_Shape[1], Raw_Data_Shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Initial_Guess_Norm = W_Initial_Guess/np.linalg.norm(W_Initial_Guess, axis=1).reshape(SOM_Network_Shape[0]*SOM_Network_Shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = np.mgrid[0:SOM_Network_Shape[0],0:SOM_Network_Shape[1]].reshape(2, SOM_Network_Shape[0]*SOM_Network_Shape[1]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = 0 \n",
    "Max_Epoch = 20000\n",
    "eta_0 = 0.1\n",
    "eta_time_const = 1000\n",
    "sigma_0 = np.max(SOM_Network_Shape) * 0.5\n",
    "sigma_time_const = 1000/np.log10(sigma_0)\n",
    "\n",
    "Saturation = np.array([[221, 65, 50], [107, 91, 149], [254, 132, 14], [103, 46, 159], [198, 33, 104], [255, 0, 0], \n",
    "                 [0, 110, 109], [0, 83, 156], [117, 81, 57], [191, 214, 65]])\n",
    "Saturation_Norm = Saturation/np.linalg.norm(Saturation, axis=1).reshape(10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winning_neuron(x, W):\n",
    "    # Also called as Best Matching Neuron/Best Matching Unit (BMU)\n",
    "    return np.argmin(np.linalg.norm(x - W, axis=1))\n",
    "\n",
    "def update_weights(lr, var, x, W, Grid):\n",
    "    i = winning_neuron(x, W)\n",
    "    d = np.square(np.linalg.norm(Grid - Grid[i], axis=1))\n",
    "    # Topological Neighbourhood Function\n",
    "    h = np.exp(-d/(2 * var * var))\n",
    "    W = W + lr * h[:, np.newaxis] * (x - W)\n",
    "    return W\n",
    "\n",
    "def decay_learning_rate(eta_initial, epoch, time_const):\n",
    "    return eta_initial * np.exp(-epoch/time_const)\n",
    "\n",
    "def decay_variance(sigma_initial, epoch, time_const):\n",
    "    return sigma_initial * np.exp(-epoch/time_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Learning Rate:  0.1  Varinance:  25.0 \n",
      "\n",
      "Epoch:  1  Learning Rate:  0.1  Varinance:  25.0 \n",
      "\n",
      "Epoch:  2  Learning Rate:  0.09990004998333751  Varinance:  24.965075916357588 \n",
      "\n",
      "Epoch:  3  Learning Rate:  0.09980019986673332  Varinance:  24.930200620379903 \n",
      "\n",
      "Epoch:  4  Learning Rate:  0.0997004495503373  Varinance:  24.895374043912373 \n",
      "\n",
      "Epoch:  5  Learning Rate:  0.09960079893439916  Varinance:  24.86059611889562 \n",
      "\n",
      "Epoch:  6  Learning Rate:  0.09950124791926823  Varinance:  24.825866777365356 \n",
      "\n",
      "Epoch:  7  Learning Rate:  0.09940179640539354  Varinance:  24.79118595145223 \n",
      "\n",
      "Epoch:  8  Learning Rate:  0.09930244429332352  Varinance:  24.756553573381705 \n",
      "\n",
      "Epoch:  9  Learning Rate:  0.09920319148370607  Varinance:  24.72196957547392 \n",
      "\n",
      "Epoch:  10  Learning Rate:  0.09910403787728837  Varinance:  24.687433890143556 \n",
      "\n",
      "Epoch:  11  Learning Rate:  0.09900498337491681  Varinance:  24.65294644989972 \n",
      "\n",
      "Epoch:  12  Learning Rate:  0.09890602787753688  Varinance:  24.61850718734579 \n",
      "\n",
      "Epoch:  13  Learning Rate:  0.09880717128619305  Varinance:  24.584116035179303 \n",
      "\n",
      "Epoch:  14  Learning Rate:  0.09870841350202876  Varinance:  24.549772926191807 \n",
      "\n",
      "Epoch:  15  Learning Rate:  0.0986097544262862  Varinance:  24.51547779326874 \n",
      "\n",
      "Epoch:  16  Learning Rate:  0.09851119396030628  Varinance:  24.481230569389307 \n",
      "\n",
      "Epoch:  17  Learning Rate:  0.09841273200552852  Varinance:  24.447031187626326 \n",
      "\n",
      "Epoch:  18  Learning Rate:  0.09831436846349097  Varinance:  24.412879581146115 \n",
      "\n",
      "Epoch:  19  Learning Rate:  0.09821610323583008  Varinance:  24.37877568320835 \n",
      "\n",
      "Epoch:  20  Learning Rate:  0.09811793622428061  Varinance:  24.34471942716595 \n",
      "\n",
      "Epoch:  21  Learning Rate:  0.09801986733067553  Varinance:  24.310710746464927 \n",
      "\n",
      "Epoch:  22  Learning Rate:  0.09792189645694596  Varinance:  24.27674957464429 \n",
      "\n",
      "Epoch:  23  Learning Rate:  0.09782402350512101  Varinance:  24.242835845335854 \n",
      "\n",
      "Epoch:  24  Learning Rate:  0.09772624837732771  Varinance:  24.20896949226418 \n",
      "\n",
      "Epoch:  25  Learning Rate:  0.09762857097579093  Varinance:  24.175150449246402 \n",
      "\n",
      "Epoch:  26  Learning Rate:  0.09753099120283326  Varinance:  24.141378650192106 \n",
      "\n",
      "Epoch:  27  Learning Rate:  0.09743350896087494  Varinance:  24.107654029103205 \n",
      "\n",
      "Epoch:  28  Learning Rate:  0.09733612415243369  Varinance:  24.073976520073817 \n",
      "\n",
      "Epoch:  29  Learning Rate:  0.09723883668012469  Varinance:  24.04034605729011 \n",
      "\n",
      "Epoch:  30  Learning Rate:  0.09714164644666048  Varinance:  24.00676257503022 \n",
      "\n",
      "Epoch:  31  Learning Rate:  0.09704455335485082  Varinance:  23.97322600766406 \n",
      "\n",
      "Epoch:  32  Learning Rate:  0.0969475573076026  Varinance:  23.939736289653258 \n",
      "\n",
      "Epoch:  33  Learning Rate:  0.09685065820791977  Varinance:  23.90629335555097 \n",
      "\n",
      "Epoch:  34  Learning Rate:  0.0967538559589032  Varinance:  23.872897140001797 \n",
      "\n",
      "Epoch:  35  Learning Rate:  0.09665715046375067  Varinance:  23.83954757774163 \n",
      "\n",
      "Epoch:  36  Learning Rate:  0.09656054162575665  Varinance:  23.806244603597538 \n",
      "\n",
      "Epoch:  37  Learning Rate:  0.09646402934831232  Varinance:  23.772988152487624 \n",
      "\n",
      "Epoch:  38  Learning Rate:  0.09636761353490536  Varinance:  23.739778159420922 \n",
      "\n",
      "Epoch:  39  Learning Rate:  0.09627129408911995  Varinance:  23.70661455949724 \n",
      "\n",
      "Epoch:  40  Learning Rate:  0.09617507091463667  Varinance:  23.673497287907068 \n",
      "\n",
      "Epoch:  41  Learning Rate:  0.09607894391523232  Varinance:  23.640426279931415 \n",
      "\n",
      "Epoch:  42  Learning Rate:  0.0959829129947799  Varinance:  23.60740147094171 \n",
      "\n",
      "Epoch:  43  Learning Rate:  0.09588697805724845  Varinance:  23.574422796399663 \n",
      "\n",
      "Epoch:  44  Learning Rate:  0.09579113900670307  Varinance:  23.54149019185714 \n",
      "\n",
      "Epoch:  45  Learning Rate:  0.09569539574730468  Varinance:  23.50860359295604 \n",
      "\n",
      "Epoch:  46  Learning Rate:  0.09559974818331  Varinance:  23.475762935428172 \n",
      "\n",
      "Epoch:  47  Learning Rate:  0.09550419621907147  Varinance:  23.442968155095116 \n",
      "\n",
      "Epoch:  48  Learning Rate:  0.09540873975903712  Varinance:  23.410219187868115 \n",
      "\n",
      "Epoch:  49  Learning Rate:  0.09531337870775047  Varinance:  23.377515969747943 \n",
      "\n",
      "Epoch:  50  Learning Rate:  0.0952181129698505  Varinance:  23.34485843682477 \n",
      "\n",
      "Epoch:  51  Learning Rate:  0.09512294245007141  Varinance:  23.31224652527805 \n",
      "\n",
      "Epoch:  52  Learning Rate:  0.0950278670532427  Varinance:  23.279680171376395 \n",
      "\n",
      "Epoch:  53  Learning Rate:  0.09493288668428897  Varinance:  23.247159311477443 \n",
      "\n",
      "Epoch:  54  Learning Rate:  0.09483800124822983  Varinance:  23.214683882027742 \n",
      "\n",
      "Epoch:  55  Learning Rate:  0.09474321065017983  Varinance:  23.182253819562618 \n",
      "\n",
      "Epoch:  56  Learning Rate:  0.09464851479534839  Varinance:  23.149869060706056 \n",
      "\n",
      "Epoch:  57  Learning Rate:  0.09455391358903964  Varinance:  23.117529542170573 \n",
      "\n",
      "Epoch:  58  Learning Rate:  0.09445940693665233  Varinance:  23.085235200757104 \n",
      "\n",
      "Epoch:  59  Learning Rate:  0.09436499474367986  Varinance:  23.05298597335486 \n",
      "\n",
      "Epoch:  60  Learning Rate:  0.09427067691570998  Varinance:  23.02078179694123 \n",
      "\n",
      "Epoch:  61  Learning Rate:  0.09417645335842488  Varinance:  22.988622608581625 \n",
      "\n",
      "Epoch:  62  Learning Rate:  0.09408232397760097  Varinance:  22.956508345429384 \n",
      "\n",
      "Epoch:  63  Learning Rate:  0.0939882886791089  Varinance:  22.92443894472564 \n",
      "\n",
      "Epoch:  64  Learning Rate:  0.09389434736891333  Varinance:  22.892414343799203 \n",
      "\n",
      "Epoch:  65  Learning Rate:  0.09380049995307295  Varinance:  22.86043448006642 \n",
      "\n",
      "Epoch:  66  Learning Rate:  0.09370674633774034  Varinance:  22.82849929103107 \n",
      "\n",
      "Epoch:  67  Learning Rate:  0.09361308642916188  Varinance:  22.79660871428424 \n",
      "\n",
      "Epoch:  68  Learning Rate:  0.09351952013367766  Varinance:  22.7647626875042 \n",
      "\n",
      "Epoch:  69  Learning Rate:  0.09342604735772136  Varinance:  22.732961148456273 \n",
      "\n",
      "Epoch:  70  Learning Rate:  0.0933326680078202  Varinance:  22.701204034992735 \n",
      "\n",
      "Epoch:  71  Learning Rate:  0.09323938199059484  Varinance:  22.669491285052672 \n",
      "\n",
      "Epoch:  72  Learning Rate:  0.09314618921275922  Varinance:  22.637822836661865 \n",
      "\n",
      "Epoch:  73  Learning Rate:  0.09305308958112057  Varinance:  22.606198627932677 \n",
      "\n",
      "Epoch:  74  Learning Rate:  0.09296008300257928  Varinance:  22.57461859706392 \n",
      "\n",
      "Epoch:  75  Learning Rate:  0.09286716938412873  Varinance:  22.543082682340742 \n",
      "\n",
      "Epoch:  76  Learning Rate:  0.09277434863285529  Varinance:  22.511590822134504 \n",
      "\n",
      "Epoch:  77  Learning Rate:  0.09268162065593823  Varinance:  22.480142954902664 \n",
      "\n",
      "Epoch:  78  Learning Rate:  0.09258898536064954  Varinance:  22.44873901918865 \n",
      "\n",
      "Epoch:  79  Learning Rate:  0.09249644265435393  Varinance:  22.417378953621732 \n",
      "\n",
      "Epoch:  80  Learning Rate:  0.09240399244450868  Varinance:  22.386062696916934 \n",
      "\n",
      "Epoch:  81  Learning Rate:  0.09231163463866358  Varinance:  22.354790187874876 \n",
      "\n",
      "Epoch:  82  Learning Rate:  0.09221936914446081  Varinance:  22.32356136538168 \n",
      "\n",
      "Epoch:  83  Learning Rate:  0.09212719586963487  Varinance:  22.292376168408833 \n",
      "\n",
      "Epoch:  84  Learning Rate:  0.09203511472201248  Varinance:  22.261234536013085 \n",
      "\n",
      "Epoch:  85  Learning Rate:  0.09194312560951247  Varinance:  22.230136407336325 \n",
      "\n",
      "Epoch:  86  Learning Rate:  0.09185122844014575  Varinance:  22.19908172160544 \n",
      "\n",
      "Epoch:  87  Learning Rate:  0.0917594231220151  Varinance:  22.168070418132235 \n",
      "\n",
      "Epoch:  88  Learning Rate:  0.09166770956331523  Varinance:  22.137102436313285 \n",
      "\n",
      "Epoch:  89  Learning Rate:  0.09157608767233257  Varinance:  22.106177715629823 \n",
      "\n",
      "Epoch:  90  Learning Rate:  0.0914845573574452  Varinance:  22.07529619564764 \n",
      "\n",
      "Epoch:  91  Learning Rate:  0.09139311852712283  Varinance:  22.044457816016923 \n",
      "\n",
      "Epoch:  92  Learning Rate:  0.09130177108992658  Varinance:  22.013662516472195 \n",
      "\n",
      "Epoch:  93  Learning Rate:  0.09121051495450905  Varinance:  21.982910236832147 \n",
      "\n",
      "Epoch:  94  Learning Rate:  0.09111935002961406  Varinance:  21.952200916999555 \n",
      "\n",
      "Epoch:  95  Learning Rate:  0.0910282762240767  Varinance:  21.92153449696114 \n",
      "\n",
      "Epoch:  96  Learning Rate:  0.09093729344682315  Varinance:  21.890910916787465 \n",
      "\n",
      "Epoch:  97  Learning Rate:  0.09084640160687063  Varinance:  21.860330116632802 \n",
      "\n",
      "Epoch:  98  Learning Rate:  0.09075560061332727  Varinance:  21.82979203673504 \n",
      "\n",
      "Epoch:  99  Learning Rate:  0.0906648903753921  Varinance:  21.799296617415546 \n",
      "\n",
      "Epoch:  100  Learning Rate:  0.09057427080235486  Varinance:  21.76884379907905 \n",
      "\n",
      "Epoch:  101  Learning Rate:  0.09048374180359596  Varinance:  21.73843352221354 \n",
      "\n",
      "Epoch:  102  Learning Rate:  0.09039330328858641  Varinance:  21.708065727390146 \n",
      "\n",
      "Epoch:  103  Learning Rate:  0.09030295516688769  Varinance:  21.677740355263012 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  104  Learning Rate:  0.09021269734815165  Varinance:  21.647457346569183 \n",
      "\n",
      "Epoch:  105  Learning Rate:  0.09012252974212048  Varinance:  21.617216642128497 \n",
      "\n",
      "Epoch:  106  Learning Rate:  0.09003245225862656  Varinance:  21.58701818284346 \n",
      "\n",
      "Epoch:  107  Learning Rate:  0.08994246480759241  Varinance:  21.556861909699148 \n",
      "\n",
      "Epoch:  108  Learning Rate:  0.08985256729903056  Varinance:  21.52674776376305 \n",
      "\n",
      "Epoch:  109  Learning Rate:  0.08976275964304349  Varinance:  21.49667568618502 \n",
      "\n",
      "Epoch:  110  Learning Rate:  0.08967304174982355  Varinance:  21.466645618197095 \n",
      "\n",
      "Epoch:  111  Learning Rate:  0.08958341352965282  Varinance:  21.436657501113416 \n",
      "\n",
      "Epoch:  112  Learning Rate:  0.08949387489290311  Varinance:  21.40671127633011 \n",
      "\n",
      "Epoch:  113  Learning Rate:  0.08940442575003572  Varinance:  21.376806885325166 \n",
      "\n",
      "Epoch:  114  Learning Rate:  0.08931506601160155  Varinance:  21.346944269658337 \n",
      "\n",
      "Epoch:  115  Learning Rate:  0.08922579558824084  Varinance:  21.317123370970993 \n",
      "\n",
      "Epoch:  116  Learning Rate:  0.08913661439068314  Varinance:  21.287344130986057 \n",
      "\n",
      "Epoch:  117  Learning Rate:  0.08904752232974728  Varinance:  21.25760649150784 \n",
      "\n",
      "Epoch:  118  Learning Rate:  0.08895851931634113  Varinance:  21.227910394421965 \n",
      "\n",
      "Epoch:  119  Learning Rate:  0.08886960526146175  Varinance:  21.198255781695227 \n",
      "\n",
      "Epoch:  120  Learning Rate:  0.08878078007619501  Varinance:  21.1686425953755 \n",
      "\n",
      "Epoch:  121  Learning Rate:  0.08869204367171575  Varinance:  21.13907077759161 \n",
      "\n",
      "Epoch:  122  Learning Rate:  0.08860339595928757  Varinance:  21.10954027055323 \n",
      "\n",
      "Epoch:  123  Learning Rate:  0.08851483685026272  Varinance:  21.08005101655076 \n",
      "\n",
      "Epoch:  124  Learning Rate:  0.08842636625608209  Varinance:  21.050602957955224 \n",
      "\n",
      "Epoch:  125  Learning Rate:  0.08833798408827509  Varinance:  21.02119603721815 \n",
      "\n",
      "Epoch:  126  Learning Rate:  0.08824969025845955  Varinance:  20.991830196871454 \n",
      "\n",
      "Epoch:  127  Learning Rate:  0.08816148467834162  Varinance:  20.96250537952734 \n",
      "\n",
      "Epoch:  128  Learning Rate:  0.0880733672597157  Varinance:  20.933221527878175 \n",
      "\n",
      "Epoch:  129  Learning Rate:  0.08798533791446439  Varinance:  20.903978584696382 \n",
      "\n",
      "Epoch:  130  Learning Rate:  0.08789739655455832  Varinance:  20.874776492834336 \n",
      "\n",
      "Epoch:  131  Learning Rate:  0.08780954309205613  Varinance:  20.845615195224237 \n",
      "\n",
      "Epoch:  132  Learning Rate:  0.08772177743910436  Varinance:  20.816494634878012 \n",
      "\n",
      "Epoch:  133  Learning Rate:  0.08763409950793732  Varinance:  20.7874147548872 \n",
      "\n",
      "Epoch:  134  Learning Rate:  0.08754650921087712  Varinance:  20.758375498422833 \n",
      "\n",
      "Epoch:  135  Learning Rate:  0.0874590064603334  Varinance:  20.72937680873533 \n",
      "\n",
      "Epoch:  136  Learning Rate:  0.08737159116880344  Varinance:  20.700418629154395 \n",
      "\n",
      "Epoch:  137  Learning Rate:  0.08728426324887194  Varinance:  20.671500903088894 \n",
      "\n",
      "Epoch:  138  Learning Rate:  0.08719702261321094  Varinance:  20.642623574026743 \n",
      "\n",
      "Epoch:  139  Learning Rate:  0.08710986917457984  Varinance:  20.613786585534818 \n",
      "\n",
      "Epoch:  140  Learning Rate:  0.08702280284582516  Varinance:  20.584989881258814 \n",
      "\n",
      "Epoch:  141  Learning Rate:  0.0869358235398806  Varinance:  20.556233404923162 \n",
      "\n",
      "Epoch:  142  Learning Rate:  0.08684893116976679  Varinance:  20.527517100330904 \n",
      "\n",
      "Epoch:  143  Learning Rate:  0.08676212564859141  Varinance:  20.49884091136358 \n",
      "\n",
      "Epoch:  144  Learning Rate:  0.0866754068895489  Varinance:  20.470204781981142 \n",
      "\n",
      "Epoch:  145  Learning Rate:  0.0865887748059205  Varinance:  20.44160865622181 \n",
      "\n",
      "Epoch:  146  Learning Rate:  0.08650222931107414  Varinance:  20.413052478201994 \n",
      "\n",
      "Epoch:  147  Learning Rate:  0.08641577031846429  Varinance:  20.38453619211617 \n",
      "\n",
      "Epoch:  148  Learning Rate:  0.08632939774163195  Varinance:  20.356059742236756 \n",
      "\n",
      "Epoch:  149  Learning Rate:  0.08624311149420455  Varinance:  20.327623072914044 \n",
      "\n",
      "Epoch:  150  Learning Rate:  0.08615691148989583  Varinance:  20.29922612857605 \n",
      "\n",
      "Epoch:  151  Learning Rate:  0.08607079764250579  Varinance:  20.270868853728423 \n",
      "\n",
      "Epoch:  152  Learning Rate:  0.08598476986592056  Varinance:  20.242551192954338 \n",
      "\n",
      "Epoch:  153  Learning Rate:  0.08589882807411235  Varinance:  20.214273090914396 \n",
      "\n",
      "Epoch:  154  Learning Rate:  0.08581297218113938  Varinance:  20.18603449234649 \n",
      "\n",
      "Epoch:  155  Learning Rate:  0.08572720210114576  Varinance:  20.157835342065717 \n",
      "\n",
      "Epoch:  156  Learning Rate:  0.08564151774836136  Varinance:  20.129675584964264 \n",
      "\n",
      "Epoch:  157  Learning Rate:  0.08555591903710186  Varinance:  20.101555166011305 \n",
      "\n",
      "Epoch:  158  Learning Rate:  0.08547040588176852  Varinance:  20.073474030252893 \n",
      "\n",
      "Epoch:  159  Learning Rate:  0.08538497819684818  Varinance:  20.045432122811835 \n",
      "\n",
      "Epoch:  160  Learning Rate:  0.08529963589691315  Varinance:  20.01742938888762 \n",
      "\n",
      "Epoch:  161  Learning Rate:  0.08521437889662115  Varinance:  19.989465773756276 \n",
      "\n",
      "Epoch:  162  Learning Rate:  0.08512920711071512  Varinance:  19.961541222770283 \n",
      "\n",
      "Epoch:  163  Learning Rate:  0.0850441204540233  Varinance:  19.933655681358463 \n",
      "\n",
      "Epoch:  164  Learning Rate:  0.08495911884145903  Varinance:  19.905809095025866 \n",
      "\n",
      "Epoch:  165  Learning Rate:  0.08487420218802068  Varinance:  19.878001409353686 \n",
      "\n",
      "Epoch:  166  Learning Rate:  0.08478937040879159  Varinance:  19.85023256999911 \n",
      "\n",
      "Epoch:  167  Learning Rate:  0.08470462341893997  Varinance:  19.82250252269527 \n",
      "\n",
      "Epoch:  168  Learning Rate:  0.08461996113371884  Varinance:  19.79481121325109 \n",
      "\n",
      "Epoch:  169  Learning Rate:  0.08453538346846588  Varinance:  19.767158587551194 \n",
      "\n",
      "Epoch:  170  Learning Rate:  0.08445089033860344  Varinance:  19.739544591555816 \n",
      "\n",
      "Epoch:  171  Learning Rate:  0.08436648165963838  Varinance:  19.71196917130067 \n",
      "\n",
      "Epoch:  172  Learning Rate:  0.08428215734716199  Varinance:  19.684432272896863 \n",
      "\n",
      "Epoch:  173  Learning Rate:  0.08419791731685  Varinance:  19.65693384253078 \n",
      "\n",
      "Epoch:  174  Learning Rate:  0.08411376148446233  Varinance:  19.62947382646398 \n",
      "\n",
      "Epoch:  175  Learning Rate:  0.08402968976584314  Varinance:  19.6020521710331 \n",
      "\n",
      "Epoch:  176  Learning Rate:  0.08394570207692074  Varinance:  19.574668822649738 \n",
      "\n",
      "Epoch:  177  Learning Rate:  0.08386179833370741  Varinance:  19.547323727800347 \n",
      "\n",
      "Epoch:  178  Learning Rate:  0.0837779784522994  Varinance:  19.520016833046146 \n",
      "\n",
      "Epoch:  179  Learning Rate:  0.08369424234887682  Varinance:  19.492748085023003 \n",
      "\n",
      "Epoch:  180  Learning Rate:  0.08361058993970355  Varinance:  19.465517430441327 \n",
      "\n",
      "Epoch:  181  Learning Rate:  0.0835270211411272  Varinance:  19.438324816085984 \n",
      "\n",
      "Epoch:  182  Learning Rate:  0.08344353586957896  Varinance:  19.41117018881617 \n",
      "\n",
      "Epoch:  183  Learning Rate:  0.08336013404157354  Varinance:  19.384053495565315 \n",
      "\n",
      "Epoch:  184  Learning Rate:  0.0832768155737091  Varinance:  19.356974683340987 \n",
      "\n",
      "Epoch:  185  Learning Rate:  0.08319358038266718  Varinance:  19.329933699224785 \n",
      "\n",
      "Epoch:  186  Learning Rate:  0.08311042838521257  Varinance:  19.302930490372223 \n",
      "\n",
      "Epoch:  187  Learning Rate:  0.08302735949819327  Varinance:  19.275965004012644 \n",
      "\n",
      "Epoch:  188  Learning Rate:  0.0829443736385404  Varinance:  19.24903718744911 \n",
      "\n",
      "Epoch:  189  Learning Rate:  0.08286147072326806  Varinance:  19.222146988058295 \n",
      "\n",
      "Epoch:  190  Learning Rate:  0.08277865066947337  Varinance:  19.195294353290386 \n",
      "\n",
      "Epoch:  191  Learning Rate:  0.08269591339433624  Varinance:  19.168479230668982 \n",
      "\n",
      "Epoch:  192  Learning Rate:  0.08261325881511938  Varinance:  19.141701567790992 \n",
      "\n",
      "Epoch:  193  Learning Rate:  0.08253068684916824  Varinance:  19.11496131232653 \n",
      "\n",
      "Epoch:  194  Learning Rate:  0.08244819741391082  Varinance:  19.0882584120188 \n",
      "\n",
      "Epoch:  195  Learning Rate:  0.0823657904268577  Varinance:  19.061592814684026 \n",
      "\n",
      "Epoch:  196  Learning Rate:  0.08228346580560185  Varinance:  19.034964468211317 \n",
      "\n",
      "Epoch:  197  Learning Rate:  0.08220122346781866  Varinance:  19.00837332056259 \n",
      "\n",
      "Epoch:  198  Learning Rate:  0.0821190633312658  Varinance:  18.98181931977245 \n",
      "\n",
      "Epoch:  199  Learning Rate:  0.08203698531378312  Varinance:  18.955302413948093 \n",
      "\n",
      "Epoch:  200  Learning Rate:  0.08195498933329257  Varinance:  18.928822551269214 \n",
      "\n",
      "Epoch:  201  Learning Rate:  0.0818730753077982  Varinance:  18.902379679987895 \n",
      "\n",
      "Epoch:  202  Learning Rate:  0.08179124315538594  Varinance:  18.875973748428514 \n",
      "\n",
      "Epoch:  203  Learning Rate:  0.08170949279422367  Varinance:  18.84960470498763 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  204  Learning Rate:  0.08162782414256099  Varinance:  18.823272498133893 \n",
      "\n",
      "Epoch:  205  Learning Rate:  0.08154623711872927  Varinance:  18.79697707640794 \n",
      "\n",
      "Epoch:  206  Learning Rate:  0.08146473164114146  Varinance:  18.770718388422303 \n",
      "\n",
      "Epoch:  207  Learning Rate:  0.08138330762829207  Varinance:  18.744496382861282 \n",
      "\n",
      "Epoch:  208  Learning Rate:  0.08130196499875711  Varinance:  18.718311008480885 \n",
      "\n",
      "Epoch:  209  Learning Rate:  0.08122070367119391  Varinance:  18.69216221410869 \n",
      "\n",
      "Epoch:  210  Learning Rate:  0.08113952356434115  Varinance:  18.666049948643764 \n",
      "\n",
      "Epoch:  211  Learning Rate:  0.08105842459701872  Varinance:  18.639974161056568 \n",
      "\n",
      "Epoch:  212  Learning Rate:  0.08097740668812764  Varinance:  18.613934800388844 \n",
      "\n",
      "Epoch:  213  Learning Rate:  0.08089646975664999  Varinance:  18.587931815753514 \n",
      "\n",
      "Epoch:  214  Learning Rate:  0.08081561372164885  Varinance:  18.5619651563346 \n",
      "\n",
      "Epoch:  215  Learning Rate:  0.08073483850226815  Varinance:  18.536034771387104 \n",
      "\n",
      "Epoch:  216  Learning Rate:  0.08065414401773269  Varinance:  18.51014061023692 \n",
      "\n",
      "Epoch:  217  Learning Rate:  0.08057353018734797  Varinance:  18.48428262228073 \n",
      "\n",
      "Epoch:  218  Learning Rate:  0.08049299693050016  Varinance:  18.458460756985907 \n",
      "\n",
      "Epoch:  219  Learning Rate:  0.08041254416665597  Varinance:  18.43267496389042 \n",
      "\n",
      "Epoch:  220  Learning Rate:  0.08033217181536266  Varinance:  18.406925192602728 \n",
      "\n",
      "Epoch:  221  Learning Rate:  0.08025187979624786  Varinance:  18.38121139280168 \n",
      "\n",
      "Epoch:  222  Learning Rate:  0.08017166802901954  Varinance:  18.355533514236438 \n",
      "\n",
      "Epoch:  223  Learning Rate:  0.08009153643346592  Varinance:  18.329891506726348 \n",
      "\n",
      "Epoch:  224  Learning Rate:  0.08001148492945542  Varinance:  18.304285320160854 \n",
      "\n",
      "Epoch:  225  Learning Rate:  0.07993151343693651  Varinance:  18.27871490449942 \n",
      "\n",
      "Epoch:  226  Learning Rate:  0.07985162187593771  Varinance:  18.253180209771397 \n",
      "\n",
      "Epoch:  227  Learning Rate:  0.07977181016656743  Varinance:  18.22768118607595 \n",
      "\n",
      "Epoch:  228  Learning Rate:  0.07969207822901397  Varinance:  18.20221778358196 \n",
      "\n",
      "Epoch:  229  Learning Rate:  0.07961242598354538  Varinance:  18.176789952527912 \n",
      "\n",
      "Epoch:  230  Learning Rate:  0.0795328533505094  Varinance:  18.151397643221806 \n",
      "\n",
      "Epoch:  231  Learning Rate:  0.07945336025033341  Varinance:  18.126040806041065 \n",
      "\n",
      "Epoch:  232  Learning Rate:  0.07937394660352427  Varinance:  18.100719391432424 \n",
      "\n",
      "Epoch:  233  Learning Rate:  0.07929461233066837  Varinance:  18.075433349911854 \n",
      "\n",
      "Epoch:  234  Learning Rate:  0.0792153573524314  Varinance:  18.050182632064445 \n",
      "\n",
      "Epoch:  235  Learning Rate:  0.07913618158955839  Varinance:  18.024967188544323 \n",
      "\n",
      "Epoch:  236  Learning Rate:  0.07905708496287356  Varinance:  17.999786970074545 \n",
      "\n",
      "Epoch:  237  Learning Rate:  0.07897806739328028  Varinance:  17.974641927447003 \n",
      "\n",
      "Epoch:  238  Learning Rate:  0.07889912880176098  Varinance:  17.94953201152234 \n",
      "\n",
      "Epoch:  239  Learning Rate:  0.07882026910937705  Varinance:  17.924457173229836 \n",
      "\n",
      "Epoch:  240  Learning Rate:  0.0787414882372688  Varinance:  17.899417363567327 \n",
      "\n",
      "Epoch:  241  Learning Rate:  0.07866278610665535  Varinance:  17.8744125336011 \n",
      "\n",
      "Epoch:  242  Learning Rate:  0.07858416263883455  Varinance:  17.8494426344658 \n",
      "\n",
      "Epoch:  243  Learning Rate:  0.07850561775518296  Varinance:  17.82450761736434 \n",
      "\n",
      "Epoch:  244  Learning Rate:  0.07842715137715565  Varinance:  17.79960743356779 \n",
      "\n",
      "Epoch:  245  Learning Rate:  0.07834876342628626  Varinance:  17.774742034415308 \n",
      "\n",
      "Epoch:  246  Learning Rate:  0.07827045382418682  Varinance:  17.74991137131402 \n",
      "\n",
      "Epoch:  247  Learning Rate:  0.07819222249254773  Varinance:  17.72511539573893 \n",
      "\n",
      "Epoch:  248  Learning Rate:  0.07811406935313765  Varinance:  17.700354059232843 \n",
      "\n",
      "Epoch:  249  Learning Rate:  0.07803599432780343  Varinance:  17.675627313406242 \n",
      "\n",
      "Epoch:  250  Learning Rate:  0.07795799733847004  Varinance:  17.650935109937222 \n",
      "\n",
      "Epoch:  251  Learning Rate:  0.0778800783071405  Varinance:  17.62627740057137 \n",
      "\n",
      "Epoch:  252  Learning Rate:  0.07780223715589574  Varinance:  17.601654137121695 \n",
      "\n",
      "Epoch:  253  Learning Rate:  0.07772447380689462  Varinance:  17.577065271468506 \n",
      "\n",
      "Epoch:  254  Learning Rate:  0.07764678818237379  Varinance:  17.55251075555935 \n",
      "\n",
      "Epoch:  255  Learning Rate:  0.0775691802046476  Varinance:  17.52799054140889 \n",
      "\n",
      "Epoch:  256  Learning Rate:  0.0774916497961081  Varinance:  17.503504581098824 \n",
      "\n",
      "Epoch:  257  Learning Rate:  0.07741419687922485  Varinance:  17.479052826777796 \n",
      "\n",
      "Epoch:  258  Learning Rate:  0.0773368213765449  Varinance:  17.454635230661296 \n",
      "\n",
      "Epoch:  259  Learning Rate:  0.07725952321069281  Varinance:  17.43025174503156 \n",
      "\n",
      "Epoch:  260  Learning Rate:  0.07718230230437034  Varinance:  17.405902322237488 \n",
      "\n",
      "Epoch:  261  Learning Rate:  0.07710515858035663  Varinance:  17.381586914694548 \n",
      "\n",
      "Epoch:  262  Learning Rate:  0.07702809196150792  Varinance:  17.35730547488468 \n",
      "\n",
      "Epoch:  263  Learning Rate:  0.07695110237075758  Varinance:  17.333057955356207 \n",
      "\n",
      "Epoch:  264  Learning Rate:  0.07687418973111604  Varinance:  17.308844308723742 \n",
      "\n",
      "Epoch:  265  Learning Rate:  0.07679735396567061  Varinance:  17.284664487668085 \n",
      "\n",
      "Epoch:  266  Learning Rate:  0.07672059499758557  Varinance:  17.260518444936153 \n",
      "\n",
      "Epoch:  267  Learning Rate:  0.07664391275010192  Varinance:  17.236406133340857 \n",
      "\n",
      "Epoch:  268  Learning Rate:  0.0765673071465374  Varinance:  17.212327505761042 \n",
      "\n",
      "Epoch:  269  Learning Rate:  0.0764907781102864  Varinance:  17.188282515141367 \n",
      "\n",
      "Epoch:  270  Learning Rate:  0.0764143255648199  Varinance:  17.164271114492237 \n",
      "\n",
      "Epoch:  271  Learning Rate:  0.07633794943368533  Varinance:  17.140293256889695 \n",
      "\n",
      "Epoch:  272  Learning Rate:  0.07626164964050654  Varinance:  17.116348895475326 \n",
      "\n",
      "Epoch:  273  Learning Rate:  0.07618542610898377  Varinance:  17.092437983456197 \n",
      "\n",
      "Epoch:  274  Learning Rate:  0.07610927876289343  Varinance:  17.068560474104714 \n",
      "\n",
      "Epoch:  275  Learning Rate:  0.07603320752608822  Varinance:  17.04471632075859 \n",
      "\n",
      "Epoch:  276  Learning Rate:  0.07595721232249686  Varinance:  17.02090547682069 \n",
      "\n",
      "Epoch:  277  Learning Rate:  0.07588129307612414  Varinance:  16.99712789575901 \n",
      "\n",
      "Epoch:  278  Learning Rate:  0.07580544971105084  Varinance:  16.973383531106517 \n",
      "\n",
      "Epoch:  279  Learning Rate:  0.07572968215143355  Varinance:  16.94967233646111 \n",
      "\n",
      "Epoch:  280  Learning Rate:  0.07565399032150474  Varinance:  16.925994265485507 \n",
      "\n",
      "Epoch:  281  Learning Rate:  0.07557837414557256  Varinance:  16.902349271907156 \n",
      "\n",
      "Epoch:  282  Learning Rate:  0.0755028335480208  Varinance:  16.87873730951814 \n",
      "\n",
      "Epoch:  283  Learning Rate:  0.0754273684533089  Varinance:  16.855158332175105 \n",
      "\n",
      "Epoch:  284  Learning Rate:  0.07535197878597173  Varinance:  16.831612293799143 \n",
      "\n",
      "Epoch:  285  Learning Rate:  0.07527666447061963  Varinance:  16.808099148375728 \n",
      "\n",
      "Epoch:  286  Learning Rate:  0.07520142543193827  Varinance:  16.784618849954615 \n",
      "\n",
      "Epoch:  287  Learning Rate:  0.0751262615946886  Varinance:  16.761171352649743 \n",
      "\n",
      "Epoch:  288  Learning Rate:  0.0750511728837068  Varinance:  16.737756610639153 \n",
      "\n",
      "Epoch:  289  Learning Rate:  0.07497615922390413  Varinance:  16.714374578164897 \n",
      "\n",
      "Epoch:  290  Learning Rate:  0.07490122054026693  Varinance:  16.69102520953296 \n",
      "\n",
      "Epoch:  291  Learning Rate:  0.07482635675785652  Varinance:  16.667708459113147 \n",
      "\n",
      "Epoch:  292  Learning Rate:  0.07475156780180911  Varinance:  16.64442428133901 \n",
      "\n",
      "Epoch:  293  Learning Rate:  0.07467685359733571  Varinance:  16.621172630707758 \n",
      "\n",
      "Epoch:  294  Learning Rate:  0.07460221406972216  Varinance:  16.59795346178016 \n",
      "\n",
      "Epoch:  295  Learning Rate:  0.07452764914432887  Varinance:  16.574766729180478 \n",
      "\n",
      "Epoch:  296  Learning Rate:  0.07445315874659095  Varinance:  16.551612387596343 \n",
      "\n",
      "Epoch:  297  Learning Rate:  0.07437874280201796  Varinance:  16.528490391778693 \n",
      "\n",
      "Epoch:  298  Learning Rate:  0.07430440123619399  Varinance:  16.505400696541685 \n",
      "\n",
      "Epoch:  299  Learning Rate:  0.07423013397477744  Varinance:  16.48234325676258 \n",
      "\n",
      "Epoch:  300  Learning Rate:  0.07415594094350106  Varinance:  16.45931802738169 \n",
      "\n",
      "Epoch:  301  Learning Rate:  0.0740818220681718  Varinance:  16.436324963402278 \n",
      "\n",
      "Epoch:  302  Learning Rate:  0.07400777727467077  Varinance:  16.413364019890448 \n",
      "\n",
      "Epoch:  303  Learning Rate:  0.07393380648895319  Varinance:  16.390435151975087 \n",
      "\n",
      "Epoch:  304  Learning Rate:  0.07385990963704826  Varinance:  16.367538314847764 \n",
      "\n",
      "Epoch:  305  Learning Rate:  0.07378608664505912  Varinance:  16.344673463762636 \n",
      "\n",
      "Epoch:  306  Learning Rate:  0.07371233743916278  Varinance:  16.32184055403638 \n",
      "\n",
      "Epoch:  307  Learning Rate:  0.07363866194561001  Varinance:  16.29903954104809 \n",
      "\n",
      "Epoch:  308  Learning Rate:  0.07356506009072533  Varinance:  16.276270380239186 \n",
      "\n",
      "Epoch:  309  Learning Rate:  0.07349153180090688  Varinance:  16.253533027113345 \n",
      "\n",
      "Epoch:  310  Learning Rate:  0.07341807700262634  Varinance:  16.230827437236396 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  311  Learning Rate:  0.07334469562242893  Varinance:  16.208153566236255 \n",
      "\n",
      "Epoch:  312  Learning Rate:  0.07327138758693325  Varinance:  16.185511369802803 \n",
      "\n",
      "Epoch:  313  Learning Rate:  0.07319815282283126  Varinance:  16.162900803687833 \n",
      "\n",
      "Epoch:  314  Learning Rate:  0.0731249912568882  Varinance:  16.14032182370495 \n",
      "\n",
      "Epoch:  315  Learning Rate:  0.0730519028159425  Varinance:  16.11777438572949 \n",
      "\n",
      "Epoch:  316  Learning Rate:  0.07297888742690568  Varinance:  16.09525844569842 \n",
      "\n",
      "Epoch:  317  Learning Rate:  0.07290594501676238  Varinance:  16.072773959610267 \n",
      "\n",
      "Epoch:  318  Learning Rate:  0.07283307551257016  Varinance:  16.050320883525025 \n",
      "\n",
      "Epoch:  319  Learning Rate:  0.07276027884145955  Varinance:  16.027899173564073 \n",
      "\n",
      "Epoch:  320  Learning Rate:  0.07268755493063382  Varinance:  16.005508785910084 \n",
      "\n",
      "Epoch:  321  Learning Rate:  0.0726149037073691  Varinance:  15.983149676806944 \n",
      "\n",
      "Epoch:  322  Learning Rate:  0.07254232509901412  Varinance:  15.96082180255966 \n",
      "\n",
      "Epoch:  323  Learning Rate:  0.0724698190329903  Varinance:  15.938525119534292 \n",
      "\n",
      "Epoch:  324  Learning Rate:  0.07239738543679153  Varinance:  15.916259584157839 \n",
      "\n",
      "Epoch:  325  Learning Rate:  0.07232502423798425  Varinance:  15.894025152918179 \n",
      "\n",
      "Epoch:  326  Learning Rate:  0.07225273536420722  Varinance:  15.871821782363973 \n",
      "\n",
      "Epoch:  327  Learning Rate:  0.07218051874317159  Varinance:  15.84964942910458 \n",
      "\n",
      "Epoch:  328  Learning Rate:  0.0721083743026607  Varinance:  15.827508049809982 \n",
      "\n",
      "Epoch:  329  Learning Rate:  0.07203630197053014  Varinance:  15.80539760121068 \n",
      "\n",
      "Epoch:  330  Learning Rate:  0.07196430167470755  Varinance:  15.78331804009763 \n",
      "\n",
      "Epoch:  331  Learning Rate:  0.07189237334319262  Varinance:  15.761269323322145 \n",
      "\n",
      "Epoch:  332  Learning Rate:  0.07182051690405704  Varinance:  15.739251407795809 \n",
      "\n",
      "Epoch:  333  Learning Rate:  0.07174873228544433  Varinance:  15.717264250490418 \n",
      "\n",
      "Epoch:  334  Learning Rate:  0.0716770194155699  Varinance:  15.695307808437855 \n",
      "\n",
      "Epoch:  335  Learning Rate:  0.07160537822272085  Varinance:  15.673382038730043 \n",
      "\n",
      "Epoch:  336  Learning Rate:  0.07153380863525599  Varinance:  15.651486898518838 \n",
      "\n",
      "Epoch:  337  Learning Rate:  0.07146231058160572  Varinance:  15.629622345015958 \n",
      "\n",
      "Epoch:  338  Learning Rate:  0.071390883990272  Varinance:  15.607788335492891 \n",
      "\n",
      "Epoch:  339  Learning Rate:  0.07131952878982822  Varinance:  15.585984827280818 \n",
      "\n",
      "Epoch:  340  Learning Rate:  0.07124824490891918  Varinance:  15.564211777770524 \n",
      "\n",
      "Epoch:  341  Learning Rate:  0.07117703227626097  Varinance:  15.542469144412319 \n",
      "\n",
      "Epoch:  342  Learning Rate:  0.07110589082064098  Varinance:  15.520756884715956 \n",
      "\n",
      "Epoch:  343  Learning Rate:  0.07103482047091773  Varinance:  15.499074956250542 \n",
      "\n",
      "Epoch:  344  Learning Rate:  0.07096382115602087  Varinance:  15.477423316644456 \n",
      "\n",
      "Epoch:  345  Learning Rate:  0.07089289280495108  Varinance:  15.455801923585275 \n",
      "\n",
      "Epoch:  346  Learning Rate:  0.07082203534678  Varinance:  15.434210734819679 \n",
      "\n",
      "Epoch:  347  Learning Rate:  0.07075124871065018  Varinance:  15.412649708153381 \n",
      "\n",
      "Epoch:  348  Learning Rate:  0.07068053282577495  Varinance:  15.391118801451029 \n",
      "\n",
      "Epoch:  349  Learning Rate:  0.07060988762143844  Varinance:  15.36961797263614 \n",
      "\n",
      "Epoch:  350  Learning Rate:  0.07053931302699544  Varinance:  15.34814717969101 \n",
      "\n",
      "Epoch:  351  Learning Rate:  0.07046880897187134  Varinance:  15.326706380656626 \n",
      "\n",
      "Epoch:  352  Learning Rate:  0.0703983753855621  Varinance:  15.305295533632595 \n",
      "\n",
      "Epoch:  353  Learning Rate:  0.0703280121976341  Varinance:  15.283914596777057 \n",
      "\n",
      "Epoch:  354  Learning Rate:  0.07025771933772416  Varinance:  15.262563528306602 \n",
      "\n",
      "Epoch:  355  Learning Rate:  0.07018749673553941  Varinance:  15.241242286496195 \n",
      "\n",
      "Epoch:  356  Learning Rate:  0.07011734432085724  Varinance:  15.21995082967908 \n",
      "\n",
      "Epoch:  357  Learning Rate:  0.07004726202352524  Varinance:  15.198689116246713 \n",
      "\n",
      "Epoch:  358  Learning Rate:  0.0699772497734611  Varinance:  15.177457104648681 \n",
      "\n",
      "Epoch:  359  Learning Rate:  0.06990730750065258  Varinance:  15.156254753392604 \n",
      "\n",
      "Epoch:  360  Learning Rate:  0.06983743513515736  Varinance:  15.135082021044074 \n",
      "\n",
      "Epoch:  361  Learning Rate:  0.0697676326071031  Varinance:  15.113938866226567 \n",
      "\n",
      "Epoch:  362  Learning Rate:  0.06969789984668727  Varinance:  15.09282524762135 \n",
      "\n",
      "Epoch:  363  Learning Rate:  0.06962823678417711  Varinance:  15.071741123967419 \n",
      "\n",
      "Epoch:  364  Learning Rate:  0.06955864334990951  Varinance:  15.05068645406141 \n",
      "\n",
      "Epoch:  365  Learning Rate:  0.06948911947429107  Varinance:  15.029661196757516 \n",
      "\n",
      "Epoch:  366  Learning Rate:  0.06941966508779789  Varinance:  15.008665310967405 \n",
      "\n",
      "Epoch:  367  Learning Rate:  0.06935028012097559  Varinance:  14.987698755660158 \n",
      "\n",
      "Epoch:  368  Learning Rate:  0.06928096450443917  Varinance:  14.966761489862158 \n",
      "\n",
      "Epoch:  369  Learning Rate:  0.06921171816887305  Varinance:  14.945853472657038 \n",
      "\n",
      "Epoch:  370  Learning Rate:  0.06914254104503086  Varinance:  14.924974663185587 \n",
      "\n",
      "Epoch:  371  Learning Rate:  0.06907343306373546  Varinance:  14.904125020645665 \n",
      "\n",
      "Epoch:  372  Learning Rate:  0.0690043941558789  Varinance:  14.883304504292145 \n",
      "\n",
      "Epoch:  373  Learning Rate:  0.06893542425242224  Varinance:  14.862513073436808 \n",
      "\n",
      "Epoch:  374  Learning Rate:  0.06886652328439558  Varinance:  14.841750687448283 \n",
      "\n",
      "Epoch:  375  Learning Rate:  0.06879769118289795  Varinance:  14.82101730575195 \n",
      "\n",
      "Epoch:  376  Learning Rate:  0.06872892787909722  Varinance:  14.80031288782988 \n",
      "\n",
      "Epoch:  377  Learning Rate:  0.0686602333042301  Varinance:  14.779637393220742 \n",
      "\n",
      "Epoch:  378  Learning Rate:  0.06859160738960202  Varinance:  14.758990781519726 \n",
      "\n",
      "Epoch:  379  Learning Rate:  0.06852305006658703  Varinance:  14.738373012378469 \n",
      "\n",
      "Epoch:  380  Learning Rate:  0.06845456126662783  Varinance:  14.717784045504976 \n",
      "\n",
      "Epoch:  381  Learning Rate:  0.06838614092123559  Varinance:  14.697223840663526 \n",
      "\n",
      "Epoch:  382  Learning Rate:  0.06831778896198996  Varinance:  14.676692357674623 \n",
      "\n",
      "Epoch:  383  Learning Rate:  0.06824950532053901  Varinance:  14.656189556414887 \n",
      "\n",
      "Epoch:  384  Learning Rate:  0.06818128992859905  Varinance:  14.635715396816993 \n",
      "\n",
      "Epoch:  385  Learning Rate:  0.06811314271795471  Varinance:  14.615269838869594 \n",
      "\n",
      "Epoch:  386  Learning Rate:  0.06804506362045877  Varinance:  14.594852842617229 \n",
      "\n",
      "Epoch:  387  Learning Rate:  0.0679770525680321  Varinance:  14.574464368160259 \n",
      "\n",
      "Epoch:  388  Learning Rate:  0.06790910949266368  Varinance:  14.554104375654777 \n",
      "\n",
      "Epoch:  389  Learning Rate:  0.06784123432641041  Varinance:  14.533772825312546 \n",
      "\n",
      "Epoch:  390  Learning Rate:  0.06777342700139712  Varinance:  14.513469677400902 \n",
      "\n",
      "Epoch:  391  Learning Rate:  0.06770568744981646  Varinance:  14.493194892242697 \n",
      "\n",
      "Epoch:  392  Learning Rate:  0.06763801560392892  Varinance:  14.472948430216197 \n",
      "\n",
      "Epoch:  393  Learning Rate:  0.06757041139606261  Varinance:  14.452730251755028 \n",
      "\n",
      "Epoch:  394  Learning Rate:  0.06750287475861332  Varinance:  14.432540317348085 \n",
      "\n",
      "Epoch:  395  Learning Rate:  0.06743540562404442  Varinance:  14.412378587539465 \n",
      "\n",
      "Epoch:  396  Learning Rate:  0.06736800392488677  Varinance:  14.392245022928368 \n",
      "\n",
      "Epoch:  397  Learning Rate:  0.06730066959373864  Varinance:  14.372139584169055 \n",
      "\n",
      "Epoch:  398  Learning Rate:  0.06723340256326572  Varinance:  14.352062231970738 \n",
      "\n",
      "Epoch:  399  Learning Rate:  0.06716620276620099  Varinance:  14.332012927097518 \n",
      "\n",
      "Epoch:  400  Learning Rate:  0.0670990701353446  Varinance:  14.311991630368315 \n",
      "\n",
      "Epoch:  401  Learning Rate:  0.06703200460356394  Varinance:  14.291998302656772 \n",
      "\n",
      "Epoch:  402  Learning Rate:  0.06696500610379345  Varinance:  14.272032904891205 \n",
      "\n",
      "Epoch:  403  Learning Rate:  0.06689807456903467  Varinance:  14.252095398054498 \n",
      "\n",
      "Epoch:  404  Learning Rate:  0.06683120993235603  Varinance:  14.232185743184045 \n",
      "\n",
      "Epoch:  405  Learning Rate:  0.0667644121268929  Varinance:  14.212303901371673 \n",
      "\n",
      "Epoch:  406  Learning Rate:  0.06669768108584744  Varinance:  14.192449833763554 \n",
      "\n",
      "Epoch:  407  Learning Rate:  0.06663101674248864  Varinance:  14.17262350156015 \n",
      "\n",
      "Epoch:  408  Learning Rate:  0.06656441903015213  Varinance:  14.152824866016115 \n",
      "\n",
      "Epoch:  409  Learning Rate:  0.06649788788224019  Varinance:  14.133053888440225 \n",
      "\n",
      "Epoch:  410  Learning Rate:  0.06643142323222168  Varinance:  14.113310530195323 \n",
      "\n",
      "Epoch:  411  Learning Rate:  0.06636502501363194  Varinance:  14.093594752698205 \n",
      "\n",
      "Epoch:  412  Learning Rate:  0.06629869316007274  Varinance:  14.073906517419585 \n",
      "\n",
      "Epoch:  413  Learning Rate:  0.06623242760521224  Varinance:  14.054245785883989 \n",
      "\n",
      "Epoch:  414  Learning Rate:  0.06616622828278483  Varinance:  14.0346125196697 \n",
      "\n",
      "Epoch:  415  Learning Rate:  0.06610009512659125  Varinance:  14.015006680408668 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  416  Learning Rate:  0.06603402807049828  Varinance:  13.995428229786445 \n",
      "\n",
      "Epoch:  417  Learning Rate:  0.0659680270484389  Varinance:  13.975877129542106 \n",
      "\n",
      "Epoch:  418  Learning Rate:  0.06590209199441208  Varinance:  13.956353341468178 \n",
      "\n",
      "Epoch:  419  Learning Rate:  0.06583622284248272  Varinance:  13.936856827410557 \n",
      "\n",
      "Epoch:  420  Learning Rate:  0.0657704195267817  Varinance:  13.917387549268446 \n",
      "\n",
      "Epoch:  421  Learning Rate:  0.06570468198150568  Varinance:  13.89794546899426 \n",
      "\n",
      "Epoch:  422  Learning Rate:  0.06563901014091712  Varinance:  13.878530548593588 \n",
      "\n",
      "Epoch:  423  Learning Rate:  0.06557340393934417  Varinance:  13.859142750125072 \n",
      "\n",
      "Epoch:  424  Learning Rate:  0.06550786331118062  Varinance:  13.83978203570037 \n",
      "\n",
      "Epoch:  425  Learning Rate:  0.06544238819088587  Varinance:  13.82044836748407 \n",
      "\n",
      "Epoch:  426  Learning Rate:  0.06537697851298473  Varinance:  13.801141707693601 \n",
      "\n",
      "Epoch:  427  Learning Rate:  0.06531163421206757  Varinance:  13.781862018599192 \n",
      "\n",
      "Epoch:  428  Learning Rate:  0.06524635522279004  Varinance:  13.762609262523762 \n",
      "\n",
      "Epoch:  429  Learning Rate:  0.0651811414798732  Varinance:  13.743383401842872 \n",
      "\n",
      "Epoch:  430  Learning Rate:  0.06511599291810326  Varinance:  13.724184398984644 \n",
      "\n",
      "Epoch:  431  Learning Rate:  0.06505090947233165  Varinance:  13.705012216429683 \n",
      "\n",
      "Epoch:  432  Learning Rate:  0.06498589107747496  Varinance:  13.685866816711004 \n",
      "\n",
      "Epoch:  433  Learning Rate:  0.06492093766851474  Varinance:  13.666748162413974 \n",
      "\n",
      "Epoch:  434  Learning Rate:  0.06485604918049762  Varinance:  13.647656216176216 \n",
      "\n",
      "Epoch:  435  Learning Rate:  0.06479122554853506  Varinance:  13.628590940687552 \n",
      "\n",
      "Epoch:  436  Learning Rate:  0.06472646670780347  Varinance:  13.60955229868992 \n",
      "\n",
      "Epoch:  437  Learning Rate:  0.06466177259354397  Varinance:  13.59054025297731 \n",
      "\n",
      "Epoch:  438  Learning Rate:  0.06459714314106245  Varinance:  13.571554766395685 \n",
      "\n",
      "Epoch:  439  Learning Rate:  0.06453257828572946  Varinance:  13.552595801842918 \n",
      "\n",
      "Epoch:  440  Learning Rate:  0.06446807796298014  Varinance:  13.533663322268701 \n",
      "\n",
      "Epoch:  441  Learning Rate:  0.06440364210831415  Varinance:  13.514757290674495 \n",
      "\n",
      "Epoch:  442  Learning Rate:  0.06433927065729562  Varinance:  13.495877670113437 \n",
      "\n",
      "Epoch:  443  Learning Rate:  0.06427496354555312  Varinance:  13.477024423690285 \n",
      "\n",
      "Epoch:  444  Learning Rate:  0.06421072070877952  Varinance:  13.458197514561332 \n",
      "\n",
      "Epoch:  445  Learning Rate:  0.06414654208273199  Varinance:  13.439396905934345 \n",
      "\n",
      "Epoch:  446  Learning Rate:  0.06408242760323188  Varinance:  13.420622561068487 \n",
      "\n",
      "Epoch:  447  Learning Rate:  0.06401837720616471  Varinance:  13.401874443274247 \n",
      "\n",
      "Epoch:  448  Learning Rate:  0.0639543908274801  Varinance:  13.383152515913366 \n",
      "\n",
      "Epoch:  449  Learning Rate:  0.06389046840319162  Varinance:  13.364456742398767 \n",
      "\n",
      "Epoch:  450  Learning Rate:  0.06382660986937688  Varinance:  13.345787086194491 \n",
      "\n",
      "Epoch:  451  Learning Rate:  0.06376281516217734  Varinance:  13.327143510815606 \n",
      "\n",
      "Epoch:  452  Learning Rate:  0.06369908421779825  Varinance:  13.30852597982816 \n",
      "\n",
      "Epoch:  453  Learning Rate:  0.06363541697250871  Varinance:  13.289934456849089 \n",
      "\n",
      "Epoch:  454  Learning Rate:  0.06357181336264144  Varinance:  13.27136890554616 \n",
      "\n",
      "Epoch:  455  Learning Rate:  0.06350827332459281  Varinance:  13.252829289637896 \n",
      "\n",
      "Epoch:  456  Learning Rate:  0.06344479679482283  Varinance:  13.234315572893498 \n",
      "\n",
      "Epoch:  457  Learning Rate:  0.0633813837098549  Varinance:  13.215827719132786 \n",
      "\n",
      "Epoch:  458  Learning Rate:  0.06331803400627599  Varinance:  13.197365692226118 \n",
      "\n",
      "Epoch:  459  Learning Rate:  0.06325474762073634  Varinance:  13.178929456094323 \n",
      "\n",
      "Epoch:  460  Learning Rate:  0.0631915244899496  Varinance:  13.16051897470864 \n",
      "\n",
      "Epoch:  461  Learning Rate:  0.0631283645506926  Varinance:  13.142134212090628 \n",
      "\n",
      "Epoch:  462  Learning Rate:  0.06306526773980542  Varinance:  13.123775132312113 \n",
      "\n",
      "Epoch:  463  Learning Rate:  0.06300223399419123  Varinance:  13.10544169949511 \n",
      "\n",
      "Epoch:  464  Learning Rate:  0.0629392632508163  Varinance:  13.087133877811752 \n",
      "\n",
      "Epoch:  465  Learning Rate:  0.06287635544670984  Varinance:  13.068851631484224 \n",
      "\n",
      "Epoch:  466  Learning Rate:  0.06281351051896408  Varinance:  13.050594924784695 \n",
      "\n",
      "Epoch:  467  Learning Rate:  0.06275072840473407  Varinance:  13.032363722035237 \n",
      "\n",
      "Epoch:  468  Learning Rate:  0.06268800904123771  Varinance:  13.014157987607769 \n",
      "\n",
      "Epoch:  469  Learning Rate:  0.0626253523657556  Varinance:  12.995977685923979 \n",
      "\n",
      "Epoch:  470  Learning Rate:  0.06256275831563107  Varinance:  12.97782278145525 \n",
      "\n",
      "Epoch:  471  Learning Rate:  0.06250022682827008  Varinance:  12.959693238722615 \n",
      "\n",
      "Epoch:  472  Learning Rate:  0.06243775784114113  Varinance:  12.941589022296645 \n",
      "\n",
      "Epoch:  473  Learning Rate:  0.06237535129177522  Varinance:  12.923510096797427 \n",
      "\n",
      "Epoch:  474  Learning Rate:  0.06231300711776579  Varinance:  12.905456426894462 \n",
      "\n",
      "Epoch:  475  Learning Rate:  0.06225072525676868  Varinance:  12.887427977306606 \n",
      "\n",
      "Epoch:  476  Learning Rate:  0.06218850564650202  Varinance:  12.869424712802008 \n",
      "\n",
      "Epoch:  477  Learning Rate:  0.06212634822474618  Varinance:  12.851446598198022 \n",
      "\n",
      "Epoch:  478  Learning Rate:  0.062064252929343734  Varinance:  12.833493598361162 \n",
      "\n",
      "Epoch:  479  Learning Rate:  0.062002219698199405  Varinance:  12.815565678207019 \n",
      "\n",
      "Epoch:  480  Learning Rate:  0.061940248469279924  Varinance:  12.797662802700197 \n",
      "\n",
      "Epoch:  481  Learning Rate:  0.061878339180614084  Varinance:  12.779784936854242 \n",
      "\n",
      "Epoch:  482  Learning Rate:  0.06181649177029258  Varinance:  12.761932045731573 \n",
      "\n",
      "Epoch:  483  Learning Rate:  0.061754706176468  Varinance:  12.744104094443415 \n",
      "\n",
      "Epoch:  484  Learning Rate:  0.06169298233735474  Varinance:  12.726301048149736 \n",
      "\n",
      "Epoch:  485  Learning Rate:  0.06163132019122897  Varinance:  12.708522872059172 \n",
      "\n",
      "Epoch:  486  Learning Rate:  0.06156971967642852  Varinance:  12.690769531428959 \n",
      "\n",
      "Epoch:  487  Learning Rate:  0.06150818073135287  Varinance:  12.673040991564868 \n",
      "\n",
      "Epoch:  488  Learning Rate:  0.061446703294463084  Varinance:  12.655337217821144 \n",
      "\n",
      "Epoch:  489  Learning Rate:  0.061385287304281715  Varinance:  12.63765817560042 \n",
      "\n",
      "Epoch:  490  Learning Rate:  0.06132393269939275  Varinance:  12.620003830353662 \n",
      "\n",
      "Epoch:  491  Learning Rate:  0.061262639418441615  Varinance:  12.60237414758011 \n",
      "\n",
      "Epoch:  492  Learning Rate:  0.061201407400134994  Varinance:  12.584769092827184 \n",
      "\n",
      "Epoch:  493  Learning Rate:  0.06114023658324087  Varinance:  12.56718863169045 \n",
      "\n",
      "Epoch:  494  Learning Rate:  0.06107912690658843  Varinance:  12.549632729813526 \n",
      "\n",
      "Epoch:  495  Learning Rate:  0.06101807830906798  Varinance:  12.53210135288802 \n",
      "\n",
      "Epoch:  496  Learning Rate:  0.06095709072963093  Varinance:  12.514594466653485 \n",
      "\n",
      "Epoch:  497  Learning Rate:  0.06089616410728969  Varinance:  12.497112036897311 \n",
      "\n",
      "Epoch:  498  Learning Rate:  0.06083529838111763  Varinance:  12.479654029454702 \n",
      "\n",
      "Epoch:  499  Learning Rate:  0.06077449349024902  Varinance:  12.462220410208582 \n",
      "\n",
      "Epoch:  500  Learning Rate:  0.06071374937387897  Varinance:  12.444811145089526 \n",
      "\n",
      "Epoch:  501  Learning Rate:  0.06065306597126335  Varinance:  12.427426200075722 \n",
      "\n",
      "Epoch:  502  Learning Rate:  0.06059244322171875  Varinance:  12.410065541192868 \n",
      "\n",
      "Epoch:  503  Learning Rate:  0.06053188106462243  Varinance:  12.392729134514129 \n",
      "\n",
      "Epoch:  504  Learning Rate:  0.060471379439412214  Varinance:  12.375416946160067 \n",
      "\n",
      "Epoch:  505  Learning Rate:  0.06041093828558647  Varinance:  12.358128942298569 \n",
      "\n",
      "Epoch:  506  Learning Rate:  0.060350557542704054  Varinance:  12.340865089144787 \n",
      "\n",
      "Epoch:  507  Learning Rate:  0.06029023715038421  Varinance:  12.323625352961066 \n",
      "\n",
      "Epoch:  508  Learning Rate:  0.06022997704830654  Varinance:  12.306409700056882 \n",
      "\n",
      "Epoch:  509  Learning Rate:  0.06016977717621094  Varinance:  12.28921809678878 \n",
      "\n",
      "Epoch:  510  Learning Rate:  0.060109637473897526  Varinance:  12.272050509560293 \n",
      "\n",
      "Epoch:  511  Learning Rate:  0.060049557881226595  Varinance:  12.254906904821901 \n",
      "\n",
      "Epoch:  512  Learning Rate:  0.059989538338118556  Varinance:  12.237787249070943 \n",
      "\n",
      "Epoch:  513  Learning Rate:  0.05992957878455384  Varinance:  12.220691508851557 \n",
      "\n",
      "Epoch:  514  Learning Rate:  0.059869679160572925  Varinance:  12.203619650754627 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  515  Learning Rate:  0.059809839406276134  Varinance:  12.1865716414177 \n",
      "\n",
      "Epoch:  516  Learning Rate:  0.05975005946182375  Varinance:  12.169547447524936 \n",
      "\n",
      "Epoch:  517  Learning Rate:  0.059690339267435805  Varinance:  12.152547035807029 \n",
      "\n",
      "Epoch:  518  Learning Rate:  0.0596306787633921  Varinance:  12.135570373041151 \n",
      "\n",
      "Epoch:  519  Learning Rate:  0.059571077890032124  Varinance:  12.118617426050893 \n",
      "\n",
      "Epoch:  520  Learning Rate:  0.059511536587755015  Varinance:  12.10168816170618 \n",
      "\n",
      "Epoch:  521  Learning Rate:  0.05945205479701944  Varinance:  12.084782546923227 \n",
      "\n",
      "Epoch:  522  Learning Rate:  0.059392632458343614  Varinance:  12.067900548664461 \n",
      "\n",
      "Epoch:  523  Learning Rate:  0.0593332695123052  Varinance:  12.051042133938465 \n",
      "\n",
      "Epoch:  524  Learning Rate:  0.05927396589954125  Varinance:  12.034207269799909 \n",
      "\n",
      "Epoch:  525  Learning Rate:  0.05921472156074813  Varinance:  12.017395923349483 \n",
      "\n",
      "Epoch:  526  Learning Rate:  0.05915553643668151  Varinance:  12.000608061733843 \n",
      "\n",
      "Epoch:  527  Learning Rate:  0.05909641046815626  Varinance:  11.98384365214553 \n",
      "\n",
      "Epoch:  528  Learning Rate:  0.05903734359604639  Varinance:  11.967102661822922 \n",
      "\n",
      "Epoch:  529  Learning Rate:  0.05897833576128504  Varinance:  11.95038505805017 \n",
      "\n",
      "Epoch:  530  Learning Rate:  0.05891938690486437  Varinance:  11.933690808157113 \n",
      "\n",
      "Epoch:  531  Learning Rate:  0.05886049696783552  Varinance:  11.917019879519243 \n",
      "\n",
      "Epoch:  532  Learning Rate:  0.058801665891308544  Varinance:  11.900372239557617 \n",
      "\n",
      "Epoch:  533  Learning Rate:  0.058742893616452345  Varinance:  11.883747855738811 \n",
      "\n",
      "Epoch:  534  Learning Rate:  0.058684180084494664  Varinance:  11.867146695574844 \n",
      "\n",
      "Epoch:  535  Learning Rate:  0.058625525236721966  Varinance:  11.850568726623122 \n",
      "\n",
      "Epoch:  536  Learning Rate:  0.05856692901447938  Varinance:  11.834013916486372 \n",
      "\n",
      "Epoch:  537  Learning Rate:  0.058508391359170686  Varinance:  11.817482232812578 \n",
      "\n",
      "Epoch:  538  Learning Rate:  0.05844991221225824  Varinance:  11.800973643294917 \n",
      "\n",
      "Epoch:  539  Learning Rate:  0.05839149151526288  Varinance:  11.784488115671705 \n",
      "\n",
      "Epoch:  540  Learning Rate:  0.058333129209763884  Varinance:  11.768025617726314 \n",
      "\n",
      "Epoch:  541  Learning Rate:  0.058274825237398964  Varinance:  11.75158611728713 \n",
      "\n",
      "Epoch:  542  Learning Rate:  0.058216579539864144  Varinance:  11.735169582227487 \n",
      "\n",
      "Epoch:  543  Learning Rate:  0.05815839205891371  Varinance:  11.71877598046558 \n",
      "\n",
      "Epoch:  544  Learning Rate:  0.05810026273636019  Varinance:  11.702405279964442 \n",
      "\n",
      "Epoch:  545  Learning Rate:  0.05804219151407424  Varinance:  11.686057448731846 \n",
      "\n",
      "Epoch:  546  Learning Rate:  0.05798417833398464  Varinance:  11.669732454820263 \n",
      "\n",
      "Epoch:  547  Learning Rate:  0.05792622313807821  Varinance:  11.653430266326794 \n",
      "\n",
      "Epoch:  548  Learning Rate:  0.05786832586839974  Varinance:  11.637150851393104 \n",
      "\n",
      "Epoch:  549  Learning Rate:  0.05781048646705196  Varinance:  11.620894178205367 \n",
      "\n",
      "Epoch:  550  Learning Rate:  0.05775270487619547  Varinance:  11.604660214994196 \n",
      "\n",
      "Epoch:  551  Learning Rate:  0.05769498103804867  Varinance:  11.588448930034586 \n",
      "\n",
      "Epoch:  552  Learning Rate:  0.057637314894887715  Varinance:  11.57226029164585 \n",
      "\n",
      "Epoch:  553  Learning Rate:  0.05757970638904645  Varinance:  11.556094268191561 \n",
      "\n",
      "Epoch:  554  Learning Rate:  0.057522155462916384  Varinance:  11.539950828079485 \n",
      "\n",
      "Epoch:  555  Learning Rate:  0.05746466205894657  Varinance:  11.523829939761518 \n",
      "\n",
      "Epoch:  556  Learning Rate:  0.0574072261196436  Varinance:  11.50773157173363 \n",
      "\n",
      "Epoch:  557  Learning Rate:  0.057349847587571536  Varinance:  11.491655692535803 \n",
      "\n",
      "Epoch:  558  Learning Rate:  0.057292526405351846  Varinance:  11.475602270751965 \n",
      "\n",
      "Epoch:  559  Learning Rate:  0.05723526251566333  Varinance:  11.459571275009933 \n",
      "\n",
      "Epoch:  560  Learning Rate:  0.0571780558612421  Varinance:  11.443562673981347 \n",
      "\n",
      "Epoch:  561  Learning Rate:  0.05712090638488149  Varinance:  11.427576436381614 \n",
      "\n",
      "Epoch:  562  Learning Rate:  0.05706381402943203  Varinance:  11.411612530969844 \n",
      "\n",
      "Epoch:  563  Learning Rate:  0.05700677873780135  Varinance:  11.395670926548787 \n",
      "\n",
      "Epoch:  564  Learning Rate:  0.05694980045295417  Varinance:  11.379751591964778 \n",
      "\n",
      "Epoch:  565  Learning Rate:  0.05689287911791218  Varinance:  11.363854496107672 \n",
      "\n",
      "Epoch:  566  Learning Rate:  0.056836014675754054  Varinance:  11.347979607910782 \n",
      "\n",
      "Epoch:  567  Learning Rate:  0.05677920706961534  Varinance:  11.332126896350818 \n",
      "\n",
      "Epoch:  568  Learning Rate:  0.056722456242688415  Varinance:  11.316296330447836 \n",
      "\n",
      "Epoch:  569  Learning Rate:  0.05666576213822247  Varinance:  11.300487879265157 \n",
      "\n",
      "Epoch:  570  Learning Rate:  0.05660912469952339  Varinance:  11.284701511909336 \n",
      "\n",
      "Epoch:  571  Learning Rate:  0.05655254386995371  Varinance:  11.268937197530073 \n",
      "\n",
      "Epoch:  572  Learning Rate:  0.05649601959293263  Varinance:  11.253194905320168 \n",
      "\n",
      "Epoch:  573  Learning Rate:  0.05643955181193584  Varinance:  11.237474604515455 \n",
      "\n",
      "Epoch:  574  Learning Rate:  0.05638314047049558  Varinance:  11.221776264394752 \n",
      "\n",
      "Epoch:  575  Learning Rate:  0.056326785512200474  Varinance:  11.206099854279785 \n",
      "\n",
      "Epoch:  576  Learning Rate:  0.056270486880695574  Varinance:  11.19044534353514 \n",
      "\n",
      "Epoch:  577  Learning Rate:  0.056214244519682245  Varinance:  11.174812701568202 \n",
      "\n",
      "Epoch:  578  Learning Rate:  0.05615805837291813  Varinance:  11.159201897829087 \n",
      "\n",
      "Epoch:  579  Learning Rate:  0.05610192838421706  Varinance:  11.143612901810592 \n",
      "\n",
      "Epoch:  580  Learning Rate:  0.056045854497449046  Varinance:  11.128045683048132 \n",
      "\n",
      "Epoch:  581  Learning Rate:  0.055989836656540205  Varinance:  11.112500211119675 \n",
      "\n",
      "Epoch:  582  Learning Rate:  0.05593387480547268  Varinance:  11.096976455645697 \n",
      "\n",
      "Epoch:  583  Learning Rate:  0.05587796888828464  Varinance:  11.081474386289102 \n",
      "\n",
      "Epoch:  584  Learning Rate:  0.055822118849070124  Varinance:  11.06599397275518 \n",
      "\n",
      "Epoch:  585  Learning Rate:  0.05576632463197913  Varinance:  11.050535184791544 \n",
      "\n",
      "Epoch:  586  Learning Rate:  0.055710586181217395  Varinance:  11.035097992188058 \n",
      "\n",
      "Epoch:  587  Learning Rate:  0.05565490344104649  Varinance:  11.019682364776804 \n",
      "\n",
      "Epoch:  588  Learning Rate:  0.05559927635578367  Varinance:  11.004288272431992 \n",
      "\n",
      "Epoch:  589  Learning Rate:  0.05554370486980183  Varinance:  10.988915685069927 \n",
      "\n",
      "Epoch:  590  Learning Rate:  0.05548818892752949  Varinance:  10.973564572648934 \n",
      "\n",
      "Epoch:  591  Learning Rate:  0.05543272847345071  Varinance:  10.95823490516931 \n",
      "\n",
      "Epoch:  592  Learning Rate:  0.05537732345210501  Varinance:  10.942926652673254 \n",
      "\n",
      "Epoch:  593  Learning Rate:  0.05532197380808739  Varinance:  10.927639785244825 \n",
      "\n",
      "Epoch:  594  Learning Rate:  0.05526667948604818  Varinance:  10.912374273009862 \n",
      "\n",
      "Epoch:  595  Learning Rate:  0.055211440430693065  Varinance:  10.897130086135947 \n",
      "\n",
      "Epoch:  596  Learning Rate:  0.055156256586782984  Varinance:  10.881907194832326 \n",
      "\n",
      "Epoch:  597  Learning Rate:  0.05510112789913407  Varinance:  10.866705569349874 \n",
      "\n",
      "Epoch:  598  Learning Rate:  0.055046054312617665  Varinance:  10.851525179981017 \n",
      "\n",
      "Epoch:  599  Learning Rate:  0.05499103577216016  Varinance:  10.83636599705968 \n",
      "\n",
      "Epoch:  600  Learning Rate:  0.05493607222274301  Varinance:  10.821227990961235 \n",
      "\n",
      "Epoch:  601  Learning Rate:  0.05488116360940265  Varinance:  10.806111132102437 \n",
      "\n",
      "Epoch:  602  Learning Rate:  0.05482630987723047  Varinance:  10.791015390941366 \n",
      "\n",
      "Epoch:  603  Learning Rate:  0.054771510971372755  Varinance:  10.775940737977374 \n",
      "\n",
      "Epoch:  604  Learning Rate:  0.054716766837030556  Varinance:  10.76088714375102 \n",
      "\n",
      "Epoch:  605  Learning Rate:  0.05466207741945977  Varinance:  10.745854578844023 \n",
      "\n",
      "Epoch:  606  Learning Rate:  0.05460744266397094  Varinance:  10.730843013879193 \n",
      "\n",
      "Epoch:  607  Learning Rate:  0.054552862515929335  Varinance:  10.715852419520381 \n",
      "\n",
      "Epoch:  608  Learning Rate:  0.0544983369207548  Varinance:  10.700882766472416 \n",
      "\n",
      "Epoch:  609  Learning Rate:  0.05444386582392172  Varinance:  10.685934025481059 \n",
      "\n",
      "Epoch:  610  Learning Rate:  0.054389449170958996  Varinance:  10.67100616733293 \n",
      "\n",
      "Epoch:  611  Learning Rate:  0.05433508690744998  Varinance:  10.656099162855465 \n",
      "\n",
      "Epoch:  612  Learning Rate:  0.0542807789790324  Varinance:  10.641212982916848 \n",
      "\n",
      "Epoch:  613  Learning Rate:  0.05422652533139832  Varinance:  10.626347598425962 \n",
      "\n",
      "Epoch:  614  Learning Rate:  0.0541723259102941  Varinance:  10.61150298033233 \n",
      "\n",
      "Epoch:  615  Learning Rate:  0.05411818066152029  Varinance:  10.596679099626058 \n",
      "\n",
      "Epoch:  616  Learning Rate:  0.05406408953093166  Varinance:  10.581875927337773 \n",
      "\n",
      "Epoch:  617  Learning Rate:  0.05401005246443707  Varinance:  10.567093434538572 \n",
      "\n",
      "Epoch:  618  Learning Rate:  0.053956069407999435  Varinance:  10.552331592339971 \n",
      "\n",
      "Epoch:  619  Learning Rate:  0.05390214030763571  Varinance:  10.537590371893836 \n",
      "\n",
      "Epoch:  620  Learning Rate:  0.05384826510941679  Varinance:  10.522869744392334 \n",
      "\n",
      "Epoch:  621  Learning Rate:  0.05379444375946745  Varinance:  10.50816968106788 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  622  Learning Rate:  0.053740676203966366  Varinance:  10.493490153193068 \n",
      "\n",
      "Epoch:  623  Learning Rate:  0.053686962389145956  Varinance:  10.47883113208063 \n",
      "\n",
      "Epoch:  624  Learning Rate:  0.05363330226129242  Varinance:  10.46419258908337 \n",
      "\n",
      "Epoch:  625  Learning Rate:  0.05357969576674561  Varinance:  10.44957449559411 \n",
      "\n",
      "Epoch:  626  Learning Rate:  0.05352614285189903  Varinance:  10.434976823045641 \n",
      "\n",
      "Epoch:  627  Learning Rate:  0.05347264346319976  Varinance:  10.420399542910651 \n",
      "\n",
      "Epoch:  628  Learning Rate:  0.05341919754714841  Varinance:  10.405842626701693 \n",
      "\n",
      "Epoch:  629  Learning Rate:  0.053365805050299066  Varinance:  10.391306045971104 \n",
      "\n",
      "Epoch:  630  Learning Rate:  0.053312465919259214  Varinance:  10.376789772310968 \n",
      "\n",
      "Epoch:  631  Learning Rate:  0.053259180100689724  Varinance:  10.362293777353052 \n",
      "\n",
      "Epoch:  632  Learning Rate:  0.05320594754130477  Varinance:  10.34781803276875 \n",
      "\n",
      "Epoch:  633  Learning Rate:  0.05315276818787179  Varinance:  10.333362510269033 \n",
      "\n",
      "Epoch:  634  Learning Rate:  0.053099641987211434  Varinance:  10.318927181604394 \n",
      "\n",
      "Epoch:  635  Learning Rate:  0.05304656888619749  Varinance:  10.304512018564779 \n",
      "\n",
      "Epoch:  636  Learning Rate:  0.05299354883175686  Varinance:  10.290116992979556 \n",
      "\n",
      "Epoch:  637  Learning Rate:  0.05294058177086946  Varinance:  10.275742076717433 \n",
      "\n",
      "Epoch:  638  Learning Rate:  0.05288766765056825  Varinance:  10.261387241686428 \n",
      "\n",
      "Epoch:  639  Learning Rate:  0.0528348064179391  Varinance:  10.247052459833794 \n",
      "\n",
      "Epoch:  640  Learning Rate:  0.05278199802012077  Varinance:  10.232737703145975 \n",
      "\n",
      "Epoch:  641  Learning Rate:  0.052729242404304856  Varinance:  10.218442943648554 \n",
      "\n",
      "Epoch:  642  Learning Rate:  0.052676539517735745  Varinance:  10.204168153406187 \n",
      "\n",
      "Epoch:  643  Learning Rate:  0.05262388930771054  Varinance:  10.189913304522554 \n",
      "\n",
      "Epoch:  644  Learning Rate:  0.05257129172157903  Varinance:  10.17567836914031 \n",
      "\n",
      "Epoch:  645  Learning Rate:  0.05251874670674361  Varinance:  10.161463319441024 \n",
      "\n",
      "Epoch:  646  Learning Rate:  0.05246625421065929  Varinance:  10.147268127645125 \n",
      "\n",
      "Epoch:  647  Learning Rate:  0.052413814180833546  Varinance:  10.133092766011849 \n",
      "\n",
      "Epoch:  648  Learning Rate:  0.052361426564826355  Varinance:  10.118937206839188 \n",
      "\n",
      "Epoch:  649  Learning Rate:  0.05230909131025008  Varinance:  10.10480142246383 \n",
      "\n",
      "Epoch:  650  Learning Rate:  0.05225680836476948  Varinance:  10.090685385261104 \n",
      "\n",
      "Epoch:  651  Learning Rate:  0.05220457767610161  Varinance:  10.076589067644939 \n",
      "\n",
      "Epoch:  652  Learning Rate:  0.05215239919201575  Varinance:  10.062512442067792 \n",
      "\n",
      "Epoch:  653  Learning Rate:  0.052100272860333446  Varinance:  10.048455481020609 \n",
      "\n",
      "Epoch:  654  Learning Rate:  0.05204819862892832  Varinance:  10.034418157032759 \n",
      "\n",
      "Epoch:  655  Learning Rate:  0.05199617644572618  Varinance:  10.020400442671992 \n",
      "\n",
      "Epoch:  656  Learning Rate:  0.05194420625870482  Varinance:  10.00640231054438 \n",
      "\n",
      "Epoch:  657  Learning Rate:  0.051892288015894045  Varinance:  9.992423733294254 \n",
      "\n",
      "Epoch:  658  Learning Rate:  0.05184042166537559  Varinance:  9.978464683604175 \n",
      "\n",
      "Epoch:  659  Learning Rate:  0.05178860715528314  Varinance:  9.964525134194853 \n",
      "\n",
      "Epoch:  660  Learning Rate:  0.051736844433802165  Varinance:  9.95060505782511 \n",
      "\n",
      "Epoch:  661  Learning Rate:  0.05168513344916992  Varinance:  9.936704427291826 \n",
      "\n",
      "Epoch:  662  Learning Rate:  0.051633474149675444  Varinance:  9.922823215429878 \n",
      "\n",
      "Epoch:  663  Learning Rate:  0.051581866483659415  Varinance:  9.908961395112097 \n",
      "\n",
      "Epoch:  664  Learning Rate:  0.05153031039951417  Varinance:  9.895118939249203 \n",
      "\n",
      "Epoch:  665  Learning Rate:  0.05147880584568362  Varinance:  9.881295820789763 \n",
      "\n",
      "Epoch:  666  Learning Rate:  0.051427352770663196  Varinance:  9.867492012720136 \n",
      "\n",
      "Epoch:  667  Learning Rate:  0.051375951122999836  Varinance:  9.853707488064412 \n",
      "\n",
      "Epoch:  668  Learning Rate:  0.05132460085129188  Varinance:  9.839942219884371 \n",
      "\n",
      "Epoch:  669  Learning Rate:  0.05127330190418905  Varinance:  9.826196181279421 \n",
      "\n",
      "Epoch:  670  Learning Rate:  0.0512220542303924  Varinance:  9.81246934538655 \n",
      "\n",
      "Epoch:  671  Learning Rate:  0.051170857778654245  Varinance:  9.798761685380274 \n",
      "\n",
      "Epoch:  672  Learning Rate:  0.051119712497778136  Varinance:  9.785073174472583 \n",
      "\n",
      "Epoch:  673  Learning Rate:  0.05106861833661879  Varinance:  9.771403785912886 \n",
      "\n",
      "Epoch:  674  Learning Rate:  0.05101757524408203  Varinance:  9.757753492987966 \n",
      "\n",
      "Epoch:  675  Learning Rate:  0.05096658316912476  Varinance:  9.744122269021918 \n",
      "\n",
      "Epoch:  676  Learning Rate:  0.05091564206075492  Varinance:  9.730510087376109 \n",
      "\n",
      "Epoch:  677  Learning Rate:  0.05086475186803137  Varinance:  9.716916921449116 \n",
      "\n",
      "Epoch:  678  Learning Rate:  0.050813912540063934  Varinance:  9.703342744676672 \n",
      "\n",
      "Epoch:  679  Learning Rate:  0.050763124026013275  Varinance:  9.689787530531628 \n",
      "\n",
      "Epoch:  680  Learning Rate:  0.050712386275090865  Varinance:  9.676251252523887 \n",
      "\n",
      "Epoch:  681  Learning Rate:  0.05066169923655896  Varinance:  9.662733884200362 \n",
      "\n",
      "Epoch:  682  Learning Rate:  0.05061106285973052  Varinance:  9.649235399144914 \n",
      "\n",
      "Epoch:  683  Learning Rate:  0.05056047709396915  Varinance:  9.63575577097831 \n",
      "\n",
      "Epoch:  684  Learning Rate:  0.05050994188868908  Varinance:  9.62229497335817 \n",
      "\n",
      "Epoch:  685  Learning Rate:  0.05045945719335512  Varinance:  9.608852979978908 \n",
      "\n",
      "Epoch:  686  Learning Rate:  0.050409022957482556  Varinance:  9.59542976457169 \n",
      "\n",
      "Epoch:  687  Learning Rate:  0.050358639130637144  Varinance:  9.582025300904379 \n",
      "\n",
      "Epoch:  688  Learning Rate:  0.05030830566243506  Varinance:  9.568639562781478 \n",
      "\n",
      "Epoch:  689  Learning Rate:  0.05025802250254285  Varinance:  9.55527252404409 \n",
      "\n",
      "Epoch:  690  Learning Rate:  0.050207789600677316  Varinance:  9.54192415856986 \n",
      "\n",
      "Epoch:  691  Learning Rate:  0.05015760690660556  Varinance:  9.52859444027292 \n",
      "\n",
      "Epoch:  692  Learning Rate:  0.05010747437014489  Varinance:  9.515283343103853 \n",
      "\n",
      "Epoch:  693  Learning Rate:  0.05005739194116278  Varinance:  9.501990841049619 \n",
      "\n",
      "Epoch:  694  Learning Rate:  0.050007359569576776  Varinance:  9.488716908133528 \n",
      "\n",
      "Epoch:  695  Learning Rate:  0.0499573772053545  Varinance:  9.475461518415175 \n",
      "\n",
      "Epoch:  696  Learning Rate:  0.049907444798513605  Varinance:  9.462224645990391 \n",
      "\n",
      "Epoch:  697  Learning Rate:  0.04985756229912166  Varinance:  9.449006264991196 \n",
      "\n",
      "Epoch:  698  Learning Rate:  0.04980772965729617  Varinance:  9.435806349585746 \n",
      "\n",
      "Epoch:  699  Learning Rate:  0.04975794682320449  Varinance:  9.422624873978284 \n",
      "\n",
      "Epoch:  700  Learning Rate:  0.04970821374706377  Varinance:  9.409461812409086 \n",
      "\n",
      "Epoch:  701  Learning Rate:  0.049658530379140954  Varinance:  9.396317139154421 \n",
      "\n",
      "Epoch:  702  Learning Rate:  0.049608896669752656  Varinance:  9.383190828526482 \n",
      "\n",
      "Epoch:  703  Learning Rate:  0.04955931256926515  Varinance:  9.370082854873354 \n",
      "\n",
      "Epoch:  704  Learning Rate:  0.04950977802809435  Varinance:  9.356993192578956 \n",
      "\n",
      "Epoch:  705  Learning Rate:  0.04946029299670571  Varinance:  9.343921816062993 \n",
      "\n",
      "Epoch:  706  Learning Rate:  0.04941085742561417  Varinance:  9.330868699780897 \n",
      "\n",
      "Epoch:  707  Learning Rate:  0.049361471265384184  Varinance:  9.317833818223797 \n",
      "\n",
      "Epoch:  708  Learning Rate:  0.049312134466629576  Varinance:  9.304817145918445 \n",
      "\n",
      "Epoch:  709  Learning Rate:  0.04926284698001355  Varinance:  9.29181865742719 \n",
      "\n",
      "Epoch:  710  Learning Rate:  0.049213608756248604  Varinance:  9.278838327347906 \n",
      "\n",
      "Epoch:  711  Learning Rate:  0.04916441974609651  Varinance:  9.265876130313956 \n",
      "\n",
      "Epoch:  712  Learning Rate:  0.04911527990036827  Varinance:  9.252932040994143 \n",
      "\n",
      "Epoch:  713  Learning Rate:  0.049066189169924015  Varinance:  9.240006034092652 \n",
      "\n",
      "Epoch:  714  Learning Rate:  0.04901714750567302  Varinance:  9.22709808434901 \n",
      "\n",
      "Epoch:  715  Learning Rate:  0.04896815485857362  Varinance:  9.214208166538027 \n",
      "\n",
      "Epoch:  716  Learning Rate:  0.04891921117963316  Varinance:  9.201336255469755 \n",
      "\n",
      "Epoch:  717  Learning Rate:  0.04887031641990795  Varinance:  9.188482325989437 \n",
      "\n",
      "Epoch:  718  Learning Rate:  0.04882147053050323  Varinance:  9.175646352977449 \n",
      "\n",
      "Epoch:  719  Learning Rate:  0.04877267346257312  Varinance:  9.162828311349264 \n",
      "\n",
      "Epoch:  720  Learning Rate:  0.04872392516732053  Varinance:  9.1500281760554 \n",
      "\n",
      "Epoch:  721  Learning Rate:  0.04867522559599717  Varinance:  9.137245922081359 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  722  Learning Rate:  0.04862657469990346  Varinance:  9.124481524447596 \n",
      "\n",
      "Epoch:  723  Learning Rate:  0.0485779724303885  Varinance:  9.111734958209459 \n",
      "\n",
      "Epoch:  724  Learning Rate:  0.04852941873885003  Varinance:  9.099006198457138 \n",
      "\n",
      "Epoch:  725  Learning Rate:  0.04848091357673433  Varinance:  9.086295220315627 \n",
      "\n",
      "Epoch:  726  Learning Rate:  0.04843245689553625  Varinance:  9.07360199894467 \n",
      "\n",
      "Epoch:  727  Learning Rate:  0.0483840486467991  Varinance:  9.060926509538705 \n",
      "\n",
      "Epoch:  728  Learning Rate:  0.04833568878211464  Varinance:  9.048268727326828 \n",
      "\n",
      "Epoch:  729  Learning Rate:  0.04828737725312298  Varinance:  9.035628627572741 \n",
      "\n",
      "Epoch:  730  Learning Rate:  0.048239114011512596  Varinance:  9.023006185574694 \n",
      "\n",
      "Epoch:  731  Learning Rate:  0.048190899009020245  Varinance:  9.010401376665454 \n",
      "\n",
      "Epoch:  732  Learning Rate:  0.048142732197430926  Varinance:  8.997814176212238 \n",
      "\n",
      "Epoch:  733  Learning Rate:  0.0480946135285778  Varinance:  8.985244559616682 \n",
      "\n",
      "Epoch:  734  Learning Rate:  0.04804654295434223  Varinance:  8.972692502314777 \n",
      "\n",
      "Epoch:  735  Learning Rate:  0.047998520426653604  Varinance:  8.960157979776836 \n",
      "\n",
      "Epoch:  736  Learning Rate:  0.04795054589748941  Varinance:  8.94764096750744 \n",
      "\n",
      "Epoch:  737  Learning Rate:  0.047902619318875114  Varinance:  8.935141441045378 \n",
      "\n",
      "Epoch:  738  Learning Rate:  0.04785474064288412  Varinance:  8.922659375963624 \n",
      "\n",
      "Epoch:  739  Learning Rate:  0.047806909821637766  Varinance:  8.910194747869268 \n",
      "\n",
      "Epoch:  740  Learning Rate:  0.04775912680730521  Varinance:  8.897747532403475 \n",
      "\n",
      "Epoch:  741  Learning Rate:  0.04771139155210344  Varinance:  8.885317705241444 \n",
      "\n",
      "Epoch:  742  Learning Rate:  0.047663704008297204  Varinance:  8.872905242092354 \n",
      "\n",
      "Epoch:  743  Learning Rate:  0.047616064128198944  Varinance:  8.860510118699313 \n",
      "\n",
      "Epoch:  744  Learning Rate:  0.047568471864168786  Varinance:  8.848132310839317 \n",
      "\n",
      "Epoch:  745  Learning Rate:  0.047520927168614446  Varinance:  8.8357717943232 \n",
      "\n",
      "Epoch:  746  Learning Rate:  0.04747342999399124  Varinance:  8.823428544995593 \n",
      "\n",
      "Epoch:  747  Learning Rate:  0.04742598029280199  Varinance:  8.811102538734861 \n",
      "\n",
      "Epoch:  748  Learning Rate:  0.047378578017596984  Varinance:  8.798793751453074 \n",
      "\n",
      "Epoch:  749  Learning Rate:  0.04733122312097393  Varinance:  8.786502159095951 \n",
      "\n",
      "Epoch:  750  Learning Rate:  0.047283915555577954  Varinance:  8.77422773764281 \n",
      "\n",
      "Epoch:  751  Learning Rate:  0.04723665527410147  Varinance:  8.761970463106529 \n",
      "\n",
      "Epoch:  752  Learning Rate:  0.0471894422292842  Varinance:  8.749730311533494 \n",
      "\n",
      "Epoch:  753  Learning Rate:  0.04714227637391309  Varinance:  8.737507259003552 \n",
      "\n",
      "Epoch:  754  Learning Rate:  0.047095157660822284  Varinance:  8.725301281629966 \n",
      "\n",
      "Epoch:  755  Learning Rate:  0.04704808604289306  Varinance:  8.71311235555937 \n",
      "\n",
      "Epoch:  756  Learning Rate:  0.0470010614730538  Varinance:  8.700940456971718 \n",
      "\n",
      "Epoch:  757  Learning Rate:  0.04695408390427993  Varinance:  8.688785562080241 \n",
      "\n",
      "Epoch:  758  Learning Rate:  0.04690715328959388  Varinance:  8.676647647131396 \n",
      "\n",
      "Epoch:  759  Learning Rate:  0.046860269582065024  Varinance:  8.66452668840483 \n",
      "\n",
      "Epoch:  760  Learning Rate:  0.04681343273480965  Varinance:  8.652422662213318 \n",
      "\n",
      "Epoch:  761  Learning Rate:  0.04676664270099093  Varinance:  8.640335544902733 \n",
      "\n",
      "Epoch:  762  Learning Rate:  0.046719899433818796  Varinance:  8.628265312851987 \n",
      "\n",
      "Epoch:  763  Learning Rate:  0.04667320288654999  Varinance:  8.616211942472987 \n",
      "\n",
      "Epoch:  764  Learning Rate:  0.046626553012487956  Varinance:  8.604175410210598 \n",
      "\n",
      "Epoch:  765  Learning Rate:  0.04657994976498283  Varinance:  8.59215569254259 \n",
      "\n",
      "Epoch:  766  Learning Rate:  0.04653339309743135  Varinance:  8.58015276597959 \n",
      "\n",
      "Epoch:  767  Learning Rate:  0.04648688296327683  Varinance:  8.56816660706504 \n",
      "\n",
      "Epoch:  768  Learning Rate:  0.04644041931600916  Varinance:  8.55619719237515 \n",
      "\n",
      "Epoch:  769  Learning Rate:  0.046394002109164674  Varinance:  8.54424449851885 \n",
      "\n",
      "Epoch:  770  Learning Rate:  0.04634763129632616  Varinance:  8.532308502137749 \n",
      "\n",
      "Epoch:  771  Learning Rate:  0.04630130683112281  Varinance:  8.520389179906088 \n",
      "\n",
      "Epoch:  772  Learning Rate:  0.04625502866723014  Varinance:  8.50848650853069 \n",
      "\n",
      "Epoch:  773  Learning Rate:  0.04620879675837  Varinance:  8.49660046475092 \n",
      "\n",
      "Epoch:  774  Learning Rate:  0.046162611058310474  Varinance:  8.484731025338634 \n",
      "\n",
      "Epoch:  775  Learning Rate:  0.04611647152086584  Varinance:  8.47287816709814 \n",
      "\n",
      "Epoch:  776  Learning Rate:  0.046070378099896586  Varinance:  8.461041866866154 \n",
      "\n",
      "Epoch:  777  Learning Rate:  0.046024330749309256  Varinance:  8.449222101511737 \n",
      "\n",
      "Epoch:  778  Learning Rate:  0.04597832942305652  Varinance:  8.437418847936275 \n",
      "\n",
      "Epoch:  779  Learning Rate:  0.045932374075137034  Varinance:  8.42563208307342 \n",
      "\n",
      "Epoch:  780  Learning Rate:  0.04588646465959545  Varinance:  8.413861783889041 \n",
      "\n",
      "Epoch:  781  Learning Rate:  0.04584060113052235  Varinance:  8.40210792738119 \n",
      "\n",
      "Epoch:  782  Learning Rate:  0.045794783442054204  Varinance:  8.390370490580054 \n",
      "\n",
      "Epoch:  783  Learning Rate:  0.04574901154837332  Varinance:  8.378649450547899 \n",
      "\n",
      "Epoch:  784  Learning Rate:  0.04570328540370779  Varinance:  8.366944784379044 \n",
      "\n",
      "Epoch:  785  Learning Rate:  0.045657604962331476  Varinance:  8.355256469199798 \n",
      "\n",
      "Epoch:  786  Learning Rate:  0.04561197017856392  Varinance:  8.343584482168433 \n",
      "\n",
      "Epoch:  787  Learning Rate:  0.04556638100677035  Varinance:  8.33192880047512 \n",
      "\n",
      "Epoch:  788  Learning Rate:  0.04552083740136159  Varinance:  8.320289401341903 \n",
      "\n",
      "Epoch:  789  Learning Rate:  0.04547533931679402  Varinance:  8.308666262022642 \n",
      "\n",
      "Epoch:  790  Learning Rate:  0.045429886707569554  Varinance:  8.29705935980297 \n",
      "\n",
      "Epoch:  791  Learning Rate:  0.045384479528235586  Varinance:  8.285468672000256 \n",
      "\n",
      "Epoch:  792  Learning Rate:  0.04533911773338492  Varinance:  8.273894175963557 \n",
      "\n",
      "Epoch:  793  Learning Rate:  0.045293801277655775  Varinance:  8.262335849073562 \n",
      "\n",
      "Epoch:  794  Learning Rate:  0.045248530115731676  Varinance:  8.250793668742572 \n",
      "\n",
      "Epoch:  795  Learning Rate:  0.04520330420234147  Varinance:  8.239267612414434 \n",
      "\n",
      "Epoch:  796  Learning Rate:  0.04515812349225923  Varinance:  8.227757657564506 \n",
      "\n",
      "Epoch:  797  Learning Rate:  0.04511298794030424  Varinance:  8.216263781699615 \n",
      "\n",
      "Epoch:  798  Learning Rate:  0.04506789750134095  Varinance:  8.204785962358006 \n",
      "\n",
      "Epoch:  799  Learning Rate:  0.04502285213027893  Varinance:  8.193324177109307 \n",
      "\n",
      "Epoch:  800  Learning Rate:  0.04497785178207278  Varinance:  8.181878403554474 \n",
      "\n",
      "Epoch:  801  Learning Rate:  0.044932896411722156  Varinance:  8.170448619325764 \n",
      "\n",
      "Epoch:  802  Learning Rate:  0.0448879859742717  Varinance:  8.159034802086671 \n",
      "\n",
      "Epoch:  803  Learning Rate:  0.04484312042481095  Varinance:  8.147636929531892 \n",
      "\n",
      "Epoch:  804  Learning Rate:  0.04479829971847437  Varinance:  8.136254979387292 \n",
      "\n",
      "Epoch:  805  Learning Rate:  0.04475352381044124  Varinance:  8.124888929409847 \n",
      "\n",
      "Epoch:  806  Learning Rate:  0.04470879265593564  Varinance:  8.113538757387605 \n",
      "\n",
      "Epoch:  807  Learning Rate:  0.044664106210226436  Varinance:  8.102204441139648 \n",
      "\n",
      "Epoch:  808  Learning Rate:  0.04461946442862716  Varinance:  8.090885958516035 \n",
      "\n",
      "Epoch:  809  Learning Rate:  0.044574867266496024  Varinance:  8.079583287397778 \n",
      "\n",
      "Epoch:  810  Learning Rate:  0.04453031467923588  Varinance:  8.06829640569678 \n",
      "\n",
      "Epoch:  811  Learning Rate:  0.044485806622294115  Varinance:  8.057025291355806 \n",
      "\n",
      "Epoch:  812  Learning Rate:  0.04444134305116268  Varinance:  8.045769922348434 \n",
      "\n",
      "Epoch:  813  Learning Rate:  0.044396923921378006  Varinance:  8.034530276679003 \n",
      "\n",
      "Epoch:  814  Learning Rate:  0.044352549188520954  Varinance:  8.023306332382594 \n",
      "\n",
      "Epoch:  815  Learning Rate:  0.04430821880821678  Varinance:  8.012098067524962 \n",
      "\n",
      "Epoch:  816  Learning Rate:  0.04426393273613512  Varinance:  8.000905460202503 \n",
      "\n",
      "Epoch:  817  Learning Rate:  0.04421969092798987  Varinance:  7.989728488542217 \n",
      "\n",
      "Epoch:  818  Learning Rate:  0.04417549333953924  Varinance:  7.978567130701654 \n",
      "\n",
      "Epoch:  819  Learning Rate:  0.044131339926585626  Varinance:  7.967421364868886 \n",
      "\n",
      "Epoch:  820  Learning Rate:  0.04408723064497562  Varinance:  7.956291169262444 \n",
      "\n",
      "Epoch:  821  Learning Rate:  0.044043165450599935  Varinance:  7.945176522131295 \n",
      "\n",
      "Epoch:  822  Learning Rate:  0.04399914429939336  Varinance:  7.93407740175479 \n",
      "\n",
      "Epoch:  823  Learning Rate:  0.043955167147334766  Varinance:  7.9229937864426185 \n",
      "\n",
      "Epoch:  824  Learning Rate:  0.04391123395044697  Varinance:  7.911925654534777 \n",
      "\n",
      "Epoch:  825  Learning Rate:  0.04386734466479679  Varinance:  7.900872984401516 \n",
      "\n",
      "Epoch:  826  Learning Rate:  0.043823499246494924  Varinance:  7.889835754443303 \n",
      "\n",
      "Epoch:  827  Learning Rate:  0.04377969765169597  Varinance:  7.878813943090779 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  828  Learning Rate:  0.0437359398365983  Varinance:  7.86780752880472 \n",
      "\n",
      "Epoch:  829  Learning Rate:  0.04369222575744412  Varinance:  7.856816490075984 \n",
      "\n",
      "Epoch:  830  Learning Rate:  0.04364855537051934  Varinance:  7.845840805425483 \n",
      "\n",
      "Epoch:  831  Learning Rate:  0.04360492863215357  Varinance:  7.834880453404133 \n",
      "\n",
      "Epoch:  832  Learning Rate:  0.04356134549872005  Varinance:  7.823935412592814 \n",
      "\n",
      "Epoch:  833  Learning Rate:  0.04351780592663568  Varinance:  7.813005661602325 \n",
      "\n",
      "Epoch:  834  Learning Rate:  0.043474309872360846  Varinance:  7.802091179073346 \n",
      "\n",
      "Epoch:  835  Learning Rate:  0.04343085729239951  Varinance:  7.7911919436764 \n",
      "\n",
      "Epoch:  836  Learning Rate:  0.04338744814329909  Varinance:  7.780307934111796 \n",
      "\n",
      "Epoch:  837  Learning Rate:  0.04334408238165043  Varinance:  7.769439129109609 \n",
      "\n",
      "Epoch:  838  Learning Rate:  0.04330075996408777  Varinance:  7.758585507429625 \n",
      "\n",
      "Epoch:  839  Learning Rate:  0.04325748084728867  Varinance:  7.747747047861292 \n",
      "\n",
      "Epoch:  840  Learning Rate:  0.04321424498797403  Varinance:  7.736923729223701 \n",
      "\n",
      "Epoch:  841  Learning Rate:  0.043171052342907974  Varinance:  7.726115530365525 \n",
      "\n",
      "Epoch:  842  Learning Rate:  0.04312790286889786  Varinance:  7.715322430164989 \n",
      "\n",
      "Epoch:  843  Learning Rate:  0.043084796522794205  Varinance:  7.704544407529817 \n",
      "\n",
      "Epoch:  844  Learning Rate:  0.04304173326149067  Varinance:  7.693781441397207 \n",
      "\n",
      "Epoch:  845  Learning Rate:  0.042998713041923986  Varinance:  7.683033510733775 \n",
      "\n",
      "Epoch:  846  Learning Rate:  0.04295573582107392  Varinance:  7.672300594535522 \n",
      "\n",
      "Epoch:  847  Learning Rate:  0.042912801555963255  Varinance:  7.661582671827789 \n",
      "\n",
      "Epoch:  848  Learning Rate:  0.04286991020365773  Varinance:  7.6508797216652225 \n",
      "\n",
      "Epoch:  849  Learning Rate:  0.04282706172126597  Varinance:  7.640191723131723 \n",
      "\n",
      "Epoch:  850  Learning Rate:  0.042784256065939504  Varinance:  7.629518655340414 \n",
      "\n",
      "Epoch:  851  Learning Rate:  0.04274149319487267  Varinance:  7.618860497433595 \n",
      "\n",
      "Epoch:  852  Learning Rate:  0.04269877306530259  Varinance:  7.608217228582706 \n",
      "\n",
      "Epoch:  853  Learning Rate:  0.042656095634509145  Varinance:  7.597588827988279 \n",
      "\n",
      "Epoch:  854  Learning Rate:  0.04261346085981488  Varinance:  7.586975274879905 \n",
      "\n",
      "Epoch:  855  Learning Rate:  0.04257086869858502  Varinance:  7.576376548516192 \n",
      "\n",
      "Epoch:  856  Learning Rate:  0.042528319108227415  Varinance:  7.56579262818472 \n",
      "\n",
      "Epoch:  857  Learning Rate:  0.04248581204619246  Varinance:  7.555223493202004 \n",
      "\n",
      "Epoch:  858  Learning Rate:  0.042443347469973094  Varinance:  7.544669122913457 \n",
      "\n",
      "Epoch:  859  Learning Rate:  0.04240092533710473  Varinance:  7.534129496693338 \n",
      "\n",
      "Epoch:  860  Learning Rate:  0.04235854560516524  Varinance:  7.523604593944726 \n",
      "\n",
      "Epoch:  861  Learning Rate:  0.042316208231774885  Varinance:  7.513094394099472 \n",
      "\n",
      "Epoch:  862  Learning Rate:  0.042273913174596285  Varinance:  7.502598876618157 \n",
      "\n",
      "Epoch:  863  Learning Rate:  0.042231660391334386  Varinance:  7.4921180209900555 \n",
      "\n",
      "Epoch:  864  Learning Rate:  0.042189449839736395  Varinance:  7.4816518067331 \n",
      "\n",
      "Epoch:  865  Learning Rate:  0.042147281477591766  Varinance:  7.47120021339383 \n",
      "\n",
      "Epoch:  866  Learning Rate:  0.04210515526273212  Varinance:  7.460763220547359 \n",
      "\n",
      "Epoch:  867  Learning Rate:  0.04206307115303125  Varinance:  7.450340807797333 \n",
      "\n",
      "Epoch:  868  Learning Rate:  0.04202102910640503  Varinance:  7.439932954775894 \n",
      "\n",
      "Epoch:  869  Learning Rate:  0.04197902908081143  Varinance:  7.429539641143632 \n",
      "\n",
      "Epoch:  870  Learning Rate:  0.0419370710342504  Varinance:  7.419160846589554 \n",
      "\n",
      "Epoch:  871  Learning Rate:  0.0418951549247639  Varinance:  7.408796550831043 \n",
      "\n",
      "Epoch:  872  Learning Rate:  0.041853280710435814  Varinance:  7.398446733613809 \n",
      "\n",
      "Epoch:  873  Learning Rate:  0.04181144834939193  Varinance:  7.388111374711862 \n",
      "\n",
      "Epoch:  874  Learning Rate:  0.04176965779979988  Varinance:  7.377790453927465 \n",
      "\n",
      "Epoch:  875  Learning Rate:  0.041727909019869114  Varinance:  7.367483951091098 \n",
      "\n",
      "Epoch:  876  Learning Rate:  0.04168620196785084  Varinance:  7.357191846061417 \n",
      "\n",
      "Epoch:  877  Learning Rate:  0.04164453660203801  Varinance:  7.34691411872521 \n",
      "\n",
      "Epoch:  878  Learning Rate:  0.04160291288076526  Varinance:  7.336650748997372 \n",
      "\n",
      "Epoch:  879  Learning Rate:  0.04156133076240884  Varinance:  7.326401716820845 \n",
      "\n",
      "Epoch:  880  Learning Rate:  0.04151979020538666  Varinance:  7.3161670021665985 \n",
      "\n",
      "Epoch:  881  Learning Rate:  0.04147829116815814  Varinance:  7.305946585033578 \n",
      "\n",
      "Epoch:  882  Learning Rate:  0.04143683360922425  Varinance:  7.2957404454486685 \n",
      "\n",
      "Epoch:  883  Learning Rate:  0.04139541748712741  Varinance:  7.285548563466661 \n",
      "\n",
      "Epoch:  884  Learning Rate:  0.041354042760451515  Varinance:  7.275370919170205 \n",
      "\n",
      "Epoch:  885  Learning Rate:  0.041312709387821826  Varinance:  7.265207492669779 \n",
      "\n",
      "Epoch:  886  Learning Rate:  0.04127141732790497  Varinance:  7.255058264103639 \n",
      "\n",
      "Epoch:  887  Learning Rate:  0.04123016653940888  Varinance:  7.244923213637793 \n",
      "\n",
      "Epoch:  888  Learning Rate:  0.04118895698108276  Varinance:  7.234802321465955 \n",
      "\n",
      "Epoch:  889  Learning Rate:  0.04114778861171706  Varinance:  7.224695567809507 \n",
      "\n",
      "Epoch:  890  Learning Rate:  0.0411066613901434  Varinance:  7.214602932917461 \n",
      "\n",
      "Epoch:  891  Learning Rate:  0.04106557527523455  Varinance:  7.204524397066421 \n",
      "\n",
      "Epoch:  892  Learning Rate:  0.0410245302259044  Varinance:  7.1944599405605425 \n",
      "\n",
      "Epoch:  893  Learning Rate:  0.0409835262011079  Varinance:  7.184409543731498 \n",
      "\n",
      "Epoch:  894  Learning Rate:  0.04094256315984101  Varinance:  7.174373186938432 \n",
      "\n",
      "Epoch:  895  Learning Rate:  0.04090164106114069  Varinance:  7.164350850567931 \n",
      "\n",
      "Epoch:  896  Learning Rate:  0.04086075986408485  Varinance:  7.154342515033978 \n",
      "\n",
      "Epoch:  897  Learning Rate:  0.04081991952779227  Varinance:  7.144348160777916 \n",
      "\n",
      "Epoch:  898  Learning Rate:  0.04077912001142262  Varinance:  7.134367768268415 \n",
      "\n",
      "Epoch:  899  Learning Rate:  0.040738361274176384  Varinance:  7.124401318001425 \n",
      "\n",
      "Epoch:  900  Learning Rate:  0.04069764327529482  Varinance:  7.114448790500144 \n",
      "\n",
      "Epoch:  901  Learning Rate:  0.04065696597405991  Varinance:  7.1045101663149826 \n",
      "\n",
      "Epoch:  902  Learning Rate:  0.04061632932979437  Varinance:  7.094585426023511 \n",
      "\n",
      "Epoch:  903  Learning Rate:  0.040575733301861545  Varinance:  7.084674550230442 \n",
      "\n",
      "Epoch:  904  Learning Rate:  0.0405351778496654  Varinance:  7.074777519567582 \n",
      "\n",
      "Epoch:  905  Learning Rate:  0.04049466293265049  Varinance:  7.064894314693788 \n",
      "\n",
      "Epoch:  906  Learning Rate:  0.040454188510301886  Varinance:  7.055024916294942 \n",
      "\n",
      "Epoch:  907  Learning Rate:  0.04041375454214516  Varinance:  7.045169305083901 \n",
      "\n",
      "Epoch:  908  Learning Rate:  0.04037336098774634  Varinance:  7.035327461800472 \n",
      "\n",
      "Epoch:  909  Learning Rate:  0.040333007806711875  Varinance:  7.025499367211364 \n",
      "\n",
      "Epoch:  910  Learning Rate:  0.04029269495868858  Varinance:  7.015685002110156 \n",
      "\n",
      "Epoch:  911  Learning Rate:  0.0402524224033636  Varinance:  7.005884347317257 \n",
      "\n",
      "Epoch:  912  Learning Rate:  0.040212190100464375  Varinance:  6.996097383679864 \n",
      "\n",
      "Epoch:  913  Learning Rate:  0.040171998009758604  Varinance:  6.98632409207194 \n",
      "\n",
      "Epoch:  914  Learning Rate:  0.04013184609105419  Varinance:  6.97656445339416 \n",
      "\n",
      "Epoch:  915  Learning Rate:  0.04009173430419921  Varinance:  6.9668184485738776 \n",
      "\n",
      "Epoch:  916  Learning Rate:  0.040051662609081884  Varinance:  6.957086058565097 \n",
      "\n",
      "Epoch:  917  Learning Rate:  0.0400116309656305  Varinance:  6.947367264348426 \n",
      "\n",
      "Epoch:  918  Learning Rate:  0.03997163933381341  Varinance:  6.93766204693104 \n",
      "\n",
      "Epoch:  919  Learning Rate:  0.03993168767363899  Varinance:  6.9279703873466465 \n",
      "\n",
      "Epoch:  920  Learning Rate:  0.03989177594515557  Varinance:  6.918292266655452 \n",
      "\n",
      "Epoch:  921  Learning Rate:  0.039851904108451415  Varinance:  6.908627665944119 \n",
      "\n",
      "Epoch:  922  Learning Rate:  0.0398120721236547  Varinance:  6.898976566325729 \n",
      "\n",
      "Epoch:  923  Learning Rate:  0.039772279950933416  Varinance:  6.889338948939752 \n",
      "\n",
      "Epoch:  924  Learning Rate:  0.0397325275504954  Varinance:  6.8797147949520046 \n",
      "\n",
      "Epoch:  925  Learning Rate:  0.039692814882588245  Varinance:  6.870104085554611 \n",
      "\n",
      "Epoch:  926  Learning Rate:  0.03965314190749929  Varinance:  6.860506801965969 \n",
      "\n",
      "Epoch:  927  Learning Rate:  0.03961350858555554  Varinance:  6.850922925430723 \n",
      "\n",
      "Epoch:  928  Learning Rate:  0.039573914877123674  Varinance:  6.841352437219704 \n",
      "\n",
      "Epoch:  929  Learning Rate:  0.039534360742609985  Varinance:  6.831795318629916 \n",
      "\n",
      "Epoch:  930  Learning Rate:  0.03949484614246033  Varinance:  6.822251550984489 \n",
      "\n",
      "Epoch:  931  Learning Rate:  0.03945537103716011  Varinance:  6.812721115632642 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  932  Learning Rate:  0.03941593538723422  Varinance:  6.80320399394965 \n",
      "\n",
      "Epoch:  933  Learning Rate:  0.039376539153247  Varinance:  6.793700167336805 \n",
      "\n",
      "Epoch:  934  Learning Rate:  0.039337182295802214  Varinance:  6.784209617221385 \n",
      "\n",
      "Epoch:  935  Learning Rate:  0.039297864775543  Varinance:  6.774732325056604 \n",
      "\n",
      "Epoch:  936  Learning Rate:  0.03925858655315184  Varinance:  6.765268272321594 \n",
      "\n",
      "Epoch:  937  Learning Rate:  0.0392193475893505  Varinance:  6.7558174405213585 \n",
      "\n",
      "Epoch:  938  Learning Rate:  0.039180147844900017  Varinance:  6.746379811186731 \n",
      "\n",
      "Epoch:  939  Learning Rate:  0.039140987280600656  Varinance:  6.736955365874356 \n",
      "\n",
      "Epoch:  940  Learning Rate:  0.03910186585729183  Varinance:  6.727544086166636 \n",
      "\n",
      "Epoch:  941  Learning Rate:  0.039062783535852116  Varinance:  6.718145953671704 \n",
      "\n",
      "Epoch:  942  Learning Rate:  0.039023740277199195  Varinance:  6.708760950023384 \n",
      "\n",
      "Epoch:  943  Learning Rate:  0.0389847360422898  Varinance:  6.699389056881161 \n",
      "\n",
      "Epoch:  944  Learning Rate:  0.0389457707921197  Varinance:  6.6900302559301394 \n",
      "\n",
      "Epoch:  945  Learning Rate:  0.038906844487723635  Varinance:  6.680684528881003 \n",
      "\n",
      "Epoch:  946  Learning Rate:  0.038867957090175306  Varinance:  6.671351857469994 \n",
      "\n",
      "Epoch:  947  Learning Rate:  0.0388291085605873  Varinance:  6.662032223458865 \n",
      "\n",
      "Epoch:  948  Learning Rate:  0.038790298860111094  Varinance:  6.652725608634843 \n",
      "\n",
      "Epoch:  949  Learning Rate:  0.03875152794993698  Varinance:  6.643431994810603 \n",
      "\n",
      "Epoch:  950  Learning Rate:  0.038712795791294045  Varinance:  6.634151363824224 \n",
      "\n",
      "Epoch:  951  Learning Rate:  0.03867410234545013  Varinance:  6.62488369753916 \n",
      "\n",
      "Epoch:  952  Learning Rate:  0.038635447573711776  Varinance:  6.615628977844196 \n",
      "\n",
      "Epoch:  953  Learning Rate:  0.03859683143742422  Varinance:  6.606387186653419 \n",
      "\n",
      "Epoch:  954  Learning Rate:  0.038558253897971316  Varinance:  6.597158305906185 \n",
      "\n",
      "Epoch:  955  Learning Rate:  0.03851971491677553  Varinance:  6.587942317567077 \n",
      "\n",
      "Epoch:  956  Learning Rate:  0.03848121445529786  Varinance:  6.578739203625872 \n",
      "\n",
      "Epoch:  957  Learning Rate:  0.03844275247503786  Varinance:  6.56954894609751 \n",
      "\n",
      "Epoch:  958  Learning Rate:  0.03840432893753353  Varinance:  6.5603715270220535 \n",
      "\n",
      "Epoch:  959  Learning Rate:  0.03836594380436134  Varinance:  6.551206928464651 \n",
      "\n",
      "Epoch:  960  Learning Rate:  0.03832759703713616  Varinance:  6.542055132515514 \n",
      "\n",
      "Epoch:  961  Learning Rate:  0.03828928859751121  Varinance:  6.532916121289864 \n",
      "\n",
      "Epoch:  962  Learning Rate:  0.03825101844717804  Varinance:  6.5237898769279115 \n",
      "\n",
      "Epoch:  963  Learning Rate:  0.03821278654786651  Varinance:  6.514676381594817 \n",
      "\n",
      "Epoch:  964  Learning Rate:  0.03817459286134471  Varinance:  6.505575617480654 \n",
      "\n",
      "Epoch:  965  Learning Rate:  0.03813643734941896  Varinance:  6.496487566800376 \n",
      "\n",
      "Epoch:  966  Learning Rate:  0.03809831997393373  Varinance:  6.487412211793782 \n",
      "\n",
      "Epoch:  967  Learning Rate:  0.03806024069677165  Varinance:  6.478349534725483 \n",
      "\n",
      "Epoch:  968  Learning Rate:  0.03802219947985344  Varinance:  6.469299517884861 \n",
      "\n",
      "Epoch:  969  Learning Rate:  0.037984196285137876  Varinance:  6.460262143586043 \n",
      "\n",
      "Epoch:  970  Learning Rate:  0.03794623107462176  Varinance:  6.451237394167864 \n",
      "\n",
      "Epoch:  971  Learning Rate:  0.03790830381033988  Varinance:  6.442225251993823 \n",
      "\n",
      "Epoch:  972  Learning Rate:  0.03787041445436498  Varinance:  6.433225699452066 \n",
      "\n",
      "Epoch:  973  Learning Rate:  0.03783256296880769  Varinance:  6.424238718955339 \n",
      "\n",
      "Epoch:  974  Learning Rate:  0.03779474931581651  Varinance:  6.415264292940955 \n",
      "\n",
      "Epoch:  975  Learning Rate:  0.0377569734575778  Varinance:  6.40630240387076 \n",
      "\n",
      "Epoch:  976  Learning Rate:  0.037719235356315696  Varinance:  6.397353034231104 \n",
      "\n",
      "Epoch:  977  Learning Rate:  0.03768153497429209  Varinance:  6.388416166532804 \n",
      "\n",
      "Epoch:  978  Learning Rate:  0.0376438722738066  Varinance:  6.379491783311102 \n",
      "\n",
      "Epoch:  979  Learning Rate:  0.03760624721719652  Varinance:  6.3705798671256435 \n",
      "\n",
      "Epoch:  980  Learning Rate:  0.03756865976683679  Varinance:  6.361680400560438 \n",
      "\n",
      "Epoch:  981  Learning Rate:  0.03753110988513996  Varinance:  6.352793366223819 \n",
      "\n",
      "Epoch:  982  Learning Rate:  0.03749359753455613  Varinance:  6.34391874674842 \n",
      "\n",
      "Epoch:  983  Learning Rate:  0.03745612267757298  Varinance:  6.335056524791134 \n",
      "\n",
      "Epoch:  984  Learning Rate:  0.037418685276715616  Varinance:  6.326206683033087 \n",
      "\n",
      "Epoch:  985  Learning Rate:  0.03738128529454665  Varinance:  6.317369204179587 \n",
      "\n",
      "Epoch:  986  Learning Rate:  0.03734392269366609  Varinance:  6.308544070960116 \n",
      "\n",
      "Epoch:  987  Learning Rate:  0.037306597436711345  Varinance:  6.299731266128275 \n",
      "\n",
      "Epoch:  988  Learning Rate:  0.03726930948635714  Varinance:  6.2909307724617545 \n",
      "\n",
      "Epoch:  989  Learning Rate:  0.03723205880531552  Varinance:  6.282142572762312 \n",
      "\n",
      "Epoch:  990  Learning Rate:  0.03719484535633582  Varinance:  6.273366649855723 \n",
      "\n",
      "Epoch:  991  Learning Rate:  0.037157669102204575  Varinance:  6.264602986591759 \n",
      "\n",
      "Epoch:  992  Learning Rate:  0.03712053000574552  Varinance:  6.255851565844149 \n",
      "\n",
      "Epoch:  993  Learning Rate:  0.037083428029819565  Varinance:  6.247112370510547 \n",
      "\n",
      "Epoch:  994  Learning Rate:  0.037046363137324734  Varinance:  6.2383853835124965 \n",
      "\n",
      "Epoch:  995  Learning Rate:  0.03700933529119613  Varinance:  6.2296705877953995 \n",
      "\n",
      "Epoch:  996  Learning Rate:  0.0369723444544059  Varinance:  6.220967966328486 \n",
      "\n",
      "Epoch:  997  Learning Rate:  0.0369353905899632  Varinance:  6.212277502104773 \n",
      "\n",
      "Epoch:  998  Learning Rate:  0.03689847366091418  Varinance:  6.203599178141038 \n",
      "\n",
      "Epoch:  999  Learning Rate:  0.03686159363034188  Varinance:  6.19493297747778 \n",
      "\n",
      "Epoch:  1000  Learning Rate:  0.03682475046136629  Varinance:  6.186278883179198 \n",
      "\n",
      "Epoch:  1001  Learning Rate:  0.036787944117144235  Varinance:  6.17763687833314 \n",
      "\n",
      "Epoch:  1002  Learning Rate:  0.03675117456086936  Varinance:  6.169006946051084 \n",
      "\n",
      "Epoch:  1003  Learning Rate:  0.036714441755772105  Varinance:  6.160389069468105 \n",
      "\n",
      "Epoch:  1004  Learning Rate:  0.036677745665119665  Varinance:  6.151783231742828 \n",
      "\n",
      "Epoch:  1005  Learning Rate:  0.03664108625221595  Varinance:  6.143189416057412 \n",
      "\n",
      "Epoch:  1006  Learning Rate:  0.03660446348040154  Varinance:  6.134607605617508 \n",
      "\n",
      "Epoch:  1007  Learning Rate:  0.03656787731305366  Varinance:  6.12603778365223 \n",
      "\n",
      "Epoch:  1008  Learning Rate:  0.03653132771358614  Varinance:  6.117479933414116 \n",
      "\n",
      "Epoch:  1009  Learning Rate:  0.036494814645449375  Varinance:  6.108934038179102 \n",
      "\n",
      "Epoch:  1010  Learning Rate:  0.0364583380721303  Varinance:  6.100400081246489 \n",
      "\n",
      "Epoch:  1011  Learning Rate:  0.03642189795715233  Varinance:  6.091878045938903 \n",
      "\n",
      "Epoch:  1012  Learning Rate:  0.03638549426407536  Varinance:  6.083367915602272 \n",
      "\n",
      "Epoch:  1013  Learning Rate:  0.036349126956495684  Varinance:  6.07486967360579 \n",
      "\n",
      "Epoch:  1014  Learning Rate:  0.036312795998045995  Varinance:  6.066383303341879 \n",
      "\n",
      "Epoch:  1015  Learning Rate:  0.036276501352395324  Varinance:  6.057908788226165 \n",
      "\n",
      "Epoch:  1016  Learning Rate:  0.03624024298324904  Varinance:  6.049446111697439 \n",
      "\n",
      "Epoch:  1017  Learning Rate:  0.03620402085434875  Varinance:  6.040995257217633 \n",
      "\n",
      "Epoch:  1018  Learning Rate:  0.036167834929472335  Varinance:  6.032556208271772 \n",
      "\n",
      "Epoch:  1019  Learning Rate:  0.036131685172433856  Varinance:  6.024128948367962 \n",
      "\n",
      "Epoch:  1020  Learning Rate:  0.036095571547083566  Varinance:  6.015713461037343 \n",
      "\n",
      "Epoch:  1021  Learning Rate:  0.03605949401730783  Varinance:  6.007309729834061 \n",
      "\n",
      "Epoch:  1022  Learning Rate:  0.03602345254702912  Varinance:  5.998917738335236 \n",
      "\n",
      "Epoch:  1023  Learning Rate:  0.03598744710020595  Varinance:  5.990537470140934 \n",
      "\n",
      "Epoch:  1024  Learning Rate:  0.03595147764083289  Varinance:  5.982168908874125 \n",
      "\n",
      "Epoch:  1025  Learning Rate:  0.03591554413294046  Varinance:  5.973812038180662 \n",
      "\n",
      "Epoch:  1026  Learning Rate:  0.03587964654059516  Varinance:  5.965466841729242 \n",
      "\n",
      "Epoch:  1027  Learning Rate:  0.03584378482789939  Varinance:  5.957133303211379 \n",
      "\n",
      "Epoch:  1028  Learning Rate:  0.035807958958991436  Varinance:  5.948811406341364 \n",
      "\n",
      "Epoch:  1029  Learning Rate:  0.03577216889804542  Varinance:  5.940501134856244 \n",
      "\n",
      "Epoch:  1030  Learning Rate:  0.03573641460927129  Varinance:  5.932202472515781 \n",
      "\n",
      "Epoch:  1031  Learning Rate:  0.03570069605691474  Varinance:  5.923915403102426 \n",
      "\n",
      "Epoch:  1032  Learning Rate:  0.03566501320525722  Varinance:  5.915639910421285 \n",
      "\n",
      "Epoch:  1033  Learning Rate:  0.03562936601861588  Varinance:  5.907375978300087 \n",
      "\n",
      "Epoch:  1034  Learning Rate:  0.03559375446134354  Varinance:  5.899123590589153 \n",
      "\n",
      "Epoch:  1035  Learning Rate:  0.035558178497828614  Varinance:  5.8908827311613665 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1036  Learning Rate:  0.03552263809249515  Varinance:  5.882653383912138 \n",
      "\n",
      "Epoch:  1037  Learning Rate:  0.03548713320980274  Varinance:  5.8744355327593745 \n",
      "\n",
      "Epoch:  1038  Learning Rate:  0.035451663814246505  Varinance:  5.866229161643452 \n",
      "\n",
      "Epoch:  1039  Learning Rate:  0.03541622987035702  Varinance:  5.85803425452718 \n",
      "\n",
      "Epoch:  1040  Learning Rate:  0.035380831342700376  Varinance:  5.849850795395771 \n",
      "\n",
      "Epoch:  1041  Learning Rate:  0.03534546819587802  Varinance:  5.841678768256809 \n",
      "\n",
      "Epoch:  1042  Learning Rate:  0.035310140394526804  Varinance:  5.833518157140221 \n",
      "\n",
      "Epoch:  1043  Learning Rate:  0.03527484790331891  Varinance:  5.825368946098241 \n",
      "\n",
      "Epoch:  1044  Learning Rate:  0.03523959068696188  Varinance:  5.817231119205382 \n",
      "\n",
      "Epoch:  1045  Learning Rate:  0.03520436871019846  Varinance:  5.809104660558407 \n",
      "\n",
      "Epoch:  1046  Learning Rate:  0.035169181937806694  Varinance:  5.800989554276293 \n",
      "\n",
      "Epoch:  1047  Learning Rate:  0.03513403033459978  Varinance:  5.7928857845002 \n",
      "\n",
      "Epoch:  1048  Learning Rate:  0.03509891386542615  Varinance:  5.784793335393446 \n",
      "\n",
      "Epoch:  1049  Learning Rate:  0.0350638324951693  Varinance:  5.776712191141471 \n",
      "\n",
      "Epoch:  1050  Learning Rate:  0.035028786188747864  Varinance:  5.768642335951809 \n",
      "\n",
      "Epoch:  1051  Learning Rate:  0.034993774911115536  Varinance:  5.760583754054051 \n",
      "\n",
      "Epoch:  1052  Learning Rate:  0.03495879862726104  Varinance:  5.752536429699822 \n",
      "\n",
      "Epoch:  1053  Learning Rate:  0.03492385730220808  Varinance:  5.744500347162748 \n",
      "\n",
      "Epoch:  1054  Learning Rate:  0.03488895090101534  Varinance:  5.736475490738421 \n",
      "\n",
      "Epoch:  1055  Learning Rate:  0.0348540793887764  Varinance:  5.728461844744372 \n",
      "\n",
      "Epoch:  1056  Learning Rate:  0.03481924273061976  Varinance:  5.720459393520043 \n",
      "\n",
      "Epoch:  1057  Learning Rate:  0.03478444089170874  Varinance:  5.712468121426751 \n",
      "\n",
      "Epoch:  1058  Learning Rate:  0.03474967383724153  Varinance:  5.704488012847657 \n",
      "\n",
      "Epoch:  1059  Learning Rate:  0.034714941532451034  Varinance:  5.6965190521877425 \n",
      "\n",
      "Epoch:  1060  Learning Rate:  0.03468024394260498  Varinance:  5.6885612238737755 \n",
      "\n",
      "Epoch:  1061  Learning Rate:  0.03464558103300574  Varinance:  5.680614512354273 \n",
      "\n",
      "Epoch:  1062  Learning Rate:  0.03461095276899044  Varinance:  5.672678902099482 \n",
      "\n",
      "Epoch:  1063  Learning Rate:  0.03457635911593078  Varinance:  5.664754377601343 \n",
      "\n",
      "Epoch:  1064  Learning Rate:  0.03454180003923312  Varinance:  5.65684092337346 \n",
      "\n",
      "Epoch:  1065  Learning Rate:  0.034507275504338374  Varinance:  5.64893852395107 \n",
      "\n",
      "Epoch:  1066  Learning Rate:  0.03447278547672202  Varinance:  5.6410471638910185 \n",
      "\n",
      "Epoch:  1067  Learning Rate:  0.034438329921894  Varinance:  5.633166827771717 \n",
      "\n",
      "Epoch:  1068  Learning Rate:  0.034403908805398786  Varinance:  5.625297500193126 \n",
      "\n",
      "Epoch:  1069  Learning Rate:  0.034369522092815234  Varinance:  5.617439165776719 \n",
      "\n",
      "Epoch:  1070  Learning Rate:  0.03433516974975665  Varinance:  5.6095918091654475 \n",
      "\n",
      "Epoch:  1071  Learning Rate:  0.03430085174187066  Varinance:  5.601755415023724 \n",
      "\n",
      "Epoch:  1072  Learning Rate:  0.03426656803483929  Varinance:  5.593929968037378 \n",
      "\n",
      "Epoch:  1073  Learning Rate:  0.0342323185943788  Varinance:  5.586115452913638 \n",
      "\n",
      "Epoch:  1074  Learning Rate:  0.03419810338623976  Varinance:  5.578311854381088 \n",
      "\n",
      "Epoch:  1075  Learning Rate:  0.03416392237620695  Varinance:  5.570519157189652 \n",
      "\n",
      "Epoch:  1076  Learning Rate:  0.034129775530099375  Varinance:  5.562737346110559 \n",
      "\n",
      "Epoch:  1077  Learning Rate:  0.034095662813770154  Varinance:  5.554966405936304 \n",
      "\n",
      "Epoch:  1078  Learning Rate:  0.03406158419310661  Varinance:  5.547206321480635 \n",
      "\n",
      "Epoch:  1079  Learning Rate:  0.03402753963403008  Varinance:  5.539457077578512 \n",
      "\n",
      "Epoch:  1080  Learning Rate:  0.033993529102496034  Varinance:  5.531718659086075 \n",
      "\n",
      "Epoch:  1081  Learning Rate:  0.033959552564493914  Varinance:  5.523991050880626 \n",
      "\n",
      "Epoch:  1082  Learning Rate:  0.0339256099860472  Varinance:  5.51627423786059 \n",
      "\n",
      "Epoch:  1083  Learning Rate:  0.03389170133321328  Varinance:  5.5085682049454885 \n",
      "\n",
      "Epoch:  1084  Learning Rate:  0.033857826572083534  Varinance:  5.50087293707591 \n",
      "\n",
      "Epoch:  1085  Learning Rate:  0.03382398566878317  Varinance:  5.493188419213481 \n",
      "\n",
      "Epoch:  1086  Learning Rate:  0.0337901785894713  Varinance:  5.485514636340836 \n",
      "\n",
      "Epoch:  1087  Learning Rate:  0.03375640530034084  Varinance:  5.477851573461585 \n",
      "\n",
      "Epoch:  1088  Learning Rate:  0.03372266576761849  Varinance:  5.470199215600293 \n",
      "\n",
      "Epoch:  1089  Learning Rate:  0.033688959957564706  Varinance:  5.462557547802442 \n",
      "\n",
      "Epoch:  1090  Learning Rate:  0.0336552878364737  Varinance:  5.454926555134404 \n",
      "\n",
      "Epoch:  1091  Learning Rate:  0.03362164937067333  Varinance:  5.447306222683414 \n",
      "\n",
      "Epoch:  1092  Learning Rate:  0.03358804452652514  Varinance:  5.43969653555754 \n",
      "\n",
      "Epoch:  1093  Learning Rate:  0.03355447327042427  Varinance:  5.432097478885655 \n",
      "\n",
      "Epoch:  1094  Learning Rate:  0.03352093556879947  Varinance:  5.424509037817401 \n",
      "\n",
      "Epoch:  1095  Learning Rate:  0.03348743138811302  Varinance:  5.41693119752317 \n",
      "\n",
      "Epoch:  1096  Learning Rate:  0.033453960694860764  Varinance:  5.40936394319407 \n",
      "\n",
      "Epoch:  1097  Learning Rate:  0.03342052345557198  Varinance:  5.401807260041895 \n",
      "\n",
      "Epoch:  1098  Learning Rate:  0.033387119636809445  Varinance:  5.394261133299099 \n",
      "\n",
      "Epoch:  1099  Learning Rate:  0.03335374920516932  Varinance:  5.386725548218766 \n",
      "\n",
      "Epoch:  1100  Learning Rate:  0.033320412127281186  Varinance:  5.3792004900745765 \n",
      "\n",
      "Epoch:  1101  Learning Rate:  0.03328710836980796  Varinance:  5.37168594416079 \n",
      "\n",
      "Epoch:  1102  Learning Rate:  0.03325383789944587  Varinance:  5.364181895792204 \n",
      "\n",
      "Epoch:  1103  Learning Rate:  0.03322060068292445  Varinance:  5.3566883303041335 \n",
      "\n",
      "Epoch:  1104  Learning Rate:  0.03318739668700649  Varinance:  5.349205233052377 \n",
      "\n",
      "Epoch:  1105  Learning Rate:  0.03315422587848797  Varinance:  5.341732589413195 \n",
      "\n",
      "Epoch:  1106  Learning Rate:  0.0331210882241981  Varinance:  5.334270384783273 \n",
      "\n",
      "Epoch:  1107  Learning Rate:  0.03308798369099921  Varinance:  5.326818604579695 \n",
      "\n",
      "Epoch:  1108  Learning Rate:  0.03305491224578677  Varinance:  5.319377234239923 \n",
      "\n",
      "Epoch:  1109  Learning Rate:  0.03302187385548933  Varinance:  5.311946259221758 \n",
      "\n",
      "Epoch:  1110  Learning Rate:  0.0329888684870685  Varinance:  5.3045256650033155 \n",
      "\n",
      "Epoch:  1111  Learning Rate:  0.03295589610751891  Varinance:  5.297115437082998 \n",
      "\n",
      "Epoch:  1112  Learning Rate:  0.03292295668386817  Varinance:  5.289715560979471 \n",
      "\n",
      "Epoch:  1113  Learning Rate:  0.03289005018317685  Varinance:  5.282326022231622 \n",
      "\n",
      "Epoch:  1114  Learning Rate:  0.03285717657253846  Varinance:  5.274946806398544 \n",
      "\n",
      "Epoch:  1115  Learning Rate:  0.03282433581907938  Varinance:  5.267577899059506 \n",
      "\n",
      "Epoch:  1116  Learning Rate:  0.03279152788995886  Varinance:  5.260219285813919 \n",
      "\n",
      "Epoch:  1117  Learning Rate:  0.032758752752368954  Varinance:  5.252870952281311 \n",
      "\n",
      "Epoch:  1118  Learning Rate:  0.03272601037353453  Varinance:  5.2455328841013 \n",
      "\n",
      "Epoch:  1119  Learning Rate:  0.03269330072071321  Varinance:  5.238205066933565 \n",
      "\n",
      "Epoch:  1120  Learning Rate:  0.03266062376119534  Varinance:  5.230887486457817 \n",
      "\n",
      "Epoch:  1121  Learning Rate:  0.032627979462303947  Varinance:  5.223580128373772 \n",
      "\n",
      "Epoch:  1122  Learning Rate:  0.032595367791394735  Varinance:  5.216282978401126 \n",
      "\n",
      "Epoch:  1123  Learning Rate:  0.032562788715856036  Varinance:  5.208996022279519 \n",
      "\n",
      "Epoch:  1124  Learning Rate:  0.03253024220310876  Varinance:  5.201719245768515 \n",
      "\n",
      "Epoch:  1125  Learning Rate:  0.0324977282206064  Varinance:  5.194452634647571 \n",
      "\n",
      "Epoch:  1126  Learning Rate:  0.03246524673583497  Varinance:  5.1871961747160125 \n",
      "\n",
      "Epoch:  1127  Learning Rate:  0.03243279771631299  Varinance:  5.179949851792997 \n",
      "\n",
      "Epoch:  1128  Learning Rate:  0.03240038112959142  Varinance:  5.172713651717496 \n",
      "\n",
      "Epoch:  1129  Learning Rate:  0.03236799694325368  Varinance:  5.165487560348263 \n",
      "\n",
      "Epoch:  1130  Learning Rate:  0.032335645124915574  Varinance:  5.158271563563805 \n",
      "\n",
      "Epoch:  1131  Learning Rate:  0.032303325642225295  Varinance:  5.151065647262357 \n",
      "\n",
      "Epoch:  1132  Learning Rate:  0.032271038462863354  Varinance:  5.143869797361855 \n",
      "\n",
      "Epoch:  1133  Learning Rate:  0.03223878355454256  Varinance:  5.136683999799905 \n",
      "\n",
      "Epoch:  1134  Learning Rate:  0.032206560885008  Varinance:  5.129508240533758 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1135  Learning Rate:  0.03217437042203702  Varinance:  5.122342505540285 \n",
      "\n",
      "Epoch:  1136  Learning Rate:  0.03214221213343914  Varinance:  5.115186780815941 \n",
      "\n",
      "Epoch:  1137  Learning Rate:  0.03211008598705607  Varinance:  5.108041052376749 \n",
      "\n",
      "Epoch:  1138  Learning Rate:  0.03207799195076166  Varinance:  5.100905306258266 \n",
      "\n",
      "Epoch:  1139  Learning Rate:  0.032045929992461884  Varinance:  5.0937795285155545 \n",
      "\n",
      "Epoch:  1140  Learning Rate:  0.03201390008009476  Varinance:  5.086663705223159 \n",
      "\n",
      "Epoch:  1141  Learning Rate:  0.0319819021816304  Varinance:  5.079557822475077 \n",
      "\n",
      "Epoch:  1142  Learning Rate:  0.031949936265070866  Varinance:  5.072461866384733 \n",
      "\n",
      "Epoch:  1143  Learning Rate:  0.03191800229845027  Varinance:  5.065375823084951 \n",
      "\n",
      "Epoch:  1144  Learning Rate:  0.03188610024983463  Varinance:  5.0582996787279235 \n",
      "\n",
      "Epoch:  1145  Learning Rate:  0.031854230087321904  Varinance:  5.051233419485192 \n",
      "\n",
      "Epoch:  1146  Learning Rate:  0.03182239177904191  Varinance:  5.044177031547614 \n",
      "\n",
      "Epoch:  1147  Learning Rate:  0.031790585293156357  Varinance:  5.037130501125337 \n",
      "\n",
      "Epoch:  1148  Learning Rate:  0.03175881059785873  Varinance:  5.030093814447775 \n",
      "\n",
      "Epoch:  1149  Learning Rate:  0.03172706766137436  Varinance:  5.023066957763576 \n",
      "\n",
      "Epoch:  1150  Learning Rate:  0.031695356451960296  Varinance:  5.016049917340601 \n",
      "\n",
      "Epoch:  1151  Learning Rate:  0.03166367693790533  Varinance:  5.009042679465892 \n",
      "\n",
      "Epoch:  1152  Learning Rate:  0.03163202908752993  Varinance:  5.002045230445649 \n",
      "\n",
      "Epoch:  1153  Learning Rate:  0.03160041286918626  Varinance:  4.9950575566052 \n",
      "\n",
      "Epoch:  1154  Learning Rate:  0.031568828251258084  Varinance:  4.988079644288978 \n",
      "\n",
      "Epoch:  1155  Learning Rate:  0.0315372752021608  Varinance:  4.981111479860491 \n",
      "\n",
      "Epoch:  1156  Learning Rate:  0.03150575369034133  Varinance:  4.974153049702298 \n",
      "\n",
      "Epoch:  1157  Learning Rate:  0.03147426368427819  Varinance:  4.967204340215979 \n",
      "\n",
      "Epoch:  1158  Learning Rate:  0.03144280515248136  Varinance:  4.960265337822111 \n",
      "\n",
      "Epoch:  1159  Learning Rate:  0.0314113780634923  Varinance:  4.953336028960247 \n",
      "\n",
      "Epoch:  1160  Learning Rate:  0.03137998238588391  Varinance:  4.946416400088871 \n",
      "\n",
      "Epoch:  1161  Learning Rate:  0.03134861808826053  Varinance:  4.939506437685394 \n",
      "\n",
      "Epoch:  1162  Learning Rate:  0.03131728513925785  Varinance:  4.932606128246116 \n",
      "\n",
      "Epoch:  1163  Learning Rate:  0.03128598350754292  Varinance:  4.925715458286198 \n",
      "\n",
      "Epoch:  1164  Learning Rate:  0.0312547131618141  Varinance:  4.918834414339641 \n",
      "\n",
      "Epoch:  1165  Learning Rate:  0.031223474070801057  Varinance:  4.911962982959258 \n",
      "\n",
      "Epoch:  1166  Learning Rate:  0.031192266203264676  Varinance:  4.9051011507166455 \n",
      "\n",
      "Epoch:  1167  Learning Rate:  0.031161089527997107  Varinance:  4.89824890420216 \n",
      "\n",
      "Epoch:  1168  Learning Rate:  0.03112994401382165  Varinance:  4.891406230024892 \n",
      "\n",
      "Epoch:  1169  Learning Rate:  0.031098829629592813  Varinance:  4.8845731148126355 \n",
      "\n",
      "Epoch:  1170  Learning Rate:  0.031067746344196186  Varinance:  4.877749545211866 \n",
      "\n",
      "Epoch:  1171  Learning Rate:  0.031036694126548504  Varinance:  4.870935507887718 \n",
      "\n",
      "Epoch:  1172  Learning Rate:  0.031005672945597525  Varinance:  4.8641309895239475 \n",
      "\n",
      "Epoch:  1173  Learning Rate:  0.030974682770322084  Varinance:  4.857335976822916 \n",
      "\n",
      "Epoch:  1174  Learning Rate:  0.030943723569731987  Varinance:  4.850550456505561 \n",
      "\n",
      "Epoch:  1175  Learning Rate:  0.030912795312868044  Varinance:  4.843774415311372 \n",
      "\n",
      "Epoch:  1176  Learning Rate:  0.03088189796880199  Varinance:  4.837007839998359 \n",
      "\n",
      "Epoch:  1177  Learning Rate:  0.030851031506636473  Varinance:  4.830250717343033 \n",
      "\n",
      "Epoch:  1178  Learning Rate:  0.030820195895505033  Varinance:  4.823503034140382 \n",
      "\n",
      "Epoch:  1179  Learning Rate:  0.030789391104572062  Varinance:  4.816764777203831 \n",
      "\n",
      "Epoch:  1180  Learning Rate:  0.03075861710303276  Varinance:  4.810035933365235 \n",
      "\n",
      "Epoch:  1181  Learning Rate:  0.030727873860113126  Varinance:  4.80331648947484 \n",
      "\n",
      "Epoch:  1182  Learning Rate:  0.030697161345069907  Varinance:  4.796606432401265 \n",
      "\n",
      "Epoch:  1183  Learning Rate:  0.0306664795271906  Varinance:  4.789905749031467 \n",
      "\n",
      "Epoch:  1184  Learning Rate:  0.03063582837579337  Varinance:  4.783214426270729 \n",
      "\n",
      "Epoch:  1185  Learning Rate:  0.03060520786022707  Varinance:  4.776532451042622 \n",
      "\n",
      "Epoch:  1186  Learning Rate:  0.030574617949871177  Varinance:  4.7698598102889855 \n",
      "\n",
      "Epoch:  1187  Learning Rate:  0.03054405861413579  Varinance:  4.7631964909699 \n",
      "\n",
      "Epoch:  1188  Learning Rate:  0.03051352982246155  Varinance:  4.756542480063666 \n",
      "\n",
      "Epoch:  1189  Learning Rate:  0.030483031544319684  Varinance:  4.749897764566768 \n",
      "\n",
      "Epoch:  1190  Learning Rate:  0.0304525637492119  Varinance:  4.743262331493862 \n",
      "\n",
      "Epoch:  1191  Learning Rate:  0.03042212640667041  Varinance:  4.736636167877742 \n",
      "\n",
      "Epoch:  1192  Learning Rate:  0.03039171948625785  Varinance:  4.730019260769317 \n",
      "\n",
      "Epoch:  1193  Learning Rate:  0.030361342957567317  Varinance:  4.723411597237583 \n",
      "\n",
      "Epoch:  1194  Learning Rate:  0.030330996790222265  Varinance:  4.716813164369603 \n",
      "\n",
      "Epoch:  1195  Learning Rate:  0.030300680953876544  Varinance:  4.710223949270481 \n",
      "\n",
      "Epoch:  1196  Learning Rate:  0.030270395418214288  Varinance:  4.703643939063328 \n",
      "\n",
      "Epoch:  1197  Learning Rate:  0.03024014015294998  Varinance:  4.697073120889248 \n",
      "\n",
      "Epoch:  1198  Learning Rate:  0.030209915127828342  Varinance:  4.69051148190731 \n",
      "\n",
      "Epoch:  1199  Learning Rate:  0.03017972031262435  Varinance:  4.683959009294517 \n",
      "\n",
      "Epoch:  1200  Learning Rate:  0.030149555677143183  Varinance:  4.677415690245787 \n",
      "\n",
      "Epoch:  1201  Learning Rate:  0.030119421191220214  Varinance:  4.670881511973929 \n",
      "\n",
      "Epoch:  1202  Learning Rate:  0.030089316824720935  Varinance:  4.664356461709609 \n",
      "\n",
      "Epoch:  1203  Learning Rate:  0.030059242547541  Varinance:  4.657840526701337 \n",
      "\n",
      "Epoch:  1204  Learning Rate:  0.030029198329606106  Varinance:  4.6513336942154355 \n",
      "\n",
      "Epoch:  1205  Learning Rate:  0.02999918414087205  Varinance:  4.644835951536014 \n",
      "\n",
      "Epoch:  1206  Learning Rate:  0.029969199951324632  Varinance:  4.638347285964945 \n",
      "\n",
      "Epoch:  1207  Learning Rate:  0.02993924573097967  Varinance:  4.63186768482184 \n",
      "\n",
      "Epoch:  1208  Learning Rate:  0.029909321449882925  Varinance:  4.625397135444028 \n",
      "\n",
      "Epoch:  1209  Learning Rate:  0.02987942707811013  Varinance:  4.6189356251865235 \n",
      "\n",
      "Epoch:  1210  Learning Rate:  0.029849562585766893  Varinance:  4.612483141422005 \n",
      "\n",
      "Epoch:  1211  Learning Rate:  0.02981972794298874  Varinance:  4.606039671540795 \n",
      "\n",
      "Epoch:  1212  Learning Rate:  0.02978992311994101  Varinance:  4.59960520295083 \n",
      "\n",
      "Epoch:  1213  Learning Rate:  0.029760148086818886  Varinance:  4.593179723077631 \n",
      "\n",
      "Epoch:  1214  Learning Rate:  0.02973040281384732  Varinance:  4.586763219364295 \n",
      "\n",
      "Epoch:  1215  Learning Rate:  0.029700687271281053  Varinance:  4.580355679271455 \n",
      "\n",
      "Epoch:  1216  Learning Rate:  0.029671001429404528  Varinance:  4.5739570902772595 \n",
      "\n",
      "Epoch:  1217  Learning Rate:  0.02964134525853191  Varinance:  4.567567439877353 \n",
      "\n",
      "Epoch:  1218  Learning Rate:  0.029611718729007017  Varinance:  4.5611867155848485 \n",
      "\n",
      "Epoch:  1219  Learning Rate:  0.029582121811203323  Varinance:  4.554814904930297 \n",
      "\n",
      "Epoch:  1220  Learning Rate:  0.029552554475523903  Varinance:  4.5484519954616776 \n",
      "\n",
      "Epoch:  1221  Learning Rate:  0.029523016692401424  Varinance:  4.542097974744358 \n",
      "\n",
      "Epoch:  1222  Learning Rate:  0.029493508432298088  Varinance:  4.535752830361077 \n",
      "\n",
      "Epoch:  1223  Learning Rate:  0.029464029665705656  Varinance:  4.5294165499119226 \n",
      "\n",
      "Epoch:  1224  Learning Rate:  0.029434580363145335  Varinance:  4.523089121014304 \n",
      "\n",
      "Epoch:  1225  Learning Rate:  0.029405160495167837  Varinance:  4.516770531302929 \n",
      "\n",
      "Epoch:  1226  Learning Rate:  0.02937577003235328  Varinance:  4.510460768429776 \n",
      "\n",
      "Epoch:  1227  Learning Rate:  0.02934640894531121  Varinance:  4.504159820064078 \n",
      "\n",
      "Epoch:  1228  Learning Rate:  0.02931707720468052  Varinance:  4.49786767389229 \n",
      "\n",
      "Epoch:  1229  Learning Rate:  0.02928777478112949  Varinance:  4.491584317618068 \n",
      "\n",
      "Epoch:  1230  Learning Rate:  0.029258501645355667  Varinance:  4.4853097389622505 \n",
      "\n",
      "Epoch:  1231  Learning Rate:  0.029229257768085944  Varinance:  4.4790439256628245 \n",
      "\n",
      "Epoch:  1232  Learning Rate:  0.02920004312007641  Varinance:  4.472786865474909 \n",
      "\n",
      "Epoch:  1233  Learning Rate:  0.02917085767211244  Varinance:  4.466538546170727 \n",
      "\n",
      "Epoch:  1234  Learning Rate:  0.02914170139500857  Varinance:  4.460298955539586 \n",
      "\n",
      "Epoch:  1235  Learning Rate:  0.02911257425960852  Varinance:  4.454068081387849 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1236  Learning Rate:  0.029083476236785158  Varinance:  4.447845911538913 \n",
      "\n",
      "Epoch:  1237  Learning Rate:  0.029054407297440462  Varinance:  4.4416324338331865 \n",
      "\n",
      "Epoch:  1238  Learning Rate:  0.02902536741250547  Varinance:  4.435427636128066 \n",
      "\n",
      "Epoch:  1239  Learning Rate:  0.028996356552940323  Varinance:  4.429231506297905 \n",
      "\n",
      "Epoch:  1240  Learning Rate:  0.02896737468973414  Varinance:  4.423044032234002 \n",
      "\n",
      "Epoch:  1241  Learning Rate:  0.028938421793905062  Varinance:  4.41686520184457 \n",
      "\n",
      "Epoch:  1242  Learning Rate:  0.028909497836500188  Varinance:  4.41069500305471 \n",
      "\n",
      "Epoch:  1243  Learning Rate:  0.02888060278859557  Varinance:  4.404533423806396 \n",
      "\n",
      "Epoch:  1244  Learning Rate:  0.028851736621296132  Varinance:  4.3983804520584435 \n",
      "\n",
      "Epoch:  1245  Learning Rate:  0.028822899305735727  Varinance:  4.392236075786489 \n",
      "\n",
      "Epoch:  1246  Learning Rate:  0.028794090813077024  Varinance:  4.386100282982969 \n",
      "\n",
      "Epoch:  1247  Learning Rate:  0.02876531111451154  Varinance:  4.379973061657092 \n",
      "\n",
      "Epoch:  1248  Learning Rate:  0.028736560181259564  Varinance:  4.37385439983482 \n",
      "\n",
      "Epoch:  1249  Learning Rate:  0.02870783798457017  Varinance:  4.367744285558837 \n",
      "\n",
      "Epoch:  1250  Learning Rate:  0.028679144495721145  Varinance:  4.361642706888535 \n",
      "\n",
      "Epoch:  1251  Learning Rate:  0.028650479686019012  Varinance:  4.355549651899988 \n",
      "\n",
      "Epoch:  1252  Learning Rate:  0.028621843526798953  Varinance:  4.349465108685922 \n",
      "\n",
      "Epoch:  1253  Learning Rate:  0.0285932359894248  Varinance:  4.3433890653557015 \n",
      "\n",
      "Epoch:  1254  Learning Rate:  0.028564657045289023  Varinance:  4.337321510035301 \n",
      "\n",
      "Epoch:  1255  Learning Rate:  0.028536106665812667  Varinance:  4.33126243086728 \n",
      "\n",
      "Epoch:  1256  Learning Rate:  0.02850758482244536  Varinance:  4.325211816010766 \n",
      "\n",
      "Epoch:  1257  Learning Rate:  0.028479091486665248  Varinance:  4.319169653641425 \n",
      "\n",
      "Epoch:  1258  Learning Rate:  0.028450626629979  Varinance:  4.313135931951444 \n",
      "\n",
      "Epoch:  1259  Learning Rate:  0.028422190223921746  Varinance:  4.307110639149501 \n",
      "\n",
      "Epoch:  1260  Learning Rate:  0.02839378224005709  Varinance:  4.301093763460749 \n",
      "\n",
      "Epoch:  1261  Learning Rate:  0.028365402649977042  Varinance:  4.295085293126791 \n",
      "\n",
      "Epoch:  1262  Learning Rate:  0.028337051425302004  Varinance:  4.289085216405652 \n",
      "\n",
      "Epoch:  1263  Learning Rate:  0.028308728537680752  Varinance:  4.283093521571764 \n",
      "\n",
      "Epoch:  1264  Learning Rate:  0.0282804339587904  Varinance:  4.277110196915939 \n",
      "\n",
      "Epoch:  1265  Learning Rate:  0.02825216766033636  Varinance:  4.271135230745342 \n",
      "\n",
      "Epoch:  1266  Learning Rate:  0.028223929614052335  Varinance:  4.265168611383477 \n",
      "\n",
      "Epoch:  1267  Learning Rate:  0.028195719791700276  Varinance:  4.25921032717016 \n",
      "\n",
      "Epoch:  1268  Learning Rate:  0.028167538165070363  Varinance:  4.2532603664614905 \n",
      "\n",
      "Epoch:  1269  Learning Rate:  0.028139384705980954  Varinance:  4.24731871762984 \n",
      "\n",
      "Epoch:  1270  Learning Rate:  0.028111259386278606  Varinance:  4.24138536906382 \n",
      "\n",
      "Epoch:  1271  Learning Rate:  0.02808316217783798  Varinance:  4.235460309168264 \n",
      "\n",
      "Epoch:  1272  Learning Rate:  0.028055093052561875  Varinance:  4.229543526364203 \n",
      "\n",
      "Epoch:  1273  Learning Rate:  0.02802705198238116  Varinance:  4.223635009088845 \n",
      "\n",
      "Epoch:  1274  Learning Rate:  0.02799903893925476  Varinance:  4.217734745795547 \n",
      "\n",
      "Epoch:  1275  Learning Rate:  0.027971053895169636  Varinance:  4.2118427249538 \n",
      "\n",
      "Epoch:  1276  Learning Rate:  0.027943096822140735  Varinance:  4.205958935049201 \n",
      "\n",
      "Epoch:  1277  Learning Rate:  0.027915167692210988  Varinance:  4.200083364583432 \n",
      "\n",
      "Epoch:  1278  Learning Rate:  0.027887266477451256  Varinance:  4.194216002074239 \n",
      "\n",
      "Epoch:  1279  Learning Rate:  0.02785939314996033  Varinance:  4.188356836055408 \n",
      "\n",
      "Epoch:  1280  Learning Rate:  0.027831547681864872  Varinance:  4.18250585507674 \n",
      "\n",
      "Epoch:  1281  Learning Rate:  0.027803730045319414  Varinance:  4.176663047704038 \n",
      "\n",
      "Epoch:  1282  Learning Rate:  0.027775940212506324  Varinance:  4.17082840251907 \n",
      "\n",
      "Epoch:  1283  Learning Rate:  0.027748178155635753  Varinance:  4.16500190811956 \n",
      "\n",
      "Epoch:  1284  Learning Rate:  0.02772044384694566  Varinance:  4.159183553119162 \n",
      "\n",
      "Epoch:  1285  Learning Rate:  0.02769273725870171  Varinance:  4.153373326147429 \n",
      "\n",
      "Epoch:  1286  Learning Rate:  0.027665058363197343  Varinance:  4.147571215849808 \n",
      "\n",
      "Epoch:  1287  Learning Rate:  0.027637407132753634  Varinance:  4.1417772108876 \n",
      "\n",
      "Epoch:  1288  Learning Rate:  0.027609783539719365  Varinance:  4.135991299937949 \n",
      "\n",
      "Epoch:  1289  Learning Rate:  0.027582187556470933  Varinance:  4.130213471693815 \n",
      "\n",
      "Epoch:  1290  Learning Rate:  0.02755461915541236  Varinance:  4.124443714863957 \n",
      "\n",
      "Epoch:  1291  Learning Rate:  0.027527078308975234  Varinance:  4.118682018172904 \n",
      "\n",
      "Epoch:  1292  Learning Rate:  0.027499564989618714  Varinance:  4.112928370360937 \n",
      "\n",
      "Epoch:  1293  Learning Rate:  0.027472079169829473  Varinance:  4.107182760184067 \n",
      "\n",
      "Epoch:  1294  Learning Rate:  0.027444620822121697  Varinance:  4.101445176414013 \n",
      "\n",
      "Epoch:  1295  Learning Rate:  0.02741718991903702  Varinance:  4.095715607838179 \n",
      "\n",
      "Epoch:  1296  Learning Rate:  0.02738978643314456  Varinance:  4.089994043259631 \n",
      "\n",
      "Epoch:  1297  Learning Rate:  0.027362410337040805  Varinance:  4.084280471497081 \n",
      "\n",
      "Epoch:  1298  Learning Rate:  0.027335061603349677  Varinance:  4.0785748813848555 \n",
      "\n",
      "Epoch:  1299  Learning Rate:  0.02730774020472242  Varinance:  4.072877261772883 \n",
      "\n",
      "Epoch:  1300  Learning Rate:  0.027280446113837648  Varinance:  4.067187601526664 \n",
      "\n",
      "Epoch:  1301  Learning Rate:  0.02725317930340126  Varinance:  4.06150588952726 \n",
      "\n",
      "Epoch:  1302  Learning Rate:  0.02722593974614645  Varinance:  4.055832114671261 \n",
      "\n",
      "Epoch:  1303  Learning Rate:  0.027198727414833652  Varinance:  4.050166265870765 \n",
      "\n",
      "Epoch:  1304  Learning Rate:  0.027171542282250546  Varinance:  4.0445083320533675 \n",
      "\n",
      "Epoch:  1305  Learning Rate:  0.027144384321211974  Varinance:  4.038858302162125 \n",
      "\n",
      "Epoch:  1306  Learning Rate:  0.027117253504559992  Varinance:  4.033216165155542 \n",
      "\n",
      "Epoch:  1307  Learning Rate:  0.027090149805163766  Varinance:  4.02758191000755 \n",
      "\n",
      "Epoch:  1308  Learning Rate:  0.027063073195919614  Varinance:  4.021955525707478 \n",
      "\n",
      "Epoch:  1309  Learning Rate:  0.0270360236497509  Varinance:  4.016337001260043 \n",
      "\n",
      "Epoch:  1310  Learning Rate:  0.027009001139608104  Varinance:  4.010726325685318 \n",
      "\n",
      "Epoch:  1311  Learning Rate:  0.026982005638468684  Varinance:  4.005123488018715 \n",
      "\n",
      "Epoch:  1312  Learning Rate:  0.026955037119337163  Varinance:  3.9995284773109643 \n",
      "\n",
      "Epoch:  1313  Learning Rate:  0.026928095555245  Varinance:  3.993941282628091 \n",
      "\n",
      "Epoch:  1314  Learning Rate:  0.02690118091925064  Varinance:  3.988361893051396 \n",
      "\n",
      "Epoch:  1315  Learning Rate:  0.026874293184439436  Varinance:  3.9827902976774308 \n",
      "\n",
      "Epoch:  1316  Learning Rate:  0.02684743232392366  Varinance:  3.9772264856179786 \n",
      "\n",
      "Epoch:  1317  Learning Rate:  0.026820598310842443  Varinance:  3.971670446000038 \n",
      "\n",
      "Epoch:  1318  Learning Rate:  0.026793791118361776  Varinance:  3.9661221679657888 \n",
      "\n",
      "Epoch:  1319  Learning Rate:  0.026767010719674456  Varinance:  3.9605816406725864 \n",
      "\n",
      "Epoch:  1320  Learning Rate:  0.026740257088000093  Varinance:  3.9550488532929284 \n",
      "\n",
      "Epoch:  1321  Learning Rate:  0.026713530196585036  Varinance:  3.949523795014439 \n",
      "\n",
      "Epoch:  1322  Learning Rate:  0.026686830018702413  Varinance:  3.9440064550398475 \n",
      "\n",
      "Epoch:  1323  Learning Rate:  0.02666015652765202  Varinance:  3.9384968225869654 \n",
      "\n",
      "Epoch:  1324  Learning Rate:  0.02663350969676039  Varinance:  3.9329948868886704 \n",
      "\n",
      "Epoch:  1325  Learning Rate:  0.026606889499380667  Varinance:  3.9275006371928747 \n",
      "\n",
      "Epoch:  1326  Learning Rate:  0.02658029590889266  Varinance:  3.922014062762516 \n",
      "\n",
      "Epoch:  1327  Learning Rate:  0.026553728898702778  Varinance:  3.9165351528755306 \n",
      "\n",
      "Epoch:  1328  Learning Rate:  0.026527188442244012  Varinance:  3.9110638968248312 \n",
      "\n",
      "Epoch:  1329  Learning Rate:  0.026500674512975893  Varinance:  3.9056002839182895 \n",
      "\n",
      "Epoch:  1330  Learning Rate:  0.026474187084384506  Varinance:  3.900144303478714 \n",
      "\n",
      "Epoch:  1331  Learning Rate:  0.026447726129982398  Varinance:  3.8946959448438276 \n",
      "\n",
      "Epoch:  1332  Learning Rate:  0.02642129162330863  Varinance:  3.8892551973662473 \n",
      "\n",
      "Epoch:  1333  Learning Rate:  0.02639488353792868  Varinance:  3.8838220504134666 \n",
      "\n",
      "Epoch:  1334  Learning Rate:  0.02636850184743448  Varinance:  3.8783964933678314 \n",
      "\n",
      "Epoch:  1335  Learning Rate:  0.02634214652544431  Varinance:  3.872978515626518 \n",
      "\n",
      "Epoch:  1336  Learning Rate:  0.026315817545602874  Varinance:  3.8675681066015177 \n",
      "\n",
      "Epoch:  1337  Learning Rate:  0.02628951488158117  Varinance:  3.862165255719611 \n",
      "\n",
      "Epoch:  1338  Learning Rate:  0.026263238507076538  Varinance:  3.8567699524223475 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1339  Learning Rate:  0.0262369883958126  Varinance:  3.8513821861660293 \n",
      "\n",
      "Epoch:  1340  Learning Rate:  0.02621076452153925  Varinance:  3.846001946421688 \n",
      "\n",
      "Epoch:  1341  Learning Rate:  0.0261845668580326  Varinance:  3.8406292226750587 \n",
      "\n",
      "Epoch:  1342  Learning Rate:  0.026158395379094996  Varinance:  3.8352640044265702 \n",
      "\n",
      "Epoch:  1343  Learning Rate:  0.02613225005855494  Varinance:  3.8299062811913176 \n",
      "\n",
      "Epoch:  1344  Learning Rate:  0.02610613087026713  Varinance:  3.82455604249904 \n",
      "\n",
      "Epoch:  1345  Learning Rate:  0.026080037788112367  Varinance:  3.819213277894107 \n",
      "\n",
      "Epoch:  1346  Learning Rate:  0.02605397078599756  Varinance:  3.813877976935491 \n",
      "\n",
      "Epoch:  1347  Learning Rate:  0.026027929837855714  Varinance:  3.8085501291967536 \n",
      "\n",
      "Epoch:  1348  Learning Rate:  0.026001914917645877  Varinance:  3.803229724266017 \n",
      "\n",
      "Epoch:  1349  Learning Rate:  0.02597592599935311  Varinance:  3.7979167517459533 \n",
      "\n",
      "Epoch:  1350  Learning Rate:  0.025949963056988525  Varinance:  3.7926112012537585 \n",
      "\n",
      "Epoch:  1351  Learning Rate:  0.025924026064589153  Varinance:  3.7873130624211284 \n",
      "\n",
      "Epoch:  1352  Learning Rate:  0.025898114996218004  Varinance:  3.7820223248942484 \n",
      "\n",
      "Epoch:  1353  Learning Rate:  0.025872229825964006  Varinance:  3.776738978333766 \n",
      "\n",
      "Epoch:  1354  Learning Rate:  0.025846370527942  Varinance:  3.77146301241477 \n",
      "\n",
      "Epoch:  1355  Learning Rate:  0.025820537076292668  Varinance:  3.766194416826776 \n",
      "\n",
      "Epoch:  1356  Learning Rate:  0.02579472944518257  Varinance:  3.7609331812737015 \n",
      "\n",
      "Epoch:  1357  Learning Rate:  0.025768947608804056  Varinance:  3.7556792954738487 \n",
      "\n",
      "Epoch:  1358  Learning Rate:  0.02574319154137531  Varinance:  3.75043274915988 \n",
      "\n",
      "Epoch:  1359  Learning Rate:  0.025717461217140244  Varinance:  3.7451935320788032 \n",
      "\n",
      "Epoch:  1360  Learning Rate:  0.025691756610368544  Varinance:  3.7399616339919506 \n",
      "\n",
      "Epoch:  1361  Learning Rate:  0.025666077695355588  Varinance:  3.7347370446749517 \n",
      "\n",
      "Epoch:  1362  Learning Rate:  0.025640424446422474  Varinance:  3.7295197539177254 \n",
      "\n",
      "Epoch:  1363  Learning Rate:  0.025614796837915933  Varinance:  3.7243097515244523 \n",
      "\n",
      "Epoch:  1364  Learning Rate:  0.025589194844208376  Varinance:  3.719107027313552 \n",
      "\n",
      "Epoch:  1365  Learning Rate:  0.025563618439697785  Varinance:  3.7139115711176722 \n",
      "\n",
      "Epoch:  1366  Learning Rate:  0.02553806759880777  Varinance:  3.708723372783662 \n",
      "\n",
      "Epoch:  1367  Learning Rate:  0.02551254229598748  Varinance:  3.7035424221725557 \n",
      "\n",
      "Epoch:  1368  Learning Rate:  0.025487042505711616  Varinance:  3.6983687091595483 \n",
      "\n",
      "Epoch:  1369  Learning Rate:  0.025461568202480373  Varinance:  3.693202223633981 \n",
      "\n",
      "Epoch:  1370  Learning Rate:  0.025436119360819465  Varinance:  3.6880429554993204 \n",
      "\n",
      "Epoch:  1371  Learning Rate:  0.02541069595528003  Varinance:  3.6828908946731334 \n",
      "\n",
      "Epoch:  1372  Learning Rate:  0.02538529796043867  Varinance:  3.677746031087075 \n",
      "\n",
      "Epoch:  1373  Learning Rate:  0.025359925350897386  Varinance:  3.672608354686866 \n",
      "\n",
      "Epoch:  1374  Learning Rate:  0.02533457810128357  Varinance:  3.667477855432269 \n",
      "\n",
      "Epoch:  1375  Learning Rate:  0.02530925618624996  Varinance:  3.662354523297076 \n",
      "\n",
      "Epoch:  1376  Learning Rate:  0.02528395958047465  Varinance:  3.657238348269085 \n",
      "\n",
      "Epoch:  1377  Learning Rate:  0.025258688258661028  Varinance:  3.6521293203500766 \n",
      "\n",
      "Epoch:  1378  Learning Rate:  0.025233442195537765  Varinance:  3.647027429555804 \n",
      "\n",
      "Epoch:  1379  Learning Rate:  0.025208221365858804  Varinance:  3.6419326659159648 \n",
      "\n",
      "Epoch:  1380  Learning Rate:  0.025183025744403304  Varinance:  3.636845019474186 \n",
      "\n",
      "Epoch:  1381  Learning Rate:  0.025157855305975654  Varinance:  3.631764480288001 \n",
      "\n",
      "Epoch:  1382  Learning Rate:  0.025132710025405403  Varinance:  3.626691038428836 \n",
      "\n",
      "Epoch:  1383  Learning Rate:  0.02510758987754727  Varinance:  3.6216246839819854 \n",
      "\n",
      "Epoch:  1384  Learning Rate:  0.025082494837281113  Varinance:  3.6165654070465925 \n",
      "\n",
      "Epoch:  1385  Learning Rate:  0.025057424879511882  Varinance:  3.6115131977356336 \n",
      "\n",
      "Epoch:  1386  Learning Rate:  0.025032379979169614  Varinance:  3.6064680461758982 \n",
      "\n",
      "Epoch:  1387  Learning Rate:  0.025007360111209416  Varinance:  3.6014299425079646 \n",
      "\n",
      "Epoch:  1388  Learning Rate:  0.024982365250611406  Varinance:  3.5963988768861865 \n",
      "\n",
      "Epoch:  1389  Learning Rate:  0.024957395372380734  Varinance:  3.591374839478672 \n",
      "\n",
      "Epoch:  1390  Learning Rate:  0.024932450451547513  Varinance:  3.586357820467264 \n",
      "\n",
      "Epoch:  1391  Learning Rate:  0.02490753046316682  Varinance:  3.5813478100475185 \n",
      "\n",
      "Epoch:  1392  Learning Rate:  0.024882635382318666  Varinance:  3.576344798428692 \n",
      "\n",
      "Epoch:  1393  Learning Rate:  0.024857765184107972  Varinance:  3.571348775833715 \n",
      "\n",
      "Epoch:  1394  Learning Rate:  0.02483291984366453  Varinance:  3.5663597324991767 \n",
      "\n",
      "Epoch:  1395  Learning Rate:  0.024808099336143002  Varinance:  3.561377658675307 \n",
      "\n",
      "Epoch:  1396  Learning Rate:  0.024783303636722875  Varinance:  3.5564025446259557 \n",
      "\n",
      "Epoch:  1397  Learning Rate:  0.024758532720608458  Varinance:  3.551434380628571 \n",
      "\n",
      "Epoch:  1398  Learning Rate:  0.02473378656302881  Varinance:  3.5464731569741863 \n",
      "\n",
      "Epoch:  1399  Learning Rate:  0.024709065139237804  Varinance:  3.541518863967396 \n",
      "\n",
      "Epoch:  1400  Learning Rate:  0.024684368424513985  Varinance:  3.5365714919263413 \n",
      "\n",
      "Epoch:  1401  Learning Rate:  0.02465969639416065  Varinance:  3.5316310311826844 \n",
      "\n",
      "Epoch:  1402  Learning Rate:  0.024635049023505762  Varinance:  3.526697472081598 \n",
      "\n",
      "Epoch:  1403  Learning Rate:  0.024610426287901957  Varinance:  3.52177080498174 \n",
      "\n",
      "Epoch:  1404  Learning Rate:  0.024585828162726482  Varinance:  3.5168510202552357 \n",
      "\n",
      "Epoch:  1405  Learning Rate:  0.024561254623381226  Varinance:  3.5119381082876635 \n",
      "\n",
      "Epoch:  1406  Learning Rate:  0.024536705645292634  Varinance:  3.507032059478031 \n",
      "\n",
      "Epoch:  1407  Learning Rate:  0.02451218120391174  Varinance:  3.502132864238758 \n",
      "\n",
      "Epoch:  1408  Learning Rate:  0.024487681274714082  Varinance:  3.4972405129956563 \n",
      "\n",
      "Epoch:  1409  Learning Rate:  0.02446320583319975  Varinance:  3.4923549961879172 \n",
      "\n",
      "Epoch:  1410  Learning Rate:  0.02443875485489328  Varinance:  3.487476304268082 \n",
      "\n",
      "Epoch:  1411  Learning Rate:  0.024414328315343712  Varinance:  3.4826044277020345 \n",
      "\n",
      "Epoch:  1412  Learning Rate:  0.024389926190124485  Varinance:  3.4777393569689736 \n",
      "\n",
      "Epoch:  1413  Learning Rate:  0.024365548454833488  Varinance:  3.4728810825614027 \n",
      "\n",
      "Epoch:  1414  Learning Rate:  0.02434119508509297  Varinance:  3.468029594985101 \n",
      "\n",
      "Epoch:  1415  Learning Rate:  0.024316866056549567  Varinance:  3.463184884759116 \n",
      "\n",
      "Epoch:  1416  Learning Rate:  0.024292561344874244  Varinance:  3.4583469424157376 \n",
      "\n",
      "Epoch:  1417  Learning Rate:  0.0242682809257623  Varinance:  3.4535157585004805 \n",
      "\n",
      "Epoch:  1418  Learning Rate:  0.024244024774933294  Varinance:  3.44869132357207 \n",
      "\n",
      "Epoch:  1419  Learning Rate:  0.02421979286813109  Varinance:  3.4438736282024185 \n",
      "\n",
      "Epoch:  1420  Learning Rate:  0.024195585181123767  Varinance:  3.4390626629766086 \n",
      "\n",
      "Epoch:  1421  Learning Rate:  0.024171401689703647  Varinance:  3.434258418492877 \n",
      "\n",
      "Epoch:  1422  Learning Rate:  0.024147242369687225  Varinance:  3.429460885362592 \n",
      "\n",
      "Epoch:  1423  Learning Rate:  0.024123107196915194  Varinance:  3.424670054210241 \n",
      "\n",
      "Epoch:  1424  Learning Rate:  0.02409899614725236  Varinance:  3.419885915673405 \n",
      "\n",
      "Epoch:  1425  Learning Rate:  0.02407490919658769  Varinance:  3.4151084604027444 \n",
      "\n",
      "Epoch:  1426  Learning Rate:  0.024050846320834214  Varinance:  3.4103376790619846 \n",
      "\n",
      "Epoch:  1427  Learning Rate:  0.024026807495929073  Varinance:  3.405573562327887 \n",
      "\n",
      "Epoch:  1428  Learning Rate:  0.02400279269783342  Varinance:  3.4008161008902418 \n",
      "\n",
      "Epoch:  1429  Learning Rate:  0.023978801902532473  Varinance:  3.3960652854518436 \n",
      "\n",
      "Epoch:  1430  Learning Rate:  0.023954835086035423  Varinance:  3.3913211067284745 \n",
      "\n",
      "Epoch:  1431  Learning Rate:  0.023930892224375458  Varinance:  3.3865835554488877 \n",
      "\n",
      "Epoch:  1432  Learning Rate:  0.023906973293609704  Varinance:  3.3818526223547876 \n",
      "\n",
      "Epoch:  1433  Learning Rate:  0.023883078269819245  Varinance:  3.3771282982008097 \n",
      "\n",
      "Epoch:  1434  Learning Rate:  0.02385920712910904  Varinance:  3.3724105737545083 \n",
      "\n",
      "Epoch:  1435  Learning Rate:  0.023835359847607956  Varinance:  3.3676994397963336 \n",
      "\n",
      "Epoch:  1436  Learning Rate:  0.023811536401468703  Varinance:  3.362994887119615 \n",
      "\n",
      "Epoch:  1437  Learning Rate:  0.02378773676686784  Varinance:  3.3582969065305437 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1438  Learning Rate:  0.02376396092000572  Varinance:  3.353605488848156 \n",
      "\n",
      "Epoch:  1439  Learning Rate:  0.023740208837106508  Varinance:  3.3489206249043084 \n",
      "\n",
      "Epoch:  1440  Learning Rate:  0.023716480494418105  Varinance:  3.3442423055436694 \n",
      "\n",
      "Epoch:  1441  Learning Rate:  0.02369277586821218  Varinance:  3.3395705216236973 \n",
      "\n",
      "Epoch:  1442  Learning Rate:  0.02366909493478409  Varinance:  3.3349052640146195 \n",
      "\n",
      "Epoch:  1443  Learning Rate:  0.02364543767045291  Varinance:  3.3302465235994205 \n",
      "\n",
      "Epoch:  1444  Learning Rate:  0.02362180405156137  Varinance:  3.32559429127382 \n",
      "\n",
      "Epoch:  1445  Learning Rate:  0.023598194054475854  Varinance:  3.320948557946253 \n",
      "\n",
      "Epoch:  1446  Learning Rate:  0.023574607655586353  Varinance:  3.3163093145378584 \n",
      "\n",
      "Epoch:  1447  Learning Rate:  0.023551044831306475  Varinance:  3.3116765519824565 \n",
      "\n",
      "Epoch:  1448  Learning Rate:  0.02352750555807339  Varinance:  3.3070502612265336 \n",
      "\n",
      "Epoch:  1449  Learning Rate:  0.02350398981234783  Varinance:  3.302430433229224 \n",
      "\n",
      "Epoch:  1450  Learning Rate:  0.02348049757061403  Varinance:  3.29781705896229 \n",
      "\n",
      "Epoch:  1451  Learning Rate:  0.023457028809379766  Varinance:  3.293210129410108 \n",
      "\n",
      "Epoch:  1452  Learning Rate:  0.023433583505176263  Varinance:  3.2886096355696455 \n",
      "\n",
      "Epoch:  1453  Learning Rate:  0.02341016163455822  Varinance:  3.28401556845045 \n",
      "\n",
      "Epoch:  1454  Learning Rate:  0.023386763174103757  Varinance:  3.2794279190746276 \n",
      "\n",
      "Epoch:  1455  Learning Rate:  0.023363388100414426  Varinance:  3.274846678476826 \n",
      "\n",
      "Epoch:  1456  Learning Rate:  0.023340036390115133  Varinance:  3.2702718377042173 \n",
      "\n",
      "Epoch:  1457  Learning Rate:  0.02331670801985418  Varinance:  3.2657033878164823 \n",
      "\n",
      "Epoch:  1458  Learning Rate:  0.023293402966303188  Varinance:  3.2611413198857853 \n",
      "\n",
      "Epoch:  1459  Learning Rate:  0.02327012120615711  Varinance:  3.2565856249967684 \n",
      "\n",
      "Epoch:  1460  Learning Rate:  0.023246862716134166  Varinance:  3.2520362942465257 \n",
      "\n",
      "Epoch:  1461  Learning Rate:  0.023223627472975884  Varinance:  3.2474933187445885 \n",
      "\n",
      "Epoch:  1462  Learning Rate:  0.023200415453447004  Varinance:  3.242956689612907 \n",
      "\n",
      "Epoch:  1463  Learning Rate:  0.023177226634335517  Varinance:  3.2384263979858363 \n",
      "\n",
      "Epoch:  1464  Learning Rate:  0.023154060992452593  Varinance:  3.233902435010115 \n",
      "\n",
      "Epoch:  1465  Learning Rate:  0.023130918504632593  Varinance:  3.229384791844847 \n",
      "\n",
      "Epoch:  1466  Learning Rate:  0.02310779914773302  Varinance:  3.22487345966149 \n",
      "\n",
      "Epoch:  1467  Learning Rate:  0.023084702898634527  Varinance:  3.220368429643833 \n",
      "\n",
      "Epoch:  1468  Learning Rate:  0.02306162973424085  Varinance:  3.2158696929879818 \n",
      "\n",
      "Epoch:  1469  Learning Rate:  0.023038579631478835  Varinance:  3.2113772409023413 \n",
      "\n",
      "Epoch:  1470  Learning Rate:  0.023015552567298366  Varinance:  3.2068910646075963 \n",
      "\n",
      "Epoch:  1471  Learning Rate:  0.022992548518672384  Varinance:  3.2024111553366987 \n",
      "\n",
      "Epoch:  1472  Learning Rate:  0.022969567462596836  Varinance:  3.1979375043348433 \n",
      "\n",
      "Epoch:  1473  Learning Rate:  0.02294660937609067  Varinance:  3.1934701028594596 \n",
      "\n",
      "Epoch:  1474  Learning Rate:  0.022923674236195787  Varinance:  3.1890089421801866 \n",
      "\n",
      "Epoch:  1475  Learning Rate:  0.022900762019977053  Varinance:  3.1845540135788624 \n",
      "\n",
      "Epoch:  1476  Learning Rate:  0.022877872704522243  Varinance:  3.1801053083495012 \n",
      "\n",
      "Epoch:  1477  Learning Rate:  0.02285500626694205  Varinance:  3.1756628177982833 \n",
      "\n",
      "Epoch:  1478  Learning Rate:  0.022832162684370022  Varinance:  3.171226533243528 \n",
      "\n",
      "Epoch:  1479  Learning Rate:  0.022809341933962588  Varinance:  3.166796446015686 \n",
      "\n",
      "Epoch:  1480  Learning Rate:  0.022786543992898985  Varinance:  3.16237254745732 \n",
      "\n",
      "Epoch:  1481  Learning Rate:  0.022763768838381274  Varinance:  3.1579548289230845 \n",
      "\n",
      "Epoch:  1482  Learning Rate:  0.022741016447634297  Varinance:  3.153543281779713 \n",
      "\n",
      "Epoch:  1483  Learning Rate:  0.022718286797905666  Varinance:  3.1491378974059994 \n",
      "\n",
      "Epoch:  1484  Learning Rate:  0.022695579866465726  Varinance:  3.1447386671927804 \n",
      "\n",
      "Epoch:  1485  Learning Rate:  0.022672895630607544  Varinance:  3.1403455825429174 \n",
      "\n",
      "Epoch:  1486  Learning Rate:  0.022650234067646876  Varinance:  3.1359586348712845 \n",
      "\n",
      "Epoch:  1487  Learning Rate:  0.022627595154922173  Varinance:  3.1315778156047482 \n",
      "\n",
      "Epoch:  1488  Learning Rate:  0.022604978869794498  Varinance:  3.127203116182152 \n",
      "\n",
      "Epoch:  1489  Learning Rate:  0.022582385189647586  Varinance:  3.1228345280542977 \n",
      "\n",
      "Epoch:  1490  Learning Rate:  0.022559814091887745  Varinance:  3.118472042683931 \n",
      "\n",
      "Epoch:  1491  Learning Rate:  0.022537265553943874  Varinance:  3.1141156515457222 \n",
      "\n",
      "Epoch:  1492  Learning Rate:  0.022514739553267434  Varinance:  3.1097653461262524 \n",
      "\n",
      "Epoch:  1493  Learning Rate:  0.022492236067332427  Varinance:  3.105421117923997 \n",
      "\n",
      "Epoch:  1494  Learning Rate:  0.022469755073635356  Varinance:  3.1010829584493047 \n",
      "\n",
      "Epoch:  1495  Learning Rate:  0.022447296549695236  Varinance:  3.0967508592243864 \n",
      "\n",
      "Epoch:  1496  Learning Rate:  0.02242486047305353  Varinance:  3.0924248117832955 \n",
      "\n",
      "Epoch:  1497  Learning Rate:  0.022402446821274177  Varinance:  3.088104807671913 \n",
      "\n",
      "Epoch:  1498  Learning Rate:  0.022380055571943502  Varinance:  3.083790838447926 \n",
      "\n",
      "Epoch:  1499  Learning Rate:  0.02235768670267027  Varinance:  3.0794828956808193 \n",
      "\n",
      "Epoch:  1500  Learning Rate:  0.0223353401910856  Varinance:  3.075180970951853 \n",
      "\n",
      "Epoch:  1501  Learning Rate:  0.022313016014842982  Varinance:  3.07088505585405 \n",
      "\n",
      "Epoch:  1502  Learning Rate:  0.022290714151618245  Varinance:  3.0665951419921744 \n",
      "\n",
      "Epoch:  1503  Learning Rate:  0.02226843457910951  Varinance:  3.06231122098272 \n",
      "\n",
      "Epoch:  1504  Learning Rate:  0.022246177275037214  Varinance:  3.058033284453893 \n",
      "\n",
      "Epoch:  1505  Learning Rate:  0.022223942217144042  Varinance:  3.0537613240455905 \n",
      "\n",
      "Epoch:  1506  Learning Rate:  0.022201729383194944  Varinance:  3.049495331409393 \n",
      "\n",
      "Epoch:  1507  Learning Rate:  0.022179538750977074  Varinance:  3.0452352982085413 \n",
      "\n",
      "Epoch:  1508  Learning Rate:  0.02215737029829981  Varinance:  3.0409812161179226 \n",
      "\n",
      "Epoch:  1509  Learning Rate:  0.022135224002994683  Varinance:  3.0367330768240537 \n",
      "\n",
      "Epoch:  1510  Learning Rate:  0.022113099842915415  Varinance:  3.0324908720250674 \n",
      "\n",
      "Epoch:  1511  Learning Rate:  0.02209099779593782  Varinance:  3.0282545934306886 \n",
      "\n",
      "Epoch:  1512  Learning Rate:  0.022068917839959873  Varinance:  3.024024232762229 \n",
      "\n",
      "Epoch:  1513  Learning Rate:  0.022046859952901593  Varinance:  3.0197997817525617 \n",
      "\n",
      "Epoch:  1514  Learning Rate:  0.022024824112705114  Varinance:  3.0155812321461104 \n",
      "\n",
      "Epoch:  1515  Learning Rate:  0.022002810297334575  Varinance:  3.011368575698832 \n",
      "\n",
      "Epoch:  1516  Learning Rate:  0.021980818484776177  Varinance:  3.007161804178198 \n",
      "\n",
      "Epoch:  1517  Learning Rate:  0.021958848653038085  Varinance:  3.0029609093631833 \n",
      "\n",
      "Epoch:  1518  Learning Rate:  0.02193690078015048  Varinance:  2.998765883044243 \n",
      "\n",
      "Epoch:  1519  Learning Rate:  0.02191497484416548  Varinance:  2.994576717023305 \n",
      "\n",
      "Epoch:  1520  Learning Rate:  0.021893070823157158  Varinance:  2.990393403113747 \n",
      "\n",
      "Epoch:  1521  Learning Rate:  0.021871188695221477  Varinance:  2.986215933140384 \n",
      "\n",
      "Epoch:  1522  Learning Rate:  0.02184932843847632  Varinance:  2.9820442989394516 \n",
      "\n",
      "Epoch:  1523  Learning Rate:  0.021827490031061415  Varinance:  2.9778784923585913 \n",
      "\n",
      "Epoch:  1524  Learning Rate:  0.021805673451138364  Varinance:  2.973718505256828 \n",
      "\n",
      "Epoch:  1525  Learning Rate:  0.021783878676890578  Varinance:  2.9695643295045646 \n",
      "\n",
      "Epoch:  1526  Learning Rate:  0.02176210568652329  Varinance:  2.9654159569835583 \n",
      "\n",
      "Epoch:  1527  Learning Rate:  0.0217403544582635  Varinance:  2.9612733795869084 \n",
      "\n",
      "Epoch:  1528  Learning Rate:  0.02171862497035998  Varinance:  2.9571365892190387 \n",
      "\n",
      "Epoch:  1529  Learning Rate:  0.021696917201083236  Varinance:  2.953005577795681 \n",
      "\n",
      "Epoch:  1530  Learning Rate:  0.021675231128725506  Varinance:  2.9488803372438643 \n",
      "\n",
      "Epoch:  1531  Learning Rate:  0.02165356673160071  Varinance:  2.9447608595018893 \n",
      "\n",
      "Epoch:  1532  Learning Rate:  0.02163192398804445  Varinance:  2.940647136519323 \n",
      "\n",
      "Epoch:  1533  Learning Rate:  0.021610302876413977  Varinance:  2.936539160256978 \n",
      "\n",
      "Epoch:  1534  Learning Rate:  0.021588703375088183  Varinance:  2.9324369226868963 \n",
      "\n",
      "Epoch:  1535  Learning Rate:  0.021567125462467567  Varinance:  2.928340415792335 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1536  Learning Rate:  0.02154556911697421  Varinance:  2.924249631567751 \n",
      "\n",
      "Epoch:  1537  Learning Rate:  0.021524034317051757  Varinance:  2.920164562018786 \n",
      "\n",
      "Epoch:  1538  Learning Rate:  0.02150252104116543  Varinance:  2.916085199162243 \n",
      "\n",
      "Epoch:  1539  Learning Rate:  0.02148102926780192  Varinance:  2.912011535026085 \n",
      "\n",
      "Epoch:  1540  Learning Rate:  0.02145955897546948  Varinance:  2.907943561649408 \n",
      "\n",
      "Epoch:  1541  Learning Rate:  0.021438110142697794  Varinance:  2.903881271082429 \n",
      "\n",
      "Epoch:  1542  Learning Rate:  0.021416682748038048  Varinance:  2.8998246553864724 \n",
      "\n",
      "Epoch:  1543  Learning Rate:  0.021395276770062824  Varinance:  2.8957737066339515 \n",
      "\n",
      "Epoch:  1544  Learning Rate:  0.021373892187366163  Varinance:  2.8917284169083515 \n",
      "\n",
      "Epoch:  1545  Learning Rate:  0.021352528978563462  Varinance:  2.8876887783042213 \n",
      "\n",
      "Epoch:  1546  Learning Rate:  0.021331187122291526  Varinance:  2.883654782927151 \n",
      "\n",
      "Epoch:  1547  Learning Rate:  0.021309866597208484  Varinance:  2.879626422893759 \n",
      "\n",
      "Epoch:  1548  Learning Rate:  0.02128856738199382  Varinance:  2.875603690331677 \n",
      "\n",
      "Epoch:  1549  Learning Rate:  0.021267289455348306  Varinance:  2.8715865773795337 \n",
      "\n",
      "Epoch:  1550  Learning Rate:  0.021246032795994026  Varinance:  2.8675750761869416 \n",
      "\n",
      "Epoch:  1551  Learning Rate:  0.021224797382674306  Varinance:  2.8635691789144753 \n",
      "\n",
      "Epoch:  1552  Learning Rate:  0.021203583194153743  Varinance:  2.859568877733665 \n",
      "\n",
      "Epoch:  1553  Learning Rate:  0.021182390209218138  Varinance:  2.8555741648269763 \n",
      "\n",
      "Epoch:  1554  Learning Rate:  0.02116121840667451  Varinance:  2.8515850323877947 \n",
      "\n",
      "Epoch:  1555  Learning Rate:  0.02114006776535105  Varinance:  2.847601472620412 \n",
      "\n",
      "Epoch:  1556  Learning Rate:  0.02111893826409712  Varinance:  2.8436234777400107 \n",
      "\n",
      "Epoch:  1557  Learning Rate:  0.021097829881783207  Varinance:  2.8396510399726456 \n",
      "\n",
      "Epoch:  1558  Learning Rate:  0.02107674259730094  Varinance:  2.8356841515552347 \n",
      "\n",
      "Epoch:  1559  Learning Rate:  0.021055676389563027  Varinance:  2.831722804735539 \n",
      "\n",
      "Epoch:  1560  Learning Rate:  0.021034631237503255  Varinance:  2.8277669917721506 \n",
      "\n",
      "Epoch:  1561  Learning Rate:  0.021013607120076473  Varinance:  2.8238167049344742 \n",
      "\n",
      "Epoch:  1562  Learning Rate:  0.020992604016258565  Varinance:  2.819871936502715 \n",
      "\n",
      "Epoch:  1563  Learning Rate:  0.020971621905046423  Varinance:  2.8159326787678634 \n",
      "\n",
      "Epoch:  1564  Learning Rate:  0.020950660765457933  Varinance:  2.8119989240316756 \n",
      "\n",
      "Epoch:  1565  Learning Rate:  0.020929720576531953  Varinance:  2.8080706646066647 \n",
      "\n",
      "Epoch:  1566  Learning Rate:  0.020908801317328293  Varinance:  2.8041478928160837 \n",
      "\n",
      "Epoch:  1567  Learning Rate:  0.020887902966927694  Varinance:  2.8002306009939066 \n",
      "\n",
      "Epoch:  1568  Learning Rate:  0.020867025504431806  Varinance:  2.7963187814848203 \n",
      "\n",
      "Epoch:  1569  Learning Rate:  0.020846168908963153  Varinance:  2.792412426644203 \n",
      "\n",
      "Epoch:  1570  Learning Rate:  0.020825333159665158  Varinance:  2.788511528838115 \n",
      "\n",
      "Epoch:  1571  Learning Rate:  0.02080451823570205  Varinance:  2.7846160804432754 \n",
      "\n",
      "Epoch:  1572  Learning Rate:  0.020783724116258912  Varinance:  2.7807260738470587 \n",
      "\n",
      "Epoch:  1573  Learning Rate:  0.02076295078054162  Varinance:  2.776841501447471 \n",
      "\n",
      "Epoch:  1574  Learning Rate:  0.020742198207776844  Varinance:  2.7729623556531404 \n",
      "\n",
      "Epoch:  1575  Learning Rate:  0.020721466377212002  Varinance:  2.769088628883296 \n",
      "\n",
      "Epoch:  1576  Learning Rate:  0.020700755268115267  Varinance:  2.765220313567762 \n",
      "\n",
      "Epoch:  1577  Learning Rate:  0.020680064859775516  Varinance:  2.7613574021469325 \n",
      "\n",
      "Epoch:  1578  Learning Rate:  0.020659395131502358  Varinance:  2.757499887071765 \n",
      "\n",
      "Epoch:  1579  Learning Rate:  0.020638746062626046  Varinance:  2.7536477608037635 \n",
      "\n",
      "Epoch:  1580  Learning Rate:  0.020618117632497525  Varinance:  2.749801015814961 \n",
      "\n",
      "Epoch:  1581  Learning Rate:  0.020597509820488344  Varinance:  2.7459596445879084 \n",
      "\n",
      "Epoch:  1582  Learning Rate:  0.02057692260599071  Varinance:  2.7421236396156567 \n",
      "\n",
      "Epoch:  1583  Learning Rate:  0.02055635596841739  Varinance:  2.7382929934017466 \n",
      "\n",
      "Epoch:  1584  Learning Rate:  0.02053580988720176  Varinance:  2.7344676984601866 \n",
      "\n",
      "Epoch:  1585  Learning Rate:  0.020515284341797717  Varinance:  2.7306477473154462 \n",
      "\n",
      "Epoch:  1586  Learning Rate:  0.020494779311679732  Varinance:  2.7268331325024375 \n",
      "\n",
      "Epoch:  1587  Learning Rate:  0.020474294776342765  Varinance:  2.723023846566501 \n",
      "\n",
      "Epoch:  1588  Learning Rate:  0.020453830715302282  Varinance:  2.7192198820633893 \n",
      "\n",
      "Epoch:  1589  Learning Rate:  0.020433387108094215  Varinance:  2.7154212315592585 \n",
      "\n",
      "Epoch:  1590  Learning Rate:  0.020412963934274966  Varinance:  2.711627887630644 \n",
      "\n",
      "Epoch:  1591  Learning Rate:  0.020392561173421343  Varinance:  2.7078398428644554 \n",
      "\n",
      "Epoch:  1592  Learning Rate:  0.020372178805130602  Varinance:  2.704057089857957 \n",
      "\n",
      "Epoch:  1593  Learning Rate:  0.020351816809020356  Varinance:  2.700279621218754 \n",
      "\n",
      "Epoch:  1594  Learning Rate:  0.02033147516472862  Varinance:  2.69650742956478 \n",
      "\n",
      "Epoch:  1595  Learning Rate:  0.02031115385191374  Varinance:  2.6927405075242796 \n",
      "\n",
      "Epoch:  1596  Learning Rate:  0.020290852850254407  Varinance:  2.6889788477357968 \n",
      "\n",
      "Epoch:  1597  Learning Rate:  0.020270572139449612  Varinance:  2.6852224428481564 \n",
      "\n",
      "Epoch:  1598  Learning Rate:  0.020250311699218648  Varinance:  2.681471285520456 \n",
      "\n",
      "Epoch:  1599  Learning Rate:  0.020230071509301072  Varinance:  2.6777253684220454 \n",
      "\n",
      "Epoch:  1600  Learning Rate:  0.02020985154945669  Varinance:  2.673984684232518 \n",
      "\n",
      "Epoch:  1601  Learning Rate:  0.02018965179946554  Varinance:  2.670249225641691 \n",
      "\n",
      "Epoch:  1602  Learning Rate:  0.020169472239127878  Varinance:  2.666518985349595 \n",
      "\n",
      "Epoch:  1603  Learning Rate:  0.020149312848264128  Varinance:  2.662793956066458 \n",
      "\n",
      "Epoch:  1604  Learning Rate:  0.02012917360671491  Varinance:  2.6590741305126913 \n",
      "\n",
      "Epoch:  1605  Learning Rate:  0.020109054494340972  Varinance:  2.655359501418875 \n",
      "\n",
      "Epoch:  1606  Learning Rate:  0.02008895549102321  Varinance:  2.6516500615257454 \n",
      "\n",
      "Epoch:  1607  Learning Rate:  0.020068876576662606  Varinance:  2.64794580358418 \n",
      "\n",
      "Epoch:  1608  Learning Rate:  0.02004881773118026  Varinance:  2.6442467203551816 \n",
      "\n",
      "Epoch:  1609  Learning Rate:  0.020028778934517308  Varinance:  2.640552804609868 \n",
      "\n",
      "Epoch:  1610  Learning Rate:  0.020008760166634963  Varinance:  2.6368640491294517 \n",
      "\n",
      "Epoch:  1611  Learning Rate:  0.01998876140751445  Varinance:  2.6331804467052327 \n",
      "\n",
      "Epoch:  1612  Learning Rate:  0.019968782637157012  Varinance:  2.6295019901385808 \n",
      "\n",
      "Epoch:  1613  Learning Rate:  0.019948823835583874  Varinance:  2.625828672240921 \n",
      "\n",
      "Epoch:  1614  Learning Rate:  0.019928884982836237  Varinance:  2.622160485833721 \n",
      "\n",
      "Epoch:  1615  Learning Rate:  0.01990896605897524  Varinance:  2.6184974237484773 \n",
      "\n",
      "Epoch:  1616  Learning Rate:  0.019889067044081964  Varinance:  2.6148394788267004 \n",
      "\n",
      "Epoch:  1617  Learning Rate:  0.019869187918257387  Varinance:  2.6111866439198996 \n",
      "\n",
      "Epoch:  1618  Learning Rate:  0.019849328661622388  Varinance:  2.607538911889571 \n",
      "\n",
      "Epoch:  1619  Learning Rate:  0.019829489254317698  Varinance:  2.603896275607183 \n",
      "\n",
      "Epoch:  1620  Learning Rate:  0.019809669676503924  Varinance:  2.6002587279541642 \n",
      "\n",
      "Epoch:  1621  Learning Rate:  0.019789869908361468  Varinance:  2.5966262618218843 \n",
      "\n",
      "Epoch:  1622  Learning Rate:  0.019770089930090575  Varinance:  2.5929988701116473 \n",
      "\n",
      "Epoch:  1623  Learning Rate:  0.019750329721911257  Varinance:  2.5893765457346687 \n",
      "\n",
      "Epoch:  1624  Learning Rate:  0.01973058926406331  Varinance:  2.585759281612071 \n",
      "\n",
      "Epoch:  1625  Learning Rate:  0.019710868536806266  Varinance:  2.5821470706748637 \n",
      "\n",
      "Epoch:  1626  Learning Rate:  0.019691167520419408  Varinance:  2.5785399058639333 \n",
      "\n",
      "Epoch:  1627  Learning Rate:  0.019671486195201707  Varinance:  2.5749377801300253 \n",
      "\n",
      "Epoch:  1628  Learning Rate:  0.019651824541471838  Varinance:  2.571340686433734 \n",
      "\n",
      "Epoch:  1629  Learning Rate:  0.019632182539568156  Varinance:  2.567748617745489 \n",
      "\n",
      "Epoch:  1630  Learning Rate:  0.019612560169848643  Varinance:  2.5641615670455353 \n",
      "\n",
      "Epoch:  1631  Learning Rate:  0.019592957412690938  Varinance:  2.560579527323929 \n",
      "\n",
      "Epoch:  1632  Learning Rate:  0.019573374248492273  Varinance:  2.557002491580516 \n",
      "\n",
      "Epoch:  1633  Learning Rate:  0.019553810657669493  Varinance:  2.5534304528249234 \n",
      "\n",
      "Epoch:  1634  Learning Rate:  0.019534266620658998  Varinance:  2.5498634040765413 \n",
      "\n",
      "Epoch:  1635  Learning Rate:  0.019514742117916754  Varinance:  2.5463013383645134 \n",
      "\n",
      "Epoch:  1636  Learning Rate:  0.019495237129918252  Varinance:  2.542744248727721 \n",
      "\n",
      "Epoch:  1637  Learning Rate:  0.019475751637158506  Varinance:  2.5391921282147676 \n",
      "\n",
      "Epoch:  1638  Learning Rate:  0.019456285620152017  Varinance:  2.53564496988397 \n",
      "\n",
      "Epoch:  1639  Learning Rate:  0.019436839059432772  Varinance:  2.5321027668033422 \n",
      "\n",
      "Epoch:  1640  Learning Rate:  0.019417411935554203  Varinance:  2.528565512050581 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1641  Learning Rate:  0.019398004229089192  Varinance:  2.5250331987130537 \n",
      "\n",
      "Epoch:  1642  Learning Rate:  0.019378615920630022  Varinance:  2.5215058198877855 \n",
      "\n",
      "Epoch:  1643  Learning Rate:  0.019359246990788392  Varinance:  2.5179833686814415 \n",
      "\n",
      "Epoch:  1644  Learning Rate:  0.019339897420195363  Varinance:  2.51446583821032 \n",
      "\n",
      "Epoch:  1645  Learning Rate:  0.01932056718950137  Varinance:  2.510953221600334 \n",
      "\n",
      "Epoch:  1646  Learning Rate:  0.019301256279376174  Varinance:  2.5074455119869996 \n",
      "\n",
      "Epoch:  1647  Learning Rate:  0.019281964670508866  Varinance:  2.503942702515422 \n",
      "\n",
      "Epoch:  1648  Learning Rate:  0.019262692343607835  Varinance:  2.500444786340284 \n",
      "\n",
      "Epoch:  1649  Learning Rate:  0.019243439279400754  Varinance:  2.4969517566258292 \n",
      "\n",
      "Epoch:  1650  Learning Rate:  0.01922420545863455  Varinance:  2.49346360654585 \n",
      "\n",
      "Epoch:  1651  Learning Rate:  0.019204990862075416  Varinance:  2.489980329283677 \n",
      "\n",
      "Epoch:  1652  Learning Rate:  0.019185795470508735  Varinance:  2.486501918032162 \n",
      "\n",
      "Epoch:  1653  Learning Rate:  0.01916661926473913  Varinance:  2.483028365993667 \n",
      "\n",
      "Epoch:  1654  Learning Rate:  0.019147462225590384  Varinance:  2.479559666380049 \n",
      "\n",
      "Epoch:  1655  Learning Rate:  0.01912832433390546  Varinance:  2.4760958124126495 \n",
      "\n",
      "Epoch:  1656  Learning Rate:  0.019109205570546465  Varinance:  2.472636797322276 \n",
      "\n",
      "Epoch:  1657  Learning Rate:  0.019090105916394637  Varinance:  2.469182614349196 \n",
      "\n",
      "Epoch:  1658  Learning Rate:  0.019071025352350307  Varinance:  2.465733256743119 \n",
      "\n",
      "Epoch:  1659  Learning Rate:  0.019051963859332926  Varinance:  2.4622887177631836 \n",
      "\n",
      "Epoch:  1660  Learning Rate:  0.019032921418280987  Varinance:  2.458848990677946 \n",
      "\n",
      "Epoch:  1661  Learning Rate:  0.019013898010152058  Varinance:  2.455414068765366 \n",
      "\n",
      "Epoch:  1662  Learning Rate:  0.018994893615922714  Varinance:  2.451983945312794 \n",
      "\n",
      "Epoch:  1663  Learning Rate:  0.01897590821658858  Varinance:  2.4485586136169557 \n",
      "\n",
      "Epoch:  1664  Learning Rate:  0.018956941793164234  Varinance:  2.445138066983943 \n",
      "\n",
      "Epoch:  1665  Learning Rate:  0.01893799432668327  Varinance:  2.441722298729199 \n",
      "\n",
      "Epoch:  1666  Learning Rate:  0.018919065798198204  Varinance:  2.438311302177504 \n",
      "\n",
      "Epoch:  1667  Learning Rate:  0.018900156188780517  Varinance:  2.4349050706629645 \n",
      "\n",
      "Epoch:  1668  Learning Rate:  0.01888126547952059  Varinance:  2.4315035975289976 \n",
      "\n",
      "Epoch:  1669  Learning Rate:  0.018862393651527722  Varinance:  2.428106876128321 \n",
      "\n",
      "Epoch:  1670  Learning Rate:  0.01884354068593007  Varinance:  2.424714899822936 \n",
      "\n",
      "Epoch:  1671  Learning Rate:  0.01882470656387468  Varinance:  2.421327661984119 \n",
      "\n",
      "Epoch:  1672  Learning Rate:  0.018805891266527416  Varinance:  2.4179451559924057 \n",
      "\n",
      "Epoch:  1673  Learning Rate:  0.018787094775072996  Varinance:  2.4145673752375796 \n",
      "\n",
      "Epoch:  1674  Learning Rate:  0.018768317070714906  Varinance:  2.4111943131186577 \n",
      "\n",
      "Epoch:  1675  Learning Rate:  0.018749558134675458  Varinance:  2.40782596304388 \n",
      "\n",
      "Epoch:  1676  Learning Rate:  0.018730817948195703  Varinance:  2.4044623184306912 \n",
      "\n",
      "Epoch:  1677  Learning Rate:  0.01871209649253546  Varinance:  2.401103372705735 \n",
      "\n",
      "Epoch:  1678  Learning Rate:  0.018693393748973264  Varinance:  2.3977491193048364 \n",
      "\n",
      "Epoch:  1679  Learning Rate:  0.018674709698806382  Varinance:  2.3943995516729912 \n",
      "\n",
      "Epoch:  1680  Learning Rate:  0.01865604432335075  Varinance:  2.3910546632643515 \n",
      "\n",
      "Epoch:  1681  Learning Rate:  0.018637397603940998  Varinance:  2.3877144475422147 \n",
      "\n",
      "Epoch:  1682  Learning Rate:  0.018618769521930402  Varinance:  2.384378897979009 \n",
      "\n",
      "Epoch:  1683  Learning Rate:  0.01860016005869088  Varinance:  2.3810480080562795 \n",
      "\n",
      "Epoch:  1684  Learning Rate:  0.018581569195612966  Varinance:  2.3777217712646808 \n",
      "\n",
      "Epoch:  1685  Learning Rate:  0.0185629969141058  Varinance:  2.3744001811039595 \n",
      "\n",
      "Epoch:  1686  Learning Rate:  0.018544443195597088  Varinance:  2.3710832310829417 \n",
      "\n",
      "Epoch:  1687  Learning Rate:  0.018525908021533123  Varinance:  2.367770914719523 \n",
      "\n",
      "Epoch:  1688  Learning Rate:  0.01850739137337872  Varinance:  2.3644632255406544 \n",
      "\n",
      "Epoch:  1689  Learning Rate:  0.018488893232617234  Varinance:  2.3611601570823266 \n",
      "\n",
      "Epoch:  1690  Learning Rate:  0.01847041358075052  Varinance:  2.357861702889563 \n",
      "\n",
      "Epoch:  1691  Learning Rate:  0.018451952399298928  Varinance:  2.3545678565164048 \n",
      "\n",
      "Epoch:  1692  Learning Rate:  0.01843350966980127  Varinance:  2.351278611525896 \n",
      "\n",
      "Epoch:  1693  Learning Rate:  0.01841508537381482  Varinance:  2.347993961490074 \n",
      "\n",
      "Epoch:  1694  Learning Rate:  0.018396679492915277  Varinance:  2.344713899989955 \n",
      "\n",
      "Epoch:  1695  Learning Rate:  0.018378292008696766  Varinance:  2.3414384206155248 \n",
      "\n",
      "Epoch:  1696  Learning Rate:  0.018359922902771785  Varinance:  2.338167516965719 \n",
      "\n",
      "Epoch:  1697  Learning Rate:  0.018341572156771246  Varinance:  2.33490118264842 \n",
      "\n",
      "Epoch:  1698  Learning Rate:  0.018323239752344386  Varinance:  2.331639411280436 \n",
      "\n",
      "Epoch:  1699  Learning Rate:  0.018304925671158812  Varinance:  2.3283821964874956 \n",
      "\n",
      "Epoch:  1700  Learning Rate:  0.018286629894900427  Varinance:  2.32512953190423 \n",
      "\n",
      "Epoch:  1701  Learning Rate:  0.018268352405273466  Varinance:  2.3218814111741635 \n",
      "\n",
      "Epoch:  1702  Learning Rate:  0.01825009318400043  Varinance:  2.3186378279496997 \n",
      "\n",
      "Epoch:  1703  Learning Rate:  0.0182318522128221  Varinance:  2.3153987758921084 \n",
      "\n",
      "Epoch:  1704  Learning Rate:  0.0182136294734975  Varinance:  2.3121642486715164 \n",
      "\n",
      "Epoch:  1705  Learning Rate:  0.018195424947803896  Varinance:  2.3089342399668924 \n",
      "\n",
      "Epoch:  1706  Learning Rate:  0.018177238617536753  Varinance:  2.305708743466034 \n",
      "\n",
      "Epoch:  1707  Learning Rate:  0.018159070464509743  Varinance:  2.3024877528655603 \n",
      "\n",
      "Epoch:  1708  Learning Rate:  0.018140920470554708  Varinance:  2.2992712618708926 \n",
      "\n",
      "Epoch:  1709  Learning Rate:  0.01812278861752166  Varinance:  2.2960592641962454 \n",
      "\n",
      "Epoch:  1710  Learning Rate:  0.018104674887278734  Varinance:  2.2928517535646162 \n",
      "\n",
      "Epoch:  1711  Learning Rate:  0.01808657926171221  Varinance:  2.2896487237077703 \n",
      "\n",
      "Epoch:  1712  Learning Rate:  0.01806850172272645  Varinance:  2.2864501683662293 \n",
      "\n",
      "Epoch:  1713  Learning Rate:  0.018050442252243924  Varinance:  2.28325608128926 \n",
      "\n",
      "Epoch:  1714  Learning Rate:  0.01803240083220515  Varinance:  2.28006645623486 \n",
      "\n",
      "Epoch:  1715  Learning Rate:  0.01801437744456871  Varinance:  2.2768812869697483 \n",
      "\n",
      "Epoch:  1716  Learning Rate:  0.017996372071311217  Varinance:  2.273700567269349 \n",
      "\n",
      "Epoch:  1717  Learning Rate:  0.017978384694427297  Varinance:  2.2705242909177845 \n",
      "\n",
      "Epoch:  1718  Learning Rate:  0.017960415295929566  Varinance:  2.2673524517078585 \n",
      "\n",
      "Epoch:  1719  Learning Rate:  0.017942463857848635  Varinance:  2.264185043441047 \n",
      "\n",
      "Epoch:  1720  Learning Rate:  0.01792453036223305  Varinance:  2.2610220599274857 \n",
      "\n",
      "Epoch:  1721  Learning Rate:  0.017906614791149324  Varinance:  2.257863494985956 \n",
      "\n",
      "Epoch:  1722  Learning Rate:  0.01788871712668188  Varinance:  2.2547093424438747 \n",
      "\n",
      "Epoch:  1723  Learning Rate:  0.017870837350933054  Varinance:  2.2515595961372807 \n",
      "\n",
      "Epoch:  1724  Learning Rate:  0.017852975446023066  Varinance:  2.2484142499108257 \n",
      "\n",
      "Epoch:  1725  Learning Rate:  0.017835131394090015  Varinance:  2.245273297617758 \n",
      "\n",
      "Epoch:  1726  Learning Rate:  0.01781730517728984  Varinance:  2.242136733119915 \n",
      "\n",
      "Epoch:  1727  Learning Rate:  0.017799496777796336  Varinance:  2.2390045502877065 \n",
      "\n",
      "Epoch:  1728  Learning Rate:  0.017781706177801084  Varinance:  2.2358767430001074 \n",
      "\n",
      "Epoch:  1729  Learning Rate:  0.017763933359513494  Varinance:  2.232753305144641 \n",
      "\n",
      "Epoch:  1730  Learning Rate:  0.017746178305160745  Varinance:  2.229634230617371 \n",
      "\n",
      "Epoch:  1731  Learning Rate:  0.017728440996987782  Varinance:  2.226519513322888 \n",
      "\n",
      "Epoch:  1732  Learning Rate:  0.01771072141725729  Varinance:  2.2234091471742974 \n",
      "\n",
      "Epoch:  1733  Learning Rate:  0.017693019548249693  Varinance:  2.2203031260932087 \n",
      "\n",
      "Epoch:  1734  Learning Rate:  0.017675335372263117  Varinance:  2.2172014440097207 \n",
      "\n",
      "Epoch:  1735  Learning Rate:  0.01765766887161339  Varinance:  2.2141040948624147 \n",
      "\n",
      "Epoch:  1736  Learning Rate:  0.017640020028634  Varinance:  2.211011072598335 \n",
      "\n",
      "Epoch:  1737  Learning Rate:  0.01762238882567611  Varinance:  2.2079223711729856 \n",
      "\n",
      "Epoch:  1738  Learning Rate:  0.017604775245108516  Varinance:  2.2048379845503137 \n",
      "\n",
      "Epoch:  1739  Learning Rate:  0.01758717926931764  Varinance:  2.201757906702697 \n",
      "\n",
      "Epoch:  1740  Learning Rate:  0.017569600880707487  Varinance:  2.1986821316109357 \n",
      "\n",
      "Epoch:  1741  Learning Rate:  0.01755204006169969  Varinance:  2.195610653264238 \n",
      "\n",
      "Epoch:  1742  Learning Rate:  0.01753449679473341  Varinance:  2.192543465660207 \n",
      "\n",
      "Epoch:  1743  Learning Rate:  0.01751697106226539  Varinance:  2.189480562804833 \n",
      "\n",
      "Epoch:  1744  Learning Rate:  0.017499462846769887  Varinance:  2.1864219387124795 \n",
      "\n",
      "Epoch:  1745  Learning Rate:  0.017481972130738693  Varinance:  2.1833675874058716 \n",
      "\n",
      "Epoch:  1746  Learning Rate:  0.017464498896681085  Varinance:  2.1803175029160835 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1747  Learning Rate:  0.01744704312712383  Varinance:  2.177271679282529 \n",
      "\n",
      "Epoch:  1748  Learning Rate:  0.017429604804611157  Varinance:  2.1742301105529487 \n",
      "\n",
      "Epoch:  1749  Learning Rate:  0.01741218391170474  Varinance:  2.171192790783396 \n",
      "\n",
      "Epoch:  1750  Learning Rate:  0.017394780430983685  Varinance:  2.1681597140382314 \n",
      "\n",
      "Epoch:  1751  Learning Rate:  0.017377394345044515  Varinance:  2.165130874390104 \n",
      "\n",
      "Epoch:  1752  Learning Rate:  0.017360025636501134  Varinance:  2.162106265919945 \n",
      "\n",
      "Epoch:  1753  Learning Rate:  0.017342674287984836  Varinance:  2.1590858827169535 \n",
      "\n",
      "Epoch:  1754  Learning Rate:  0.01732534028214427  Varinance:  2.156069718878588 \n",
      "\n",
      "Epoch:  1755  Learning Rate:  0.017308023601645434  Varinance:  2.153057768510548 \n",
      "\n",
      "Epoch:  1756  Learning Rate:  0.01729072422917164  Varinance:  2.1500500257267716 \n",
      "\n",
      "Epoch:  1757  Learning Rate:  0.017273442147423514  Varinance:  2.147046484649417 \n",
      "\n",
      "Epoch:  1758  Learning Rate:  0.017256177339118977  Varinance:  2.144047139408855 \n",
      "\n",
      "Epoch:  1759  Learning Rate:  0.017238929786993217  Varinance:  2.1410519841436555 \n",
      "\n",
      "Epoch:  1760  Learning Rate:  0.017221699473798684  Varinance:  2.1380610130005757 \n",
      "\n",
      "Epoch:  1761  Learning Rate:  0.017204486382305054  Varinance:  2.135074220134552 \n",
      "\n",
      "Epoch:  1762  Learning Rate:  0.017187290495299243  Varinance:  2.1320915997086822 \n",
      "\n",
      "Epoch:  1763  Learning Rate:  0.01717011179558536  Varinance:  2.1291131458942214 \n",
      "\n",
      "Epoch:  1764  Learning Rate:  0.017152950265984703  Varinance:  2.1261388528705667 \n",
      "\n",
      "Epoch:  1765  Learning Rate:  0.017135805889335737  Varinance:  2.123168714825245 \n",
      "\n",
      "Epoch:  1766  Learning Rate:  0.017118678648494097  Varinance:  2.120202725953904 \n",
      "\n",
      "Epoch:  1767  Learning Rate:  0.017101568526332524  Varinance:  2.1172408804603005 \n",
      "\n",
      "Epoch:  1768  Learning Rate:  0.017084475505740906  Varinance:  2.1142831725562883 \n",
      "\n",
      "Epoch:  1769  Learning Rate:  0.017067399569626215  Varinance:  2.111329596461804 \n",
      "\n",
      "Epoch:  1770  Learning Rate:  0.01705034070091252  Varinance:  2.1083801464048624 \n",
      "\n",
      "Epoch:  1771  Learning Rate:  0.017033298882540942  Varinance:  2.1054348166215404 \n",
      "\n",
      "Epoch:  1772  Learning Rate:  0.017016274097469673  Varinance:  2.1024936013559663 \n",
      "\n",
      "Epoch:  1773  Learning Rate:  0.016999266328673913  Varinance:  2.0995564948603103 \n",
      "\n",
      "Epoch:  1774  Learning Rate:  0.0169822755591459  Varinance:  2.096623491394772 \n",
      "\n",
      "Epoch:  1775  Learning Rate:  0.016965301771894863  Varinance:  2.093694585227567 \n",
      "\n",
      "Epoch:  1776  Learning Rate:  0.01694834494994701  Varinance:  2.090769770634921 \n",
      "\n",
      "Epoch:  1777  Learning Rate:  0.01693140507634552  Varinance:  2.0878490419010536 \n",
      "\n",
      "Epoch:  1778  Learning Rate:  0.01691448213415052  Varinance:  2.08493239331817 \n",
      "\n",
      "Epoch:  1779  Learning Rate:  0.01689757610643906  Varinance:  2.0820198191864487 \n",
      "\n",
      "Epoch:  1780  Learning Rate:  0.016880686976305116  Varinance:  2.0791113138140314 \n",
      "\n",
      "Epoch:  1781  Learning Rate:  0.01686381472685955  Varinance:  2.0762068715170114 \n",
      "\n",
      "Epoch:  1782  Learning Rate:  0.016846959341230122  Varinance:  2.0733064866194186 \n",
      "\n",
      "Epoch:  1783  Learning Rate:  0.016830120802561438  Varinance:  2.070410153453216 \n",
      "\n",
      "Epoch:  1784  Learning Rate:  0.016813299094014963  Varinance:  2.067517866358284 \n",
      "\n",
      "Epoch:  1785  Learning Rate:  0.016796494198768976  Varinance:  2.0646296196824085 \n",
      "\n",
      "Epoch:  1786  Learning Rate:  0.016779706100018593  Varinance:  2.0617454077812725 \n",
      "\n",
      "Epoch:  1787  Learning Rate:  0.0167629347809757  Varinance:  2.0588652250184447 \n",
      "\n",
      "Epoch:  1788  Learning Rate:  0.016746180224868994  Varinance:  2.0559890657653646 \n",
      "\n",
      "Epoch:  1789  Learning Rate:  0.016729442414943903  Varinance:  2.0531169244013374 \n",
      "\n",
      "Epoch:  1790  Learning Rate:  0.016712721334462624  Varinance:  2.0502487953135193 \n",
      "\n",
      "Epoch:  1791  Learning Rate:  0.01669601696670407  Varinance:  2.0473846728969076 \n",
      "\n",
      "Epoch:  1792  Learning Rate:  0.016679329294963876  Varinance:  2.0445245515543298 \n",
      "\n",
      "Epoch:  1793  Learning Rate:  0.016662658302554364  Varinance:  2.0416684256964315 \n",
      "\n",
      "Epoch:  1794  Learning Rate:  0.016646003972804548  Varinance:  2.0388162897416686 \n",
      "\n",
      "Epoch:  1795  Learning Rate:  0.016629366289060088  Varinance:  2.03596813811629 \n",
      "\n",
      "Epoch:  1796  Learning Rate:  0.01661274523468331  Varinance:  2.0331239652543354 \n",
      "\n",
      "Epoch:  1797  Learning Rate:  0.01659614079305314  Varinance:  2.0302837655976176 \n",
      "\n",
      "Epoch:  1798  Learning Rate:  0.016579552947565152  Varinance:  2.027447533595715 \n",
      "\n",
      "Epoch:  1799  Learning Rate:  0.016562981681631492  Varinance:  2.024615263705959 \n",
      "\n",
      "Epoch:  1800  Learning Rate:  0.016546426978680896  Varinance:  2.0217869503934236 \n",
      "\n",
      "Epoch:  1801  Learning Rate:  0.016529888822158653  Varinance:  2.0189625881309174 \n",
      "\n",
      "Epoch:  1802  Learning Rate:  0.016513367195526617  Varinance:  2.0161421713989656 \n",
      "\n",
      "Epoch:  1803  Learning Rate:  0.016496862082263145  Varinance:  2.013325694685808 \n",
      "\n",
      "Epoch:  1804  Learning Rate:  0.016480373465863136  Varinance:  2.010513152487383 \n",
      "\n",
      "Epoch:  1805  Learning Rate:  0.016463901329837962  Varinance:  2.007704539307317 \n",
      "\n",
      "Epoch:  1806  Learning Rate:  0.01644744565771549  Varinance:  2.004899849656916 \n",
      "\n",
      "Epoch:  1807  Learning Rate:  0.016431006433040046  Varinance:  2.0020990780551537 \n",
      "\n",
      "Epoch:  1808  Learning Rate:  0.01641458363937241  Varinance:  1.9993022190286573 \n",
      "\n",
      "Epoch:  1809  Learning Rate:  0.016398177260289772  Varinance:  1.9965092671117048 \n",
      "\n",
      "Epoch:  1810  Learning Rate:  0.01638178727938577  Varinance:  1.993720216846206 \n",
      "\n",
      "Epoch:  1811  Learning Rate:  0.016365413680270405  Varinance:  1.9909350627816975 \n",
      "\n",
      "Epoch:  1812  Learning Rate:  0.01634905644657009  Varinance:  1.9881537994753296 \n",
      "\n",
      "Epoch:  1813  Learning Rate:  0.016332715561927582  Varinance:  1.9853764214918548 \n",
      "\n",
      "Epoch:  1814  Learning Rate:  0.016316391010001995  Varinance:  1.9826029234036213 \n",
      "\n",
      "Epoch:  1815  Learning Rate:  0.016300082774468778  Varinance:  1.9798332997905554 \n",
      "\n",
      "Epoch:  1816  Learning Rate:  0.016283790839019697  Varinance:  1.9770675452401587 \n",
      "\n",
      "Epoch:  1817  Learning Rate:  0.01626751518736281  Varinance:  1.9743056543474917 \n",
      "\n",
      "Epoch:  1818  Learning Rate:  0.016251255803222467  Varinance:  1.9715476217151666 \n",
      "\n",
      "Epoch:  1819  Learning Rate:  0.016235012670339277  Varinance:  1.9687934419533353 \n",
      "\n",
      "Epoch:  1820  Learning Rate:  0.01621878577247012  Varinance:  1.9660431096796795 \n",
      "\n",
      "Epoch:  1821  Learning Rate:  0.016202575093388075  Varinance:  1.9632966195193973 \n",
      "\n",
      "Epoch:  1822  Learning Rate:  0.016186380616882483  Varinance:  1.960553966105199 \n",
      "\n",
      "Epoch:  1823  Learning Rate:  0.01617020232675885  Varinance:  1.95781514407729 \n",
      "\n",
      "Epoch:  1824  Learning Rate:  0.016154040206838898  Varinance:  1.955080148083364 \n",
      "\n",
      "Epoch:  1825  Learning Rate:  0.016137894240960494  Varinance:  1.9523489727785923 \n",
      "\n",
      "Epoch:  1826  Learning Rate:  0.016121764412977677  Varinance:  1.9496216128256125 \n",
      "\n",
      "Epoch:  1827  Learning Rate:  0.016105650706760618  Varinance:  1.946898062894518 \n",
      "\n",
      "Epoch:  1828  Learning Rate:  0.016089553106195607  Varinance:  1.9441783176628467 \n",
      "\n",
      "Epoch:  1829  Learning Rate:  0.01607347159518504  Varinance:  1.9414623718155735 \n",
      "\n",
      "Epoch:  1830  Learning Rate:  0.016057406157647412  Varinance:  1.938750220045098 \n",
      "\n",
      "Epoch:  1831  Learning Rate:  0.016041356777517276  Varinance:  1.9360418570512337 \n",
      "\n",
      "Epoch:  1832  Learning Rate:  0.016025323438745256  Varinance:  1.9333372775411986 \n",
      "\n",
      "Epoch:  1833  Learning Rate:  0.01600930612529801  Varinance:  1.9306364762296049 \n",
      "\n",
      "Epoch:  1834  Learning Rate:  0.015993304821158225  Varinance:  1.927939447838448 \n",
      "\n",
      "Epoch:  1835  Learning Rate:  0.015977319510324592  Varinance:  1.925246187097095 \n",
      "\n",
      "Epoch:  1836  Learning Rate:  0.015961350176811804  Varinance:  1.9225566887422785 \n",
      "\n",
      "Epoch:  1837  Learning Rate:  0.015945396804650517  Varinance:  1.9198709475180815 \n",
      "\n",
      "Epoch:  1838  Learning Rate:  0.01592945937788737  Varinance:  1.917188958175931 \n",
      "\n",
      "Epoch:  1839  Learning Rate:  0.015913537880584923  Varinance:  1.9145107154745848 \n",
      "\n",
      "Epoch:  1840  Learning Rate:  0.015897632296821687  Varinance:  1.9118362141801244 \n",
      "\n",
      "Epoch:  1841  Learning Rate:  0.015881742610692067  Varinance:  1.9091654490659395 \n",
      "\n",
      "Epoch:  1842  Learning Rate:  0.015865868806306388  Varinance:  1.9064984149127238 \n",
      "\n",
      "Epoch:  1843  Learning Rate:  0.015850010867790833  Varinance:  1.903835106508462 \n",
      "\n",
      "Epoch:  1844  Learning Rate:  0.01583416877928747  Varinance:  1.9011755186484194 \n",
      "\n",
      "Epoch:  1845  Learning Rate:  0.0158183425249542  Varinance:  1.8985196461351317 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1846  Learning Rate:  0.01580253208896478  Varinance:  1.8958674837783962 \n",
      "\n",
      "Epoch:  1847  Learning Rate:  0.01578673745550876  Varinance:  1.8932190263952604 \n",
      "\n",
      "Epoch:  1848  Learning Rate:  0.015770958608791515  Varinance:  1.8905742688100107 \n",
      "\n",
      "Epoch:  1849  Learning Rate:  0.015755195533034193  Varinance:  1.887933205854166 \n",
      "\n",
      "Epoch:  1850  Learning Rate:  0.015739448212473715  Varinance:  1.8852958323664644 \n",
      "\n",
      "Epoch:  1851  Learning Rate:  0.015723716631362763  Varinance:  1.8826621431928536 \n",
      "\n",
      "Epoch:  1852  Learning Rate:  0.01570800077396975  Varinance:  1.8800321331864827 \n",
      "\n",
      "Epoch:  1853  Learning Rate:  0.015692300624578822  Varinance:  1.87740579720769 \n",
      "\n",
      "Epoch:  1854  Learning Rate:  0.01567661616748983  Varinance:  1.8747831301239928 \n",
      "\n",
      "Epoch:  1855  Learning Rate:  0.0156609473870183  Varinance:  1.8721641268100793 \n",
      "\n",
      "Epoch:  1856  Learning Rate:  0.015645294267495474  Varinance:  1.8695487821477972 \n",
      "\n",
      "Epoch:  1857  Learning Rate:  0.015629656793268214  Varinance:  1.8669370910261451 \n",
      "\n",
      "Epoch:  1858  Learning Rate:  0.015614034948699052  Varinance:  1.8643290483412602 \n",
      "\n",
      "Epoch:  1859  Learning Rate:  0.015598428718166135  Varinance:  1.86172464899641 \n",
      "\n",
      "Epoch:  1860  Learning Rate:  0.015582838086063242  Varinance:  1.859123887901983 \n",
      "\n",
      "Epoch:  1861  Learning Rate:  0.015567263036799731  Varinance:  1.856526759975475 \n",
      "\n",
      "Epoch:  1862  Learning Rate:  0.015551703554800556  Varinance:  1.8539332601414844 \n",
      "\n",
      "Epoch:  1863  Learning Rate:  0.015536159624506227  Varinance:  1.8513433833316988 \n",
      "\n",
      "Epoch:  1864  Learning Rate:  0.015520631230372823  Varinance:  1.8487571244848864 \n",
      "\n",
      "Epoch:  1865  Learning Rate:  0.015505118356871937  Varinance:  1.8461744785468854 \n",
      "\n",
      "Epoch:  1866  Learning Rate:  0.015489620988490705  Varinance:  1.843595440470595 \n",
      "\n",
      "Epoch:  1867  Learning Rate:  0.01547413910973175  Varinance:  1.8410200052159649 \n",
      "\n",
      "Epoch:  1868  Learning Rate:  0.015458672705113195  Varinance:  1.8384481677499842 \n",
      "\n",
      "Epoch:  1869  Learning Rate:  0.015443221759168633  Varinance:  1.8358799230466745 \n",
      "\n",
      "Epoch:  1870  Learning Rate:  0.015427786256447116  Varinance:  1.833315266087078 \n",
      "\n",
      "Epoch:  1871  Learning Rate:  0.015412366181513142  Varinance:  1.8307541918592483 \n",
      "\n",
      "Epoch:  1872  Learning Rate:  0.015396961518946635  Varinance:  1.8281966953582405 \n",
      "\n",
      "Epoch:  1873  Learning Rate:  0.015381572253342926  Varinance:  1.825642771586102 \n",
      "\n",
      "Epoch:  1874  Learning Rate:  0.015366198369312759  Varinance:  1.8230924155518604 \n",
      "\n",
      "Epoch:  1875  Learning Rate:  0.015350839851482235  Varinance:  1.8205456222715168 \n",
      "\n",
      "Epoch:  1876  Learning Rate:  0.015335496684492848  Varinance:  1.818002386768035 \n",
      "\n",
      "Epoch:  1877  Learning Rate:  0.015320168853001422  Varinance:  1.8154627040713314 \n",
      "\n",
      "Epoch:  1878  Learning Rate:  0.01530485634168012  Varinance:  1.8129265692182643 \n",
      "\n",
      "Epoch:  1879  Learning Rate:  0.015289559135216442  Varinance:  1.8103939772526267 \n",
      "\n",
      "Epoch:  1880  Learning Rate:  0.015274277218313169  Varinance:  1.8078649232251358 \n",
      "\n",
      "Epoch:  1881  Learning Rate:  0.01525901057568839  Varinance:  1.8053394021934197 \n",
      "\n",
      "Epoch:  1882  Learning Rate:  0.015243759192075455  Varinance:  1.802817409222014 \n",
      "\n",
      "Epoch:  1883  Learning Rate:  0.015228523052222984  Varinance:  1.8002989393823468 \n",
      "\n",
      "Epoch:  1884  Learning Rate:  0.015213302140894834  Varinance:  1.797783987752733 \n",
      "\n",
      "Epoch:  1885  Learning Rate:  0.015198096442870094  Varinance:  1.7952725494183623 \n",
      "\n",
      "Epoch:  1886  Learning Rate:  0.01518290594294306  Varinance:  1.7927646194712903 \n",
      "\n",
      "Epoch:  1887  Learning Rate:  0.015167730625923241  Varinance:  1.7902601930104272 \n",
      "\n",
      "Epoch:  1888  Learning Rate:  0.015152570476635303  Varinance:  1.787759265141532 \n",
      "\n",
      "Epoch:  1889  Learning Rate:  0.015137425479919113  Varinance:  1.7852618309771997 \n",
      "\n",
      "Epoch:  1890  Learning Rate:  0.015122295620629656  Varinance:  1.7827678856368534 \n",
      "\n",
      "Epoch:  1891  Learning Rate:  0.015107180883637087  Varinance:  1.7802774242467339 \n",
      "\n",
      "Epoch:  1892  Learning Rate:  0.015092081253826656  Varinance:  1.7777904419398898 \n",
      "\n",
      "Epoch:  1893  Learning Rate:  0.01507699671609874  Varinance:  1.7753069338561707 \n",
      "\n",
      "Epoch:  1894  Learning Rate:  0.015061927255368793  Varinance:  1.7728268951422126 \n",
      "\n",
      "Epoch:  1895  Learning Rate:  0.01504687285656736  Varinance:  1.770350320951434 \n",
      "\n",
      "Epoch:  1896  Learning Rate:  0.015031833504640033  Varinance:  1.7678772064440227 \n",
      "\n",
      "Epoch:  1897  Learning Rate:  0.015016809184547467  Varinance:  1.7654075467869277 \n",
      "\n",
      "Epoch:  1898  Learning Rate:  0.015001799881265335  Varinance:  1.76294133715385 \n",
      "\n",
      "Epoch:  1899  Learning Rate:  0.014986805579784336  Varinance:  1.760478572725233 \n",
      "\n",
      "Epoch:  1900  Learning Rate:  0.014971826265110164  Varinance:  1.7580192486882522 \n",
      "\n",
      "Epoch:  1901  Learning Rate:  0.014956861922263507  Varinance:  1.7555633602368055 \n",
      "\n",
      "Epoch:  1902  Learning Rate:  0.014941912536280017  Varinance:  1.7531109025715068 \n",
      "\n",
      "Epoch:  1903  Learning Rate:  0.01492697809221031  Varinance:  1.7506618708996735 \n",
      "\n",
      "Epoch:  1904  Learning Rate:  0.014912058575119937  Varinance:  1.7482162604353177 \n",
      "\n",
      "Epoch:  1905  Learning Rate:  0.014897153970089384  Varinance:  1.745774066399139 \n",
      "\n",
      "Epoch:  1906  Learning Rate:  0.014882264262214038  Varinance:  1.7433352840185121 \n",
      "\n",
      "Epoch:  1907  Learning Rate:  0.0148673894366042  Varinance:  1.7408999085274788 \n",
      "\n",
      "Epoch:  1908  Learning Rate:  0.014852529478385033  Varinance:  1.7384679351667394 \n",
      "\n",
      "Epoch:  1909  Learning Rate:  0.014837684372696586  Varinance:  1.7360393591836425 \n",
      "\n",
      "Epoch:  1910  Learning Rate:  0.014822854104693745  Varinance:  1.7336141758321766 \n",
      "\n",
      "Epoch:  1911  Learning Rate:  0.014808038659546247  Varinance:  1.731192380372959 \n",
      "\n",
      "Epoch:  1912  Learning Rate:  0.014793238022438641  Varinance:  1.7287739680732286 \n",
      "\n",
      "Epoch:  1913  Learning Rate:  0.01477845217857029  Varinance:  1.7263589342068366 \n",
      "\n",
      "Epoch:  1914  Learning Rate:  0.014763681113155347  Varinance:  1.7239472740542339 \n",
      "\n",
      "Epoch:  1915  Learning Rate:  0.014748924811422752  Varinance:  1.7215389829024663 \n",
      "\n",
      "Epoch:  1916  Learning Rate:  0.014734183258616194  Varinance:  1.719134056045164 \n",
      "\n",
      "Epoch:  1917  Learning Rate:  0.014719456439994125  Varinance:  1.7167324887825302 \n",
      "\n",
      "Epoch:  1918  Learning Rate:  0.01470474434082972  Varinance:  1.7143342764213345 \n",
      "\n",
      "Epoch:  1919  Learning Rate:  0.014690046946410885  Varinance:  1.7119394142749031 \n",
      "\n",
      "Epoch:  1920  Learning Rate:  0.014675364242040216  Varinance:  1.709547897663108 \n",
      "\n",
      "Epoch:  1921  Learning Rate:  0.014660696213035016  Varinance:  1.7071597219123595 \n",
      "\n",
      "Epoch:  1922  Learning Rate:  0.014646042844727248  Varinance:  1.7047748823555986 \n",
      "\n",
      "Epoch:  1923  Learning Rate:  0.014631404122463547  Varinance:  1.7023933743322834 \n",
      "\n",
      "Epoch:  1924  Learning Rate:  0.014616780031605185  Varinance:  1.7000151931883847 \n",
      "\n",
      "Epoch:  1925  Learning Rate:  0.014602170557528075  Varinance:  1.6976403342763728 \n",
      "\n",
      "Epoch:  1926  Learning Rate:  0.014587575685622738  Varinance:  1.6952687929552135 \n",
      "\n",
      "Epoch:  1927  Learning Rate:  0.014572995401294303  Varinance:  1.6929005645903519 \n",
      "\n",
      "Epoch:  1928  Learning Rate:  0.014558429689962482  Varinance:  1.69053564455371 \n",
      "\n",
      "Epoch:  1929  Learning Rate:  0.01454387853706157  Varinance:  1.6881740282236748 \n",
      "\n",
      "Epoch:  1930  Learning Rate:  0.014529341928040403  Varinance:  1.6858157109850895 \n",
      "\n",
      "Epoch:  1931  Learning Rate:  0.014514819848362373  Varinance:  1.6834606882292436 \n",
      "\n",
      "Epoch:  1932  Learning Rate:  0.014500312283505401  Varinance:  1.681108955353866 \n",
      "\n",
      "Epoch:  1933  Learning Rate:  0.014485819218961927  Varinance:  1.6787605077631151 \n",
      "\n",
      "Epoch:  1934  Learning Rate:  0.014471340640238873  Varinance:  1.6764153408675673 \n",
      "\n",
      "Epoch:  1935  Learning Rate:  0.014456876532857668  Varinance:  1.6740734500842118 \n",
      "\n",
      "Epoch:  1936  Learning Rate:  0.014442426882354198  Varinance:  1.6717348308364404 \n",
      "\n",
      "Epoch:  1937  Learning Rate:  0.014427991674278818  Varinance:  1.6693994785540371 \n",
      "\n",
      "Epoch:  1938  Learning Rate:  0.01441357089419631  Varinance:  1.6670673886731724 \n",
      "\n",
      "Epoch:  1939  Learning Rate:  0.014399164527685901  Varinance:  1.6647385566363904 \n",
      "\n",
      "Epoch:  1940  Learning Rate:  0.014384772560341217  Varinance:  1.6624129778926016 \n",
      "\n",
      "Epoch:  1941  Learning Rate:  0.014370394977770293  Varinance:  1.6600906478970754 \n",
      "\n",
      "Epoch:  1942  Learning Rate:  0.014356031765595544  Varinance:  1.6577715621114293 \n",
      "\n",
      "Epoch:  1943  Learning Rate:  0.014341682909453757  Varinance:  1.6554557160036214 \n",
      "\n",
      "Epoch:  1944  Learning Rate:  0.014327348394996074  Varinance:  1.6531431050479404 \n",
      "\n",
      "Epoch:  1945  Learning Rate:  0.014313028207887983  Varinance:  1.6508337247249976 \n",
      "\n",
      "Epoch:  1946  Learning Rate:  0.014298722333809289  Varinance:  1.6485275705217173 \n",
      "\n",
      "Epoch:  1947  Learning Rate:  0.014284430758454123  Varinance:  1.6462246379313286 \n",
      "\n",
      "Epoch:  1948  Learning Rate:  0.014270153467530904  Varinance:  1.6439249224533559 \n",
      "\n",
      "Epoch:  1949  Learning Rate:  0.014255890446762343  Varinance:  1.641628419593611 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1950  Learning Rate:  0.014241641681885416  Varinance:  1.639335124864185 \n",
      "\n",
      "Epoch:  1951  Learning Rate:  0.01422740715865136  Varinance:  1.6370450337834366 \n",
      "\n",
      "Epoch:  1952  Learning Rate:  0.014213186862825642  Varinance:  1.6347581418759873 \n",
      "\n",
      "Epoch:  1953  Learning Rate:  0.014198980780187978  Varinance:  1.6324744446727073 \n",
      "\n",
      "Epoch:  1954  Learning Rate:  0.014184788896532272  Varinance:  1.630193937710713 \n",
      "\n",
      "Epoch:  1955  Learning Rate:  0.01417061119766665  Varinance:  1.6279166165333547 \n",
      "\n",
      "Epoch:  1956  Learning Rate:  0.014156447669413402  Varinance:  1.6256424766902071 \n",
      "\n",
      "Epoch:  1957  Learning Rate:  0.014142298297609008  Varinance:  1.6233715137370635 \n",
      "\n",
      "Epoch:  1958  Learning Rate:  0.014128163068104086  Varinance:  1.6211037232359247 \n",
      "\n",
      "Epoch:  1959  Learning Rate:  0.014114041966763411  Varinance:  1.6188391007549925 \n",
      "\n",
      "Epoch:  1960  Learning Rate:  0.014099934979465876  Varinance:  1.6165776418686575 \n",
      "\n",
      "Epoch:  1961  Learning Rate:  0.0140858420921045  Varinance:  1.6143193421574942 \n",
      "\n",
      "Epoch:  1962  Learning Rate:  0.014071763290586388  Varinance:  1.612064197208251 \n",
      "\n",
      "Epoch:  1963  Learning Rate:  0.014057698560832742  Varinance:  1.6098122026138413 \n",
      "\n",
      "Epoch:  1964  Learning Rate:  0.01404364788877882  Varinance:  1.6075633539733347 \n",
      "\n",
      "Epoch:  1965  Learning Rate:  0.014029611260373965  Varinance:  1.6053176468919488 \n",
      "\n",
      "Epoch:  1966  Learning Rate:  0.014015588661581535  Varinance:  1.6030750769810416 \n",
      "\n",
      "Epoch:  1967  Learning Rate:  0.01400158007837894  Varinance:  1.6008356398580992 \n",
      "\n",
      "Epoch:  1968  Learning Rate:  0.013987585496757585  Varinance:  1.5985993311467328 \n",
      "\n",
      "Epoch:  1969  Learning Rate:  0.013973604902722895  Varinance:  1.5963661464766656 \n",
      "\n",
      "Epoch:  1970  Learning Rate:  0.013959638282294271  Varinance:  1.5941360814837269 \n",
      "\n",
      "Epoch:  1971  Learning Rate:  0.013945685621505095  Varinance:  1.5919091318098415 \n",
      "\n",
      "Epoch:  1972  Learning Rate:  0.013931746906402698  Varinance:  1.5896852931030243 \n",
      "\n",
      "Epoch:  1973  Learning Rate:  0.013917822123048373  Varinance:  1.5874645610173665 \n",
      "\n",
      "Epoch:  1974  Learning Rate:  0.013903911257517327  Varinance:  1.5852469312130326 \n",
      "\n",
      "Epoch:  1975  Learning Rate:  0.0138900142958987  Varinance:  1.58303239935625 \n",
      "\n",
      "Epoch:  1976  Learning Rate:  0.013876131224295525  Varinance:  1.5808209611192992 \n",
      "\n",
      "Epoch:  1977  Learning Rate:  0.013862262028824733  Varinance:  1.5786126121805066 \n",
      "\n",
      "Epoch:  1978  Learning Rate:  0.013848406695617122  Varinance:  1.576407348224236 \n",
      "\n",
      "Epoch:  1979  Learning Rate:  0.013834565210817364  Varinance:  1.5742051649408808 \n",
      "\n",
      "Epoch:  1980  Learning Rate:  0.013820737560583966  Varinance:  1.5720060580268522 \n",
      "\n",
      "Epoch:  1981  Learning Rate:  0.013806923731089283  Varinance:  1.5698100231845753 \n",
      "\n",
      "Epoch:  1982  Learning Rate:  0.013793123708519478  Varinance:  1.5676170561224796 \n",
      "\n",
      "Epoch:  1983  Learning Rate:  0.013779337479074536  Varinance:  1.5654271525549874 \n",
      "\n",
      "Epoch:  1984  Learning Rate:  0.013765565028968217  Varinance:  1.5632403082025101 \n",
      "\n",
      "Epoch:  1985  Learning Rate:  0.013751806344428075  Varinance:  1.5610565187914365 \n",
      "\n",
      "Epoch:  1986  Learning Rate:  0.013738061411695424  Varinance:  1.558875780054124 \n",
      "\n",
      "Epoch:  1987  Learning Rate:  0.013724330217025333  Varinance:  1.5566980877288943 \n",
      "\n",
      "Epoch:  1988  Learning Rate:  0.013710612746686597  Varinance:  1.5545234375600208 \n",
      "\n",
      "Epoch:  1989  Learning Rate:  0.013696908986961754  Varinance:  1.5523518252977233 \n",
      "\n",
      "Epoch:  1990  Learning Rate:  0.013683218924147037  Varinance:  1.550183246698157 \n",
      "\n",
      "Epoch:  1991  Learning Rate:  0.013669542544552386  Varinance:  1.5480176975234068 \n",
      "\n",
      "Epoch:  1992  Learning Rate:  0.013655879834501417  Varinance:  1.5458551735414776 \n",
      "\n",
      "Epoch:  1993  Learning Rate:  0.013642230780331424  Varinance:  1.5436956705262848 \n",
      "\n",
      "Epoch:  1994  Learning Rate:  0.013628595368393344  Varinance:  1.541539184257649 \n",
      "\n",
      "Epoch:  1995  Learning Rate:  0.013614973585051772  Varinance:  1.539385710521286 \n",
      "\n",
      "Epoch:  1996  Learning Rate:  0.013601365416684916  Varinance:  1.5372352451087985 \n",
      "\n",
      "Epoch:  1997  Learning Rate:  0.013587770849684613  Varinance:  1.5350877838176686 \n",
      "\n",
      "Epoch:  1998  Learning Rate:  0.013574189870456289  Varinance:  1.5329433224512488 \n",
      "\n",
      "Epoch:  1999  Learning Rate:  0.01356062246541897  Varinance:  1.5308018568187547 \n",
      "\n",
      "Epoch:  2000  Learning Rate:  0.013547068621005243  Varinance:  1.5286633827352545 \n",
      "\n",
      "Epoch:  2001  Learning Rate:  0.013533528323661271  Varinance:  1.5265278960216648 \n",
      "\n",
      "Epoch:  2002  Learning Rate:  0.013520001559846749  Varinance:  1.5243953925047393 \n",
      "\n",
      "Epoch:  2003  Learning Rate:  0.013506488316034912  Varinance:  1.5222658680170613 \n",
      "\n",
      "Epoch:  2004  Learning Rate:  0.013492988578712513  Varinance:  1.5201393183970364 \n",
      "\n",
      "Epoch:  2005  Learning Rate:  0.01347950233437982  Varinance:  1.5180157394888842 \n",
      "\n",
      "Epoch:  2006  Learning Rate:  0.013466029569550586  Varinance:  1.5158951271426275 \n",
      "\n",
      "Epoch:  2007  Learning Rate:  0.013452570270752046  Varinance:  1.5137774772140893 \n",
      "\n",
      "Epoch:  2008  Learning Rate:  0.013439124424524891  Varinance:  1.51166278556488 \n",
      "\n",
      "Epoch:  2009  Learning Rate:  0.013425692017423286  Varinance:  1.5095510480623924 \n",
      "\n",
      "Epoch:  2010  Learning Rate:  0.013412273036014817  Varinance:  1.5074422605797912 \n",
      "\n",
      "Epoch:  2011  Learning Rate:  0.0133988674668805  Varinance:  1.5053364189960072 \n",
      "\n",
      "Epoch:  2012  Learning Rate:  0.013385475296614763  Varinance:  1.5032335191957282 \n",
      "\n",
      "Epoch:  2013  Learning Rate:  0.013372096511825441  Varinance:  1.5011335570693893 \n",
      "\n",
      "Epoch:  2014  Learning Rate:  0.013358731099133749  Varinance:  1.4990365285131682 \n",
      "\n",
      "Epoch:  2015  Learning Rate:  0.013345379045174266  Varinance:  1.496942429428975 \n",
      "\n",
      "Epoch:  2016  Learning Rate:  0.013332040336594936  Varinance:  1.4948512557244447 \n",
      "\n",
      "Epoch:  2017  Learning Rate:  0.01331871496005706  Varinance:  1.492763003312929 \n",
      "\n",
      "Epoch:  2018  Learning Rate:  0.013305402902235253  Varinance:  1.490677668113489 \n",
      "\n",
      "Epoch:  2019  Learning Rate:  0.013292104149817458  Varinance:  1.4885952460508867 \n",
      "\n",
      "Epoch:  2020  Learning Rate:  0.013278818689504916  Varinance:  1.4865157330555754 \n",
      "\n",
      "Epoch:  2021  Learning Rate:  0.013265546508012172  Varinance:  1.4844391250636955 \n",
      "\n",
      "Epoch:  2022  Learning Rate:  0.013252287592067044  Varinance:  1.4823654180170633 \n",
      "\n",
      "Epoch:  2023  Learning Rate:  0.013239041928410612  Varinance:  1.4802946078631654 \n",
      "\n",
      "Epoch:  2024  Learning Rate:  0.013225809503797207  Varinance:  1.4782266905551482 \n",
      "\n",
      "Epoch:  2025  Learning Rate:  0.01321259030499441  Varinance:  1.4761616620518128 \n",
      "\n",
      "Epoch:  2026  Learning Rate:  0.013199384318783022  Varinance:  1.4740995183176038 \n",
      "\n",
      "Epoch:  2027  Learning Rate:  0.013186191531957055  Varinance:  1.4720402553226053 \n",
      "\n",
      "Epoch:  2028  Learning Rate:  0.01317301193132371  Varinance:  1.4699838690425298 \n",
      "\n",
      "Epoch:  2029  Learning Rate:  0.0131598455037034  Varinance:  1.467930355458712 \n",
      "\n",
      "Epoch:  2030  Learning Rate:  0.013146692235929689  Varinance:  1.4658797105581007 \n",
      "\n",
      "Epoch:  2031  Learning Rate:  0.013133552114849312  Varinance:  1.4638319303332508 \n",
      "\n",
      "Epoch:  2032  Learning Rate:  0.013120425127322137  Varinance:  1.4617870107823154 \n",
      "\n",
      "Epoch:  2033  Learning Rate:  0.013107311260221188  Varinance:  1.4597449479090372 \n",
      "\n",
      "Epoch:  2034  Learning Rate:  0.013094210500432593  Varinance:  1.4577057377227425 \n",
      "\n",
      "Epoch:  2035  Learning Rate:  0.013081122834855588  Varinance:  1.4556693762383321 \n",
      "\n",
      "Epoch:  2036  Learning Rate:  0.013068048250402503  Varinance:  1.4536358594762742 \n",
      "\n",
      "Epoch:  2037  Learning Rate:  0.013054986733998764  Varinance:  1.4516051834625956 \n",
      "\n",
      "Epoch:  2038  Learning Rate:  0.013041938272582848  Varinance:  1.4495773442288757 \n",
      "\n",
      "Epoch:  2039  Learning Rate:  0.013028902853106289  Varinance:  1.447552337812236 \n",
      "\n",
      "Epoch:  2040  Learning Rate:  0.013015880462533665  Varinance:  1.4455301602553345 \n",
      "\n",
      "Epoch:  2041  Learning Rate:  0.013002871087842592  Varinance:  1.443510807606359 \n",
      "\n",
      "Epoch:  2042  Learning Rate:  0.012989874716023692  Varinance:  1.4414942759190161 \n",
      "\n",
      "Epoch:  2043  Learning Rate:  0.012976891334080587  Varinance:  1.4394805612525257 \n",
      "\n",
      "Epoch:  2044  Learning Rate:  0.012963920929029897  Varinance:  1.437469659671613 \n",
      "\n",
      "Epoch:  2045  Learning Rate:  0.012950963487901219  Varinance:  1.4354615672465014 \n",
      "\n",
      "Epoch:  2046  Learning Rate:  0.012938018997737109  Varinance:  1.433456280052902 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2047  Learning Rate:  0.012925087445593073  Varinance:  1.4314537941720096 \n",
      "\n",
      "Epoch:  2048  Learning Rate:  0.012912168818537557  Varinance:  1.4294541056904928 \n",
      "\n",
      "Epoch:  2049  Learning Rate:  0.01289926310365194  Varinance:  1.4274572107004877 \n",
      "\n",
      "Epoch:  2050  Learning Rate:  0.012886370288030503  Varinance:  1.4254631052995888 \n",
      "\n",
      "Epoch:  2051  Learning Rate:  0.012873490358780423  Varinance:  1.4234717855908423 \n",
      "\n",
      "Epoch:  2052  Learning Rate:  0.012860623303021774  Varinance:  1.4214832476827393 \n",
      "\n",
      "Epoch:  2053  Learning Rate:  0.012847769107887503  Varinance:  1.4194974876892048 \n",
      "\n",
      "Epoch:  2054  Learning Rate:  0.012834927760523413  Varinance:  1.4175145017295945 \n",
      "\n",
      "Epoch:  2055  Learning Rate:  0.01282209924808815  Varinance:  1.4155342859286848 \n",
      "\n",
      "Epoch:  2056  Learning Rate:  0.012809283557753199  Varinance:  1.4135568364166655 \n",
      "\n",
      "Epoch:  2057  Learning Rate:  0.01279648067670288  Varinance:  1.4115821493291327 \n",
      "\n",
      "Epoch:  2058  Learning Rate:  0.012783690592134304  Varinance:  1.4096102208070809 \n",
      "\n",
      "Epoch:  2059  Learning Rate:  0.012770913291257383  Varinance:  1.407641046996894 \n",
      "\n",
      "Epoch:  2060  Learning Rate:  0.012758148761294814  Varinance:  1.4056746240503415 \n",
      "\n",
      "Epoch:  2061  Learning Rate:  0.012745396989482075  Varinance:  1.4037109481245673 \n",
      "\n",
      "Epoch:  2062  Learning Rate:  0.012732657963067388  Varinance:  1.401750015382084 \n",
      "\n",
      "Epoch:  2063  Learning Rate:  0.012719931669311721  Varinance:  1.3997918219907657 \n",
      "\n",
      "Epoch:  2064  Learning Rate:  0.012707218095488783  Varinance:  1.3978363641238387 \n",
      "\n",
      "Epoch:  2065  Learning Rate:  0.012694517228885004  Varinance:  1.3958836379598767 \n",
      "\n",
      "Epoch:  2066  Learning Rate:  0.01268182905679951  Varinance:  1.393933639682789 \n",
      "\n",
      "Epoch:  2067  Learning Rate:  0.012669153566544132  Varinance:  1.3919863654818188 \n",
      "\n",
      "Epoch:  2068  Learning Rate:  0.012656490745443366  Varinance:  1.390041811551531 \n",
      "\n",
      "Epoch:  2069  Learning Rate:  0.01264384058083441  Varinance:  1.3880999740918079 \n",
      "\n",
      "Epoch:  2070  Learning Rate:  0.012631203060067087  Varinance:  1.3861608493078392 \n",
      "\n",
      "Epoch:  2071  Learning Rate:  0.012618578170503878  Varinance:  1.384224433410117 \n",
      "\n",
      "Epoch:  2072  Learning Rate:  0.012605965899519884  Varinance:  1.3822907226144254 \n",
      "\n",
      "Epoch:  2073  Learning Rate:  0.012593366234502846  Varinance:  1.3803597131418366 \n",
      "\n",
      "Epoch:  2074  Learning Rate:  0.012580779162853092  Varinance:  1.3784314012187013 \n",
      "\n",
      "Epoch:  2075  Learning Rate:  0.01256820467198355  Varinance:  1.3765057830766414 \n",
      "\n",
      "Epoch:  2076  Learning Rate:  0.012555642749319721  Varinance:  1.374582854952544 \n",
      "\n",
      "Epoch:  2077  Learning Rate:  0.012543093382299692  Varinance:  1.3726626130885524 \n",
      "\n",
      "Epoch:  2078  Learning Rate:  0.012530556558374093  Varinance:  1.37074505373206 \n",
      "\n",
      "Epoch:  2079  Learning Rate:  0.012518032265006094  Varinance:  1.3688301731357015 \n",
      "\n",
      "Epoch:  2080  Learning Rate:  0.0125055204896714  Varinance:  1.3669179675573473 \n",
      "\n",
      "Epoch:  2081  Learning Rate:  0.012493021219858243  Varinance:  1.3650084332600958 \n",
      "\n",
      "Epoch:  2082  Learning Rate:  0.012480534443067346  Varinance:  1.3631015665122646 \n",
      "\n",
      "Epoch:  2083  Learning Rate:  0.012468060146811932  Varinance:  1.3611973635873853 \n",
      "\n",
      "Epoch:  2084  Learning Rate:  0.0124555983186177  Varinance:  1.3592958207641948 \n",
      "\n",
      "Epoch:  2085  Learning Rate:  0.012443148946022827  Varinance:  1.3573969343266292 \n",
      "\n",
      "Epoch:  2086  Learning Rate:  0.012430712016577939  Varinance:  1.355500700563814 \n",
      "\n",
      "Epoch:  2087  Learning Rate:  0.012418287517846103  Varinance:  1.3536071157700602 \n",
      "\n",
      "Epoch:  2088  Learning Rate:  0.012405875437402814  Varinance:  1.3517161762448553 \n",
      "\n",
      "Epoch:  2089  Learning Rate:  0.012393475762836004  Varinance:  1.349827878292856 \n",
      "\n",
      "Epoch:  2090  Learning Rate:  0.012381088481745987  Varinance:  1.3479422182238816 \n",
      "\n",
      "Epoch:  2091  Learning Rate:  0.012368713581745483  Varinance:  1.3460591923529064 \n",
      "\n",
      "Epoch:  2092  Learning Rate:  0.01235635105045959  Varinance:  1.3441787970000514 \n",
      "\n",
      "Epoch:  2093  Learning Rate:  0.012344000875525778  Varinance:  1.3423010284905799 \n",
      "\n",
      "Epoch:  2094  Learning Rate:  0.012331663044593872  Varinance:  1.3404258831548876 \n",
      "\n",
      "Epoch:  2095  Learning Rate:  0.01231933754532604  Varinance:  1.3385533573284973 \n",
      "\n",
      "Epoch:  2096  Learning Rate:  0.012307024365396772  Varinance:  1.3366834473520501 \n",
      "\n",
      "Epoch:  2097  Learning Rate:  0.012294723492492901  Varinance:  1.3348161495713 \n",
      "\n",
      "Epoch:  2098  Learning Rate:  0.012282434914313547  Varinance:  1.3329514603371055 \n",
      "\n",
      "Epoch:  2099  Learning Rate:  0.01227015861857013  Varinance:  1.3310893760054219 \n",
      "\n",
      "Epoch:  2100  Learning Rate:  0.012257894592986351  Varinance:  1.329229892937296 \n",
      "\n",
      "Epoch:  2101  Learning Rate:  0.012245642825298192  Varinance:  1.3273730074988586 \n",
      "\n",
      "Epoch:  2102  Learning Rate:  0.012233403303253877  Varinance:  1.3255187160613155 \n",
      "\n",
      "Epoch:  2103  Learning Rate:  0.012221176014613885  Varinance:  1.323667015000943 \n",
      "\n",
      "Epoch:  2104  Learning Rate:  0.01220896094715092  Varinance:  1.3218179006990798 \n",
      "\n",
      "Epoch:  2105  Learning Rate:  0.012196758088649926  Varinance:  1.3199713695421174 \n",
      "\n",
      "Epoch:  2106  Learning Rate:  0.012184567426908037  Varinance:  1.3181274179214981 \n",
      "\n",
      "Epoch:  2107  Learning Rate:  0.012172388949734589  Varinance:  1.3162860422337042 \n",
      "\n",
      "Epoch:  2108  Learning Rate:  0.012160222644951101  Varinance:  1.3144472388802515 \n",
      "\n",
      "Epoch:  2109  Learning Rate:  0.012148068500391277  Varinance:  1.3126110042676837 \n",
      "\n",
      "Epoch:  2110  Learning Rate:  0.012135926503900966  Varinance:  1.3107773348075638 \n",
      "\n",
      "Epoch:  2111  Learning Rate:  0.012123796643338168  Varinance:  1.3089462269164682 \n",
      "\n",
      "Epoch:  2112  Learning Rate:  0.01211167890657302  Varinance:  1.307117677015978 \n",
      "\n",
      "Epoch:  2113  Learning Rate:  0.012099573281487794  Varinance:  1.3052916815326747 \n",
      "\n",
      "Epoch:  2114  Learning Rate:  0.012087479755976856  Varinance:  1.3034682368981307 \n",
      "\n",
      "Epoch:  2115  Learning Rate:  0.012075398317946682  Varinance:  1.3016473395489043 \n",
      "\n",
      "Epoch:  2116  Learning Rate:  0.012063328955315826  Varinance:  1.299828985926531 \n",
      "\n",
      "Epoch:  2117  Learning Rate:  0.012051271656014939  Varinance:  1.2980131724775177 \n",
      "\n",
      "Epoch:  2118  Learning Rate:  0.012039226407986709  Varinance:  1.2961998956533356 \n",
      "\n",
      "Epoch:  2119  Learning Rate:  0.012027193199185892  Varinance:  1.2943891519104123 \n",
      "\n",
      "Epoch:  2120  Learning Rate:  0.012015172017579269  Varinance:  1.292580937710126 \n",
      "\n",
      "Epoch:  2121  Learning Rate:  0.012003162851145673  Varinance:  1.290775249518799 \n",
      "\n",
      "Epoch:  2122  Learning Rate:  0.011991165687875928  Varinance:  1.2889720838076888 \n",
      "\n",
      "Epoch:  2123  Learning Rate:  0.011979180515772868  Varinance:  1.2871714370529832 \n",
      "\n",
      "Epoch:  2124  Learning Rate:  0.011967207322851317  Varinance:  1.2853733057357932 \n",
      "\n",
      "Epoch:  2125  Learning Rate:  0.011955246097138094  Varinance:  1.2835776863421433 \n",
      "\n",
      "Epoch:  2126  Learning Rate:  0.011943296826671963  Varinance:  1.2817845753629693 \n",
      "\n",
      "Epoch:  2127  Learning Rate:  0.011931359499503653  Varinance:  1.279993969294108 \n",
      "\n",
      "Epoch:  2128  Learning Rate:  0.011919434103695838  Varinance:  1.2782058646362913 \n",
      "\n",
      "Epoch:  2129  Learning Rate:  0.011907520627323115  Varinance:  1.27642025789514 \n",
      "\n",
      "Epoch:  2130  Learning Rate:  0.011895619058472015  Varinance:  1.274637145581156 \n",
      "\n",
      "Epoch:  2131  Learning Rate:  0.011883729385240967  Varinance:  1.2728565242097163 \n",
      "\n",
      "Epoch:  2132  Learning Rate:  0.011871851595740293  Varinance:  1.2710783903010645 \n",
      "\n",
      "Epoch:  2133  Learning Rate:  0.011859985678092199  Varinance:  1.2693027403803068 \n",
      "\n",
      "Epoch:  2134  Learning Rate:  0.011848131620430776  Varinance:  1.2675295709774033 \n",
      "\n",
      "Epoch:  2135  Learning Rate:  0.011836289410901963  Varinance:  1.2657588786271612 \n",
      "\n",
      "Epoch:  2136  Learning Rate:  0.011824459037663545  Varinance:  1.2639906598692292 \n",
      "\n",
      "Epoch:  2137  Learning Rate:  0.011812640488885147  Varinance:  1.2622249112480894 \n",
      "\n",
      "Epoch:  2138  Learning Rate:  0.011800833752748225  Varinance:  1.2604616293130506 \n",
      "\n",
      "Epoch:  2139  Learning Rate:  0.011789038817446041  Varinance:  1.2587008106182433 \n",
      "\n",
      "Epoch:  2140  Learning Rate:  0.011777255671183656  Varinance:  1.256942451722611 \n",
      "\n",
      "Epoch:  2141  Learning Rate:  0.01176548430217792  Varinance:  1.2551865491899046 \n",
      "\n",
      "Epoch:  2142  Learning Rate:  0.011753724698657469  Varinance:  1.2534330995886749 \n",
      "\n",
      "Epoch:  2143  Learning Rate:  0.011741976848862698  Varinance:  1.2516820994922666 \n",
      "\n",
      "Epoch:  2144  Learning Rate:  0.011730240741045755  Varinance:  1.2499335454788119 \n",
      "\n",
      "Epoch:  2145  Learning Rate:  0.011718516363470523  Varinance:  1.2481874341312214 \n",
      "\n",
      "Epoch:  2146  Learning Rate:  0.011706803704412638  Varinance:  1.2464437620371809 \n",
      "\n",
      "Epoch:  2147  Learning Rate:  0.011695102752159433  Varinance:  1.2447025257891426 \n",
      "\n",
      "Epoch:  2148  Learning Rate:  0.011683413495009953  Varinance:  1.2429637219843195 \n",
      "\n",
      "Epoch:  2149  Learning Rate:  0.011671735921274939  Varinance:  1.2412273472246766 \n",
      "\n",
      "Epoch:  2150  Learning Rate:  0.011660070019276823  Varinance:  1.2394933981169274 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2151  Learning Rate:  0.011648415777349697  Varinance:  1.2377618712725258 \n",
      "\n",
      "Epoch:  2152  Learning Rate:  0.01163677318383932  Varinance:  1.2360327633076573 \n",
      "\n",
      "Epoch:  2153  Learning Rate:  0.011625142227103091  Varinance:  1.2343060708432365 \n",
      "\n",
      "Epoch:  2154  Learning Rate:  0.011613522895510063  Varinance:  1.2325817905048977 \n",
      "\n",
      "Epoch:  2155  Learning Rate:  0.0116019151774409  Varinance:  1.230859918922989 \n",
      "\n",
      "Epoch:  2156  Learning Rate:  0.01159031906128788  Varinance:  1.2291404527325667 \n",
      "\n",
      "Epoch:  2157  Learning Rate:  0.011578734535454882  Varinance:  1.2274233885733867 \n",
      "\n",
      "Epoch:  2158  Learning Rate:  0.01156716158835739  Varinance:  1.225708723089899 \n",
      "\n",
      "Epoch:  2159  Learning Rate:  0.011555600208422449  Varinance:  1.223996452931242 \n",
      "\n",
      "Epoch:  2160  Learning Rate:  0.01154405038408868  Varinance:  1.2222865747512344 \n",
      "\n",
      "Epoch:  2161  Learning Rate:  0.011532512103806252  Varinance:  1.2205790852083696 \n",
      "\n",
      "Epoch:  2162  Learning Rate:  0.011520985356036894  Varinance:  1.2188739809658098 \n",
      "\n",
      "Epoch:  2163  Learning Rate:  0.011509470129253851  Varinance:  1.2171712586913772 \n",
      "\n",
      "Epoch:  2164  Learning Rate:  0.011497966411941897  Varinance:  1.2154709150575504 \n",
      "\n",
      "Epoch:  2165  Learning Rate:  0.01148647419259731  Varinance:  1.2137729467414549 \n",
      "\n",
      "Epoch:  2166  Learning Rate:  0.011474993459727877  Varinance:  1.2120773504248588 \n",
      "\n",
      "Epoch:  2167  Learning Rate:  0.011463524201852858  Varinance:  1.210384122794166 \n",
      "\n",
      "Epoch:  2168  Learning Rate:  0.011452066407502999  Varinance:  1.2086932605404097 \n",
      "\n",
      "Epoch:  2169  Learning Rate:  0.011440620065220494  Varinance:  1.207004760359244 \n",
      "\n",
      "Epoch:  2170  Learning Rate:  0.011429185163559015  Varinance:  1.2053186189509413 \n",
      "\n",
      "Epoch:  2171  Learning Rate:  0.01141776169108365  Varinance:  1.2036348330203812 \n",
      "\n",
      "Epoch:  2172  Learning Rate:  0.01140634963637093  Varinance:  1.2019533992770484 \n",
      "\n",
      "Epoch:  2173  Learning Rate:  0.011394948988008788  Varinance:  1.2002743144350227 \n",
      "\n",
      "Epoch:  2174  Learning Rate:  0.011383559734596593  Varinance:  1.1985975752129758 \n",
      "\n",
      "Epoch:  2175  Learning Rate:  0.011372181864745078  Varinance:  1.1969231783341625 \n",
      "\n",
      "Epoch:  2176  Learning Rate:  0.011360815367076378  Varinance:  1.195251120526415 \n",
      "\n",
      "Epoch:  2177  Learning Rate:  0.011349460230223984  Varinance:  1.1935813985221373 \n",
      "\n",
      "Epoch:  2178  Learning Rate:  0.011338116442832773  Varinance:  1.1919140090582967 \n",
      "\n",
      "Epoch:  2179  Learning Rate:  0.011326783993558949  Varinance:  1.1902489488764199 \n",
      "\n",
      "Epoch:  2180  Learning Rate:  0.011315462871070062  Varinance:  1.1885862147225856 \n",
      "\n",
      "Epoch:  2181  Learning Rate:  0.011304153064044985  Varinance:  1.186925803347418 \n",
      "\n",
      "Epoch:  2182  Learning Rate:  0.011292854561173918  Varinance:  1.18526771150608 \n",
      "\n",
      "Epoch:  2183  Learning Rate:  0.011281567351158355  Varinance:  1.1836119359582682 \n",
      "\n",
      "Epoch:  2184  Learning Rate:  0.011270291422711083  Varinance:  1.181958473468206 \n",
      "\n",
      "Epoch:  2185  Learning Rate:  0.011259026764556167  Varinance:  1.1803073208046355 \n",
      "\n",
      "Epoch:  2186  Learning Rate:  0.011247773365428959  Varinance:  1.178658474740814 \n",
      "\n",
      "Epoch:  2187  Learning Rate:  0.011236531214076053  Varinance:  1.1770119320545065 \n",
      "\n",
      "Epoch:  2188  Learning Rate:  0.011225300299255298  Varinance:  1.1753676895279788 \n",
      "\n",
      "Epoch:  2189  Learning Rate:  0.011214080609735772  Varinance:  1.173725743947992 \n",
      "\n",
      "Epoch:  2190  Learning Rate:  0.011202872134297798  Varinance:  1.1720860921057967 \n",
      "\n",
      "Epoch:  2191  Learning Rate:  0.011191674861732889  Varinance:  1.170448730797124 \n",
      "\n",
      "Epoch:  2192  Learning Rate:  0.011180488780843774  Varinance:  1.1688136568221834 \n",
      "\n",
      "Epoch:  2193  Learning Rate:  0.011169313880444369  Varinance:  1.1671808669856534 \n",
      "\n",
      "Epoch:  2194  Learning Rate:  0.01115815014935978  Varinance:  1.165550358096676 \n",
      "\n",
      "Epoch:  2195  Learning Rate:  0.011146997576426268  Varinance:  1.1639221269688513 \n",
      "\n",
      "Epoch:  2196  Learning Rate:  0.011135856150491263  Varinance:  1.1622961704202306 \n",
      "\n",
      "Epoch:  2197  Learning Rate:  0.011124725860413331  Varinance:  1.1606724852733106 \n",
      "\n",
      "Epoch:  2198  Learning Rate:  0.011113606695062192  Varinance:  1.159051068355025 \n",
      "\n",
      "Epoch:  2199  Learning Rate:  0.011102498643318673  Varinance:  1.1574319164967426 \n",
      "\n",
      "Epoch:  2200  Learning Rate:  0.011091401694074722  Varinance:  1.1558150265342573 \n",
      "\n",
      "Epoch:  2201  Learning Rate:  0.011080315836233388  Varinance:  1.1542003953077835 \n",
      "\n",
      "Epoch:  2202  Learning Rate:  0.011069241058708816  Varinance:  1.15258801966195 \n",
      "\n",
      "Epoch:  2203  Learning Rate:  0.011058177350426225  Varinance:  1.150977896445794 \n",
      "\n",
      "Epoch:  2204  Learning Rate:  0.011047124700321907  Varinance:  1.1493700225127519 \n",
      "\n",
      "Epoch:  2205  Learning Rate:  0.011036083097343203  Varinance:  1.1477643947206593 \n",
      "\n",
      "Epoch:  2206  Learning Rate:  0.011025052530448522  Varinance:  1.1461610099317388 \n",
      "\n",
      "Epoch:  2207  Learning Rate:  0.011014032988607292  Varinance:  1.1445598650125974 \n",
      "\n",
      "Epoch:  2208  Learning Rate:  0.011003024460799966  Varinance:  1.1429609568342194 \n",
      "\n",
      "Epoch:  2209  Learning Rate:  0.010992026936018013  Varinance:  1.1413642822719596 \n",
      "\n",
      "Epoch:  2210  Learning Rate:  0.010981040403263918  Varinance:  1.139769838205539 \n",
      "\n",
      "Epoch:  2211  Learning Rate:  0.010970064851551142  Varinance:  1.1381776215190353 \n",
      "\n",
      "Epoch:  2212  Learning Rate:  0.01095910026990413  Varinance:  1.1365876291008812 \n",
      "\n",
      "Epoch:  2213  Learning Rate:  0.010948146647358298  Varinance:  1.134999857843855 \n",
      "\n",
      "Epoch:  2214  Learning Rate:  0.010937203972960028  Varinance:  1.1334143046450762 \n",
      "\n",
      "Epoch:  2215  Learning Rate:  0.010926272235766643  Varinance:  1.131830966405999 \n",
      "\n",
      "Epoch:  2216  Learning Rate:  0.010915351424846407  Varinance:  1.1302498400324053 \n",
      "\n",
      "Epoch:  2217  Learning Rate:  0.010904441529278499  Varinance:  1.1286709224344011 \n",
      "\n",
      "Epoch:  2218  Learning Rate:  0.010893542538153033  Varinance:  1.1270942105264066 \n",
      "\n",
      "Epoch:  2219  Learning Rate:  0.010882654440571014  Varinance:  1.1255197012271543 \n",
      "\n",
      "Epoch:  2220  Learning Rate:  0.010871777225644343  Varinance:  1.1239473914596807 \n",
      "\n",
      "Epoch:  2221  Learning Rate:  0.010860910882495797  Varinance:  1.1223772781513202 \n",
      "\n",
      "Epoch:  2222  Learning Rate:  0.010850055400259044  Varinance:  1.1208093582337 \n",
      "\n",
      "Epoch:  2223  Learning Rate:  0.010839210768078596  Varinance:  1.1192436286427343 \n",
      "\n",
      "Epoch:  2224  Learning Rate:  0.01082837697510982  Varinance:  1.1176800863186158 \n",
      "\n",
      "Epoch:  2225  Learning Rate:  0.010817554010518914  Varinance:  1.1161187282058138 \n",
      "\n",
      "Epoch:  2226  Learning Rate:  0.010806741863482926  Varinance:  1.1145595512530648 \n",
      "\n",
      "Epoch:  2227  Learning Rate:  0.010795940523189704  Varinance:  1.113002552413368 \n",
      "\n",
      "Epoch:  2228  Learning Rate:  0.010785149978837902  Varinance:  1.11144772864398 \n",
      "\n",
      "Epoch:  2229  Learning Rate:  0.010774370219636975  Varinance:  1.1098950769064067 \n",
      "\n",
      "Epoch:  2230  Learning Rate:  0.01076360123480717  Varinance:  1.1083445941663996 \n",
      "\n",
      "Epoch:  2231  Learning Rate:  0.010752843013579497  Varinance:  1.1067962773939484 \n",
      "\n",
      "Epoch:  2232  Learning Rate:  0.010742095545195732  Varinance:  1.1052501235632755 \n",
      "\n",
      "Epoch:  2233  Learning Rate:  0.010731358818908403  Varinance:  1.1037061296528308 \n",
      "\n",
      "Epoch:  2234  Learning Rate:  0.010720632823980794  Varinance:  1.1021642926452853 \n",
      "\n",
      "Epoch:  2235  Learning Rate:  0.0107099175496869  Varinance:  1.100624609527524 \n",
      "\n",
      "Epoch:  2236  Learning Rate:  0.01069921298531145  Varinance:  1.099087077290643 \n",
      "\n",
      "Epoch:  2237  Learning Rate:  0.01068851912014987  Varinance:  1.0975516929299391 \n",
      "\n",
      "Epoch:  2238  Learning Rate:  0.010677835943508306  Varinance:  1.0960184534449087 \n",
      "\n",
      "Epoch:  2239  Learning Rate:  0.010667163444703576  Varinance:  1.094487355839239 \n",
      "\n",
      "Epoch:  2240  Learning Rate:  0.01065650161306318  Varinance:  1.0929583971208034 \n",
      "\n",
      "Epoch:  2241  Learning Rate:  0.010645850437925281  Varinance:  1.091431574301654 \n",
      "\n",
      "Epoch:  2242  Learning Rate:  0.01063520990863871  Varinance:  1.0899068843980189 \n",
      "\n",
      "Epoch:  2243  Learning Rate:  0.010624580014562936  Varinance:  1.0883843244302929 \n",
      "\n",
      "Epoch:  2244  Learning Rate:  0.010613960745068061  Varinance:  1.086863891423033 \n",
      "\n",
      "Epoch:  2245  Learning Rate:  0.01060335208953481  Varinance:  1.0853455824049538 \n",
      "\n",
      "Epoch:  2246  Learning Rate:  0.010592754037354537  Varinance:  1.0838293944089206 \n",
      "\n",
      "Epoch:  2247  Learning Rate:  0.010582166577929186  Varinance:  1.0823153244719426 \n",
      "\n",
      "Epoch:  2248  Learning Rate:  0.010571589700671291  Varinance:  1.0808033696351695 \n",
      "\n",
      "Epoch:  2249  Learning Rate:  0.010561023395003977  Varinance:  1.0792935269438837 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2250  Learning Rate:  0.01055046765036094  Varinance:  1.0777857934474961 \n",
      "\n",
      "Epoch:  2251  Learning Rate:  0.010539922456186434  Varinance:  1.0762801661995374 \n",
      "\n",
      "Epoch:  2252  Learning Rate:  0.010529387801935262  Varinance:  1.0747766422576566 \n",
      "\n",
      "Epoch:  2253  Learning Rate:  0.01051886367707277  Varinance:  1.0732752186836116 \n",
      "\n",
      "Epoch:  2254  Learning Rate:  0.010508350071074826  Varinance:  1.071775892543266 \n",
      "\n",
      "Epoch:  2255  Learning Rate:  0.010497846973427835  Varinance:  1.070278660906582 \n",
      "\n",
      "Epoch:  2256  Learning Rate:  0.010487354373628691  Varinance:  1.0687835208476146 \n",
      "\n",
      "Epoch:  2257  Learning Rate:  0.010476872261184794  Varinance:  1.067290469444506 \n",
      "\n",
      "Epoch:  2258  Learning Rate:  0.010466400625614027  Varinance:  1.0657995037794805 \n",
      "\n",
      "Epoch:  2259  Learning Rate:  0.010455939456444763  Varinance:  1.0643106209388389 \n",
      "\n",
      "Epoch:  2260  Learning Rate:  0.010445488743215827  Varinance:  1.062823818012952 \n",
      "\n",
      "Epoch:  2261  Learning Rate:  0.010435048475476504  Varinance:  1.0613390920962544 \n",
      "\n",
      "Epoch:  2262  Learning Rate:  0.010424618642786522  Varinance:  1.059856440287241 \n",
      "\n",
      "Epoch:  2263  Learning Rate:  0.010414199234716056  Varinance:  1.0583758596884598 \n",
      "\n",
      "Epoch:  2264  Learning Rate:  0.010403790240845694  Varinance:  1.0568973474065049 \n",
      "\n",
      "Epoch:  2265  Learning Rate:  0.010393391650766436  Varinance:  1.055420900552014 \n",
      "\n",
      "Epoch:  2266  Learning Rate:  0.010383003454079692  Varinance:  1.0539465162396606 \n",
      "\n",
      "Epoch:  2267  Learning Rate:  0.010372625640397273  Varinance:  1.0524741915881493 \n",
      "\n",
      "Epoch:  2268  Learning Rate:  0.010362258199341359  Varinance:  1.051003923720209 \n",
      "\n",
      "Epoch:  2269  Learning Rate:  0.010351901120544507  Varinance:  1.049535709762589 \n",
      "\n",
      "Epoch:  2270  Learning Rate:  0.010341554393649635  Varinance:  1.048069546846051 \n",
      "\n",
      "Epoch:  2271  Learning Rate:  0.01033121800831002  Varinance:  1.0466054321053662 \n",
      "\n",
      "Epoch:  2272  Learning Rate:  0.010320891954189277  Varinance:  1.045143362679308 \n",
      "\n",
      "Epoch:  2273  Learning Rate:  0.010310576220961347  Varinance:  1.043683335710647 \n",
      "\n",
      "Epoch:  2274  Learning Rate:  0.010300270798310493  Varinance:  1.0422253483461448 \n",
      "\n",
      "Epoch:  2275  Learning Rate:  0.0102899756759313  Varinance:  1.0407693977365493 \n",
      "\n",
      "Epoch:  2276  Learning Rate:  0.01027969084352864  Varinance:  1.039315481036589 \n",
      "\n",
      "Epoch:  2277  Learning Rate:  0.01026941629081768  Varinance:  1.037863595404966 \n",
      "\n",
      "Epoch:  2278  Learning Rate:  0.010259152007523864  Varinance:  1.0364137380043523 \n",
      "\n",
      "Epoch:  2279  Learning Rate:  0.010248897983382913  Varinance:  1.0349659060013838 \n",
      "\n",
      "Epoch:  2280  Learning Rate:  0.010238654208140801  Varinance:  1.0335200965666542 \n",
      "\n",
      "Epoch:  2281  Learning Rate:  0.010228420671553749  Varinance:  1.0320763068747096 \n",
      "\n",
      "Epoch:  2282  Learning Rate:  0.010218197363388217  Varinance:  1.030634534104044 \n",
      "\n",
      "Epoch:  2283  Learning Rate:  0.010207984273420904  Varinance:  1.0291947754370918 \n",
      "\n",
      "Epoch:  2284  Learning Rate:  0.010197781391438716  Varinance:  1.0277570280602237 \n",
      "\n",
      "Epoch:  2285  Learning Rate:  0.010187588707238768  Varinance:  1.0263212891637417 \n",
      "\n",
      "Epoch:  2286  Learning Rate:  0.010177406210628372  Varinance:  1.0248875559418718 \n",
      "\n",
      "Epoch:  2287  Learning Rate:  0.010167233891425041  Varinance:  1.0234558255927604 \n",
      "\n",
      "Epoch:  2288  Learning Rate:  0.010157071739456448  Varinance:  1.0220260953184674 \n",
      "\n",
      "Epoch:  2289  Learning Rate:  0.010146919744560438  Varinance:  1.0205983623249628 \n",
      "\n",
      "Epoch:  2290  Learning Rate:  0.010136777896585018  Varinance:  1.0191726238221168 \n",
      "\n",
      "Epoch:  2291  Learning Rate:  0.01012664618538834  Varinance:  1.0177488770236998 \n",
      "\n",
      "Epoch:  2292  Learning Rate:  0.010116524600838693  Varinance:  1.0163271191473737 \n",
      "\n",
      "Epoch:  2293  Learning Rate:  0.01010641313281449  Varinance:  1.0149073474146875 \n",
      "\n",
      "Epoch:  2294  Learning Rate:  0.010096311771204257  Varinance:  1.0134895590510709 \n",
      "\n",
      "Epoch:  2295  Learning Rate:  0.010086220505906643  Varinance:  1.0120737512858304 \n",
      "\n",
      "Epoch:  2296  Learning Rate:  0.010076139326830373  Varinance:  1.0106599213521428 \n",
      "\n",
      "Epoch:  2297  Learning Rate:  0.01006606822389427  Varinance:  1.0092480664870493 \n",
      "\n",
      "Epoch:  2298  Learning Rate:  0.010056007187027226  Varinance:  1.0078381839314519 \n",
      "\n",
      "Epoch:  2299  Learning Rate:  0.010045956206168211  Varinance:  1.006430270930106 \n",
      "\n",
      "Epoch:  2300  Learning Rate:  0.01003591527126624  Varinance:  1.0050243247316173 \n",
      "\n",
      "Epoch:  2301  Learning Rate:  0.010025884372280375  Varinance:  1.0036203425884338 \n",
      "\n",
      "Epoch:  2302  Learning Rate:  0.010015863499179715  Varinance:  1.0022183217568426 \n",
      "\n",
      "Epoch:  2303  Learning Rate:  0.010005852641943393  Varinance:  1.0008182594969626 \n",
      "\n",
      "Epoch:  2304  Learning Rate:  0.009995851790560548  Varinance:  0.9994201530727417 \n",
      "\n",
      "Epoch:  2305  Learning Rate:  0.009985860935030323  Varinance:  0.9980239997519486 \n",
      "\n",
      "Epoch:  2306  Learning Rate:  0.009975880065361863  Varinance:  0.9966297968061694 \n",
      "\n",
      "Epoch:  2307  Learning Rate:  0.009965909171574304  Varinance:  0.9952375415108022 \n",
      "\n",
      "Epoch:  2308  Learning Rate:  0.009955948243696746  Varinance:  0.9938472311450505 \n",
      "\n",
      "Epoch:  2309  Learning Rate:  0.009945997271768262  Varinance:  0.9924588629919191 \n",
      "\n",
      "Epoch:  2310  Learning Rate:  0.009936056245837876  Varinance:  0.9910724343382077 \n",
      "\n",
      "Epoch:  2311  Learning Rate:  0.009926125155964567  Varinance:  0.989687942474507 \n",
      "\n",
      "Epoch:  2312  Learning Rate:  0.009916203992217239  Varinance:  0.9883053846951921 \n",
      "\n",
      "Epoch:  2313  Learning Rate:  0.009906292744674732  Varinance:  0.9869247582984183 \n",
      "\n",
      "Epoch:  2314  Learning Rate:  0.00989639140342579  Varinance:  0.9855460605861149 \n",
      "\n",
      "Epoch:  2315  Learning Rate:  0.009886499958569082  Varinance:  0.9841692888639805 \n",
      "\n",
      "Epoch:  2316  Learning Rate:  0.009876618400213154  Varinance:  0.9827944404414776 \n",
      "\n",
      "Epoch:  2317  Learning Rate:  0.009866746718476451  Varinance:  0.9814215126318264 \n",
      "\n",
      "Epoch:  2318  Learning Rate:  0.009856884903487283  Varinance:  0.9800505027520017 \n",
      "\n",
      "Epoch:  2319  Learning Rate:  0.009847032945383846  Varinance:  0.9786814081227255 \n",
      "\n",
      "Epoch:  2320  Learning Rate:  0.009837190834314174  Varinance:  0.9773142260684634 \n",
      "\n",
      "Epoch:  2321  Learning Rate:  0.009827358560436155  Varinance:  0.9759489539174179 \n",
      "\n",
      "Epoch:  2322  Learning Rate:  0.009817536113917513  Varinance:  0.9745855890015248 \n",
      "\n",
      "Epoch:  2323  Learning Rate:  0.009807723484935806  Varinance:  0.9732241286564456 \n",
      "\n",
      "Epoch:  2324  Learning Rate:  0.009797920663678402  Varinance:  0.971864570221565 \n",
      "\n",
      "Epoch:  2325  Learning Rate:  0.009788127640342478  Varinance:  0.9705069110399842 \n",
      "\n",
      "Epoch:  2326  Learning Rate:  0.009778344405135005  Varinance:  0.9691511484585161 \n",
      "\n",
      "Epoch:  2327  Learning Rate:  0.009768570948272756  Varinance:  0.9677972798276797 \n",
      "\n",
      "Epoch:  2328  Learning Rate:  0.009758807259982271  Varinance:  0.9664453025016957 \n",
      "\n",
      "Epoch:  2329  Learning Rate:  0.009749053330499859  Varinance:  0.9650952138384804 \n",
      "\n",
      "Epoch:  2330  Learning Rate:  0.009739309150071585  Varinance:  0.9637470111996409 \n",
      "\n",
      "Epoch:  2331  Learning Rate:  0.009729574708953276  Varinance:  0.9624006919504704 \n",
      "\n",
      "Epoch:  2332  Learning Rate:  0.009719849997410488  Varinance:  0.9610562534599425 \n",
      "\n",
      "Epoch:  2333  Learning Rate:  0.009710135005718509  Varinance:  0.9597136931007064 \n",
      "\n",
      "Epoch:  2334  Learning Rate:  0.009700429724162338  Varinance:  0.9583730082490817 \n",
      "\n",
      "Epoch:  2335  Learning Rate:  0.009690734143036705  Varinance:  0.9570341962850532 \n",
      "\n",
      "Epoch:  2336  Learning Rate:  0.009681048252646022  Varinance:  0.9556972545922646 \n",
      "\n",
      "Epoch:  2337  Learning Rate:  0.0096713720433044  Varinance:  0.9543621805580164 \n",
      "\n",
      "Epoch:  2338  Learning Rate:  0.00966170550533562  Varinance:  0.9530289715732578 \n",
      "\n",
      "Epoch:  2339  Learning Rate:  0.009652048629073156  Varinance:  0.9516976250325829 \n",
      "\n",
      "Epoch:  2340  Learning Rate:  0.009642401404860126  Varinance:  0.9503681383342258 \n",
      "\n",
      "Epoch:  2341  Learning Rate:  0.009632763823049305  Varinance:  0.9490405088800551 \n",
      "\n",
      "Epoch:  2342  Learning Rate:  0.009623135874003104  Varinance:  0.9477147340755687 \n",
      "\n",
      "Epoch:  2343  Learning Rate:  0.009613517548093583  Varinance:  0.9463908113298886 \n",
      "\n",
      "Epoch:  2344  Learning Rate:  0.009603908835702411  Varinance:  0.9450687380557568 \n",
      "\n",
      "Epoch:  2345  Learning Rate:  0.009594309727220878  Varinance:  0.9437485116695291 \n",
      "\n",
      "Epoch:  2346  Learning Rate:  0.009584720213049866  Varinance:  0.942430129591171 \n",
      "\n",
      "Epoch:  2347  Learning Rate:  0.009575140283599869  Varinance:  0.941113589244252 \n",
      "\n",
      "Epoch:  2348  Learning Rate:  0.009565569929290954  Varinance:  0.9397988880559408 \n",
      "\n",
      "Epoch:  2349  Learning Rate:  0.009556009140552766  Varinance:  0.9384860234570007 \n",
      "\n",
      "Epoch:  2350  Learning Rate:  0.009546457907824511  Varinance:  0.9371749928817827 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2351  Learning Rate:  0.009536916221554961  Varinance:  0.9358657937682233 \n",
      "\n",
      "Epoch:  2352  Learning Rate:  0.00952738407220243  Varinance:  0.9345584235578378 \n",
      "\n",
      "Epoch:  2353  Learning Rate:  0.009517861450234763  Varinance:  0.9332528796957156 \n",
      "\n",
      "Epoch:  2354  Learning Rate:  0.009508348346129339  Varinance:  0.9319491596305148 \n",
      "\n",
      "Epoch:  2355  Learning Rate:  0.009498844750373055  Varinance:  0.9306472608144587 \n",
      "\n",
      "Epoch:  2356  Learning Rate:  0.009489350653462313  Varinance:  0.929347180703328 \n",
      "\n",
      "Epoch:  2357  Learning Rate:  0.009479866045903014  Varinance:  0.9280489167564588 \n",
      "\n",
      "Epoch:  2358  Learning Rate:  0.009470390918210548  Varinance:  0.9267524664367366 \n",
      "\n",
      "Epoch:  2359  Learning Rate:  0.009460925260909794  Varinance:  0.9254578272105907 \n",
      "\n",
      "Epoch:  2360  Learning Rate:  0.00945146906453509  Varinance:  0.9241649965479893 \n",
      "\n",
      "Epoch:  2361  Learning Rate:  0.009442022319630235  Varinance:  0.922873971922436 \n",
      "\n",
      "Epoch:  2362  Learning Rate:  0.009432585016748485  Varinance:  0.9215847508109634 \n",
      "\n",
      "Epoch:  2363  Learning Rate:  0.00942315714645254  Varinance:  0.9202973306941274 \n",
      "\n",
      "Epoch:  2364  Learning Rate:  0.00941373869931453  Varinance:  0.9190117090560054 \n",
      "\n",
      "Epoch:  2365  Learning Rate:  0.009404329665916  Varinance:  0.9177278833841882 \n",
      "\n",
      "Epoch:  2366  Learning Rate:  0.009394930036847918  Varinance:  0.9164458511697766 \n",
      "\n",
      "Epoch:  2367  Learning Rate:  0.00938553980271066  Varinance:  0.9151656099073767 \n",
      "\n",
      "Epoch:  2368  Learning Rate:  0.009376158954113986  Varinance:  0.9138871570950944 \n",
      "\n",
      "Epoch:  2369  Learning Rate:  0.009366787481677047  Varinance:  0.9126104902345297 \n",
      "\n",
      "Epoch:  2370  Learning Rate:  0.009357425376028367  Varinance:  0.911335606830774 \n",
      "\n",
      "Epoch:  2371  Learning Rate:  0.009348072627805846  Varinance:  0.9100625043924032 \n",
      "\n",
      "Epoch:  2372  Learning Rate:  0.009338729227656732  Varinance:  0.908791180431474 \n",
      "\n",
      "Epoch:  2373  Learning Rate:  0.009329395166237625  Varinance:  0.9075216324635189 \n",
      "\n",
      "Epoch:  2374  Learning Rate:  0.009320070434214458  Varinance:  0.9062538580075407 \n",
      "\n",
      "Epoch:  2375  Learning Rate:  0.009310755022262505  Varinance:  0.9049878545860084 \n",
      "\n",
      "Epoch:  2376  Learning Rate:  0.00930144892106635  Varinance:  0.9037236197248512 \n",
      "\n",
      "Epoch:  2377  Learning Rate:  0.00929215212131989  Varinance:  0.9024611509534551 \n",
      "\n",
      "Epoch:  2378  Learning Rate:  0.009282864613726327  Varinance:  0.9012004458046581 \n",
      "\n",
      "Epoch:  2379  Learning Rate:  0.009273586388998147  Varinance:  0.8999415018147435 \n",
      "\n",
      "Epoch:  2380  Learning Rate:  0.009264317437857132  Varinance:  0.8986843165234373 \n",
      "\n",
      "Epoch:  2381  Learning Rate:  0.009255057751034329  Varinance:  0.8974288874739016 \n",
      "\n",
      "Epoch:  2382  Learning Rate:  0.009245807319270048  Varinance:  0.8961752122127317 \n",
      "\n",
      "Epoch:  2383  Learning Rate:  0.009236566133313851  Varinance:  0.8949232882899486 \n",
      "\n",
      "Epoch:  2384  Learning Rate:  0.009227334183924562  Varinance:  0.8936731132589972 \n",
      "\n",
      "Epoch:  2385  Learning Rate:  0.009218111461870225  Varinance:  0.8924246846767397 \n",
      "\n",
      "Epoch:  2386  Learning Rate:  0.00920889795792812  Varinance:  0.8911780001034516 \n",
      "\n",
      "Epoch:  2387  Learning Rate:  0.009199693662884733  Varinance:  0.8899330571028158 \n",
      "\n",
      "Epoch:  2388  Learning Rate:  0.009190498567535784  Varinance:  0.8886898532419198 \n",
      "\n",
      "Epoch:  2389  Learning Rate:  0.009181312662686165  Varinance:  0.8874483860912484 \n",
      "\n",
      "Epoch:  2390  Learning Rate:  0.009172135939149976  Varinance:  0.8862086532246812 \n",
      "\n",
      "Epoch:  2391  Learning Rate:  0.009162968387750483  Varinance:  0.8849706522194873 \n",
      "\n",
      "Epoch:  2392  Learning Rate:  0.00915380999932015  Varinance:  0.8837343806563194 \n",
      "\n",
      "Epoch:  2393  Learning Rate:  0.009144660764700576  Varinance:  0.8824998361192107 \n",
      "\n",
      "Epoch:  2394  Learning Rate:  0.00913552067474253  Varinance:  0.8812670161955688 \n",
      "\n",
      "Epoch:  2395  Learning Rate:  0.009126389720305916  Varinance:  0.8800359184761726 \n",
      "\n",
      "Epoch:  2396  Learning Rate:  0.009117267892259785  Varinance:  0.8788065405551649 \n",
      "\n",
      "Epoch:  2397  Learning Rate:  0.009108155181482308  Varinance:  0.8775788800300509 \n",
      "\n",
      "Epoch:  2398  Learning Rate:  0.009099051578860772  Varinance:  0.8763529345016915 \n",
      "\n",
      "Epoch:  2399  Learning Rate:  0.009089957075291567  Varinance:  0.875128701574299 \n",
      "\n",
      "Epoch:  2400  Learning Rate:  0.0090808716616802  Varinance:  0.8739061788554325 \n",
      "\n",
      "Epoch:  2401  Learning Rate:  0.009071795328941252  Varinance:  0.8726853639559942 \n",
      "\n",
      "Epoch:  2402  Learning Rate:  0.009062728067998389  Varinance:  0.8714662544902216 \n",
      "\n",
      "Epoch:  2403  Learning Rate:  0.009053669869784344  Varinance:  0.8702488480756874 \n",
      "\n",
      "Epoch:  2404  Learning Rate:  0.009044620725240926  Varinance:  0.869033142333291 \n",
      "\n",
      "Epoch:  2405  Learning Rate:  0.00903558062531899  Varinance:  0.8678191348872557 \n",
      "\n",
      "Epoch:  2406  Learning Rate:  0.00902654956097843  Varinance:  0.8666068233651242 \n",
      "\n",
      "Epoch:  2407  Learning Rate:  0.009017527523188181  Varinance:  0.8653962053977525 \n",
      "\n",
      "Epoch:  2408  Learning Rate:  0.00900851450292621  Varinance:  0.8641872786193072 \n",
      "\n",
      "Epoch:  2409  Learning Rate:  0.008999510491179492  Varinance:  0.8629800406672588 \n",
      "\n",
      "Epoch:  2410  Learning Rate:  0.008990515478944016  Varinance:  0.8617744891823789 \n",
      "\n",
      "Epoch:  2411  Learning Rate:  0.008981529457224764  Varinance:  0.8605706218087347 \n",
      "\n",
      "Epoch:  2412  Learning Rate:  0.00897255241703572  Varinance:  0.8593684361936846 \n",
      "\n",
      "Epoch:  2413  Learning Rate:  0.008963584349399843  Varinance:  0.8581679299878733 \n",
      "\n",
      "Epoch:  2414  Learning Rate:  0.008954625245349061  Varinance:  0.8569691008452279 \n",
      "\n",
      "Epoch:  2415  Learning Rate:  0.008945675095924267  Varinance:  0.855771946422953 \n",
      "\n",
      "Epoch:  2416  Learning Rate:  0.00893673389217532  Varinance:  0.8545764643815247 \n",
      "\n",
      "Epoch:  2417  Learning Rate:  0.008927801625161007  Varinance:  0.8533826523846886 \n",
      "\n",
      "Epoch:  2418  Learning Rate:  0.008918878285949065  Varinance:  0.8521905080994538 \n",
      "\n",
      "Epoch:  2419  Learning Rate:  0.008909963865616146  Varinance:  0.8510000291960883 \n",
      "\n",
      "Epoch:  2420  Learning Rate:  0.008901058355247843  Varinance:  0.8498112133481147 \n",
      "\n",
      "Epoch:  2421  Learning Rate:  0.008892161745938635  Varinance:  0.8486240582323057 \n",
      "\n",
      "Epoch:  2422  Learning Rate:  0.008883274028791913  Varinance:  0.8474385615286789 \n",
      "\n",
      "Epoch:  2423  Learning Rate:  0.008874395194919957  Varinance:  0.8462547209204936 \n",
      "\n",
      "Epoch:  2424  Learning Rate:  0.00886552523544394  Varinance:  0.8450725340942449 \n",
      "\n",
      "Epoch:  2425  Learning Rate:  0.008856664141493897  Varinance:  0.8438919987396604 \n",
      "\n",
      "Epoch:  2426  Learning Rate:  0.008847811904208731  Varinance:  0.8427131125496944 \n",
      "\n",
      "Epoch:  2427  Learning Rate:  0.008838968514736206  Varinance:  0.8415358732205246 \n",
      "\n",
      "Epoch:  2428  Learning Rate:  0.008830133964232935  Varinance:  0.8403602784515469 \n",
      "\n",
      "Epoch:  2429  Learning Rate:  0.008821308243864364  Varinance:  0.8391863259453708 \n",
      "\n",
      "Epoch:  2430  Learning Rate:  0.008812491344804772  Varinance:  0.8380140134078152 \n",
      "\n",
      "Epoch:  2431  Learning Rate:  0.008803683258237256  Varinance:  0.8368433385479044 \n",
      "\n",
      "Epoch:  2432  Learning Rate:  0.008794883975353733  Varinance:  0.8356742990778626 \n",
      "\n",
      "Epoch:  2433  Learning Rate:  0.00878609348735492  Varinance:  0.8345068927131102 \n",
      "\n",
      "Epoch:  2434  Learning Rate:  0.008777311785450327  Varinance:  0.8333411171722591 \n",
      "\n",
      "Epoch:  2435  Learning Rate:  0.008768538860858249  Varinance:  0.8321769701771077 \n",
      "\n",
      "Epoch:  2436  Learning Rate:  0.008759774704805764  Varinance:  0.8310144494526374 \n",
      "\n",
      "Epoch:  2437  Learning Rate:  0.008751019308528714  Varinance:  0.8298535527270078 \n",
      "\n",
      "Epoch:  2438  Learning Rate:  0.008742272663271702  Varinance:  0.8286942777315521 \n",
      "\n",
      "Epoch:  2439  Learning Rate:  0.008733534760288077  Varinance:  0.8275366222007726 \n",
      "\n",
      "Epoch:  2440  Learning Rate:  0.008724805590839945  Varinance:  0.8263805838723364 \n",
      "\n",
      "Epoch:  2441  Learning Rate:  0.00871608514619813  Varinance:  0.8252261604870718 \n",
      "\n",
      "Epoch:  2442  Learning Rate:  0.008707373417642187  Varinance:  0.8240733497889615 \n",
      "\n",
      "Epoch:  2443  Learning Rate:  0.008698670396460386  Varinance:  0.8229221495251408 \n",
      "\n",
      "Epoch:  2444  Learning Rate:  0.008689976073949708  Varinance:  0.8217725574458924 \n",
      "\n",
      "Epoch:  2445  Learning Rate:  0.008681290441415829  Varinance:  0.8206245713046411 \n",
      "\n",
      "Epoch:  2446  Learning Rate:  0.008672613490173115  Varinance:  0.8194781888579505 \n",
      "\n",
      "Epoch:  2447  Learning Rate:  0.008663945211544611  Varinance:  0.8183334078655182 \n",
      "\n",
      "Epoch:  2448  Learning Rate:  0.008655285596862043  Varinance:  0.8171902260901713 \n",
      "\n",
      "Epoch:  2449  Learning Rate:  0.008646634637465792  Varinance:  0.8160486412978618 \n",
      "\n",
      "Epoch:  2450  Learning Rate:  0.0086379923247049  Varinance:  0.8149086512576632 \n",
      "\n",
      "Epoch:  2451  Learning Rate:  0.00862935864993705  Varinance:  0.8137702537417653 \n",
      "\n",
      "Epoch:  2452  Learning Rate:  0.008620733604528572  Varinance:  0.8126334465254696 \n",
      "\n",
      "Epoch:  2453  Learning Rate:  0.008612117179854417  Varinance:  0.8114982273871866 \n",
      "\n",
      "Epoch:  2454  Learning Rate:  0.008603509367298158  Varinance:  0.8103645941084292 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2455  Learning Rate:  0.008594910158251982  Varinance:  0.8092325444738094 \n",
      "\n",
      "Epoch:  2456  Learning Rate:  0.008586319544116683  Varinance:  0.8081020762710347 \n",
      "\n",
      "Epoch:  2457  Learning Rate:  0.008577737516301644  Varinance:  0.8069731872909028 \n",
      "\n",
      "Epoch:  2458  Learning Rate:  0.008569164066224835  Varinance:  0.8058458753272975 \n",
      "\n",
      "Epoch:  2459  Learning Rate:  0.008560599185312804  Varinance:  0.8047201381771844 \n",
      "\n",
      "Epoch:  2460  Learning Rate:  0.008552042865000675  Varinance:  0.8035959736406069 \n",
      "\n",
      "Epoch:  2461  Learning Rate:  0.008543495096732124  Varinance:  0.802473379520682 \n",
      "\n",
      "Epoch:  2462  Learning Rate:  0.008534955871959382  Varinance:  0.8013523536235942 \n",
      "\n",
      "Epoch:  2463  Learning Rate:  0.008526425182143218  Varinance:  0.8002328937585944 \n",
      "\n",
      "Epoch:  2464  Learning Rate:  0.008517903018752949  Varinance:  0.799114997737993 \n",
      "\n",
      "Epoch:  2465  Learning Rate:  0.008509389373266412  Varinance:  0.7979986633771564 \n",
      "\n",
      "Epoch:  2466  Learning Rate:  0.008500884237169956  Varinance:  0.7968838884945038 \n",
      "\n",
      "Epoch:  2467  Learning Rate:  0.008492387601958442  Varinance:  0.7957706709115011 \n",
      "\n",
      "Epoch:  2468  Learning Rate:  0.008483899459135242  Varinance:  0.7946590084526574 \n",
      "\n",
      "Epoch:  2469  Learning Rate:  0.008475419800212207  Varinance:  0.7935488989455213 \n",
      "\n",
      "Epoch:  2470  Learning Rate:  0.008466948616709678  Varinance:  0.7924403402206767 \n",
      "\n",
      "Epoch:  2471  Learning Rate:  0.00845848590015647  Varinance:  0.7913333301117369 \n",
      "\n",
      "Epoch:  2472  Learning Rate:  0.00845003164208987  Varinance:  0.7902278664553428 \n",
      "\n",
      "Epoch:  2473  Learning Rate:  0.008441585834055614  Varinance:  0.7891239470911566 \n",
      "\n",
      "Epoch:  2474  Learning Rate:  0.008433148467607897  Varinance:  0.7880215698618591 \n",
      "\n",
      "Epoch:  2475  Learning Rate:  0.008424719534309347  Varinance:  0.7869207326131439 \n",
      "\n",
      "Epoch:  2476  Learning Rate:  0.008416299025731036  Varinance:  0.7858214331937147 \n",
      "\n",
      "Epoch:  2477  Learning Rate:  0.008407886933452454  Varinance:  0.7847236694552803 \n",
      "\n",
      "Epoch:  2478  Learning Rate:  0.008399483249061505  Varinance:  0.7836274392525506 \n",
      "\n",
      "Epoch:  2479  Learning Rate:  0.008391087964154501  Varinance:  0.7825327404432327 \n",
      "\n",
      "Epoch:  2480  Learning Rate:  0.008382701070336165  Varinance:  0.781439570888026 \n",
      "\n",
      "Epoch:  2481  Learning Rate:  0.008374322559219596  Varinance:  0.7803479284506188 \n",
      "\n",
      "Epoch:  2482  Learning Rate:  0.008365952422426286  Varinance:  0.7792578109976831 \n",
      "\n",
      "Epoch:  2483  Learning Rate:  0.00835759065158609  Varinance:  0.7781692163988716 \n",
      "\n",
      "Epoch:  2484  Learning Rate:  0.008349237238337248  Varinance:  0.7770821425268128 \n",
      "\n",
      "Epoch:  2485  Learning Rate:  0.008340892174326339  Varinance:  0.7759965872571075 \n",
      "\n",
      "Epoch:  2486  Learning Rate:  0.0083325554512083  Varinance:  0.7749125484683237 \n",
      "\n",
      "Epoch:  2487  Learning Rate:  0.008324227060646401  Varinance:  0.7738300240419934 \n",
      "\n",
      "Epoch:  2488  Learning Rate:  0.008315906994312262  Varinance:  0.7727490118626072 \n",
      "\n",
      "Epoch:  2489  Learning Rate:  0.00830759524388581  Varinance:  0.771669509817612 \n",
      "\n",
      "Epoch:  2490  Learning Rate:  0.008299291801055294  Varinance:  0.770591515797405 \n",
      "\n",
      "Epoch:  2491  Learning Rate:  0.008290996657517266  Varinance:  0.7695150276953313 \n",
      "\n",
      "Epoch:  2492  Learning Rate:  0.008282709804976592  Varinance:  0.7684400434076781 \n",
      "\n",
      "Epoch:  2493  Learning Rate:  0.008274431235146413  Varinance:  0.7673665608336722 \n",
      "\n",
      "Epoch:  2494  Learning Rate:  0.008266160939748158  Varinance:  0.7662945778754746 \n",
      "\n",
      "Epoch:  2495  Learning Rate:  0.008257898910511527  Varinance:  0.7652240924381765 \n",
      "\n",
      "Epoch:  2496  Learning Rate:  0.008249645139174498  Varinance:  0.7641551024297963 \n",
      "\n",
      "Epoch:  2497  Learning Rate:  0.008241399617483297  Varinance:  0.763087605761275 \n",
      "\n",
      "Epoch:  2498  Learning Rate:  0.008233162337192401  Varinance:  0.762021600346471 \n",
      "\n",
      "Epoch:  2499  Learning Rate:  0.008224933290064523  Varinance:  0.7609570841021579 \n",
      "\n",
      "Epoch:  2500  Learning Rate:  0.008216712467870625  Varinance:  0.7598940549480194 \n",
      "\n",
      "Epoch:  2501  Learning Rate:  0.008208499862389881  Varinance:  0.7588325108066443 \n",
      "\n",
      "Epoch:  2502  Learning Rate:  0.008200295465409681  Varinance:  0.7577724496035244 \n",
      "\n",
      "Epoch:  2503  Learning Rate:  0.008192099268725632  Varinance:  0.7567138692670495 \n",
      "\n",
      "Epoch:  2504  Learning Rate:  0.008183911264141528  Varinance:  0.7556567677285033 \n",
      "\n",
      "Epoch:  2505  Learning Rate:  0.008175731443469376  Varinance:  0.7546011429220589 \n",
      "\n",
      "Epoch:  2506  Learning Rate:  0.008167559798529346  Varinance:  0.7535469927847761 \n",
      "\n",
      "Epoch:  2507  Learning Rate:  0.008159396321149799  Varinance:  0.7524943152565962 \n",
      "\n",
      "Epoch:  2508  Learning Rate:  0.008151241003167246  Varinance:  0.7514431082803377 \n",
      "\n",
      "Epoch:  2509  Learning Rate:  0.008143093836426382  Varinance:  0.7503933698016937 \n",
      "\n",
      "Epoch:  2510  Learning Rate:  0.00813495481278003  Varinance:  0.749345097769227 \n",
      "\n",
      "Epoch:  2511  Learning Rate:  0.00812682392408917  Varinance:  0.7482982901343659 \n",
      "\n",
      "Epoch:  2512  Learning Rate:  0.00811870116222291  Varinance:  0.7472529448514007 \n",
      "\n",
      "Epoch:  2513  Learning Rate:  0.00811058651905849  Varinance:  0.7462090598774794 \n",
      "\n",
      "Epoch:  2514  Learning Rate:  0.008102479986481266  Varinance:  0.7451666331726042 \n",
      "\n",
      "Epoch:  2515  Learning Rate:  0.008094381556384702  Varinance:  0.744125662699626 \n",
      "\n",
      "Epoch:  2516  Learning Rate:  0.008086291220670366  Varinance:  0.7430861464242423 \n",
      "\n",
      "Epoch:  2517  Learning Rate:  0.00807820897124793  Varinance:  0.7420480823149928 \n",
      "\n",
      "Epoch:  2518  Learning Rate:  0.008070134800035135  Varinance:  0.7410114683432543 \n",
      "\n",
      "Epoch:  2519  Learning Rate:  0.008062068698957816  Varinance:  0.739976302483238 \n",
      "\n",
      "Epoch:  2520  Learning Rate:  0.008054010659949862  Varinance:  0.7389425827119851 \n",
      "\n",
      "Epoch:  2521  Learning Rate:  0.008045960674953244  Varinance:  0.7379103070093621 \n",
      "\n",
      "Epoch:  2522  Learning Rate:  0.00803791873591797  Varinance:  0.7368794733580583 \n",
      "\n",
      "Epoch:  2523  Learning Rate:  0.008029884834802102  Varinance:  0.7358500797435807 \n",
      "\n",
      "Epoch:  2524  Learning Rate:  0.008021858963571736  Varinance:  0.7348221241542511 \n",
      "\n",
      "Epoch:  2525  Learning Rate:  0.008013841114201005  Varinance:  0.7337956045812006 \n",
      "\n",
      "Epoch:  2526  Learning Rate:  0.008005831278672054  Varinance:  0.7327705190183674 \n",
      "\n",
      "Epoch:  2527  Learning Rate:  0.007997829448975051  Varinance:  0.731746865462492 \n",
      "\n",
      "Epoch:  2528  Learning Rate:  0.00798983561710816  Varinance:  0.7307246419131126 \n",
      "\n",
      "Epoch:  2529  Learning Rate:  0.007981849775077555  Varinance:  0.7297038463725626 \n",
      "\n",
      "Epoch:  2530  Learning Rate:  0.007973871914897391  Varinance:  0.7286844768459664 \n",
      "\n",
      "Epoch:  2531  Learning Rate:  0.007965902028589805  Varinance:  0.7276665313412345 \n",
      "\n",
      "Epoch:  2532  Learning Rate:  0.007957940108184907  Varinance:  0.7266500078690605 \n",
      "\n",
      "Epoch:  2533  Learning Rate:  0.007949986145720787  Varinance:  0.7256349044429176 \n",
      "\n",
      "Epoch:  2534  Learning Rate:  0.007942040133243473  Varinance:  0.7246212190790527 \n",
      "\n",
      "Epoch:  2535  Learning Rate:  0.007934102062806954  Varinance:  0.7236089497964854 \n",
      "\n",
      "Epoch:  2536  Learning Rate:  0.007926171926473156  Varinance:  0.7225980946170016 \n",
      "\n",
      "Epoch:  2537  Learning Rate:  0.007918249716311948  Varinance:  0.7215886515651515 \n",
      "\n",
      "Epoch:  2538  Learning Rate:  0.007910335424401118  Varinance:  0.7205806186682443 \n",
      "\n",
      "Epoch:  2539  Learning Rate:  0.007902429042826368  Varinance:  0.7195739939563455 \n",
      "\n",
      "Epoch:  2540  Learning Rate:  0.007894530563681319  Varinance:  0.7185687754622722 \n",
      "\n",
      "Epoch:  2541  Learning Rate:  0.007886639979067495  Varinance:  0.7175649612215893 \n",
      "\n",
      "Epoch:  2542  Learning Rate:  0.007878757281094306  Varinance:  0.7165625492726065 \n",
      "\n",
      "Epoch:  2543  Learning Rate:  0.007870882461879055  Varinance:  0.7155615376563738 \n",
      "\n",
      "Epoch:  2544  Learning Rate:  0.00786301551354692  Varinance:  0.7145619244166775 \n",
      "\n",
      "Epoch:  2545  Learning Rate:  0.007855156428230954  Varinance:  0.7135637076000371 \n",
      "\n",
      "Epoch:  2546  Learning Rate:  0.007847305198072073  Varinance:  0.7125668852557003 \n",
      "\n",
      "Epoch:  2547  Learning Rate:  0.007839461815219042  Varinance:  0.7115714554356413 \n",
      "\n",
      "Epoch:  2548  Learning Rate:  0.00783162627182848  Varinance:  0.7105774161945536 \n",
      "\n",
      "Epoch:  2549  Learning Rate:  0.007823798560064842  Varinance:  0.7095847655898501 \n",
      "\n",
      "Epoch:  2550  Learning Rate:  0.007815978672100418  Varinance:  0.7085935016816564 \n",
      "\n",
      "Epoch:  2551  Learning Rate:  0.007808166600115317  Varinance:  0.7076036225328083 \n",
      "\n",
      "Epoch:  2552  Learning Rate:  0.007800362336297464  Varinance:  0.7066151262088478 \n",
      "\n",
      "Epoch:  2553  Learning Rate:  0.0077925658728426  Varinance:  0.7056280107780195 \n",
      "\n",
      "Epoch:  2554  Learning Rate:  0.007784777201954258  Varinance:  0.7046422743112658 \n",
      "\n",
      "Epoch:  2555  Learning Rate:  0.0077769963158437665  Varinance:  0.7036579148822247 \n",
      "\n",
      "Epoch:  2556  Learning Rate:  0.007769223206730236  Varinance:  0.702674930567225 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2557  Learning Rate:  0.007761457866840563  Varinance:  0.7016933194452826 \n",
      "\n",
      "Epoch:  2558  Learning Rate:  0.0077537002884094045  Varinance:  0.7007130795980974 \n",
      "\n",
      "Epoch:  2559  Learning Rate:  0.007745950463679179  Varinance:  0.6997342091100487 \n",
      "\n",
      "Epoch:  2560  Learning Rate:  0.00773820838490006  Varinance:  0.6987567060681922 \n",
      "\n",
      "Epoch:  2561  Learning Rate:  0.007730474044329975  Varinance:  0.6977805685622552 \n",
      "\n",
      "Epoch:  2562  Learning Rate:  0.007722747434234577  Varinance:  0.6968057946846344 \n",
      "\n",
      "Epoch:  2563  Learning Rate:  0.007715028546887258  Varinance:  0.695832382530391 \n",
      "\n",
      "Epoch:  2564  Learning Rate:  0.007707317374569124  Varinance:  0.6948603301972474 \n",
      "\n",
      "Epoch:  2565  Learning Rate:  0.007699613909569011  Varinance:  0.693889635785583 \n",
      "\n",
      "Epoch:  2566  Learning Rate:  0.007691918144183451  Varinance:  0.6929202973984319 \n",
      "\n",
      "Epoch:  2567  Learning Rate:  0.007684230070716673  Varinance:  0.6919523131414773 \n",
      "\n",
      "Epoch:  2568  Learning Rate:  0.007676549681480605  Varinance:  0.6909856811230487 \n",
      "\n",
      "Epoch:  2569  Learning Rate:  0.007668876968794861  Varinance:  0.6900203994541186 \n",
      "\n",
      "Epoch:  2570  Learning Rate:  0.007661211924986725  Varinance:  0.6890564662482983 \n",
      "\n",
      "Epoch:  2571  Learning Rate:  0.007653554542391152  Varinance:  0.6880938796218342 \n",
      "\n",
      "Epoch:  2572  Learning Rate:  0.007645904813350756  Varinance:  0.6871326376936043 \n",
      "\n",
      "Epoch:  2573  Learning Rate:  0.007638262730215813  Varinance:  0.6861727385851148 \n",
      "\n",
      "Epoch:  2574  Learning Rate:  0.007630628285344238  Varinance:  0.6852141804204952 \n",
      "\n",
      "Epoch:  2575  Learning Rate:  0.007623001471101583  Varinance:  0.6842569613264962 \n",
      "\n",
      "Epoch:  2576  Learning Rate:  0.007615382279861033  Varinance:  0.6833010794324853 \n",
      "\n",
      "Epoch:  2577  Learning Rate:  0.007607770704003399  Varinance:  0.6823465328704432 \n",
      "\n",
      "Epoch:  2578  Learning Rate:  0.0076001667359171035  Varinance:  0.68139331977496 \n",
      "\n",
      "Epoch:  2579  Learning Rate:  0.007592570367998178  Varinance:  0.6804414382832319 \n",
      "\n",
      "Epoch:  2580  Learning Rate:  0.007584981592650249  Varinance:  0.6794908865350575 \n",
      "\n",
      "Epoch:  2581  Learning Rate:  0.007577400402284548  Varinance:  0.678541662672833 \n",
      "\n",
      "Epoch:  2582  Learning Rate:  0.007569826789319881  Varinance:  0.6775937648415511 \n",
      "\n",
      "Epoch:  2583  Learning Rate:  0.007562260746182634  Varinance:  0.6766471911887948 \n",
      "\n",
      "Epoch:  2584  Learning Rate:  0.007554702265306761  Varinance:  0.6757019398647355 \n",
      "\n",
      "Epoch:  2585  Learning Rate:  0.007547151339133785  Varinance:  0.6747580090221283 \n",
      "\n",
      "Epoch:  2586  Learning Rate:  0.007539607960112777  Varinance:  0.6738153968163096 \n",
      "\n",
      "Epoch:  2587  Learning Rate:  0.007532072120700358  Varinance:  0.6728741014051911 \n",
      "\n",
      "Epoch:  2588  Learning Rate:  0.007524543813360683  Varinance:  0.6719341209492596 \n",
      "\n",
      "Epoch:  2589  Learning Rate:  0.007517023030565454  Varinance:  0.6709954536115706 \n",
      "\n",
      "Epoch:  2590  Learning Rate:  0.00750950976479388  Varinance:  0.6700580975577461 \n",
      "\n",
      "Epoch:  2591  Learning Rate:  0.0075020040085326985  Varinance:  0.6691220509559708 \n",
      "\n",
      "Epoch:  2592  Learning Rate:  0.007494505754276144  Varinance:  0.6681873119769879 \n",
      "\n",
      "Epoch:  2593  Learning Rate:  0.007487014994525975  Varinance:  0.6672538787940969 \n",
      "\n",
      "Epoch:  2594  Learning Rate:  0.007479531721791423  Varinance:  0.6663217495831476 \n",
      "\n",
      "Epoch:  2595  Learning Rate:  0.007472055928589217  Varinance:  0.6653909225225395 \n",
      "\n",
      "Epoch:  2596  Learning Rate:  0.007464587607443557  Varinance:  0.6644613957932163 \n",
      "\n",
      "Epoch:  2597  Learning Rate:  0.007457126750886132  Varinance:  0.6635331675786628 \n",
      "\n",
      "Epoch:  2598  Learning Rate:  0.007449673351456078  Varinance:  0.6626062360649013 \n",
      "\n",
      "Epoch:  2599  Learning Rate:  0.007442227401699995  Varinance:  0.6616805994404887 \n",
      "\n",
      "Epoch:  2600  Learning Rate:  0.007434788894171933  Varinance:  0.6607562558965121 \n",
      "\n",
      "Epoch:  2601  Learning Rate:  0.007427357821433388  Varinance:  0.6598332036265849 \n",
      "\n",
      "Epoch:  2602  Learning Rate:  0.007419934176053283  Varinance:  0.658911440826845 \n",
      "\n",
      "Epoch:  2603  Learning Rate:  0.007412517950607971  Varinance:  0.6579909656959498 \n",
      "\n",
      "Epoch:  2604  Learning Rate:  0.007405109137681225  Varinance:  0.6570717764350731 \n",
      "\n",
      "Epoch:  2605  Learning Rate:  0.007397707729864237  Varinance:  0.6561538712479015 \n",
      "\n",
      "Epoch:  2606  Learning Rate:  0.007390313719755595  Varinance:  0.6552372483406315 \n",
      "\n",
      "Epoch:  2607  Learning Rate:  0.00738292709996129  Varinance:  0.6543219059219645 \n",
      "\n",
      "Epoch:  2608  Learning Rate:  0.007375547863094695  Varinance:  0.6534078422031052 \n",
      "\n",
      "Epoch:  2609  Learning Rate:  0.007368176001776583  Varinance:  0.6524950553977568 \n",
      "\n",
      "Epoch:  2610  Learning Rate:  0.007360811508635085  Varinance:  0.6515835437221178 \n",
      "\n",
      "Epoch:  2611  Learning Rate:  0.00735345437630571  Varinance:  0.6506733053948789 \n",
      "\n",
      "Epoch:  2612  Learning Rate:  0.0073461045974313206  Varinance:  0.649764338637219 \n",
      "\n",
      "Epoch:  2613  Learning Rate:  0.007338762164662144  Varinance:  0.6488566416728023 \n",
      "\n",
      "Epoch:  2614  Learning Rate:  0.007331427070655744  Varinance:  0.6479502127277736 \n",
      "\n",
      "Epoch:  2615  Learning Rate:  0.0073240993080770245  Varinance:  0.6470450500307566 \n",
      "\n",
      "Epoch:  2616  Learning Rate:  0.007316778869598222  Varinance:  0.6461411518128491 \n",
      "\n",
      "Epoch:  2617  Learning Rate:  0.007309465747898901  Varinance:  0.6452385163076204 \n",
      "\n",
      "Epoch:  2618  Learning Rate:  0.007302159935665936  Varinance:  0.644337141751107 \n",
      "\n",
      "Epoch:  2619  Learning Rate:  0.007294861425593518  Varinance:  0.6434370263818101 \n",
      "\n",
      "Epoch:  2620  Learning Rate:  0.007287570210383128  Varinance:  0.6425381684406907 \n",
      "\n",
      "Epoch:  2621  Learning Rate:  0.007280286282743559  Varinance:  0.641640566171168 \n",
      "\n",
      "Epoch:  2622  Learning Rate:  0.00727300963539088  Varinance:  0.6407442178191148 \n",
      "\n",
      "Epoch:  2623  Learning Rate:  0.007265740261048443  Varinance:  0.6398491216328546 \n",
      "\n",
      "Epoch:  2624  Learning Rate:  0.007258478152446868  Varinance:  0.6389552758631573 \n",
      "\n",
      "Epoch:  2625  Learning Rate:  0.007251223302324053  Varinance:  0.638062678763237 \n",
      "\n",
      "Epoch:  2626  Learning Rate:  0.007243975703425146  Varinance:  0.6371713285887479 \n",
      "\n",
      "Epoch:  2627  Learning Rate:  0.0072367353485025465  Varinance:  0.6362812235977807 \n",
      "\n",
      "Epoch:  2628  Learning Rate:  0.007229502230315897  Varinance:  0.6353923620508596 \n",
      "\n",
      "Epoch:  2629  Learning Rate:  0.007222276341632078  Varinance:  0.634504742210939 \n",
      "\n",
      "Epoch:  2630  Learning Rate:  0.007215057675225206  Varinance:  0.6336183623433996 \n",
      "\n",
      "Epoch:  2631  Learning Rate:  0.00720784622387661  Varinance:  0.6327332207160457 \n",
      "\n",
      "Epoch:  2632  Learning Rate:  0.007200641980374839  Varinance:  0.6318493155991007 \n",
      "\n",
      "Epoch:  2633  Learning Rate:  0.007193444937515645  Varinance:  0.6309666452652056 \n",
      "\n",
      "Epoch:  2634  Learning Rate:  0.007186255088101991  Varinance:  0.6300852079894128 \n",
      "\n",
      "Epoch:  2635  Learning Rate:  0.007179072424944025  Varinance:  0.6292050020491861 \n",
      "\n",
      "Epoch:  2636  Learning Rate:  0.007171896940859081  Varinance:  0.6283260257243943 \n",
      "\n",
      "Epoch:  2637  Learning Rate:  0.007164728628671672  Varinance:  0.6274482772973102 \n",
      "\n",
      "Epoch:  2638  Learning Rate:  0.007157567481213493  Varinance:  0.6265717550526053 \n",
      "\n",
      "Epoch:  2639  Learning Rate:  0.00715041349132339  Varinance:  0.6256964572773484 \n",
      "\n",
      "Epoch:  2640  Learning Rate:  0.007143266651847376  Varinance:  0.6248223822609996 \n",
      "\n",
      "Epoch:  2641  Learning Rate:  0.007136126955638606  Varinance:  0.6239495282954102 \n",
      "\n",
      "Epoch:  2642  Learning Rate:  0.0071289943955573885  Varinance:  0.6230778936748169 \n",
      "\n",
      "Epoch:  2643  Learning Rate:  0.007121868964471161  Varinance:  0.6222074766958393 \n",
      "\n",
      "Epoch:  2644  Learning Rate:  0.007114750655254492  Varinance:  0.6213382756574768 \n",
      "\n",
      "Epoch:  2645  Learning Rate:  0.0071076394607890656  Varinance:  0.620470288861105 \n",
      "\n",
      "Epoch:  2646  Learning Rate:  0.007100535373963698  Varinance:  0.6196035146104725 \n",
      "\n",
      "Epoch:  2647  Learning Rate:  0.007093438387674295  Varinance:  0.6187379512116968 \n",
      "\n",
      "Epoch:  2648  Learning Rate:  0.007086348494823872  Varinance:  0.6178735969732627 \n",
      "\n",
      "Epoch:  2649  Learning Rate:  0.00707926568832253  Varinance:  0.6170104502060174 \n",
      "\n",
      "Epoch:  2650  Learning Rate:  0.00707218996108747  Varinance:  0.6161485092231678 \n",
      "\n",
      "Epoch:  2651  Learning Rate:  0.00706512130604296  Varinance:  0.6152877723402773 \n",
      "\n",
      "Epoch:  2652  Learning Rate:  0.0070580597161203455  Varinance:  0.614428237875263 \n",
      "\n",
      "Epoch:  2653  Learning Rate:  0.007051005184258031  Varinance:  0.6135699041483903 \n",
      "\n",
      "Epoch:  2654  Learning Rate:  0.007043957703401492  Varinance:  0.6127127694822724 \n",
      "\n",
      "Epoch:  2655  Learning Rate:  0.007036917266503243  Varinance:  0.6118568322018655 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2656  Learning Rate:  0.007029883866522847  Varinance:  0.6110020906344654 \n",
      "\n",
      "Epoch:  2657  Learning Rate:  0.007022857496426902  Varinance:  0.610148543109705 \n",
      "\n",
      "Epoch:  2658  Learning Rate:  0.0070158381491890395  Varinance:  0.6092961879595505 \n",
      "\n",
      "Epoch:  2659  Learning Rate:  0.007008825817789912  Varinance:  0.6084450235182987 \n",
      "\n",
      "Epoch:  2660  Learning Rate:  0.007001820495217186  Varinance:  0.607595048122572 \n",
      "\n",
      "Epoch:  2661  Learning Rate:  0.006994822174465536  Varinance:  0.606746260111318 \n",
      "\n",
      "Epoch:  2662  Learning Rate:  0.006987830848536646  Varinance:  0.6058986578258041 \n",
      "\n",
      "Epoch:  2663  Learning Rate:  0.006980846510439187  Varinance:  0.6050522396096146 \n",
      "\n",
      "Epoch:  2664  Learning Rate:  0.00697386915318882  Varinance:  0.6042070038086483 \n",
      "\n",
      "Epoch:  2665  Learning Rate:  0.006966898769808185  Varinance:  0.6033629487711145 \n",
      "\n",
      "Epoch:  2666  Learning Rate:  0.006959935353326902  Varinance:  0.6025200728475302 \n",
      "\n",
      "Epoch:  2667  Learning Rate:  0.006952978896781554  Varinance:  0.6016783743907157 \n",
      "\n",
      "Epoch:  2668  Learning Rate:  0.006946029393215681  Varinance:  0.6008378517557935 \n",
      "\n",
      "Epoch:  2669  Learning Rate:  0.006939086835679777  Varinance:  0.5999985033001836 \n",
      "\n",
      "Epoch:  2670  Learning Rate:  0.00693215121723129  Varinance:  0.5991603273836004 \n",
      "\n",
      "Epoch:  2671  Learning Rate:  0.0069252225309346  Varinance:  0.59832332236805 \n",
      "\n",
      "Epoch:  2672  Learning Rate:  0.006918300769861015  Varinance:  0.5974874866178266 \n",
      "\n",
      "Epoch:  2673  Learning Rate:  0.006911385927088775  Varinance:  0.5966528184995091 \n",
      "\n",
      "Epoch:  2674  Learning Rate:  0.006904477995703042  Varinance:  0.5958193163819587 \n",
      "\n",
      "Epoch:  2675  Learning Rate:  0.0068975769687958785  Varinance:  0.5949869786363151 \n",
      "\n",
      "Epoch:  2676  Learning Rate:  0.006890682839466259  Varinance:  0.5941558036359934 \n",
      "\n",
      "Epoch:  2677  Learning Rate:  0.006883795600820049  Varinance:  0.593325789756681 \n",
      "\n",
      "Epoch:  2678  Learning Rate:  0.006876915245970019  Varinance:  0.5924969353763344 \n",
      "\n",
      "Epoch:  2679  Learning Rate:  0.006870041768035806  Varinance:  0.5916692388751764 \n",
      "\n",
      "Epoch:  2680  Learning Rate:  0.006863175160143934  Varinance:  0.5908426986356916 \n",
      "\n",
      "Epoch:  2681  Learning Rate:  0.006856315415427791  Varinance:  0.5900173130426251 \n",
      "\n",
      "Epoch:  2682  Learning Rate:  0.006849462527027637  Varinance:  0.5891930804829781 \n",
      "\n",
      "Epoch:  2683  Learning Rate:  0.006842616488090584  Varinance:  0.5883699993460053 \n",
      "\n",
      "Epoch:  2684  Learning Rate:  0.006835777291770587  Varinance:  0.5875480680232114 \n",
      "\n",
      "Epoch:  2685  Learning Rate:  0.0068289449312284485  Varinance:  0.5867272849083484 \n",
      "\n",
      "Epoch:  2686  Learning Rate:  0.006822119399631813  Varinance:  0.5859076483974113 \n",
      "\n",
      "Epoch:  2687  Learning Rate:  0.006815300690155146  Varinance:  0.5850891568886368 \n",
      "\n",
      "Epoch:  2688  Learning Rate:  0.006808488795979737  Varinance:  0.5842718087824988 \n",
      "\n",
      "Epoch:  2689  Learning Rate:  0.006801683710293688  Varinance:  0.5834556024817058 \n",
      "\n",
      "Epoch:  2690  Learning Rate:  0.006794885426291921  Varinance:  0.5826405363911976 \n",
      "\n",
      "Epoch:  2691  Learning Rate:  0.006788093937176144  Varinance:  0.5818266089181421 \n",
      "\n",
      "Epoch:  2692  Learning Rate:  0.006781309236154872  Varinance:  0.5810138184719327 \n",
      "\n",
      "Epoch:  2693  Learning Rate:  0.006774531316443397  Varinance:  0.5802021634641842 \n",
      "\n",
      "Epoch:  2694  Learning Rate:  0.0067677601712638055  Varinance:  0.5793916423087309 \n",
      "\n",
      "Epoch:  2695  Learning Rate:  0.00676099579384495  Varinance:  0.5785822534216226 \n",
      "\n",
      "Epoch:  2696  Learning Rate:  0.006754238177422452  Varinance:  0.577773995221122 \n",
      "\n",
      "Epoch:  2697  Learning Rate:  0.00674748731523869  Varinance:  0.5769668661277014 \n",
      "\n",
      "Epoch:  2698  Learning Rate:  0.006740743200542811  Varinance:  0.5761608645640396 \n",
      "\n",
      "Epoch:  2699  Learning Rate:  0.006734005826590692  Varinance:  0.575355988955019 \n",
      "\n",
      "Epoch:  2700  Learning Rate:  0.006727275186644961  Varinance:  0.5745522377277218 \n",
      "\n",
      "Epoch:  2701  Learning Rate:  0.006720551273974976  Varinance:  0.5737496093114283 \n",
      "\n",
      "Epoch:  2702  Learning Rate:  0.006713834081856826  Varinance:  0.5729481021376125 \n",
      "\n",
      "Epoch:  2703  Learning Rate:  0.006707123603573319  Varinance:  0.5721477146399397 \n",
      "\n",
      "Epoch:  2704  Learning Rate:  0.006700419832413974  Varinance:  0.5713484452542636 \n",
      "\n",
      "Epoch:  2705  Learning Rate:  0.006693722761675016  Varinance:  0.570550292418623 \n",
      "\n",
      "Epoch:  2706  Learning Rate:  0.006687032384659381  Varinance:  0.5697532545732377 \n",
      "\n",
      "Epoch:  2707  Learning Rate:  0.006680348694676687  Varinance:  0.5689573301605075 \n",
      "\n",
      "Epoch:  2708  Learning Rate:  0.006673671685043247  Varinance:  0.5681625176250079 \n",
      "\n",
      "Epoch:  2709  Learning Rate:  0.006667001349082043  Varinance:  0.567368815413487 \n",
      "\n",
      "Epoch:  2710  Learning Rate:  0.006660337680122747  Varinance:  0.5665762219748631 \n",
      "\n",
      "Epoch:  2711  Learning Rate:  0.006653680671501686  Varinance:  0.5657847357602209 \n",
      "\n",
      "Epoch:  2712  Learning Rate:  0.0066470303165618505  Varinance:  0.5649943552228095 \n",
      "\n",
      "Epoch:  2713  Learning Rate:  0.006640386608652883  Varinance:  0.5642050788180377 \n",
      "\n",
      "Epoch:  2714  Learning Rate:  0.006633749541131082  Varinance:  0.5634169050034731 \n",
      "\n",
      "Epoch:  2715  Learning Rate:  0.006627119107359372  Varinance:  0.5626298322388373 \n",
      "\n",
      "Epoch:  2716  Learning Rate:  0.006620495300707324  Varinance:  0.5618438589860043 \n",
      "\n",
      "Epoch:  2717  Learning Rate:  0.006613878114551126  Varinance:  0.5610589837089962 \n",
      "\n",
      "Epoch:  2718  Learning Rate:  0.006607267542273594  Varinance:  0.5602752048739812 \n",
      "\n",
      "Epoch:  2719  Learning Rate:  0.006600663577264156  Varinance:  0.5594925209492696 \n",
      "\n",
      "Epoch:  2720  Learning Rate:  0.006594066212918847  Varinance:  0.5587109304053121 \n",
      "\n",
      "Epoch:  2721  Learning Rate:  0.006587475442640295  Varinance:  0.5579304317146957 \n",
      "\n",
      "Epoch:  2722  Learning Rate:  0.006580891259837739  Varinance:  0.5571510233521416 \n",
      "\n",
      "Epoch:  2723  Learning Rate:  0.006574313657926991  Varinance:  0.5563727037945013 \n",
      "\n",
      "Epoch:  2724  Learning Rate:  0.006567742630330448  Varinance:  0.5555954715207543 \n",
      "\n",
      "Epoch:  2725  Learning Rate:  0.006561178170477081  Varinance:  0.554819325012005 \n",
      "\n",
      "Epoch:  2726  Learning Rate:  0.006554620271802434  Varinance:  0.5540442627514791 \n",
      "\n",
      "Epoch:  2727  Learning Rate:  0.006548068927748603  Varinance:  0.5532702832245218 \n",
      "\n",
      "Epoch:  2728  Learning Rate:  0.006541524131764247  Varinance:  0.5524973849185939 \n",
      "\n",
      "Epoch:  2729  Learning Rate:  0.006534985877304566  Varinance:  0.5517255663232695 \n",
      "\n",
      "Epoch:  2730  Learning Rate:  0.006528454157831308  Varinance:  0.5509548259302322 \n",
      "\n",
      "Epoch:  2731  Learning Rate:  0.006521928966812753  Varinance:  0.550185162233273 \n",
      "\n",
      "Epoch:  2732  Learning Rate:  0.006515410297723708  Varinance:  0.5494165737282871 \n",
      "\n",
      "Epoch:  2733  Learning Rate:  0.0065088981440455  Varinance:  0.5486490589132705 \n",
      "\n",
      "Epoch:  2734  Learning Rate:  0.006502392499265983  Varinance:  0.5478826162883177 \n",
      "\n",
      "Epoch:  2735  Learning Rate:  0.006495893356879505  Varinance:  0.5471172443556186 \n",
      "\n",
      "Epoch:  2736  Learning Rate:  0.006489400710386927  Varinance:  0.5463529416194552 \n",
      "\n",
      "Epoch:  2737  Learning Rate:  0.006482914553295596  Varinance:  0.5455897065861994 \n",
      "\n",
      "Epoch:  2738  Learning Rate:  0.006476434879119363  Varinance:  0.5448275377643094 \n",
      "\n",
      "Epoch:  2739  Learning Rate:  0.006469961681378547  Varinance:  0.5440664336643265 \n",
      "\n",
      "Epoch:  2740  Learning Rate:  0.0064634949535999535  Varinance:  0.5433063927988735 \n",
      "\n",
      "Epoch:  2741  Learning Rate:  0.006457034689316847  Varinance:  0.5425474136826508 \n",
      "\n",
      "Epoch:  2742  Learning Rate:  0.006450580882068973  Varinance:  0.5417894948324337 \n",
      "\n",
      "Epoch:  2743  Learning Rate:  0.006444133525402518  Varinance:  0.5410326347670693 \n",
      "\n",
      "Epoch:  2744  Learning Rate:  0.0064376926128701245  Varinance:  0.5402768320074741 \n",
      "\n",
      "Epoch:  2745  Learning Rate:  0.006431258138030878  Varinance:  0.5395220850766307 \n",
      "\n",
      "Epoch:  2746  Learning Rate:  0.006424830094450308  Varinance:  0.5387683924995849 \n",
      "\n",
      "Epoch:  2747  Learning Rate:  0.006418408475700368  Varinance:  0.538015752803443 \n",
      "\n",
      "Epoch:  2748  Learning Rate:  0.0064119932753594405  Varinance:  0.5372641645173694 \n",
      "\n",
      "Epoch:  2749  Learning Rate:  0.006405584487012317  Varinance:  0.5365136261725822 \n",
      "\n",
      "Epoch:  2750  Learning Rate:  0.006399182104250219  Varinance:  0.5357641363023523 \n",
      "\n",
      "Epoch:  2751  Learning Rate:  0.006392786120670757  Varinance:  0.5350156934419994 \n",
      "\n",
      "Epoch:  2752  Learning Rate:  0.00638639652987795  Varinance:  0.5342682961288885 \n",
      "\n",
      "Epoch:  2753  Learning Rate:  0.006380013325482204  Varinance:  0.5335219429024286 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2754  Learning Rate:  0.0063736365011003125  Varinance:  0.5327766323040691 \n",
      "\n",
      "Epoch:  2755  Learning Rate:  0.006367266050355457  Varinance:  0.5320323628772966 \n",
      "\n",
      "Epoch:  2756  Learning Rate:  0.006360901966877181  Varinance:  0.5312891331676327 \n",
      "\n",
      "Epoch:  2757  Learning Rate:  0.006354544244301402  Varinance:  0.5305469417226306 \n",
      "\n",
      "Epoch:  2758  Learning Rate:  0.0063481928762703956  Varinance:  0.5298057870918729 \n",
      "\n",
      "Epoch:  2759  Learning Rate:  0.0063418478564327965  Varinance:  0.5290656678269676 \n",
      "\n",
      "Epoch:  2760  Learning Rate:  0.0063355091784435815  Varinance:  0.5283265824815468 \n",
      "\n",
      "Epoch:  2761  Learning Rate:  0.006329176835964074  Varinance:  0.5275885296112629 \n",
      "\n",
      "Epoch:  2762  Learning Rate:  0.006322850822661927  Varinance:  0.526851507773786 \n",
      "\n",
      "Epoch:  2763  Learning Rate:  0.006316531132211131  Varinance:  0.5261155155288011 \n",
      "\n",
      "Epoch:  2764  Learning Rate:  0.006310217758291995  Varinance:  0.525380551438005 \n",
      "\n",
      "Epoch:  2765  Learning Rate:  0.0063039106945911435  Varinance:  0.5246466140651046 \n",
      "\n",
      "Epoch:  2766  Learning Rate:  0.006297609934801507  Varinance:  0.5239137019758118 \n",
      "\n",
      "Epoch:  2767  Learning Rate:  0.006291315472622335  Varinance:  0.5231818137378433 \n",
      "\n",
      "Epoch:  2768  Learning Rate:  0.00628502730175916  Varinance:  0.5224509479209164 \n",
      "\n",
      "Epoch:  2769  Learning Rate:  0.006278745415923809  Varinance:  0.5217211030967465 \n",
      "\n",
      "Epoch:  2770  Learning Rate:  0.006272469808834395  Varinance:  0.5209922778390439 \n",
      "\n",
      "Epoch:  2771  Learning Rate:  0.006266200474215316  Varinance:  0.520264470723512 \n",
      "\n",
      "Epoch:  2772  Learning Rate:  0.006259937405797233  Varinance:  0.5195376803278431 \n",
      "\n",
      "Epoch:  2773  Learning Rate:  0.006253680597317077  Varinance:  0.5188119052317167 \n",
      "\n",
      "Epoch:  2774  Learning Rate:  0.006247430042518036  Varinance:  0.5180871440167971 \n",
      "\n",
      "Epoch:  2775  Learning Rate:  0.0062411857351495625  Varinance:  0.5173633952667289 \n",
      "\n",
      "Epoch:  2776  Learning Rate:  0.0062349476689673436  Varinance:  0.5166406575671362 \n",
      "\n",
      "Epoch:  2777  Learning Rate:  0.0062287158377333125  Varinance:  0.5159189295056184 \n",
      "\n",
      "Epoch:  2778  Learning Rate:  0.006222490235215636  Varinance:  0.5151982096717481 \n",
      "\n",
      "Epoch:  2779  Learning Rate:  0.006216270855188717  Varinance:  0.5144784966570682 \n",
      "\n",
      "Epoch:  2780  Learning Rate:  0.00621005769143317  Varinance:  0.5137597890550891 \n",
      "\n",
      "Epoch:  2781  Learning Rate:  0.006203850737735832  Varinance:  0.5130420854612864 \n",
      "\n",
      "Epoch:  2782  Learning Rate:  0.006197649987889747  Varinance:  0.5123253844730972 \n",
      "\n",
      "Epoch:  2783  Learning Rate:  0.006191455435694168  Varinance:  0.5116096846899184 \n",
      "\n",
      "Epoch:  2784  Learning Rate:  0.006185267074954541  Varinance:  0.5108949847131035 \n",
      "\n",
      "Epoch:  2785  Learning Rate:  0.006179084899482505  Varinance:  0.5101812831459589 \n",
      "\n",
      "Epoch:  2786  Learning Rate:  0.00617290890309588  Varinance:  0.5094685785937436 \n",
      "\n",
      "Epoch:  2787  Learning Rate:  0.006166739079618675  Varinance:  0.508756869663664 \n",
      "\n",
      "Epoch:  2788  Learning Rate:  0.006160575422881064  Varinance:  0.5080461549648725 \n",
      "\n",
      "Epoch:  2789  Learning Rate:  0.006154417926719389  Varinance:  0.5073364331084644 \n",
      "\n",
      "Epoch:  2790  Learning Rate:  0.00614826658497615  Varinance:  0.5066277027074755 \n",
      "\n",
      "Epoch:  2791  Learning Rate:  0.006142121391500013  Varinance:  0.5059199623768789 \n",
      "\n",
      "Epoch:  2792  Learning Rate:  0.006135982340145778  Varinance:  0.5052132107335822 \n",
      "\n",
      "Epoch:  2793  Learning Rate:  0.006129849424774396  Varinance:  0.5045074463964256 \n",
      "\n",
      "Epoch:  2794  Learning Rate:  0.006123722639252945  Varinance:  0.5038026679861789 \n",
      "\n",
      "Epoch:  2795  Learning Rate:  0.006117601977454647  Varinance:  0.503098874125538 \n",
      "\n",
      "Epoch:  2796  Learning Rate:  0.0061114874332588364  Varinance:  0.5023960634391234 \n",
      "\n",
      "Epoch:  2797  Learning Rate:  0.006105379000550968  Varinance:  0.5016942345534766 \n",
      "\n",
      "Epoch:  2798  Learning Rate:  0.006099276673222607  Varinance:  0.5009933860970585 \n",
      "\n",
      "Epoch:  2799  Learning Rate:  0.006093180445171429  Varinance:  0.5002935167002444 \n",
      "\n",
      "Epoch:  2800  Learning Rate:  0.006087090310301205  Varinance:  0.4995946249953245 \n",
      "\n",
      "Epoch:  2801  Learning Rate:  0.006081006262521798  Varinance:  0.49889670961649896 \n",
      "\n",
      "Epoch:  2802  Learning Rate:  0.006074928295749158  Varinance:  0.4981997691998761 \n",
      "\n",
      "Epoch:  2803  Learning Rate:  0.0060688564039053225  Varinance:  0.4975038023834693 \n",
      "\n",
      "Epoch:  2804  Learning Rate:  0.006062790580918396  Varinance:  0.49680880780719516 \n",
      "\n",
      "Epoch:  2805  Learning Rate:  0.006056730820722556  Varinance:  0.49611478411286924 \n",
      "\n",
      "Epoch:  2806  Learning Rate:  0.006050677117258039  Varinance:  0.4954217299442054 \n",
      "\n",
      "Epoch:  2807  Learning Rate:  0.006044629464471147  Varinance:  0.49472964394681174 \n",
      "\n",
      "Epoch:  2808  Learning Rate:  0.0060385878563142216  Varinance:  0.4940385247681885 \n",
      "\n",
      "Epoch:  2809  Learning Rate:  0.006032552286745657  Varinance:  0.49334837105772533 \n",
      "\n",
      "Epoch:  2810  Learning Rate:  0.006026522749729878  Varinance:  0.4926591814666985 \n",
      "\n",
      "Epoch:  2811  Learning Rate:  0.006020499239237354  Varinance:  0.49197095464826884 \n",
      "\n",
      "Epoch:  2812  Learning Rate:  0.006014481749244571  Varinance:  0.4912836892574779 \n",
      "\n",
      "Epoch:  2813  Learning Rate:  0.006008470273734039  Varinance:  0.4905973839512466 \n",
      "\n",
      "Epoch:  2814  Learning Rate:  0.006002464806694279  Varinance:  0.4899120373883721 \n",
      "\n",
      "Epoch:  2815  Learning Rate:  0.0059964653421198276  Varinance:  0.48922764822952497 \n",
      "\n",
      "Epoch:  2816  Learning Rate:  0.005990471874011218  Varinance:  0.48854421513724694 \n",
      "\n",
      "Epoch:  2817  Learning Rate:  0.005984484396374983  Varinance:  0.48786173677594835 \n",
      "\n",
      "Epoch:  2818  Learning Rate:  0.005978502903223638  Varinance:  0.4871802118119044 \n",
      "\n",
      "Epoch:  2819  Learning Rate:  0.0059725273885756994  Varinance:  0.4864996389132545 \n",
      "\n",
      "Epoch:  2820  Learning Rate:  0.005966557846455646  Varinance:  0.48582001674999803 \n",
      "\n",
      "Epoch:  2821  Learning Rate:  0.005960594270893937  Varinance:  0.48514134399399256 \n",
      "\n",
      "Epoch:  2822  Learning Rate:  0.0059546366559269925  Varinance:  0.484463619318951 \n",
      "\n",
      "Epoch:  2823  Learning Rate:  0.005948684995597202  Varinance:  0.4837868414004388 \n",
      "\n",
      "Epoch:  2824  Learning Rate:  0.005942739283952904  Varinance:  0.48311100891587233 \n",
      "\n",
      "Epoch:  2825  Learning Rate:  0.0059367995150483854  Varinance:  0.48243612054451435 \n",
      "\n",
      "Epoch:  2826  Learning Rate:  0.005930865682943872  Varinance:  0.4817621749674736 \n",
      "\n",
      "Epoch:  2827  Learning Rate:  0.0059249377817055405  Varinance:  0.481089170867701 \n",
      "\n",
      "Epoch:  2828  Learning Rate:  0.005919015805405483  Varinance:  0.4804171069299872 \n",
      "\n",
      "Epoch:  2829  Learning Rate:  0.005913099748121726  Varinance:  0.47974598184096046 \n",
      "\n",
      "Epoch:  2830  Learning Rate:  0.005907189603938205  Varinance:  0.47907579428908337 \n",
      "\n",
      "Epoch:  2831  Learning Rate:  0.0059012853669447845  Varinance:  0.47840654296465124 \n",
      "\n",
      "Epoch:  2832  Learning Rate:  0.005895387031237222  Varinance:  0.47773822655978815 \n",
      "\n",
      "Epoch:  2833  Learning Rate:  0.005889494590917182  Varinance:  0.477070843768446 \n",
      "\n",
      "Epoch:  2834  Learning Rate:  0.0058836080400922215  Varinance:  0.476404393286401 \n",
      "\n",
      "Epoch:  2835  Learning Rate:  0.005877727372875794  Varinance:  0.4757388738112511 \n",
      "\n",
      "Epoch:  2836  Learning Rate:  0.005871852583387229  Varinance:  0.4750742840424137 \n",
      "\n",
      "Epoch:  2837  Learning Rate:  0.005865983665751737  Varinance:  0.4744106226811236 \n",
      "\n",
      "Epoch:  2838  Learning Rate:  0.0058601206141003954  Varinance:  0.47374788843042903 \n",
      "\n",
      "Epoch:  2839  Learning Rate:  0.0058542634225701605  Varinance:  0.4730860799951905 \n",
      "\n",
      "Epoch:  2840  Learning Rate:  0.005848412085303835  Varinance:  0.47242519608207795 \n",
      "\n",
      "Epoch:  2841  Learning Rate:  0.005842566596450083  Varinance:  0.4717652353995678 \n",
      "\n",
      "Epoch:  2842  Learning Rate:  0.005836726950163412  Varinance:  0.47110619665794057 \n",
      "\n",
      "Epoch:  2843  Learning Rate:  0.0058308931406041795  Varinance:  0.4704480785692789 \n",
      "\n",
      "Epoch:  2844  Learning Rate:  0.005825065161938574  Varinance:  0.4697908798474645 \n",
      "\n",
      "Epoch:  2845  Learning Rate:  0.005819243008338615  Varinance:  0.46913459920817496 \n",
      "\n",
      "Epoch:  2846  Learning Rate:  0.005813426673982147  Varinance:  0.4684792353688831 \n",
      "\n",
      "Epoch:  2847  Learning Rate:  0.00580761615305284  Varinance:  0.46782478704885283 \n",
      "\n",
      "Epoch:  2848  Learning Rate:  0.005801811439740171  Varinance:  0.4671712529691372 \n",
      "\n",
      "Epoch:  2849  Learning Rate:  0.005796012528239424  Varinance:  0.4665186318525762 \n",
      "\n",
      "Epoch:  2850  Learning Rate:  0.005790219412751686  Varinance:  0.4658669224237938 \n",
      "\n",
      "Epoch:  2851  Learning Rate:  0.005784432087483846  Varinance:  0.46521612340919527 \n",
      "\n",
      "Epoch:  2852  Learning Rate:  0.005778650546648576  Varinance:  0.4645662335369656 \n",
      "\n",
      "Epoch:  2853  Learning Rate:  0.005772874784464334  Varinance:  0.4639172515370661 \n",
      "\n",
      "Epoch:  2854  Learning Rate:  0.005767104795155353  Varinance:  0.46326917614123253 \n",
      "\n",
      "Epoch:  2855  Learning Rate:  0.0057613405729516525  Varinance:  0.46262200608297216 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2856  Learning Rate:  0.005755582112089005  Varinance:  0.4619757400975616 \n",
      "\n",
      "Epoch:  2857  Learning Rate:  0.005749829406808949  Varinance:  0.4613303769220444 \n",
      "\n",
      "Epoch:  2858  Learning Rate:  0.005744082451358776  Varinance:  0.46068591529522795 \n",
      "\n",
      "Epoch:  2859  Learning Rate:  0.005738341239991535  Varinance:  0.46004235395768184 \n",
      "\n",
      "Epoch:  2860  Learning Rate:  0.005732605766966013  Varinance:  0.45939969165173494 \n",
      "\n",
      "Epoch:  2861  Learning Rate:  0.005726876026546736  Varinance:  0.4587579271214732 \n",
      "\n",
      "Epoch:  2862  Learning Rate:  0.00572115201300396  Varinance:  0.45811705911273665 \n",
      "\n",
      "Epoch:  2863  Learning Rate:  0.005715433720613676  Varinance:  0.45747708637311807 \n",
      "\n",
      "Epoch:  2864  Learning Rate:  0.005709721143657589  Varinance:  0.45683800765195853 \n",
      "\n",
      "Epoch:  2865  Learning Rate:  0.005704014276423122  Varinance:  0.4561998217003479 \n",
      "\n",
      "Epoch:  2866  Learning Rate:  0.005698313113203404  Varinance:  0.45556252727111896 \n",
      "\n",
      "Epoch:  2867  Learning Rate:  0.005692617648297276  Varinance:  0.4549261231188485 \n",
      "\n",
      "Epoch:  2868  Learning Rate:  0.005686927876009271  Varinance:  0.4542906079998518 \n",
      "\n",
      "Epoch:  2869  Learning Rate:  0.005681243790649616  Varinance:  0.4536559806721816 \n",
      "\n",
      "Epoch:  2870  Learning Rate:  0.005675565386534223  Varinance:  0.4530222398956267 \n",
      "\n",
      "Epoch:  2871  Learning Rate:  0.00566989265798469  Varinance:  0.45238938443170695 \n",
      "\n",
      "Epoch:  2872  Learning Rate:  0.00566422559932829  Varinance:  0.4517574130436738 \n",
      "\n",
      "Epoch:  2873  Learning Rate:  0.005658564204897961  Varinance:  0.45112632449650486 \n",
      "\n",
      "Epoch:  2874  Learning Rate:  0.005652908469032304  Varinance:  0.4504961175569046 \n",
      "\n",
      "Epoch:  2875  Learning Rate:  0.005647258386075591  Varinance:  0.4498667909932991 \n",
      "\n",
      "Epoch:  2876  Learning Rate:  0.005641613950377735  Varinance:  0.4492383435758352 \n",
      "\n",
      "Epoch:  2877  Learning Rate:  0.005635975156294299  Varinance:  0.4486107740763785 \n",
      "\n",
      "Epoch:  2878  Learning Rate:  0.005630341998186489  Varinance:  0.44798408126850897 \n",
      "\n",
      "Epoch:  2879  Learning Rate:  0.005624714470421144  Varinance:  0.44735826392752154 \n",
      "\n",
      "Epoch:  2880  Learning Rate:  0.005619092567370741  Varinance:  0.4467333208304205 \n",
      "\n",
      "Epoch:  2881  Learning Rate:  0.005613476283413373  Varinance:  0.4461092507559189 \n",
      "\n",
      "Epoch:  2882  Learning Rate:  0.005607865612932757  Varinance:  0.4454860524844369 \n",
      "\n",
      "Epoch:  2883  Learning Rate:  0.005602260550318218  Varinance:  0.4448637247980968 \n",
      "\n",
      "Epoch:  2884  Learning Rate:  0.005596661089964698  Varinance:  0.444242266480724 \n",
      "\n",
      "Epoch:  2885  Learning Rate:  0.005591067226272736  Varinance:  0.4436216763178411 \n",
      "\n",
      "Epoch:  2886  Learning Rate:  0.005585478953648465  Varinance:  0.4430019530966688 \n",
      "\n",
      "Epoch:  2887  Learning Rate:  0.005579896266503612  Varinance:  0.44238309560612094 \n",
      "\n",
      "Epoch:  2888  Learning Rate:  0.005574319159255491  Varinance:  0.4417651026368032 \n",
      "\n",
      "Epoch:  2889  Learning Rate:  0.005568747626326995  Varinance:  0.4411479729810119 \n",
      "\n",
      "Epoch:  2890  Learning Rate:  0.005563181662146589  Varinance:  0.4405317054327288 \n",
      "\n",
      "Epoch:  2891  Learning Rate:  0.0055576212611483065  Varinance:  0.43991629878762223 \n",
      "\n",
      "Epoch:  2892  Learning Rate:  0.005552066417771751  Varinance:  0.4393017518430412 \n",
      "\n",
      "Epoch:  2893  Learning Rate:  0.005546517126462075  Varinance:  0.43868806339801647 \n",
      "\n",
      "Epoch:  2894  Learning Rate:  0.005540973381669988  Varinance:  0.4380752322532549 \n",
      "\n",
      "Epoch:  2895  Learning Rate:  0.005535435177851743  Varinance:  0.43746325721113943 \n",
      "\n",
      "Epoch:  2896  Learning Rate:  0.0055299025094691385  Varinance:  0.4368521370757266 \n",
      "\n",
      "Epoch:  2897  Learning Rate:  0.0055243753709895045  Varinance:  0.4362418706527424 \n",
      "\n",
      "Epoch:  2898  Learning Rate:  0.005518853756885702  Varinance:  0.43563245674958256 \n",
      "\n",
      "Epoch:  2899  Learning Rate:  0.005513337661636114  Varinance:  0.4350238941753075 \n",
      "\n",
      "Epoch:  2900  Learning Rate:  0.0055078270797246495  Varinance:  0.43441618174064256 \n",
      "\n",
      "Epoch:  2901  Learning Rate:  0.005502322005640724  Varinance:  0.43380931825797353 \n",
      "\n",
      "Epoch:  2902  Learning Rate:  0.005496822433879262  Varinance:  0.4332033025413453 \n",
      "\n",
      "Epoch:  2903  Learning Rate:  0.00549132835894069  Varinance:  0.4325981334064605 \n",
      "\n",
      "Epoch:  2904  Learning Rate:  0.005485839775330937  Varinance:  0.4319938096706748 \n",
      "\n",
      "Epoch:  2905  Learning Rate:  0.005480356677561415  Varinance:  0.4313903301529972 \n",
      "\n",
      "Epoch:  2906  Learning Rate:  0.005474879060149029  Varinance:  0.43078769367408526 \n",
      "\n",
      "Epoch:  2907  Learning Rate:  0.005469406917616156  Varinance:  0.4301858990562456 \n",
      "\n",
      "Epoch:  2908  Learning Rate:  0.00546394024449066  Varinance:  0.4295849451234287 \n",
      "\n",
      "Epoch:  2909  Learning Rate:  0.005458479035305863  Varinance:  0.428984830701228 \n",
      "\n",
      "Epoch:  2910  Learning Rate:  0.005453023284600556  Varinance:  0.42838555461687866 \n",
      "\n",
      "Epoch:  2911  Learning Rate:  0.005447572986918986  Varinance:  0.4277871156992528 \n",
      "\n",
      "Epoch:  2912  Learning Rate:  0.005442128136810859  Varinance:  0.42718951277885986 \n",
      "\n",
      "Epoch:  2913  Learning Rate:  0.005436688728831323  Varinance:  0.426592744687842 \n",
      "\n",
      "Epoch:  2914  Learning Rate:  0.005431254757540968  Varinance:  0.42599681025997277 \n",
      "\n",
      "Epoch:  2915  Learning Rate:  0.005425826217505821  Varinance:  0.4254017083306561 \n",
      "\n",
      "Epoch:  2916  Learning Rate:  0.005420403103297347  Varinance:  0.4248074377369213 \n",
      "\n",
      "Epoch:  2917  Learning Rate:  0.005414985409492427  Varinance:  0.4242139973174237 \n",
      "\n",
      "Epoch:  2918  Learning Rate:  0.005409573130673368  Varinance:  0.42362138591243964 \n",
      "\n",
      "Epoch:  2919  Learning Rate:  0.005404166261427888  Varinance:  0.423029602363867 \n",
      "\n",
      "Epoch:  2920  Learning Rate:  0.005398764796349122  Varinance:  0.42243864551522015 \n",
      "\n",
      "Epoch:  2921  Learning Rate:  0.005393368730035602  Varinance:  0.42184851421162955 \n",
      "\n",
      "Epoch:  2922  Learning Rate:  0.005387978057091262  Varinance:  0.4212592072998395 \n",
      "\n",
      "Epoch:  2923  Learning Rate:  0.0053825927721254255  Varinance:  0.42067072362820424 \n",
      "\n",
      "Epoch:  2924  Learning Rate:  0.0053772128697528124  Varinance:  0.4200830620466881 \n",
      "\n",
      "Epoch:  2925  Learning Rate:  0.005371838344593517  Varinance:  0.41949622140686077 \n",
      "\n",
      "Epoch:  2926  Learning Rate:  0.0053664691912730135  Varinance:  0.4189102005618973 \n",
      "\n",
      "Epoch:  2927  Learning Rate:  0.0053611054044221465  Varinance:  0.4183249983665741 \n",
      "\n",
      "Epoch:  2928  Learning Rate:  0.005355746978677133  Varinance:  0.4177406136772672 \n",
      "\n",
      "Epoch:  2929  Learning Rate:  0.005350393908679544  Varinance:  0.41715704535195147 \n",
      "\n",
      "Epoch:  2930  Learning Rate:  0.00534504618907631  Varinance:  0.4165742922501955 \n",
      "\n",
      "Epoch:  2931  Learning Rate:  0.005339703814519709  Varinance:  0.41599235323316264 \n",
      "\n",
      "Epoch:  2932  Learning Rate:  0.005334366779667369  Varinance:  0.4154112271636056 \n",
      "\n",
      "Epoch:  2933  Learning Rate:  0.005329035079182253  Varinance:  0.41483091290586743 \n",
      "\n",
      "Epoch:  2934  Learning Rate:  0.005323708707732661  Varinance:  0.4142514093258763 \n",
      "\n",
      "Epoch:  2935  Learning Rate:  0.005318387659992216  Varinance:  0.41367271529114463 \n",
      "\n",
      "Epoch:  2936  Learning Rate:  0.0053130719306398785  Varinance:  0.4130948296707683 \n",
      "\n",
      "Epoch:  2937  Learning Rate:  0.005307761514359913  Varinance:  0.4125177513354213 \n",
      "\n",
      "Epoch:  2938  Learning Rate:  0.005302456405841906  Varinance:  0.41194147915735674 \n",
      "\n",
      "Epoch:  2939  Learning Rate:  0.005297156599780743  Varinance:  0.4113660120104017 \n",
      "\n",
      "Epoch:  2940  Learning Rate:  0.0052918620908766235  Varinance:  0.4107913487699579 \n",
      "\n",
      "Epoch:  2941  Learning Rate:  0.005286572873835037  Varinance:  0.4102174883129972 \n",
      "\n",
      "Epoch:  2942  Learning Rate:  0.005281288943366764  Varinance:  0.40964442951806 \n",
      "\n",
      "Epoch:  2943  Learning Rate:  0.005276010294187873  Varinance:  0.40907217126525464 \n",
      "\n",
      "Epoch:  2944  Learning Rate:  0.005270736921019717  Varinance:  0.40850071243625247 \n",
      "\n",
      "Epoch:  2945  Learning Rate:  0.005265468818588921  Varinance:  0.40793005191428816 \n",
      "\n",
      "Epoch:  2946  Learning Rate:  0.005260205981627384  Varinance:  0.407360188584156 \n",
      "\n",
      "Epoch:  2947  Learning Rate:  0.005254948404872264  Varinance:  0.4067911213322077 \n",
      "\n",
      "Epoch:  2948  Learning Rate:  0.005249696083065989  Varinance:  0.406222849046352 \n",
      "\n",
      "Epoch:  2949  Learning Rate:  0.005244449010956234  Varinance:  0.40565537061604956 \n",
      "\n",
      "Epoch:  2950  Learning Rate:  0.005239207183295928  Varinance:  0.40508868493231415 \n",
      "\n",
      "Epoch:  2951  Learning Rate:  0.005233970594843238  Varinance:  0.4045227908877071 \n",
      "\n",
      "Epoch:  2952  Learning Rate:  0.0052287392403615835  Varinance:  0.40395768737633825 \n",
      "\n",
      "Epoch:  2953  Learning Rate:  0.005223513114619604  Varinance:  0.4033933732938613 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2954  Learning Rate:  0.0052182922123911735  Varinance:  0.4028298475374727 \n",
      "\n",
      "Epoch:  2955  Learning Rate:  0.005213076528455389  Varinance:  0.40226710900591045 \n",
      "\n",
      "Epoch:  2956  Learning Rate:  0.005207866057596569  Varinance:  0.40170515659944966 \n",
      "\n",
      "Epoch:  2957  Learning Rate:  0.005202660794604241  Varinance:  0.40114398921990313 \n",
      "\n",
      "Epoch:  2958  Learning Rate:  0.005197460734273142  Varinance:  0.40058360577061625 \n",
      "\n",
      "Epoch:  2959  Learning Rate:  0.005192265871403208  Varinance:  0.4000240051564679 \n",
      "\n",
      "Epoch:  2960  Learning Rate:  0.005187076200799579  Varinance:  0.39946518628386574 \n",
      "\n",
      "Epoch:  2961  Learning Rate:  0.0051818917172725834  Varinance:  0.39890714806074506 \n",
      "\n",
      "Epoch:  2962  Learning Rate:  0.005176712415637738  Varinance:  0.398349889396568 \n",
      "\n",
      "Epoch:  2963  Learning Rate:  0.005171538290715736  Varinance:  0.3977934092023186 \n",
      "\n",
      "Epoch:  2964  Learning Rate:  0.0051663693373324585  Varinance:  0.39723770639050343 \n",
      "\n",
      "Epoch:  2965  Learning Rate:  0.005161205550318949  Varinance:  0.3966827798751471 \n",
      "\n",
      "Epoch:  2966  Learning Rate:  0.00515604692451142  Varinance:  0.39612862857179265 \n",
      "\n",
      "Epoch:  2967  Learning Rate:  0.005150893454751243  Varinance:  0.395575251397497 \n",
      "\n",
      "Epoch:  2968  Learning Rate:  0.005145745135884952  Varinance:  0.3950226472708298 \n",
      "\n",
      "Epoch:  2969  Learning Rate:  0.005140601962764226  Varinance:  0.3944708151118726 \n",
      "\n",
      "Epoch:  2970  Learning Rate:  0.005135463930245891  Varinance:  0.39391975384221406 \n",
      "\n",
      "Epoch:  2971  Learning Rate:  0.005130331033191911  Varinance:  0.39336946238495085 \n",
      "\n",
      "Epoch:  2972  Learning Rate:  0.005125203266469395  Varinance:  0.3928199396646825 \n",
      "\n",
      "Epoch:  2973  Learning Rate:  0.005120080624950573  Varinance:  0.39227118460751237 \n",
      "\n",
      "Epoch:  2974  Learning Rate:  0.005114963103512802  Varinance:  0.3917231961410428 \n",
      "\n",
      "Epoch:  2975  Learning Rate:  0.005109850697038558  Varinance:  0.3911759731943745 \n",
      "\n",
      "Epoch:  2976  Learning Rate:  0.00510474340041544  Varinance:  0.3906295146981049 \n",
      "\n",
      "Epoch:  2977  Learning Rate:  0.005099641208536147  Varinance:  0.39008381958432425 \n",
      "\n",
      "Epoch:  2978  Learning Rate:  0.005094544116298488  Varinance:  0.3895388867866158 \n",
      "\n",
      "Epoch:  2979  Learning Rate:  0.005089452118605367  Varinance:  0.38899471524005164 \n",
      "\n",
      "Epoch:  2980  Learning Rate:  0.005084365210364792  Varinance:  0.38845130388119137 \n",
      "\n",
      "Epoch:  2981  Learning Rate:  0.005079283386489851  Varinance:  0.3879086516480815 \n",
      "\n",
      "Epoch:  2982  Learning Rate:  0.0050742066418987194  Varinance:  0.38736675748025035 \n",
      "\n",
      "Epoch:  2983  Learning Rate:  0.005069134971514649  Varinance:  0.38682562031870926 \n",
      "\n",
      "Epoch:  2984  Learning Rate:  0.005064068370265976  Varinance:  0.38628523910594753 \n",
      "\n",
      "Epoch:  2985  Learning Rate:  0.005059006833086096  Varinance:  0.385745612785933 \n",
      "\n",
      "Epoch:  2986  Learning Rate:  0.00505395035491347  Varinance:  0.38520674030410795 \n",
      "\n",
      "Epoch:  2987  Learning Rate:  0.005048898930691617  Varinance:  0.3846686206073877 \n",
      "\n",
      "Epoch:  2988  Learning Rate:  0.005043852555369119  Varinance:  0.38413125264415965 \n",
      "\n",
      "Epoch:  2989  Learning Rate:  0.005038811223899597  Varinance:  0.38359463536427907 \n",
      "\n",
      "Epoch:  2990  Learning Rate:  0.005033774931241717  Varinance:  0.3830587677190695 \n",
      "\n",
      "Epoch:  2991  Learning Rate:  0.005028743672359187  Varinance:  0.38252364866131805 \n",
      "\n",
      "Epoch:  2992  Learning Rate:  0.00502371744222075  Varinance:  0.38198927714527625 \n",
      "\n",
      "Epoch:  2993  Learning Rate:  0.005018696235800174  Varinance:  0.38145565212665533 \n",
      "\n",
      "Epoch:  2994  Learning Rate:  0.005013680048076252  Varinance:  0.3809227725626254 \n",
      "\n",
      "Epoch:  2995  Learning Rate:  0.005008668874032793  Varinance:  0.38039063741181445 \n",
      "\n",
      "Epoch:  2996  Learning Rate:  0.005003662708658628  Varinance:  0.3798592456343038 \n",
      "\n",
      "Epoch:  2997  Learning Rate:  0.004998661546947589  Varinance:  0.3793285961916289 \n",
      "\n",
      "Epoch:  2998  Learning Rate:  0.004993665383898514  Varinance:  0.37879868804677447 \n",
      "\n",
      "Epoch:  2999  Learning Rate:  0.0049886742145152365  Varinance:  0.3782695201641753 \n",
      "\n",
      "Epoch:  3000  Learning Rate:  0.00498368803380659  Varinance:  0.3777410915097119 \n",
      "\n",
      "Epoch:  3001  Learning Rate:  0.004978706836786395  Varinance:  0.3772134010507093 \n",
      "\n",
      "Epoch:  3002  Learning Rate:  0.00497373061847345  Varinance:  0.376686447755936 \n",
      "\n",
      "Epoch:  3003  Learning Rate:  0.004968759373891539  Varinance:  0.3761602305956001 \n",
      "\n",
      "Epoch:  3004  Learning Rate:  0.004963793098069413  Varinance:  0.3756347485413495 \n",
      "\n",
      "Epoch:  3005  Learning Rate:  0.0049588317860408005  Varinance:  0.3751100005662671 \n",
      "\n",
      "Epoch:  3006  Learning Rate:  0.004953875432844389  Varinance:  0.3745859856448719 \n",
      "\n",
      "Epoch:  3007  Learning Rate:  0.004948924033523821  Varinance:  0.37406270275311454 \n",
      "\n",
      "Epoch:  3008  Learning Rate:  0.004943977583127698  Varinance:  0.3735401508683761 \n",
      "\n",
      "Epoch:  3009  Learning Rate:  0.004939036076709572  Varinance:  0.3730183289694672 \n",
      "\n",
      "Epoch:  3010  Learning Rate:  0.004934099509327935  Varinance:  0.37249723603662366 \n",
      "\n",
      "Epoch:  3011  Learning Rate:  0.004929167876046217  Varinance:  0.37197687105150734 \n",
      "\n",
      "Epoch:  3012  Learning Rate:  0.004924241171932785  Varinance:  0.3714572329972016 \n",
      "\n",
      "Epoch:  3013  Learning Rate:  0.004919319392060937  Varinance:  0.3709383208582105 \n",
      "\n",
      "Epoch:  3014  Learning Rate:  0.004914402531508891  Varinance:  0.3704201336204575 \n",
      "\n",
      "Epoch:  3015  Learning Rate:  0.004909490585359786  Varinance:  0.36990267027128154 \n",
      "\n",
      "Epoch:  3016  Learning Rate:  0.004904583548701673  Varinance:  0.3693859297994374 \n",
      "\n",
      "Epoch:  3017  Learning Rate:  0.00489968141662752  Varinance:  0.3688699111950913 \n",
      "\n",
      "Epoch:  3018  Learning Rate:  0.004894784184235192  Varinance:  0.3683546134498216 \n",
      "\n",
      "Epoch:  3019  Learning Rate:  0.004889891846627456  Varinance:  0.36784003555661415 \n",
      "\n",
      "Epoch:  3020  Learning Rate:  0.004885004398911972  Varinance:  0.36732617650986166 \n",
      "\n",
      "Epoch:  3021  Learning Rate:  0.004880121836201297  Varinance:  0.3668130353053627 \n",
      "\n",
      "Epoch:  3022  Learning Rate:  0.004875244153612863  Varinance:  0.3663006109403172 \n",
      "\n",
      "Epoch:  3023  Learning Rate:  0.00487037134626899  Varinance:  0.36578890241332745 \n",
      "\n",
      "Epoch:  3024  Learning Rate:  0.004865503409296867  Varinance:  0.36527790872439325 \n",
      "\n",
      "Epoch:  3025  Learning Rate:  0.0048606403378285605  Varinance:  0.3647676288749127 \n",
      "\n",
      "Epoch:  3026  Learning Rate:  0.004855782127000998  Varinance:  0.364258061867678 \n",
      "\n",
      "Epoch:  3027  Learning Rate:  0.004850928771955965  Varinance:  0.3637492067068742 \n",
      "\n",
      "Epoch:  3028  Learning Rate:  0.004846080267840108  Varinance:  0.36324106239807863 \n",
      "\n",
      "Epoch:  3029  Learning Rate:  0.004841236609804924  Varinance:  0.3627336279482564 \n",
      "\n",
      "Epoch:  3030  Learning Rate:  0.004836397793006753  Varinance:  0.3622269023657614 \n",
      "\n",
      "Epoch:  3031  Learning Rate:  0.004831563812606779  Varinance:  0.361720884660331 \n",
      "\n",
      "Epoch:  3032  Learning Rate:  0.004826734663771017  Varinance:  0.3612155738430877 \n",
      "\n",
      "Epoch:  3033  Learning Rate:  0.004821910341670324  Varinance:  0.3607109689265343 \n",
      "\n",
      "Epoch:  3034  Learning Rate:  0.0048170908414803745  Varinance:  0.36020706892455306 \n",
      "\n",
      "Epoch:  3035  Learning Rate:  0.004812276158381668  Varinance:  0.35970387285240485 \n",
      "\n",
      "Epoch:  3036  Learning Rate:  0.004807466287559518  Varinance:  0.3592013797267248 \n",
      "\n",
      "Epoch:  3037  Learning Rate:  0.004802661224204059  Varinance:  0.35869958856552303 \n",
      "\n",
      "Epoch:  3038  Learning Rate:  0.004797860963510224  Varinance:  0.35819849838818035 \n",
      "\n",
      "Epoch:  3039  Learning Rate:  0.004793065500677752  Varinance:  0.3576981082154487 \n",
      "\n",
      "Epoch:  3040  Learning Rate:  0.004788274830911179  Varinance:  0.3571984170694468 \n",
      "\n",
      "Epoch:  3041  Learning Rate:  0.004783488949419837  Varinance:  0.35669942397365983 \n",
      "\n",
      "Epoch:  3042  Learning Rate:  0.0047787078514178434  Varinance:  0.3562011279529377 \n",
      "\n",
      "Epoch:  3043  Learning Rate:  0.0047739315321241  Varinance:  0.3557035280334915 \n",
      "\n",
      "Epoch:  3044  Learning Rate:  0.004769159986762284  Varinance:  0.35520662324289387 \n",
      "\n",
      "Epoch:  3045  Learning Rate:  0.004764393210560854  Varinance:  0.354710412610075 \n",
      "\n",
      "Epoch:  3046  Learning Rate:  0.004759631198753032  Varinance:  0.3542148951653217 \n",
      "\n",
      "Epoch:  3047  Learning Rate:  0.004754873946576805  Varinance:  0.3537200699402761 \n",
      "\n",
      "Epoch:  3048  Learning Rate:  0.004750121449274919  Varinance:  0.35322593596793217 \n",
      "\n",
      "Epoch:  3049  Learning Rate:  0.00474537370209488  Varinance:  0.35273249228263576 \n",
      "\n",
      "Epoch:  3050  Learning Rate:  0.00474063070028894  Varinance:  0.3522397379200805 \n",
      "\n",
      "Epoch:  3051  Learning Rate:  0.004735892439114093  Varinance:  0.35174767191730855 \n",
      "\n",
      "Epoch:  3052  Learning Rate:  0.004731158913832079  Varinance:  0.35125629331270614 \n",
      "\n",
      "Epoch:  3053  Learning Rate:  0.004726430119709375  Varinance:  0.3507656011460028 \n",
      "\n",
      "Epoch:  3054  Learning Rate:  0.004721706052017185  Varinance:  0.3502755944582708 \n",
      "\n",
      "Epoch:  3055  Learning Rate:  0.004716986706031439  Varinance:  0.34978627229192033 \n",
      "\n",
      "Epoch:  3056  Learning Rate:  0.004712272077032792  Varinance:  0.3492976336907008 \n",
      "\n",
      "Epoch:  3057  Learning Rate:  0.004707562160306615  Varinance:  0.3488096776996962 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3058  Learning Rate:  0.0047028569511429925  Varinance:  0.34832240336532566 \n",
      "\n",
      "Epoch:  3059  Learning Rate:  0.004698156444836711  Varinance:  0.3478358097353395 \n",
      "\n",
      "Epoch:  3060  Learning Rate:  0.004693460636687265  Varinance:  0.34734989585881837 \n",
      "\n",
      "Epoch:  3061  Learning Rate:  0.004688769521998849  Varinance:  0.34686466078617223 \n",
      "\n",
      "Epoch:  3062  Learning Rate:  0.004684083096080345  Varinance:  0.3463801035691363 \n",
      "\n",
      "Epoch:  3063  Learning Rate:  0.004679401354245328  Varinance:  0.3458962232607718 \n",
      "\n",
      "Epoch:  3064  Learning Rate:  0.004674724291812053  Varinance:  0.3454130189154614 \n",
      "\n",
      "Epoch:  3065  Learning Rate:  0.004670051904103461  Varinance:  0.34493048958891026 \n",
      "\n",
      "Epoch:  3066  Learning Rate:  0.004665384186447163  Varinance:  0.34444863433814155 \n",
      "\n",
      "Epoch:  3067  Learning Rate:  0.00466072113417544  Varinance:  0.3439674522214957 \n",
      "\n",
      "Epoch:  3068  Learning Rate:  0.004656062742625237  Varinance:  0.3434869422986298 \n",
      "\n",
      "Epoch:  3069  Learning Rate:  0.004651409007138167  Varinance:  0.34300710363051307 \n",
      "\n",
      "Epoch:  3070  Learning Rate:  0.004646759923060492  Varinance:  0.3425279352794278 \n",
      "\n",
      "Epoch:  3071  Learning Rate:  0.004642115485743128  Varinance:  0.34204943630896517 \n",
      "\n",
      "Epoch:  3072  Learning Rate:  0.004637475690541633  Varinance:  0.3415716057840255 \n",
      "\n",
      "Epoch:  3073  Learning Rate:  0.004632840532816217  Varinance:  0.3410944427708147 \n",
      "\n",
      "Epoch:  3074  Learning Rate:  0.004628210007931721  Varinance:  0.34061794633684284 \n",
      "\n",
      "Epoch:  3075  Learning Rate:  0.004623584111257619  Varinance:  0.340142115550924 \n",
      "\n",
      "Epoch:  3076  Learning Rate:  0.004618962838168011  Varinance:  0.3396669494831715 \n",
      "\n",
      "Epoch:  3077  Learning Rate:  0.004614346184041627  Varinance:  0.3391924472049991 \n",
      "\n",
      "Epoch:  3078  Learning Rate:  0.0046097341442618124  Varinance:  0.3387186077891167 \n",
      "\n",
      "Epoch:  3079  Learning Rate:  0.004605126714216526  Varinance:  0.33824543030952975 \n",
      "\n",
      "Epoch:  3080  Learning Rate:  0.004600523889298336  Varinance:  0.3377729138415381 \n",
      "\n",
      "Epoch:  3081  Learning Rate:  0.00459592566490442  Varinance:  0.3373010574617322 \n",
      "\n",
      "Epoch:  3082  Learning Rate:  0.0045913320364365535  Varinance:  0.3368298602479936 \n",
      "\n",
      "Epoch:  3083  Learning Rate:  0.004586742999301104  Varinance:  0.3363593212794909 \n",
      "\n",
      "Epoch:  3084  Learning Rate:  0.004582158548909036  Varinance:  0.3358894396366802 \n",
      "\n",
      "Epoch:  3085  Learning Rate:  0.004577578680675899  Varinance:  0.3354202144013013 \n",
      "\n",
      "Epoch:  3086  Learning Rate:  0.0045730033900218256  Varinance:  0.3349516446563769 \n",
      "\n",
      "Epoch:  3087  Learning Rate:  0.004568432672371522  Varinance:  0.3344837294862113 \n",
      "\n",
      "Epoch:  3088  Learning Rate:  0.00456386652315427  Varinance:  0.33401646797638695 \n",
      "\n",
      "Epoch:  3089  Learning Rate:  0.004559304937803924  Varinance:  0.33354985921376507 \n",
      "\n",
      "Epoch:  3090  Learning Rate:  0.0045547479117588946  Varinance:  0.33308390228648105 \n",
      "\n",
      "Epoch:  3091  Learning Rate:  0.004550195440462157  Varinance:  0.33261859628394536 \n",
      "\n",
      "Epoch:  3092  Learning Rate:  0.004545647519361237  Varinance:  0.3321539402968398 \n",
      "\n",
      "Epoch:  3093  Learning Rate:  0.0045411041439082185  Varinance:  0.3316899334171162 \n",
      "\n",
      "Epoch:  3094  Learning Rate:  0.00453656530955972  Varinance:  0.3312265747379961 \n",
      "\n",
      "Epoch:  3095  Learning Rate:  0.004532031011776911  Varinance:  0.3307638633539664 \n",
      "\n",
      "Epoch:  3096  Learning Rate:  0.004527501246025489  Varinance:  0.33030179836078005 \n",
      "\n",
      "Epoch:  3097  Learning Rate:  0.004522976007775692  Varinance:  0.3298403788554522 \n",
      "\n",
      "Epoch:  3098  Learning Rate:  0.0045184552925022796  Varinance:  0.3293796039362606 \n",
      "\n",
      "Epoch:  3099  Learning Rate:  0.0045139390956845365  Varinance:  0.3289194727027417 \n",
      "\n",
      "Epoch:  3100  Learning Rate:  0.004509427412806263  Varinance:  0.32845998425569 \n",
      "\n",
      "Epoch:  3101  Learning Rate:  0.00450492023935578  Varinance:  0.3280011376971569 \n",
      "\n",
      "Epoch:  3102  Learning Rate:  0.004500417570825913  Varinance:  0.327542932130447 \n",
      "\n",
      "Epoch:  3103  Learning Rate:  0.004495919402713991  Varinance:  0.3270853666601189 \n",
      "\n",
      "Epoch:  3104  Learning Rate:  0.004491425730521844  Varinance:  0.32662844039198086 \n",
      "\n",
      "Epoch:  3105  Learning Rate:  0.004486936549755803  Varinance:  0.32617215243309133 \n",
      "\n",
      "Epoch:  3106  Learning Rate:  0.004482451855926687  Varinance:  0.3257165018917555 \n",
      "\n",
      "Epoch:  3107  Learning Rate:  0.0044779716445498005  Varinance:  0.32526148787752407 \n",
      "\n",
      "Epoch:  3108  Learning Rate:  0.0044734959111449295  Varinance:  0.3248071095011925 \n",
      "\n",
      "Epoch:  3109  Learning Rate:  0.004469024651236344  Varinance:  0.3243533658747976 \n",
      "\n",
      "Epoch:  3110  Learning Rate:  0.004464557860352782  Varinance:  0.3239002561116173 \n",
      "\n",
      "Epoch:  3111  Learning Rate:  0.004460095534027454  Varinance:  0.32344777932616775 \n",
      "\n",
      "Epoch:  3112  Learning Rate:  0.004455637667798028  Varinance:  0.322995934634202 \n",
      "\n",
      "Epoch:  3113  Learning Rate:  0.004451184257206644  Varinance:  0.32254472115270916 \n",
      "\n",
      "Epoch:  3114  Learning Rate:  0.004446735297799888  Varinance:  0.32209413799991077 \n",
      "\n",
      "Epoch:  3115  Learning Rate:  0.004442290785128799  Varinance:  0.32164418429526126 \n",
      "\n",
      "Epoch:  3116  Learning Rate:  0.004437850714748865  Varinance:  0.3211948591594441 \n",
      "\n",
      "Epoch:  3117  Learning Rate:  0.004433415082220018  Varinance:  0.32074616171437237 \n",
      "\n",
      "Epoch:  3118  Learning Rate:  0.004428983883106622  Varinance:  0.32029809108318463 \n",
      "\n",
      "Epoch:  3119  Learning Rate:  0.004424557112977477  Varinance:  0.3198506463902447 \n",
      "\n",
      "Epoch:  3120  Learning Rate:  0.004420134767405813  Varinance:  0.3194038267611403 \n",
      "\n",
      "Epoch:  3121  Learning Rate:  0.004415716841969286  Varinance:  0.3189576313226796 \n",
      "\n",
      "Epoch:  3122  Learning Rate:  0.00441130333224997  Varinance:  0.3185120592028918 \n",
      "\n",
      "Epoch:  3123  Learning Rate:  0.004406894233834353  Varinance:  0.31806710953102285 \n",
      "\n",
      "Epoch:  3124  Learning Rate:  0.004402489542313335  Varinance:  0.31762278143753647 \n",
      "\n",
      "Epoch:  3125  Learning Rate:  0.0043980892532822285  Varinance:  0.31717907405411017 \n",
      "\n",
      "Epoch:  3126  Learning Rate:  0.004393693362340742  Varinance:  0.31673598651363444 \n",
      "\n",
      "Epoch:  3127  Learning Rate:  0.004389301865092984  Varinance:  0.316293517950212 \n",
      "\n",
      "Epoch:  3128  Learning Rate:  0.004384914757147456  Varinance:  0.315851667499154 \n",
      "\n",
      "Epoch:  3129  Learning Rate:  0.004380532034117049  Varinance:  0.3154104342969806 \n",
      "\n",
      "Epoch:  3130  Learning Rate:  0.004376153691619043  Varinance:  0.31496981748141734 \n",
      "\n",
      "Epoch:  3131  Learning Rate:  0.0043717797252750944  Varinance:  0.3145298161913952 \n",
      "\n",
      "Epoch:  3132  Learning Rate:  0.004367410130711235  Varinance:  0.3140904295670473 \n",
      "\n",
      "Epoch:  3133  Learning Rate:  0.004363044903557867  Varinance:  0.31365165674970785 \n",
      "\n",
      "Epoch:  3134  Learning Rate:  0.00435868403944977  Varinance:  0.31321349688191163 \n",
      "\n",
      "Epoch:  3135  Learning Rate:  0.004354327534026074  Varinance:  0.31277594910739004 \n",
      "\n",
      "Epoch:  3136  Learning Rate:  0.004349975382930276  Varinance:  0.31233901257107166 \n",
      "\n",
      "Epoch:  3137  Learning Rate:  0.004345627581810221  Varinance:  0.31190268641907865 \n",
      "\n",
      "Epoch:  3138  Learning Rate:  0.004341284126318112  Varinance:  0.31146696979872707 \n",
      "\n",
      "Epoch:  3139  Learning Rate:  0.004336945012110491  Varinance:  0.31103186185852316 \n",
      "\n",
      "Epoch:  3140  Learning Rate:  0.004332610234848243  Varinance:  0.3105973617481629 \n",
      "\n",
      "Epoch:  3141  Learning Rate:  0.0043282797901965896  Varinance:  0.3101634686185308 \n",
      "\n",
      "Epoch:  3142  Learning Rate:  0.004323953673825089  Varinance:  0.30973018162169647 \n",
      "\n",
      "Epoch:  3143  Learning Rate:  0.004319631881407622  Varinance:  0.3092974999109151 \n",
      "\n",
      "Epoch:  3144  Learning Rate:  0.004315314408622397  Varinance:  0.3088654226406241 \n",
      "\n",
      "Epoch:  3145  Learning Rate:  0.004311001251151939  Varinance:  0.30843394896644194 \n",
      "\n",
      "Epoch:  3146  Learning Rate:  0.004306692404683093  Varinance:  0.30800307804516747 \n",
      "\n",
      "Epoch:  3147  Learning Rate:  0.004302387864907009  Varinance:  0.3075728090347765 \n",
      "\n",
      "Epoch:  3148  Learning Rate:  0.004298087627519149  Varinance:  0.30714314109442215 \n",
      "\n",
      "Epoch:  3149  Learning Rate:  0.004293791688219274  Varinance:  0.30671407338443096 \n",
      "\n",
      "Epoch:  3150  Learning Rate:  0.004289500042711446  Varinance:  0.3062856050663037 \n",
      "\n",
      "Epoch:  3151  Learning Rate:  0.004285212686704019  Varinance:  0.3058577353027117 \n",
      "\n",
      "Epoch:  3152  Learning Rate:  0.004280929615909634  Varinance:  0.30543046325749595 \n",
      "\n",
      "Epoch:  3153  Learning Rate:  0.004276650826045222  Varinance:  0.3050037880956662 \n",
      "\n",
      "Epoch:  3154  Learning Rate:  0.004272376312831993  Varinance:  0.3045777089833978 \n",
      "\n",
      "Epoch:  3155  Learning Rate:  0.004268106071995434  Varinance:  0.3041522250880319 \n",
      "\n",
      "Epoch:  3156  Learning Rate:  0.004263840099265301  Varinance:  0.30372733557807174 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3157  Learning Rate:  0.0042595783903756216  Varinance:  0.3033030396231832 \n",
      "\n",
      "Epoch:  3158  Learning Rate:  0.00425532094106469  Varinance:  0.30287933639419135 \n",
      "\n",
      "Epoch:  3159  Learning Rate:  0.004251067747075053  Varinance:  0.3024562250630797 \n",
      "\n",
      "Epoch:  3160  Learning Rate:  0.0042468188041535175  Varinance:  0.30203370480298886 \n",
      "\n",
      "Epoch:  3161  Learning Rate:  0.004242574108051139  Varinance:  0.30161177478821405 \n",
      "\n",
      "Epoch:  3162  Learning Rate:  0.004238333654523223  Varinance:  0.3011904341942045 \n",
      "\n",
      "Epoch:  3163  Learning Rate:  0.004234097439329314  Varinance:  0.3007696821975606 \n",
      "\n",
      "Epoch:  3164  Learning Rate:  0.0042298654582332  Varinance:  0.30034951797603393 \n",
      "\n",
      "Epoch:  3165  Learning Rate:  0.004225637707002893  Varinance:  0.2999299407085239 \n",
      "\n",
      "Epoch:  3166  Learning Rate:  0.004221414181410647  Varinance:  0.29951094957507707 \n",
      "\n",
      "Epoch:  3167  Learning Rate:  0.0042171948772329335  Varinance:  0.29909254375688604 \n",
      "\n",
      "Epoch:  3168  Learning Rate:  0.00421297979025045  Varinance:  0.29867472243628634 \n",
      "\n",
      "Epoch:  3169  Learning Rate:  0.004208768916248106  Varinance:  0.29825748479675684 \n",
      "\n",
      "Epoch:  3170  Learning Rate:  0.004204562251015029  Varinance:  0.297840830022916 \n",
      "\n",
      "Epoch:  3171  Learning Rate:  0.004200359790344555  Varinance:  0.2974247573005223 \n",
      "\n",
      "Epoch:  3172  Learning Rate:  0.004196161530034221  Varinance:  0.29700926581647086 \n",
      "\n",
      "Epoch:  3173  Learning Rate:  0.004191967465885765  Varinance:  0.29659435475879287 \n",
      "\n",
      "Epoch:  3174  Learning Rate:  0.004187777593705126  Varinance:  0.29618002331665444 \n",
      "\n",
      "Epoch:  3175  Learning Rate:  0.004183591909302431  Varinance:  0.2957662706803533 \n",
      "\n",
      "Epoch:  3176  Learning Rate:  0.004179410408491992  Varinance:  0.29535309604131965 \n",
      "\n",
      "Epoch:  3177  Learning Rate:  0.004175233087092309  Varinance:  0.294940498592112 \n",
      "\n",
      "Epoch:  3178  Learning Rate:  0.004171059940926062  Varinance:  0.29452847752641725 \n",
      "\n",
      "Epoch:  3179  Learning Rate:  0.004166890965820104  Varinance:  0.29411703203904915 \n",
      "\n",
      "Epoch:  3180  Learning Rate:  0.0041627261576054585  Varinance:  0.2937061613259454 \n",
      "\n",
      "Epoch:  3181  Learning Rate:  0.004158565512117316  Varinance:  0.29329586458416795 \n",
      "\n",
      "Epoch:  3182  Learning Rate:  0.004154409025195034  Varinance:  0.2928861410118993 \n",
      "\n",
      "Epoch:  3183  Learning Rate:  0.004150256692682124  Varinance:  0.2924769898084433 \n",
      "\n",
      "Epoch:  3184  Learning Rate:  0.004146108510426252  Varinance:  0.29206841017422136 \n",
      "\n",
      "Epoch:  3185  Learning Rate:  0.0041419644742792345  Varinance:  0.291660401310772 \n",
      "\n",
      "Epoch:  3186  Learning Rate:  0.004137824580097038  Varinance:  0.2912529624207498 \n",
      "\n",
      "Epoch:  3187  Learning Rate:  0.0041336888237397664  Varinance:  0.29084609270792233 \n",
      "\n",
      "Epoch:  3188  Learning Rate:  0.0041295572010716635  Varinance:  0.29043979137717035 \n",
      "\n",
      "Epoch:  3189  Learning Rate:  0.004125429707961103  Varinance:  0.29003405763448453 \n",
      "\n",
      "Epoch:  3190  Learning Rate:  0.004121306340280596  Varinance:  0.2896288906869657 \n",
      "\n",
      "Epoch:  3191  Learning Rate:  0.004117187093906774  Varinance:  0.2892242897428213 \n",
      "\n",
      "Epoch:  3192  Learning Rate:  0.004113071964720389  Varinance:  0.28882025401136535 \n",
      "\n",
      "Epoch:  3193  Learning Rate:  0.004108960948606308  Varinance:  0.28841678270301685 \n",
      "\n",
      "Epoch:  3194  Learning Rate:  0.004104854041453521  Varinance:  0.2880138750292968 \n",
      "\n",
      "Epoch:  3195  Learning Rate:  0.004100751239155117  Varinance:  0.287611530202829 \n",
      "\n",
      "Epoch:  3196  Learning Rate:  0.004096652537608295  Varinance:  0.2872097474373358 \n",
      "\n",
      "Epoch:  3197  Learning Rate:  0.0040925579327143495  Varinance:  0.28680852594763917 \n",
      "\n",
      "Epoch:  3198  Learning Rate:  0.004088467420378679  Varinance:  0.2864078649496572 \n",
      "\n",
      "Epoch:  3199  Learning Rate:  0.00408438099651077  Varinance:  0.28600776366040315 \n",
      "\n",
      "Epoch:  3200  Learning Rate:  0.004080298657024198  Varinance:  0.2856082212979851 \n",
      "\n",
      "Epoch:  3201  Learning Rate:  0.0040762203978366215  Varinance:  0.285209237081602 \n",
      "\n",
      "Epoch:  3202  Learning Rate:  0.004072146214869783  Varinance:  0.2848108102315451 \n",
      "\n",
      "Epoch:  3203  Learning Rate:  0.0040680761040495  Varinance:  0.2844129399691933 \n",
      "\n",
      "Epoch:  3204  Learning Rate:  0.00406401006130566  Varinance:  0.28401562551701465 \n",
      "\n",
      "Epoch:  3205  Learning Rate:  0.004059948082572218  Varinance:  0.2836188660985624 \n",
      "\n",
      "Epoch:  3206  Learning Rate:  0.004055890163787199  Varinance:  0.28322266093847454 \n",
      "\n",
      "Epoch:  3207  Learning Rate:  0.004051836300892681  Varinance:  0.282827009262473 \n",
      "\n",
      "Epoch:  3208  Learning Rate:  0.0040477864898348015  Varinance:  0.2824319102973602 \n",
      "\n",
      "Epoch:  3209  Learning Rate:  0.004043740726563748  Varinance:  0.2820373632710198 \n",
      "\n",
      "Epoch:  3210  Learning Rate:  0.00403969900703376  Varinance:  0.2816433674124134 \n",
      "\n",
      "Epoch:  3211  Learning Rate:  0.004035661327203115  Varinance:  0.28124992195157955 \n",
      "\n",
      "Epoch:  3212  Learning Rate:  0.004031627683034134  Varinance:  0.28085702611963337 \n",
      "\n",
      "Epoch:  3213  Learning Rate:  0.00402759807049317  Varinance:  0.2804646791487627 \n",
      "\n",
      "Epoch:  3214  Learning Rate:  0.004023572485550614  Varinance:  0.2800728802722295 \n",
      "\n",
      "Epoch:  3215  Learning Rate:  0.004019550924180879  Varinance:  0.27968162872436536 \n",
      "\n",
      "Epoch:  3216  Learning Rate:  0.004015533382362403  Varinance:  0.2792909237405728 \n",
      "\n",
      "Epoch:  3217  Learning Rate:  0.004011519856077642  Varinance:  0.2789007645573216 \n",
      "\n",
      "Epoch:  3218  Learning Rate:  0.004007510341313073  Varinance:  0.27851115041214813 \n",
      "\n",
      "Epoch:  3219  Learning Rate:  0.00400350483405918  Varinance:  0.27812208054365467 \n",
      "\n",
      "Epoch:  3220  Learning Rate:  0.003999503330310454  Varinance:  0.2777335541915062 \n",
      "\n",
      "Epoch:  3221  Learning Rate:  0.00399550582606539  Varinance:  0.2773455705964308 \n",
      "\n",
      "Epoch:  3222  Learning Rate:  0.003991512317326487  Varinance:  0.2769581290002161 \n",
      "\n",
      "Epoch:  3223  Learning Rate:  0.003987522800100234  Varinance:  0.2765712286457102 \n",
      "\n",
      "Epoch:  3224  Learning Rate:  0.003983537270397113  Varinance:  0.276184868776818 \n",
      "\n",
      "Epoch:  3225  Learning Rate:  0.003979555724231593  Varinance:  0.27579904863850063 \n",
      "\n",
      "Epoch:  3226  Learning Rate:  0.00397557815762213  Varinance:  0.2754137674767748 \n",
      "\n",
      "Epoch:  3227  Learning Rate:  0.003971604566591157  Varinance:  0.2750290245387094 \n",
      "\n",
      "Epoch:  3228  Learning Rate:  0.00396763494716508  Varinance:  0.27464481907242627 \n",
      "\n",
      "Epoch:  3229  Learning Rate:  0.00396366929537428  Varinance:  0.27426115032709647 \n",
      "\n",
      "Epoch:  3230  Learning Rate:  0.003959707607253108  Varinance:  0.27387801755294106 \n",
      "\n",
      "Epoch:  3231  Learning Rate:  0.003955749878839873  Varinance:  0.2734954200012277 \n",
      "\n",
      "Epoch:  3232  Learning Rate:  0.003951796106176846  Varinance:  0.2731133569242699 \n",
      "\n",
      "Epoch:  3233  Learning Rate:  0.0039478462853102525  Varinance:  0.2727318275754267 \n",
      "\n",
      "Epoch:  3234  Learning Rate:  0.003943900412290276  Varinance:  0.27235083120909886 \n",
      "\n",
      "Epoch:  3235  Learning Rate:  0.003939958483171039  Varinance:  0.27197036708072986 \n",
      "\n",
      "Epoch:  3236  Learning Rate:  0.003936020494010615  Varinance:  0.2715904344468023 \n",
      "\n",
      "Epoch:  3237  Learning Rate:  0.00393208644087101  Varinance:  0.2712110325648384 \n",
      "\n",
      "Epoch:  3238  Learning Rate:  0.003928156319818177  Varinance:  0.27083216069339694 \n",
      "\n",
      "Epoch:  3239  Learning Rate:  0.003924230126921989  Varinance:  0.2704538180920723 \n",
      "\n",
      "Epoch:  3240  Learning Rate:  0.0039203078582562565  Varinance:  0.2700760040214941 \n",
      "\n",
      "Epoch:  3241  Learning Rate:  0.003916389509898707  Varinance:  0.26969871774332377 \n",
      "\n",
      "Epoch:  3242  Learning Rate:  0.003912475077930995  Varinance:  0.2693219585202551 \n",
      "\n",
      "Epoch:  3243  Learning Rate:  0.003908564558438687  Varinance:  0.2689457256160112 \n",
      "\n",
      "Epoch:  3244  Learning Rate:  0.0039046579475112636  Varinance:  0.2685700182953437 \n",
      "\n",
      "Epoch:  3245  Learning Rate:  0.0039007552412421115  Varinance:  0.26819483582403214 \n",
      "\n",
      "Epoch:  3246  Learning Rate:  0.0038968564357285264  Varinance:  0.26782017746888076 \n",
      "\n",
      "Epoch:  3247  Learning Rate:  0.0038929615270717026  Varinance:  0.26744604249771886 \n",
      "\n",
      "Epoch:  3248  Learning Rate:  0.00388907051137673  Varinance:  0.26707243017939786 \n",
      "\n",
      "Epoch:  3249  Learning Rate:  0.0038851833847525913  Varinance:  0.26669933978379123 \n",
      "\n",
      "Epoch:  3250  Learning Rate:  0.003881300143312163  Varinance:  0.2663267705817919 \n",
      "\n",
      "Epoch:  3251  Learning Rate:  0.003877420783172201  Varinance:  0.26595472184531127 \n",
      "\n",
      "Epoch:  3252  Learning Rate:  0.0038735453004533453  Varinance:  0.26558319284727855 \n",
      "\n",
      "Epoch:  3253  Learning Rate:  0.0038696736912801136  Varinance:  0.2652121828616377 \n",
      "\n",
      "Epoch:  3254  Learning Rate:  0.003865805951780893  Varinance:  0.26484169116334794 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3255  Learning Rate:  0.003861942078087949  Varinance:  0.2644717170283803 \n",
      "\n",
      "Epoch:  3256  Learning Rate:  0.003858082066337404  Varinance:  0.26410225973371837 \n",
      "\n",
      "Epoch:  3257  Learning Rate:  0.0038542259126692472  Varinance:  0.2637333185573548 \n",
      "\n",
      "Epoch:  3258  Learning Rate:  0.0038503736132273225  Varinance:  0.2633648927782911 \n",
      "\n",
      "Epoch:  3259  Learning Rate:  0.003846525164159334  Varinance:  0.2629969816765366 \n",
      "\n",
      "Epoch:  3260  Learning Rate:  0.00384268056161683  Varinance:  0.2626295845331056 \n",
      "\n",
      "Epoch:  3261  Learning Rate:  0.0038388398017552075  Varinance:  0.2622627006300174 \n",
      "\n",
      "Epoch:  3262  Learning Rate:  0.0038350028807337053  Varinance:  0.26189632925029377 \n",
      "\n",
      "Epoch:  3263  Learning Rate:  0.003831169794715405  Varinance:  0.26153046967795873 \n",
      "\n",
      "Epoch:  3264  Learning Rate:  0.0038273405398672185  Varinance:  0.2611651211980359 \n",
      "\n",
      "Epoch:  3265  Learning Rate:  0.0038235151123598913  Varinance:  0.26080028309654774 \n",
      "\n",
      "Epoch:  3266  Learning Rate:  0.0038196935083679925  Varinance:  0.26043595466051467 \n",
      "\n",
      "Epoch:  3267  Learning Rate:  0.0038158757240699228  Varinance:  0.2600721351779523 \n",
      "\n",
      "Epoch:  3268  Learning Rate:  0.0038120617556478952  Varinance:  0.25970882393787176 \n",
      "\n",
      "Epoch:  3269  Learning Rate:  0.003808251599287941  Varinance:  0.2593460202302765 \n",
      "\n",
      "Epoch:  3270  Learning Rate:  0.0038044452511799016  Varinance:  0.2589837233461626 \n",
      "\n",
      "Epoch:  3271  Learning Rate:  0.0038006427075174317  Varinance:  0.2586219325775161 \n",
      "\n",
      "Epoch:  3272  Learning Rate:  0.003796843964497986  Varinance:  0.258260647217312 \n",
      "\n",
      "Epoch:  3273  Learning Rate:  0.0037930490183228217  Varinance:  0.25789986655951364 \n",
      "\n",
      "Epoch:  3274  Learning Rate:  0.00378925786519699  Varinance:  0.25753958989906983 \n",
      "\n",
      "Epoch:  3275  Learning Rate:  0.0037854705013293413  Varinance:  0.2571798165319152 \n",
      "\n",
      "Epoch:  3276  Learning Rate:  0.0037816869229325084  Varinance:  0.2568205457549672 \n",
      "\n",
      "Epoch:  3277  Learning Rate:  0.003777907126222914  Varinance:  0.25646177686612565 \n",
      "\n",
      "Epoch:  3278  Learning Rate:  0.0037741311074207594  Varinance:  0.25610350916427155 \n",
      "\n",
      "Epoch:  3279  Learning Rate:  0.0037703588627500287  Varinance:  0.2557457419492647 \n",
      "\n",
      "Epoch:  3280  Learning Rate:  0.0037665903884384743  Varinance:  0.2553884745219437 \n",
      "\n",
      "Epoch:  3281  Learning Rate:  0.003762825680717622  Varinance:  0.255031706184123 \n",
      "\n",
      "Epoch:  3282  Learning Rate:  0.003759064735822763  Varinance:  0.25467543623859346 \n",
      "\n",
      "Epoch:  3283  Learning Rate:  0.0037553075499929542  Varinance:  0.254319663989119 \n",
      "\n",
      "Epoch:  3284  Learning Rate:  0.0037515541194710084  Varinance:  0.25396438874043614 \n",
      "\n",
      "Epoch:  3285  Learning Rate:  0.0037478044405034948  Varinance:  0.25360960979825364 \n",
      "\n",
      "Epoch:  3286  Learning Rate:  0.0037440585093407325  Varinance:  0.253255326469249 \n",
      "\n",
      "Epoch:  3287  Learning Rate:  0.003740316322236793  Varinance:  0.2529015380610691 \n",
      "\n",
      "Epoch:  3288  Learning Rate:  0.0037365778754494872  Varinance:  0.2525482438823274 \n",
      "\n",
      "Epoch:  3289  Learning Rate:  0.0037328431652403683  Varinance:  0.25219544324260385 \n",
      "\n",
      "Epoch:  3290  Learning Rate:  0.0037291121878747246  Varinance:  0.2518431354524423 \n",
      "\n",
      "Epoch:  3291  Learning Rate:  0.003725384939621581  Varinance:  0.2514913198233498 \n",
      "\n",
      "Epoch:  3292  Learning Rate:  0.003721661416753687  Varinance:  0.2511399956677959 \n",
      "\n",
      "Epoch:  3293  Learning Rate:  0.0037179416155475203  Varinance:  0.25078916229920944 \n",
      "\n",
      "Epoch:  3294  Learning Rate:  0.0037142255322832776  Varinance:  0.2504388190319796 \n",
      "\n",
      "Epoch:  3295  Learning Rate:  0.0037105131632448776  Varinance:  0.25008896518145224 \n",
      "\n",
      "Epoch:  3296  Learning Rate:  0.0037068045047199505  Varinance:  0.2497396000639307 \n",
      "\n",
      "Epoch:  3297  Learning Rate:  0.003703099552999837  Varinance:  0.24939072299667253 \n",
      "\n",
      "Epoch:  3298  Learning Rate:  0.0036993983043795836  Varinance:  0.24904233329788936 \n",
      "\n",
      "Epoch:  3299  Learning Rate:  0.0036957007551579445  Varinance:  0.2486944302867455 \n",
      "\n",
      "Epoch:  3300  Learning Rate:  0.0036920069016373683  Varinance:  0.24834701328335596 \n",
      "\n",
      "Epoch:  3301  Learning Rate:  0.0036883167401240017  Varinance:  0.24800008160878595 \n",
      "\n",
      "Epoch:  3302  Learning Rate:  0.003684630266927681  Varinance:  0.2476536345850486 \n",
      "\n",
      "Epoch:  3303  Learning Rate:  0.0036809474783619355  Varinance:  0.24730767153510486 \n",
      "\n",
      "Epoch:  3304  Learning Rate:  0.0036772683707439746  Varinance:  0.24696219178286086 \n",
      "\n",
      "Epoch:  3305  Learning Rate:  0.003673592940394692  Varinance:  0.24661719465316723 \n",
      "\n",
      "Epoch:  3306  Learning Rate:  0.0036699211836386534  Varinance:  0.2462726794718183 \n",
      "\n",
      "Epoch:  3307  Learning Rate:  0.0036662530968041064  Varinance:  0.24592864556554958 \n",
      "\n",
      "Epoch:  3308  Learning Rate:  0.0036625886762229618  Varinance:  0.24558509226203779 \n",
      "\n",
      "Epoch:  3309  Learning Rate:  0.0036589279182307984  Varinance:  0.2452420188898983 \n",
      "\n",
      "Epoch:  3310  Learning Rate:  0.0036552708191668567  Varinance:  0.2448994247786844 \n",
      "\n",
      "Epoch:  3311  Learning Rate:  0.0036516173753740402  Varinance:  0.2445573092588865 \n",
      "\n",
      "Epoch:  3312  Learning Rate:  0.003647967583198904  Varinance:  0.2442156716619295 \n",
      "\n",
      "Epoch:  3313  Learning Rate:  0.003644321438991654  Varinance:  0.24387451132017324 \n",
      "\n",
      "Epoch:  3314  Learning Rate:  0.0036406789391061458  Varinance:  0.24353382756690917 \n",
      "\n",
      "Epoch:  3315  Learning Rate:  0.0036370400798998812  Varinance:  0.2431936197363611 \n",
      "\n",
      "Epoch:  3316  Learning Rate:  0.003633404857734  Varinance:  0.24285388716368222 \n",
      "\n",
      "Epoch:  3317  Learning Rate:  0.0036297732689732787  Varinance:  0.24251462918495453 \n",
      "\n",
      "Epoch:  3318  Learning Rate:  0.003626145309986128  Varinance:  0.24217584513718804 \n",
      "\n",
      "Epoch:  3319  Learning Rate:  0.003622520977144591  Varinance:  0.24183753435831817 \n",
      "\n",
      "Epoch:  3320  Learning Rate:  0.003618900266824332  Varinance:  0.24149969618720607 \n",
      "\n",
      "Epoch:  3321  Learning Rate:  0.0036152831754046426  Varinance:  0.2411623299636356 \n",
      "\n",
      "Epoch:  3322  Learning Rate:  0.003611669699268428  Varinance:  0.2408254350283137 \n",
      "\n",
      "Epoch:  3323  Learning Rate:  0.0036080598348022155  Varinance:  0.2404890107228678 \n",
      "\n",
      "Epoch:  3324  Learning Rate:  0.003604453578396138  Varinance:  0.24015305638984502 \n",
      "\n",
      "Epoch:  3325  Learning Rate:  0.003600850926443939  Varinance:  0.2398175713727115 \n",
      "\n",
      "Epoch:  3326  Learning Rate:  0.0035972518753429657  Varinance:  0.23948255501584975 \n",
      "\n",
      "Epoch:  3327  Learning Rate:  0.0035936564214941683  Varinance:  0.23914800666455893 \n",
      "\n",
      "Epoch:  3328  Learning Rate:  0.003590064561302092  Varinance:  0.23881392566505202 \n",
      "\n",
      "Epoch:  3329  Learning Rate:  0.003586476291174876  Varinance:  0.23848031136445613 \n",
      "\n",
      "Epoch:  3330  Learning Rate:  0.0035828916075242496  Varinance:  0.23814716311080977 \n",
      "\n",
      "Epoch:  3331  Learning Rate:  0.0035793105067655298  Varinance:  0.23781448025306223 \n",
      "\n",
      "Epoch:  3332  Learning Rate:  0.0035757329853176156  Varinance:  0.23748226214107293 \n",
      "\n",
      "Epoch:  3333  Learning Rate:  0.0035721590396029846  Varinance:  0.23715050812560864 \n",
      "\n",
      "Epoch:  3334  Learning Rate:  0.003568588666047689  Varinance:  0.23681921755834395 \n",
      "\n",
      "Epoch:  3335  Learning Rate:  0.003565021861081359  Varinance:  0.23648838979185827 \n",
      "\n",
      "Epoch:  3336  Learning Rate:  0.0035614586211371864  Varinance:  0.23615802417963636 \n",
      "\n",
      "Epoch:  3337  Learning Rate:  0.003557898942651932  Varinance:  0.23582812007606535 \n",
      "\n",
      "Epoch:  3338  Learning Rate:  0.0035543428220659156  Varinance:  0.23549867683643447 \n",
      "\n",
      "Epoch:  3339  Learning Rate:  0.003550790255823019  Varinance:  0.23516969381693398 \n",
      "\n",
      "Epoch:  3340  Learning Rate:  0.003547241240370673  Varinance:  0.23484117037465296 \n",
      "\n",
      "Epoch:  3341  Learning Rate:  0.003543695772159864  Varinance:  0.23451310586757915 \n",
      "\n",
      "Epoch:  3342  Learning Rate:  0.003540153847645121  Varinance:  0.2341854996545968 \n",
      "\n",
      "Epoch:  3343  Learning Rate:  0.0035366154632845218  Varinance:  0.23385835109548556 \n",
      "\n",
      "Epoch:  3344  Learning Rate:  0.003533080615539681  Varinance:  0.23353165955092023 \n",
      "\n",
      "Epoch:  3345  Learning Rate:  0.0035295493008757495  Varinance:  0.2332054243824678 \n",
      "\n",
      "Epoch:  3346  Learning Rate:  0.003526021515761412  Varinance:  0.23287964495258798 \n",
      "\n",
      "Epoch:  3347  Learning Rate:  0.0035224972566688857  Varinance:  0.23255432062463027 \n",
      "\n",
      "Epoch:  3348  Learning Rate:  0.0035189765200739093  Varinance:  0.23222945076283438 \n",
      "\n",
      "Epoch:  3349  Learning Rate:  0.003515459302455746  Varinance:  0.23190503473232757 \n",
      "\n",
      "Epoch:  3350  Learning Rate:  0.0035119456002971772  Varinance:  0.23158107189912386 \n",
      "\n",
      "Epoch:  3351  Learning Rate:  0.0035084354100845027  Varinance:  0.2312575616301238 \n",
      "\n",
      "Epoch:  3352  Learning Rate:  0.0035049287283075306  Varinance:  0.23093450329311122 \n",
      "\n",
      "Epoch:  3353  Learning Rate:  0.0035014255514595784  Varinance:  0.23061189625675418 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3354  Learning Rate:  0.003497925876037469  Varinance:  0.2302897398906018 \n",
      "\n",
      "Epoch:  3355  Learning Rate:  0.003494429698541527  Varinance:  0.22996803356508475 \n",
      "\n",
      "Epoch:  3356  Learning Rate:  0.003490937015475576  Varinance:  0.22964677665151248 \n",
      "\n",
      "Epoch:  3357  Learning Rate:  0.0034874478233469317  Varinance:  0.22932596852207285 \n",
      "\n",
      "Epoch:  3358  Learning Rate:  0.0034839621186663985  Varinance:  0.22900560854983124 \n",
      "\n",
      "Epoch:  3359  Learning Rate:  0.0034804798979482772  Varinance:  0.22868569610872808 \n",
      "\n",
      "Epoch:  3360  Learning Rate:  0.0034770011577103434  Varinance:  0.2283662305735792 \n",
      "\n",
      "Epoch:  3361  Learning Rate:  0.0034735258944738564  Varinance:  0.22804721132007288 \n",
      "\n",
      "Epoch:  3362  Learning Rate:  0.003470054104763552  Varinance:  0.22772863772477053 \n",
      "\n",
      "Epoch:  3363  Learning Rate:  0.003466585785107644  Varinance:  0.2274105091651037 \n",
      "\n",
      "Epoch:  3364  Learning Rate:  0.00346312093203781  Varinance:  0.22709282501937375 \n",
      "\n",
      "Epoch:  3365  Learning Rate:  0.0034596595420891955  Varinance:  0.22677558466675107 \n",
      "\n",
      "Epoch:  3366  Learning Rate:  0.00345620161180041  Varinance:  0.2264587874872726 \n",
      "\n",
      "Epoch:  3367  Learning Rate:  0.0034527471377135267  Varinance:  0.22614243286184207 \n",
      "\n",
      "Epoch:  3368  Learning Rate:  0.0034492961163740685  Varinance:  0.22582652017222732 \n",
      "\n",
      "Epoch:  3369  Learning Rate:  0.003445848544331014  Varinance:  0.22551104880106063 \n",
      "\n",
      "Epoch:  3370  Learning Rate:  0.0034424044181367896  Varinance:  0.22519601813183604 \n",
      "\n",
      "Epoch:  3371  Learning Rate:  0.003438963734347271  Varinance:  0.22488142754890894 \n",
      "\n",
      "Epoch:  3372  Learning Rate:  0.0034355264895217738  Varinance:  0.2245672764374953 \n",
      "\n",
      "Epoch:  3373  Learning Rate:  0.0034320926802230527  Varinance:  0.2242535641836691 \n",
      "\n",
      "Epoch:  3374  Learning Rate:  0.0034286623030172964  Varinance:  0.2239402901743628 \n",
      "\n",
      "Epoch:  3375  Learning Rate:  0.00342523535447413  Varinance:  0.22362745379736465 \n",
      "\n",
      "Epoch:  3376  Learning Rate:  0.0034218118311666033  Varinance:  0.22331505444131813 \n",
      "\n",
      "Epoch:  3377  Learning Rate:  0.0034183917296711935  Varinance:  0.22300309149572145 \n",
      "\n",
      "Epoch:  3378  Learning Rate:  0.0034149750465677984  Varinance:  0.22269156435092483 \n",
      "\n",
      "Epoch:  3379  Learning Rate:  0.003411561778439732  Varinance:  0.22238047239813083 \n",
      "\n",
      "Epoch:  3380  Learning Rate:  0.0034081519218737305  Varinance:  0.22206981502939185 \n",
      "\n",
      "Epoch:  3381  Learning Rate:  0.0034047454734599347  Varinance:  0.22175959163761028 \n",
      "\n",
      "Epoch:  3382  Learning Rate:  0.0034013424297918964  Varinance:  0.221449801616536 \n",
      "\n",
      "Epoch:  3383  Learning Rate:  0.0033979427874665695  Varinance:  0.22114044436076583 \n",
      "\n",
      "Epoch:  3384  Learning Rate:  0.003394546543084315  Varinance:  0.22083151926574285 \n",
      "\n",
      "Epoch:  3385  Learning Rate:  0.003391153693248886  Varinance:  0.22052302572775404 \n",
      "\n",
      "Epoch:  3386  Learning Rate:  0.003387764234567433  Varinance:  0.22021496314393033 \n",
      "\n",
      "Epoch:  3387  Learning Rate:  0.003384378163650495  Varinance:  0.21990733091224424 \n",
      "\n",
      "Epoch:  3388  Learning Rate:  0.0033809954771120048  Varinance:  0.21960012843150997 \n",
      "\n",
      "Epoch:  3389  Learning Rate:  0.0033776161715692735  Varinance:  0.21929335510138095 \n",
      "\n",
      "Epoch:  3390  Learning Rate:  0.0033742402436429952  Varinance:  0.2189870103223494 \n",
      "\n",
      "Epoch:  3391  Learning Rate:  0.00337086768995724  Varinance:  0.21868109349574547 \n",
      "\n",
      "Epoch:  3392  Learning Rate:  0.003367498507139457  Varinance:  0.21837560402373496 \n",
      "\n",
      "Epoch:  3393  Learning Rate:  0.0033641326918204626  Varinance:  0.21807054130931958 \n",
      "\n",
      "Epoch:  3394  Learning Rate:  0.0033607702406344394  Varinance:  0.21776590475633414 \n",
      "\n",
      "Epoch:  3395  Learning Rate:  0.0033574111502189356  Varinance:  0.21746169376944718 \n",
      "\n",
      "Epoch:  3396  Learning Rate:  0.0033540554172148637  Varinance:  0.21715790775415825 \n",
      "\n",
      "Epoch:  3397  Learning Rate:  0.003350703038266488  Varinance:  0.21685454611679741 \n",
      "\n",
      "Epoch:  3398  Learning Rate:  0.0033473540100214306  Varinance:  0.21655160826452466 \n",
      "\n",
      "Epoch:  3399  Learning Rate:  0.00334400832913066  Varinance:  0.2162490936053274 \n",
      "\n",
      "Epoch:  3400  Learning Rate:  0.003340665992248499  Varinance:  0.2159470015480207 \n",
      "\n",
      "Epoch:  3401  Learning Rate:  0.0033373269960326082  Varinance:  0.21564533150224496 \n",
      "\n",
      "Epoch:  3402  Learning Rate:  0.003333991337143992  Varinance:  0.2153440828784658 \n",
      "\n",
      "Epoch:  3403  Learning Rate:  0.0033306590122469896  Varinance:  0.21504325508797206 \n",
      "\n",
      "Epoch:  3404  Learning Rate:  0.0033273300180092775  Varinance:  0.2147428475428748 \n",
      "\n",
      "Epoch:  3405  Learning Rate:  0.0033240043511018613  Varinance:  0.21444285965610696 \n",
      "\n",
      "Epoch:  3406  Learning Rate:  0.0033206820081990735  Varinance:  0.21414329084142092 \n",
      "\n",
      "Epoch:  3407  Learning Rate:  0.0033173629859785687  Varinance:  0.2138441405133887 \n",
      "\n",
      "Epoch:  3408  Learning Rate:  0.0033140472811213277  Varinance:  0.2135454080873996 \n",
      "\n",
      "Epoch:  3409  Learning Rate:  0.003310734890311644  Varinance:  0.21324709297965957 \n",
      "\n",
      "Epoch:  3410  Learning Rate:  0.0033074258102371266  Varinance:  0.21294919460719072 \n",
      "\n",
      "Epoch:  3411  Learning Rate:  0.0033041200375886932  Varinance:  0.2126517123878288 \n",
      "\n",
      "Epoch:  3412  Learning Rate:  0.003300817569060575  Varinance:  0.21235464574022345 \n",
      "\n",
      "Epoch:  3413  Learning Rate:  0.0032975184013503004  Varinance:  0.21205799408383585 \n",
      "\n",
      "Epoch:  3414  Learning Rate:  0.003294222531158702  Varinance:  0.21176175683893889 \n",
      "\n",
      "Epoch:  3415  Learning Rate:  0.003290929955189908  Varinance:  0.21146593342661468 \n",
      "\n",
      "Epoch:  3416  Learning Rate:  0.003287640670151345  Varinance:  0.21117052326875407 \n",
      "\n",
      "Epoch:  3417  Learning Rate:  0.003284354672753726  Varinance:  0.21087552578805616 \n",
      "\n",
      "Epoch:  3418  Learning Rate:  0.0032810719597110532  Varinance:  0.21058094040802564 \n",
      "\n",
      "Epoch:  3419  Learning Rate:  0.003277792527740613  Varinance:  0.21028676655297343 \n",
      "\n",
      "Epoch:  3420  Learning Rate:  0.003274516373562974  Varinance:  0.20999300364801374 \n",
      "\n",
      "Epoch:  3421  Learning Rate:  0.003271243493901982  Varinance:  0.20969965111906483 \n",
      "\n",
      "Epoch:  3422  Learning Rate:  0.0032679738854847562  Varinance:  0.2094067083928462 \n",
      "\n",
      "Epoch:  3423  Learning Rate:  0.003264707545041687  Varinance:  0.20911417489687836 \n",
      "\n",
      "Epoch:  3424  Learning Rate:  0.0032614444693064362  Varinance:  0.2088220500594819 \n",
      "\n",
      "Epoch:  3425  Learning Rate:  0.0032581846550159264  Varinance:  0.2085303333097755 \n",
      "\n",
      "Epoch:  3426  Learning Rate:  0.0032549280989103433  Varinance:  0.20823902407767597 \n",
      "\n",
      "Epoch:  3427  Learning Rate:  0.003251674797733129  Varinance:  0.2079481217938957 \n",
      "\n",
      "Epoch:  3428  Learning Rate:  0.0032484247482309847  Varinance:  0.20765762588994327 \n",
      "\n",
      "Epoch:  3429  Learning Rate:  0.0032451779471538594  Varinance:  0.20736753579812073 \n",
      "\n",
      "Epoch:  3430  Learning Rate:  0.0032419343912549513  Varinance:  0.20707785095152323 \n",
      "\n",
      "Epoch:  3431  Learning Rate:  0.003238694077290704  Varinance:  0.20678857078403842 \n",
      "\n",
      "Epoch:  3432  Learning Rate:  0.0032354570020208043  Varinance:  0.20649969473034405 \n",
      "\n",
      "Epoch:  3433  Learning Rate:  0.003232223162208177  Varinance:  0.20621122222590832 \n",
      "\n",
      "Epoch:  3434  Learning Rate:  0.0032289925546189814  Varinance:  0.20592315270698733 \n",
      "\n",
      "Epoch:  3435  Learning Rate:  0.0032257651760226077  Varinance:  0.20563548561062547 \n",
      "\n",
      "Epoch:  3436  Learning Rate:  0.00322254102319168  Varinance:  0.20534822037465297 \n",
      "\n",
      "Epoch:  3437  Learning Rate:  0.0032193200929020445  Varinance:  0.2050613564376855 \n",
      "\n",
      "Epoch:  3438  Learning Rate:  0.0032161023819327703  Varinance:  0.20477489323912332 \n",
      "\n",
      "Epoch:  3439  Learning Rate:  0.003212887887066144  Varinance:  0.2044888302191492 \n",
      "\n",
      "Epoch:  3440  Learning Rate:  0.0032096766050876747  Varinance:  0.20420316681872877 \n",
      "\n",
      "Epoch:  3441  Learning Rate:  0.003206468532786077  Varinance:  0.20391790247960792 \n",
      "\n",
      "Epoch:  3442  Learning Rate:  0.0032032636669532804  Varinance:  0.20363303664431248 \n",
      "\n",
      "Epoch:  3443  Learning Rate:  0.003200062004384415  Varinance:  0.2033485687561476 \n",
      "\n",
      "Epoch:  3444  Learning Rate:  0.0031968635418778233  Varinance:  0.2030644982591953 \n",
      "\n",
      "Epoch:  3445  Learning Rate:  0.0031936682762350394  Varinance:  0.202780824598315 \n",
      "\n",
      "Epoch:  3446  Learning Rate:  0.0031904762042607978  Varinance:  0.20249754721914093 \n",
      "\n",
      "Epoch:  3447  Learning Rate:  0.0031872873227630247  Varinance:  0.2022146655680824 \n",
      "\n",
      "Epoch:  3448  Learning Rate:  0.0031841016285528417  Varinance:  0.20193217909232158 \n",
      "\n",
      "Epoch:  3449  Learning Rate:  0.0031809191184445523  Varinance:  0.2016500872398129 \n",
      "\n",
      "Epoch:  3450  Learning Rate:  0.0031777397892556467  Varinance:  0.20136838945928243 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3451  Learning Rate:  0.003174563637806794  Varinance:  0.20108708520022575 \n",
      "\n",
      "Epoch:  3452  Learning Rate:  0.0031713906609218447  Varinance:  0.20080617391290814 \n",
      "\n",
      "Epoch:  3453  Learning Rate:  0.0031682208554278204  Varinance:  0.20052565504836217 \n",
      "\n",
      "Epoch:  3454  Learning Rate:  0.0031650542181549165  Varinance:  0.20024552805838788 \n",
      "\n",
      "Epoch:  3455  Learning Rate:  0.003161890745936492  Varinance:  0.19996579239555073 \n",
      "\n",
      "Epoch:  3456  Learning Rate:  0.0031587304356090785  Varinance:  0.1996864475131809 \n",
      "\n",
      "Epoch:  3457  Learning Rate:  0.003155573284012364  Varinance:  0.1994074928653727 \n",
      "\n",
      "Epoch:  3458  Learning Rate:  0.0031524192879891968  Varinance:  0.19912892790698244 \n",
      "\n",
      "Epoch:  3459  Learning Rate:  0.003149268444385579  Varinance:  0.1988507520936286 \n",
      "\n",
      "Epoch:  3460  Learning Rate:  0.0031461207500506692  Varinance:  0.19857296488168952 \n",
      "\n",
      "Epoch:  3461  Learning Rate:  0.0031429762018367713  Varinance:  0.1982955657283036 \n",
      "\n",
      "Epoch:  3462  Learning Rate:  0.003139834796599337  Varinance:  0.19801855409136704 \n",
      "\n",
      "Epoch:  3463  Learning Rate:  0.00313669653119696  Varinance:  0.19774192942953348 \n",
      "\n",
      "Epoch:  3464  Learning Rate:  0.003133561402491377  Varinance:  0.1974656912022132 \n",
      "\n",
      "Epoch:  3465  Learning Rate:  0.003130429407347458  Varinance:  0.19718983886957098 \n",
      "\n",
      "Epoch:  3466  Learning Rate:  0.003127300542633206  Varinance:  0.19691437189252645 \n",
      "\n",
      "Epoch:  3467  Learning Rate:  0.0031241748052197567  Varinance:  0.1966392897327516 \n",
      "\n",
      "Epoch:  3468  Learning Rate:  0.0031210521919813744  Varinance:  0.19636459185267124 \n",
      "\n",
      "Epoch:  3469  Learning Rate:  0.003117932699795444  Varinance:  0.19609027771546045 \n",
      "\n",
      "Epoch:  3470  Learning Rate:  0.003114816325542473  Varinance:  0.19581634678504442 \n",
      "\n",
      "Epoch:  3471  Learning Rate:  0.0031117030661060863  Varinance:  0.19554279852609757 \n",
      "\n",
      "Epoch:  3472  Learning Rate:  0.003108592918373026  Varinance:  0.19526963240404158 \n",
      "\n",
      "Epoch:  3473  Learning Rate:  0.003105485879233143  Varinance:  0.1949968478850456 \n",
      "\n",
      "Epoch:  3474  Learning Rate:  0.0031023819455793986  Varinance:  0.19472444443602388 \n",
      "\n",
      "Epoch:  3475  Learning Rate:  0.0030992811143078563  Varinance:  0.19445242152463554 \n",
      "\n",
      "Epoch:  3476  Learning Rate:  0.0030961833823176885  Varinance:  0.19418077861928376 \n",
      "\n",
      "Epoch:  3477  Learning Rate:  0.0030930887465111605  Varinance:  0.1939095151891137 \n",
      "\n",
      "Epoch:  3478  Learning Rate:  0.003089997203793637  Varinance:  0.19363863070401283 \n",
      "\n",
      "Epoch:  3479  Learning Rate:  0.0030869087510735733  Varinance:  0.19336812463460834 \n",
      "\n",
      "Epoch:  3480  Learning Rate:  0.0030838233852625192  Varinance:  0.19309799645226777 \n",
      "\n",
      "Epoch:  3481  Learning Rate:  0.0030807411032751078  Varinance:  0.1928282456290966 \n",
      "\n",
      "Epoch:  3482  Learning Rate:  0.0030776619020290562  Varinance:  0.1925588716379377 \n",
      "\n",
      "Epoch:  3483  Learning Rate:  0.003074585778445162  Varinance:  0.19228987395237088 \n",
      "\n",
      "Epoch:  3484  Learning Rate:  0.003071512729447303  Varinance:  0.1920212520467107 \n",
      "\n",
      "Epoch:  3485  Learning Rate:  0.00306844275196243  Varinance:  0.19175300539600676 \n",
      "\n",
      "Epoch:  3486  Learning Rate:  0.003065375842920565  Varinance:  0.19148513347604126 \n",
      "\n",
      "Epoch:  3487  Learning Rate:  0.0030623119992547964  Varinance:  0.1912176357633295 \n",
      "\n",
      "Epoch:  3488  Learning Rate:  0.0030592512179012836  Varinance:  0.19095051173511746 \n",
      "\n",
      "Epoch:  3489  Learning Rate:  0.003056193495799244  Varinance:  0.1906837608693814 \n",
      "\n",
      "Epoch:  3490  Learning Rate:  0.003053138829890955  Varinance:  0.19041738264482738 \n",
      "\n",
      "Epoch:  3491  Learning Rate:  0.0030500872171217485  Varinance:  0.190151376540889 \n",
      "\n",
      "Epoch:  3492  Learning Rate:  0.0030470386544400145  Varinance:  0.1898857420377277 \n",
      "\n",
      "Epoch:  3493  Learning Rate:  0.003043993138797189  Varinance:  0.1896204786162305 \n",
      "\n",
      "Epoch:  3494  Learning Rate:  0.0030409506671477565  Varinance:  0.1893555857580103 \n",
      "\n",
      "Epoch:  3495  Learning Rate:  0.003037911236449243  Varinance:  0.1890910629454035 \n",
      "\n",
      "Epoch:  3496  Learning Rate:  0.0030348748436622203  Varinance:  0.1888269096614699 \n",
      "\n",
      "Epoch:  3497  Learning Rate:  0.003031841485750294  Varinance:  0.1885631253899917 \n",
      "\n",
      "Epoch:  3498  Learning Rate:  0.0030288111596801066  Varinance:  0.18829970961547182 \n",
      "\n",
      "Epoch:  3499  Learning Rate:  0.0030257838624213294  Varinance:  0.1880366618231338 \n",
      "\n",
      "Epoch:  3500  Learning Rate:  0.003022759590946668  Varinance:  0.18777398149891963 \n",
      "\n",
      "Epoch:  3501  Learning Rate:  0.00301973834223185  Varinance:  0.18751166812949022 \n",
      "\n",
      "Epoch:  3502  Learning Rate:  0.003016720113255626  Varinance:  0.18724972120222297 \n",
      "\n",
      "Epoch:  3503  Learning Rate:  0.0030137049009997664  Varinance:  0.18698814020521148 \n",
      "\n",
      "Epoch:  3504  Learning Rate:  0.0030106927024490574  Varinance:  0.1867269246272649 \n",
      "\n",
      "Epoch:  3505  Learning Rate:  0.0030076835145913033  Varinance:  0.18646607395790588 \n",
      "\n",
      "Epoch:  3506  Learning Rate:  0.003004677334417314  Varinance:  0.1862055876873708 \n",
      "\n",
      "Epoch:  3507  Learning Rate:  0.00300167415892091  Varinance:  0.18594546530660774 \n",
      "\n",
      "Epoch:  3508  Learning Rate:  0.0029986739850989138  Varinance:  0.18568570630727582 \n",
      "\n",
      "Epoch:  3509  Learning Rate:  0.0029956768099511535  Varinance:  0.18542631018174485 \n",
      "\n",
      "Epoch:  3510  Learning Rate:  0.002992682630480453  Varinance:  0.1851672764230931 \n",
      "\n",
      "Epoch:  3511  Learning Rate:  0.002989691443692632  Varinance:  0.18490860452510766 \n",
      "\n",
      "Epoch:  3512  Learning Rate:  0.002986703246596503  Varinance:  0.18465029398228208 \n",
      "\n",
      "Epoch:  3513  Learning Rate:  0.002983718036203871  Varinance:  0.18439234428981682 \n",
      "\n",
      "Epoch:  3514  Learning Rate:  0.0029807358095295233  Varinance:  0.18413475494361695 \n",
      "\n",
      "Epoch:  3515  Learning Rate:  0.002977756563591234  Varinance:  0.18387752544029182 \n",
      "\n",
      "Epoch:  3516  Learning Rate:  0.0029747802954097546  Varinance:  0.1836206552771544 \n",
      "\n",
      "Epoch:  3517  Learning Rate:  0.00297180700200882  Varinance:  0.18336414395221934 \n",
      "\n",
      "Epoch:  3518  Learning Rate:  0.002968836680415135  Varinance:  0.18310799096420313 \n",
      "\n",
      "Epoch:  3519  Learning Rate:  0.002965869327658378  Varinance:  0.1828521958125219 \n",
      "\n",
      "Epoch:  3520  Learning Rate:  0.0029629049407711946  Varinance:  0.18259675799729178 \n",
      "\n",
      "Epoch:  3521  Learning Rate:  0.0029599435167892  Varinance:  0.1823416770193266 \n",
      "\n",
      "Epoch:  3522  Learning Rate:  0.002956985052750969  Varinance:  0.18208695238013767 \n",
      "\n",
      "Epoch:  3523  Learning Rate:  0.0029540295456980375  Varinance:  0.1818325835819331 \n",
      "\n",
      "Epoch:  3524  Learning Rate:  0.002951076992674896  Varinance:  0.18157857012761575 \n",
      "\n",
      "Epoch:  3525  Learning Rate:  0.0029481273907289943  Varinance:  0.18132491152078353 \n",
      "\n",
      "Epoch:  3526  Learning Rate:  0.002945180736910729  Varinance:  0.18107160726572724 \n",
      "\n",
      "Epoch:  3527  Learning Rate:  0.0029422370282734464  Varinance:  0.18081865686743073 \n",
      "\n",
      "Epoch:  3528  Learning Rate:  0.002939296261873436  Varinance:  0.18056605983156893 \n",
      "\n",
      "Epoch:  3529  Learning Rate:  0.0029363584347699336  Varinance:  0.18031381566450727 \n",
      "\n",
      "Epoch:  3530  Learning Rate:  0.0029334235440251107  Varinance:  0.18006192387330133 \n",
      "\n",
      "Epoch:  3531  Learning Rate:  0.0029304915867040763  Varinance:  0.17981038396569465 \n",
      "\n",
      "Epoch:  3532  Learning Rate:  0.002927562559874871  Varinance:  0.179559195450119 \n",
      "\n",
      "Epoch:  3533  Learning Rate:  0.0029246364606084715  Varinance:  0.17930835783569235 \n",
      "\n",
      "Epoch:  3534  Learning Rate:  0.002921713285978776  Varinance:  0.17905787063221892 \n",
      "\n",
      "Epoch:  3535  Learning Rate:  0.00291879303306261  Varinance:  0.1788077333501873 \n",
      "\n",
      "Epoch:  3536  Learning Rate:  0.002915875698939719  Varinance:  0.17855794550076992 \n",
      "\n",
      "Epoch:  3537  Learning Rate:  0.0029129612806927715  Varinance:  0.17830850659582254 \n",
      "\n",
      "Epoch:  3538  Learning Rate:  0.0029100497754073473  Varinance:  0.17805941614788218 \n",
      "\n",
      "Epoch:  3539  Learning Rate:  0.0029071411801719405  Varinance:  0.17781067367016753 \n",
      "\n",
      "Epoch:  3540  Learning Rate:  0.0029042354920779554  Varinance:  0.17756227867657676 \n",
      "\n",
      "Epoch:  3541  Learning Rate:  0.0029013327082197055  Varinance:  0.17731423068168714 \n",
      "\n",
      "Epoch:  3542  Learning Rate:  0.002898432825694406  Varinance:  0.17706652920075452 \n",
      "\n",
      "Epoch:  3543  Learning Rate:  0.002895535841602173  Varinance:  0.17681917374971126 \n",
      "\n",
      "Epoch:  3544  Learning Rate:  0.0028926417530460223  Varinance:  0.1765721638451666 \n",
      "\n",
      "Epoch:  3545  Learning Rate:  0.0028897505571318666  Varinance:  0.1763254990044045 \n",
      "\n",
      "Epoch:  3546  Learning Rate:  0.0028868622509685086  Varinance:  0.17607917874538376 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3547  Learning Rate:  0.0028839768316676426  Varinance:  0.17583320258673618 \n",
      "\n",
      "Epoch:  3548  Learning Rate:  0.0028810942963438473  Varinance:  0.17558757004776598 \n",
      "\n",
      "Epoch:  3549  Learning Rate:  0.0028782146421145894  Varinance:  0.17534228064844942 \n",
      "\n",
      "Epoch:  3550  Learning Rate:  0.0028753378661002136  Varinance:  0.1750973339094326 \n",
      "\n",
      "Epoch:  3551  Learning Rate:  0.0028724639654239435  Varinance:  0.174852729352032 \n",
      "\n",
      "Epoch:  3552  Learning Rate:  0.002869592937211877  Varinance:  0.1746084664982321 \n",
      "\n",
      "Epoch:  3553  Learning Rate:  0.002866724778592988  Varinance:  0.1743645448706859 \n",
      "\n",
      "Epoch:  3554  Learning Rate:  0.0028638594866991166  Varinance:  0.1741209639927125 \n",
      "\n",
      "Epoch:  3555  Learning Rate:  0.002860997058664971  Varinance:  0.1738777233882973 \n",
      "\n",
      "Epoch:  3556  Learning Rate:  0.0028581374916281203  Varinance:  0.17363482258209073 \n",
      "\n",
      "Epoch:  3557  Learning Rate:  0.0028552807827290014  Varinance:  0.17339226109940692 \n",
      "\n",
      "Epoch:  3558  Learning Rate:  0.0028524269291109027  Varinance:  0.1731500384662237 \n",
      "\n",
      "Epoch:  3559  Learning Rate:  0.002849575927919971  Varinance:  0.17290815420918032 \n",
      "\n",
      "Epoch:  3560  Learning Rate:  0.0028467277763052036  Varinance:  0.1726666078555781 \n",
      "\n",
      "Epoch:  3561  Learning Rate:  0.0028438824714184506  Varinance:  0.17242539893337822 \n",
      "\n",
      "Epoch:  3562  Learning Rate:  0.0028410400104144063  Varinance:  0.17218452697120107 \n",
      "\n",
      "Epoch:  3563  Learning Rate:  0.0028382003904506093  Varinance:  0.17194399149832626 \n",
      "\n",
      "Epoch:  3564  Learning Rate:  0.0028353636086874377  Varinance:  0.17170379204469025 \n",
      "\n",
      "Epoch:  3565  Learning Rate:  0.0028325296622881122  Varinance:  0.1714639281408868 \n",
      "\n",
      "Epoch:  3566  Learning Rate:  0.0028296985484186857  Varinance:  0.17122439931816474 \n",
      "\n",
      "Epoch:  3567  Learning Rate:  0.0028268702642480427  Varinance:  0.1709852051084284 \n",
      "\n",
      "Epoch:  3568  Learning Rate:  0.0028240448069478986  Varinance:  0.1707463450442356 \n",
      "\n",
      "Epoch:  3569  Learning Rate:  0.002821222173692798  Varinance:  0.17050781865879705 \n",
      "\n",
      "Epoch:  3570  Learning Rate:  0.002818402361660106  Varinance:  0.17026962548597613 \n",
      "\n",
      "Epoch:  3571  Learning Rate:  0.0028155853680300108  Varinance:  0.17003176506028667 \n",
      "\n",
      "Epoch:  3572  Learning Rate:  0.0028127711899855173  Varinance:  0.16979423691689338 \n",
      "\n",
      "Epoch:  3573  Learning Rate:  0.002809959824712449  Varinance:  0.16955704059161003 \n",
      "\n",
      "Epoch:  3574  Learning Rate:  0.0028071512693994393  Varinance:  0.1693201756208987 \n",
      "\n",
      "Epoch:  3575  Learning Rate:  0.002804345521237933  Varinance:  0.16908364154186947 \n",
      "\n",
      "Epoch:  3576  Learning Rate:  0.002801542577422181  Varinance:  0.1688474378922785 \n",
      "\n",
      "Epoch:  3577  Learning Rate:  0.0027987424351492408  Varinance:  0.16861156421052828 \n",
      "\n",
      "Epoch:  3578  Learning Rate:  0.0027959450916189687  Varinance:  0.1683760200356655 \n",
      "\n",
      "Epoch:  3579  Learning Rate:  0.0027931505440340217  Varinance:  0.1681408049073815 \n",
      "\n",
      "Epoch:  3580  Learning Rate:  0.00279035878959985  Varinance:  0.16790591836601002 \n",
      "\n",
      "Epoch:  3581  Learning Rate:  0.0027875698255247017  Varinance:  0.1676713599525271 \n",
      "\n",
      "Epoch:  3582  Learning Rate:  0.0027847836490196114  Varinance:  0.1674371292085504 \n",
      "\n",
      "Epoch:  3583  Learning Rate:  0.002782000257298402  Varinance:  0.1672032256763373 \n",
      "\n",
      "Epoch:  3584  Learning Rate:  0.00277921964757768  Varinance:  0.1669696488987853 \n",
      "\n",
      "Epoch:  3585  Learning Rate:  0.0027764418170768395  Varinance:  0.1667363984194298 \n",
      "\n",
      "Epoch:  3586  Learning Rate:  0.002773666763018047  Varinance:  0.16650347378244446 \n",
      "\n",
      "Epoch:  3587  Learning Rate:  0.0027708944826262483  Varinance:  0.1662708745326393 \n",
      "\n",
      "Epoch:  3588  Learning Rate:  0.002768124973129162  Varinance:  0.1660386002154602 \n",
      "\n",
      "Epoch:  3589  Learning Rate:  0.0027653582317572808  Varinance:  0.16580665037698852 \n",
      "\n",
      "Epoch:  3590  Learning Rate:  0.002762594255743862  Varinance:  0.16557502456393908 \n",
      "\n",
      "Epoch:  3591  Learning Rate:  0.002759833042324929  Varinance:  0.16534372232366049 \n",
      "\n",
      "Epoch:  3592  Learning Rate:  0.0027570745887392674  Varinance:  0.16511274320413322 \n",
      "\n",
      "Epoch:  3593  Learning Rate:  0.002754318892228425  Varinance:  0.1648820867539697 \n",
      "\n",
      "Epoch:  3594  Learning Rate:  0.0027515659500367048  Varinance:  0.16465175252241251 \n",
      "\n",
      "Epoch:  3595  Learning Rate:  0.0027488157594111637  Varinance:  0.1644217400593339 \n",
      "\n",
      "Epoch:  3596  Learning Rate:  0.0027460683176016095  Varinance:  0.16419204891523545 \n",
      "\n",
      "Epoch:  3597  Learning Rate:  0.0027433236218606036  Varinance:  0.16396267864124597 \n",
      "\n",
      "Epoch:  3598  Learning Rate:  0.0027405816694434475  Varinance:  0.16373362878912198 \n",
      "\n",
      "Epoch:  3599  Learning Rate:  0.00273784245760819  Varinance:  0.16350489891124562 \n",
      "\n",
      "Epoch:  3600  Learning Rate:  0.0027351059836156167  Varinance:  0.16327648856062485 \n",
      "\n",
      "Epoch:  3601  Learning Rate:  0.0027323722447292562  Varinance:  0.16304839729089168 \n",
      "\n",
      "Epoch:  3602  Learning Rate:  0.0027296412382153677  Varinance:  0.16282062465630165 \n",
      "\n",
      "Epoch:  3603  Learning Rate:  0.0027269129613429455  Varinance:  0.16259317021173345 \n",
      "\n",
      "Epoch:  3604  Learning Rate:  0.0027241874113837104  Varinance:  0.16236603351268697 \n",
      "\n",
      "Epoch:  3605  Learning Rate:  0.0027214645856121146  Varinance:  0.16213921411528368 \n",
      "\n",
      "Epoch:  3606  Learning Rate:  0.002718744481305332  Varinance:  0.16191271157626463 \n",
      "\n",
      "Epoch:  3607  Learning Rate:  0.0027160270957432564  Varinance:  0.16168652545299017 \n",
      "\n",
      "Epoch:  3608  Learning Rate:  0.0027133124262085024  Varinance:  0.1614606553034394 \n",
      "\n",
      "Epoch:  3609  Learning Rate:  0.0027106004699864014  Varinance:  0.16123510068620825 \n",
      "\n",
      "Epoch:  3610  Learning Rate:  0.0027078912243649967  Varinance:  0.16100986116050997 \n",
      "\n",
      "Epoch:  3611  Learning Rate:  0.0027051846866350416  Varinance:  0.16078493628617296 \n",
      "\n",
      "Epoch:  3612  Learning Rate:  0.0027024808540899977  Varinance:  0.16056032562364111 \n",
      "\n",
      "Epoch:  3613  Learning Rate:  0.0026997797240260342  Varinance:  0.16033602873397182 \n",
      "\n",
      "Epoch:  3614  Learning Rate:  0.0026970812937420195  Varinance:  0.16011204517883587 \n",
      "\n",
      "Epoch:  3615  Learning Rate:  0.0026943855605395233  Varinance:  0.15988837452051657 \n",
      "\n",
      "Epoch:  3616  Learning Rate:  0.0026916925217228115  Varinance:  0.15966501632190833 \n",
      "\n",
      "Epoch:  3617  Learning Rate:  0.0026890021745988465  Varinance:  0.15944197014651665 \n",
      "\n",
      "Epoch:  3618  Learning Rate:  0.0026863145164772798  Varinance:  0.15921923555845624 \n",
      "\n",
      "Epoch:  3619  Learning Rate:  0.0026836295446704544  Varinance:  0.15899681212245131 \n",
      "\n",
      "Epoch:  3620  Learning Rate:  0.0026809472564933955  Varinance:  0.1587746994038337 \n",
      "\n",
      "Epoch:  3621  Learning Rate:  0.002678267649263818  Varinance:  0.15855289696854247 \n",
      "\n",
      "Epoch:  3622  Learning Rate:  0.0026755907203021125  Varinance:  0.1583314043831235 \n",
      "\n",
      "Epoch:  3623  Learning Rate:  0.0026729164669313504  Varinance:  0.15811022121472754 \n",
      "\n",
      "Epoch:  3624  Learning Rate:  0.002670244886477277  Varinance:  0.15788934703111063 \n",
      "\n",
      "Epoch:  3625  Learning Rate:  0.0026675759762683133  Varinance:  0.15766878140063215 \n",
      "\n",
      "Epoch:  3626  Learning Rate:  0.0026649097336355486  Varinance:  0.15744852389225486 \n",
      "\n",
      "Epoch:  3627  Learning Rate:  0.00266224615591274  Varinance:  0.15722857407554341 \n",
      "\n",
      "Epoch:  3628  Learning Rate:  0.002659585240436309  Varinance:  0.15700893152066367 \n",
      "\n",
      "Epoch:  3629  Learning Rate:  0.002656926984545338  Varinance:  0.15678959579838236 \n",
      "\n",
      "Epoch:  3630  Learning Rate:  0.002654271385581575  Varinance:  0.1565705664800654 \n",
      "\n",
      "Epoch:  3631  Learning Rate:  0.0026516184408894184  Varinance:  0.15635184313767786 \n",
      "\n",
      "Epoch:  3632  Learning Rate:  0.002648968147815924  Varinance:  0.15613342534378233 \n",
      "\n",
      "Epoch:  3633  Learning Rate:  0.0026463205037107963  Varinance:  0.15591531267153907 \n",
      "\n",
      "Epoch:  3634  Learning Rate:  0.0026436755059263944  Varinance:  0.1556975046947042 \n",
      "\n",
      "Epoch:  3635  Learning Rate:  0.002641033151817719  Varinance:  0.15548000098762918 \n",
      "\n",
      "Epoch:  3636  Learning Rate:  0.0026383934387424154  Varinance:  0.15526280112526067 \n",
      "\n",
      "Epoch:  3637  Learning Rate:  0.002635756364060769  Varinance:  0.15504590468313842 \n",
      "\n",
      "Epoch:  3638  Learning Rate:  0.002633121925135708  Varinance:  0.1548293112373958 \n",
      "\n",
      "Epoch:  3639  Learning Rate:  0.002630490119332791  Varinance:  0.15461302036475774 \n",
      "\n",
      "Epoch:  3640  Learning Rate:  0.002627860944020213  Varinance:  0.15439703164254068 \n",
      "\n",
      "Epoch:  3641  Learning Rate:  0.002625234396568796  Varinance:  0.15418134464865177 \n",
      "\n",
      "Epoch:  3642  Learning Rate:  0.002622610474351996  Varinance:  0.15396595896158732 \n",
      "\n",
      "Epoch:  3643  Learning Rate:  0.0026199891747458893  Varinance:  0.153750874160433 \n",
      "\n",
      "Epoch:  3644  Learning Rate:  0.0026173704951291754  Varinance:  0.153536089824862 \n",
      "\n",
      "Epoch:  3645  Learning Rate:  0.0026147544328831737  Varinance:  0.15332160553513516 \n",
      "\n",
      "Epoch:  3646  Learning Rate:  0.0026121409853918235  Varinance:  0.15310742087209928 \n",
      "\n",
      "Epoch:  3647  Learning Rate:  0.0026095301500416767  Varinance:  0.15289353541718675 \n",
      "\n",
      "Epoch:  3648  Learning Rate:  0.0026069219242218973  Varinance:  0.15267994875241506 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3649  Learning Rate:  0.0026043163053242582  Varinance:  0.152466660460385 \n",
      "\n",
      "Epoch:  3650  Learning Rate:  0.002601713290743143  Varinance:  0.15225367012428115 \n",
      "\n",
      "Epoch:  3651  Learning Rate:  0.002599112877875535  Varinance:  0.1520409773278697 \n",
      "\n",
      "Epoch:  3652  Learning Rate:  0.002596515064121021  Varinance:  0.1518285816554988 \n",
      "\n",
      "Epoch:  3653  Learning Rate:  0.002593919846881787  Varinance:  0.15161648269209704 \n",
      "\n",
      "Epoch:  3654  Learning Rate:  0.0025913272235626172  Varinance:  0.15140468002317267 \n",
      "\n",
      "Epoch:  3655  Learning Rate:  0.0025887371915708866  Varinance:  0.15119317323481346 \n",
      "\n",
      "Epoch:  3656  Learning Rate:  0.0025861497483165635  Varinance:  0.1509819619136848 \n",
      "\n",
      "Epoch:  3657  Learning Rate:  0.002583564891212203  Varinance:  0.1507710456470301 \n",
      "\n",
      "Epoch:  3658  Learning Rate:  0.00258098261767295  Varinance:  0.15056042402266878 \n",
      "\n",
      "Epoch:  3659  Learning Rate:  0.0025784029251165303  Varinance:  0.15035009662899662 \n",
      "\n",
      "Epoch:  3660  Learning Rate:  0.00257582581096325  Varinance:  0.15014006305498404 \n",
      "\n",
      "Epoch:  3661  Learning Rate:  0.002573251272635994  Varinance:  0.14993032289017558 \n",
      "\n",
      "Epoch:  3662  Learning Rate:  0.0025706793075602266  Varinance:  0.14972087572468962 \n",
      "\n",
      "Epoch:  3663  Learning Rate:  0.0025681099131639813  Varinance:  0.14951172114921657 \n",
      "\n",
      "Epoch:  3664  Learning Rate:  0.0025655430868778625  Varinance:  0.14930285875501917 \n",
      "\n",
      "Epoch:  3665  Learning Rate:  0.002562978826135044  Varinance:  0.1490942881339306 \n",
      "\n",
      "Epoch:  3666  Learning Rate:  0.0025604171283712656  Varinance:  0.14888600887835485 \n",
      "\n",
      "Epoch:  3667  Learning Rate:  0.0025578579910248297  Varinance:  0.1486780205812648 \n",
      "\n",
      "Epoch:  3668  Learning Rate:  0.0025553014115365976  Varinance:  0.14847032283620198 \n",
      "\n",
      "Epoch:  3669  Learning Rate:  0.0025527473873499886  Varinance:  0.1482629152372761 \n",
      "\n",
      "Epoch:  3670  Learning Rate:  0.002550195915910981  Varinance:  0.14805579737916347 \n",
      "\n",
      "Epoch:  3671  Learning Rate:  0.002547646994668102  Varinance:  0.14784896885710694 \n",
      "\n",
      "Epoch:  3672  Learning Rate:  0.0025451006210724294  Varinance:  0.1476424292669146 \n",
      "\n",
      "Epoch:  3673  Learning Rate:  0.0025425567925775896  Varinance:  0.14743617820495905 \n",
      "\n",
      "Epoch:  3674  Learning Rate:  0.002540015506639755  Varinance:  0.1472302152681772 \n",
      "\n",
      "Epoch:  3675  Learning Rate:  0.0025374767607176385  Varinance:  0.14702454005406845 \n",
      "\n",
      "Epoch:  3676  Learning Rate:  0.0025349405522724945  Varinance:  0.1468191521606951 \n",
      "\n",
      "Epoch:  3677  Learning Rate:  0.002532406878768113  Varinance:  0.14661405118668028 \n",
      "\n",
      "Epoch:  3678  Learning Rate:  0.002529875737670822  Varinance:  0.14640923673120845 \n",
      "\n",
      "Epoch:  3679  Learning Rate:  0.00252734712644948  Varinance:  0.14620470839402364 \n",
      "\n",
      "Epoch:  3680  Learning Rate:  0.0025248210425754744  Varinance:  0.14600046577542883 \n",
      "\n",
      "Epoch:  3681  Learning Rate:  0.0025222974835227216  Varinance:  0.145796508476286 \n",
      "\n",
      "Epoch:  3682  Learning Rate:  0.002519776446767663  Varinance:  0.14559283609801402 \n",
      "\n",
      "Epoch:  3683  Learning Rate:  0.002517257929789261  Varinance:  0.14538944824258915 \n",
      "\n",
      "Epoch:  3684  Learning Rate:  0.002514741930068999  Varinance:  0.14518634451254311 \n",
      "\n",
      "Epoch:  3685  Learning Rate:  0.002512228445090875  Varinance:  0.1449835245109635 \n",
      "\n",
      "Epoch:  3686  Learning Rate:  0.002509717472341407  Varinance:  0.14478098784149182 \n",
      "\n",
      "Epoch:  3687  Learning Rate:  0.0025072090093096206  Varinance:  0.14457873410832345 \n",
      "\n",
      "Epoch:  3688  Learning Rate:  0.002504703053487052  Varinance:  0.14437676291620696 \n",
      "\n",
      "Epoch:  3689  Learning Rate:  0.0025021996023677443  Varinance:  0.14417507387044262 \n",
      "\n",
      "Epoch:  3690  Learning Rate:  0.0024996986534482494  Varinance:  0.1439736665768826 \n",
      "\n",
      "Epoch:  3691  Learning Rate:  0.0024972002042276155  Varinance:  0.14377254064192907 \n",
      "\n",
      "Epoch:  3692  Learning Rate:  0.0024947042522073942  Varinance:  0.1435716956725347 \n",
      "\n",
      "Epoch:  3693  Learning Rate:  0.002492210794891632  Varinance:  0.14337113127620071 \n",
      "\n",
      "Epoch:  3694  Learning Rate:  0.002489719829786874  Varinance:  0.14317084706097674 \n",
      "\n",
      "Epoch:  3695  Learning Rate:  0.0024872313544021525  Varinance:  0.14297084263546028 \n",
      "\n",
      "Epoch:  3696  Learning Rate:  0.002484745366248993  Varinance:  0.14277111760879513 \n",
      "\n",
      "Epoch:  3697  Learning Rate:  0.0024822618628414055  Varinance:  0.14257167159067158 \n",
      "\n",
      "Epoch:  3698  Learning Rate:  0.0024797808416958888  Varinance:  0.14237250419132463 \n",
      "\n",
      "Epoch:  3699  Learning Rate:  0.00247730230033142  Varinance:  0.14217361502153436 \n",
      "\n",
      "Epoch:  3700  Learning Rate:  0.0024748262362694587  Varinance:  0.14197500369262417 \n",
      "\n",
      "Epoch:  3701  Learning Rate:  0.002472352647033939  Varinance:  0.1417766698164604 \n",
      "\n",
      "Epoch:  3702  Learning Rate:  0.002469881530151273  Varinance:  0.1415786130054519 \n",
      "\n",
      "Epoch:  3703  Learning Rate:  0.002467412883150343  Varinance:  0.14138083287254866 \n",
      "\n",
      "Epoch:  3704  Learning Rate:  0.0024649467035625016  Varinance:  0.14118332903124173 \n",
      "\n",
      "Epoch:  3705  Learning Rate:  0.0024624829889215685  Varinance:  0.14098610109556173 \n",
      "\n",
      "Epoch:  3706  Learning Rate:  0.0024600217367638302  Varinance:  0.1407891486800785 \n",
      "\n",
      "Epoch:  3707  Learning Rate:  0.002457562944628034  Varinance:  0.14059247139990064 \n",
      "\n",
      "Epoch:  3708  Learning Rate:  0.002455106610055387  Varinance:  0.14039606887067402 \n",
      "\n",
      "Epoch:  3709  Learning Rate:  0.0024526527305895537  Varinance:  0.14019994070858185 \n",
      "\n",
      "Epoch:  3710  Learning Rate:  0.0024502013037766565  Varinance:  0.14000408653034307 \n",
      "\n",
      "Epoch:  3711  Learning Rate:  0.002447752327165267  Varinance:  0.1398085059532125 \n",
      "\n",
      "Epoch:  3712  Learning Rate:  0.002445305798306409  Varinance:  0.13961319859497934 \n",
      "\n",
      "Epoch:  3713  Learning Rate:  0.002442861714753552  Varinance:  0.13941816407396662 \n",
      "\n",
      "Epoch:  3714  Learning Rate:  0.002440420074062614  Varinance:  0.13922340200903102 \n",
      "\n",
      "Epoch:  3715  Learning Rate:  0.002437980873791954  Varinance:  0.13902891201956116 \n",
      "\n",
      "Epoch:  3716  Learning Rate:  0.0024355441115023707  Varinance:  0.1388346937254778 \n",
      "\n",
      "Epoch:  3717  Learning Rate:  0.0024331097847571006  Varinance:  0.13864074674723226 \n",
      "\n",
      "Epoch:  3718  Learning Rate:  0.002430677891121819  Varinance:  0.13844707070580642 \n",
      "\n",
      "Epoch:  3719  Learning Rate:  0.0024282484281646314  Varinance:  0.1382536652227114 \n",
      "\n",
      "Epoch:  3720  Learning Rate:  0.002425821393456074  Varinance:  0.138060529919987 \n",
      "\n",
      "Epoch:  3721  Learning Rate:  0.0024233967845691115  Varinance:  0.1378676644202014 \n",
      "\n",
      "Epoch:  3722  Learning Rate:  0.0024209745990791363  Varinance:  0.1376750683464495 \n",
      "\n",
      "Epoch:  3723  Learning Rate:  0.002418554834563962  Varinance:  0.1374827413223533 \n",
      "\n",
      "Epoch:  3724  Learning Rate:  0.002416137488603824  Varinance:  0.13729068297206004 \n",
      "\n",
      "Epoch:  3725  Learning Rate:  0.002413722558781375  Varinance:  0.13709889292024247 \n",
      "\n",
      "Epoch:  3726  Learning Rate:  0.0024113100426816866  Varinance:  0.13690737079209736 \n",
      "\n",
      "Epoch:  3727  Learning Rate:  0.002408899937892242  Varinance:  0.13671611621334506 \n",
      "\n",
      "Epoch:  3728  Learning Rate:  0.002406492242002936  Varinance:  0.13652512881022905 \n",
      "\n",
      "Epoch:  3729  Learning Rate:  0.0024040869526060714  Varinance:  0.13633440820951462 \n",
      "\n",
      "Epoch:  3730  Learning Rate:  0.0024016840672963606  Varinance:  0.13614395403848872 \n",
      "\n",
      "Epoch:  3731  Learning Rate:  0.0023992835836709175  Varinance:  0.1359537659249587 \n",
      "\n",
      "Epoch:  3732  Learning Rate:  0.002396885499329258  Varinance:  0.13576384349725218 \n",
      "\n",
      "Epoch:  3733  Learning Rate:  0.002394489811873297  Varinance:  0.13557418638421567 \n",
      "\n",
      "Epoch:  3734  Learning Rate:  0.002392096518907348  Varinance:  0.13538479421521424 \n",
      "\n",
      "Epoch:  3735  Learning Rate:  0.002389705618038117  Varinance:  0.13519566662013097 \n",
      "\n",
      "Epoch:  3736  Learning Rate:  0.0023873171068747034  Varinance:  0.13500680322936556 \n",
      "\n",
      "Epoch:  3737  Learning Rate:  0.0023849309830285947  Varinance:  0.13481820367383454 \n",
      "\n",
      "Epoch:  3738  Learning Rate:  0.002382547244113669  Varinance:  0.1346298675849696 \n",
      "\n",
      "Epoch:  3739  Learning Rate:  0.0023801658877461856  Varinance:  0.13444179459471733 \n",
      "\n",
      "Epoch:  3740  Learning Rate:  0.0023777869115447882  Varinance:  0.1342539843355389 \n",
      "\n",
      "Epoch:  3741  Learning Rate:  0.0023754103131305  Varinance:  0.13406643644040836 \n",
      "\n",
      "Epoch:  3742  Learning Rate:  0.002373036090126723  Varinance:  0.13387915054281302 \n",
      "\n",
      "Epoch:  3743  Learning Rate:  0.0023706642401592347  Varinance:  0.13369212627675164 \n",
      "\n",
      "Epoch:  3744  Learning Rate:  0.0023682947608561836  Varinance:  0.13350536327673485 \n",
      "\n",
      "Epoch:  3745  Learning Rate:  0.00236592764984809  Varinance:  0.13331886117778338 \n",
      "\n",
      "Epoch:  3746  Learning Rate:  0.0023635629047678445  Varinance:  0.13313261961542797 \n",
      "\n",
      "Epoch:  3747  Learning Rate:  0.0023612005232507005  Varinance:  0.13294663822570868 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3748  Learning Rate:  0.0023588405029342768  Varinance:  0.13276091664517373 \n",
      "\n",
      "Epoch:  3749  Learning Rate:  0.0023564828414585513  Varinance:  0.13257545451087938 \n",
      "\n",
      "Epoch:  3750  Learning Rate:  0.002354127536465865  Varinance:  0.13239025146038857 \n",
      "\n",
      "Epoch:  3751  Learning Rate:  0.0023517745856009107  Varinance:  0.1322053071317709 \n",
      "\n",
      "Epoch:  3752  Learning Rate:  0.0023494239865107388  Varinance:  0.1320206211636013 \n",
      "\n",
      "Epoch:  3753  Learning Rate:  0.002347075736844749  Varinance:  0.1318361931949596 \n",
      "\n",
      "Epoch:  3754  Learning Rate:  0.00234472983425469  Varinance:  0.13165202286543012 \n",
      "\n",
      "Epoch:  3755  Learning Rate:  0.002342386276394662  Varinance:  0.13146810981510026 \n",
      "\n",
      "Epoch:  3756  Learning Rate:  0.0023400450609211056  Varinance:  0.1312844536845606 \n",
      "\n",
      "Epoch:  3757  Learning Rate:  0.0023377061854928053  Varinance:  0.13110105411490341 \n",
      "\n",
      "Epoch:  3758  Learning Rate:  0.002335369647770884  Varinance:  0.13091791074772277 \n",
      "\n",
      "Epoch:  3759  Learning Rate:  0.0023330354454188062  Varinance:  0.13073502322511305 \n",
      "\n",
      "Epoch:  3760  Learning Rate:  0.0023307035761023684  Varinance:  0.13055239118966874 \n",
      "\n",
      "Epoch:  3761  Learning Rate:  0.0023283740374897013  Varinance:  0.13037001428448378 \n",
      "\n",
      "Epoch:  3762  Learning Rate:  0.0023260468272512636  Varinance:  0.13018789215315035 \n",
      "\n",
      "Epoch:  3763  Learning Rate:  0.002323721943059849  Varinance:  0.13000602443975895 \n",
      "\n",
      "Epoch:  3764  Learning Rate:  0.002321399382590571  Varinance:  0.1298244107888968 \n",
      "\n",
      "Epoch:  3765  Learning Rate:  0.0023190791435208683  Varinance:  0.12964305084564812 \n",
      "\n",
      "Epoch:  3766  Learning Rate:  0.002316761223530502  Varinance:  0.12946194425559251 \n",
      "\n",
      "Epoch:  3767  Learning Rate:  0.0023144456203015532  Varinance:  0.12928109066480478 \n",
      "\n",
      "Epoch:  3768  Learning Rate:  0.0023121323315184175  Varinance:  0.12910048971985444 \n",
      "\n",
      "Epoch:  3769  Learning Rate:  0.0023098213548678062  Varinance:  0.12892014106780425 \n",
      "\n",
      "Epoch:  3770  Learning Rate:  0.002307512688038741  Varinance:  0.12874004435621056 \n",
      "\n",
      "Epoch:  3771  Learning Rate:  0.002305206328722557  Varinance:  0.12856019923312162 \n",
      "\n",
      "Epoch:  3772  Learning Rate:  0.002302902274612894  Varinance:  0.12838060534707743 \n",
      "\n",
      "Epoch:  3773  Learning Rate:  0.0023006005234056976  Varinance:  0.1282012623471093 \n",
      "\n",
      "Epoch:  3774  Learning Rate:  0.002298301072799215  Varinance:  0.1280221698827383 \n",
      "\n",
      "Epoch:  3775  Learning Rate:  0.0022960039204939983  Varinance:  0.12784332760397563 \n",
      "\n",
      "Epoch:  3776  Learning Rate:  0.0022937090641928933  Varinance:  0.1276647351613209 \n",
      "\n",
      "Epoch:  3777  Learning Rate:  0.0022914165016010434  Varinance:  0.12748639220576255 \n",
      "\n",
      "Epoch:  3778  Learning Rate:  0.0022891262304258852  Varinance:  0.12730829838877605 \n",
      "\n",
      "Epoch:  3779  Learning Rate:  0.002286838248377149  Varinance:  0.12713045336232387 \n",
      "\n",
      "Epoch:  3780  Learning Rate:  0.0022845525531668518  Varinance:  0.12695285677885498 \n",
      "\n",
      "Epoch:  3781  Learning Rate:  0.002282269142509298  Varinance:  0.12677550829130338 \n",
      "\n",
      "Epoch:  3782  Learning Rate:  0.002279988014121076  Varinance:  0.12659840755308843 \n",
      "\n",
      "Epoch:  3783  Learning Rate:  0.0022777091657210594  Varinance:  0.12642155421811316 \n",
      "\n",
      "Epoch:  3784  Learning Rate:  0.002275432595030398  Varinance:  0.1262449479407645 \n",
      "\n",
      "Epoch:  3785  Learning Rate:  0.0022731582997725214  Varinance:  0.12606858837591195 \n",
      "\n",
      "Epoch:  3786  Learning Rate:  0.0022708862776731334  Varinance:  0.12589247517890703 \n",
      "\n",
      "Epoch:  3787  Learning Rate:  0.002268616526460213  Varinance:  0.12571660800558312 \n",
      "\n",
      "Epoch:  3788  Learning Rate:  0.002266349043864008  Varinance:  0.12554098651225395 \n",
      "\n",
      "Epoch:  3789  Learning Rate:  0.0022640838276170353  Varinance:  0.1253656103557138 \n",
      "\n",
      "Epoch:  3790  Learning Rate:  0.0022618208754540785  Varinance:  0.12519047919323592 \n",
      "\n",
      "Epoch:  3791  Learning Rate:  0.0022595601851121863  Varinance:  0.12501559268257284 \n",
      "\n",
      "Epoch:  3792  Learning Rate:  0.002257301754330668  Varinance:  0.12484095048195479 \n",
      "\n",
      "Epoch:  3793  Learning Rate:  0.0022550455808510914  Varinance:  0.12466655225008953 \n",
      "\n",
      "Epoch:  3794  Learning Rate:  0.002252791662417283  Varinance:  0.12449239764616184 \n",
      "\n",
      "Epoch:  3795  Learning Rate:  0.002250539996775326  Varinance:  0.1243184863298322 \n",
      "\n",
      "Epoch:  3796  Learning Rate:  0.002248290581673553  Varinance:  0.124144817961237 \n",
      "\n",
      "Epoch:  3797  Learning Rate:  0.0022460434148625493  Varinance:  0.12397139220098692 \n",
      "\n",
      "Epoch:  3798  Learning Rate:  0.0022437984940951463  Varinance:  0.12379820871016721 \n",
      "\n",
      "Epoch:  3799  Learning Rate:  0.0022415558171264257  Varinance:  0.12362526715033627 \n",
      "\n",
      "Epoch:  3800  Learning Rate:  0.002239315381713709  Varinance:  0.12345256718352524 \n",
      "\n",
      "Epoch:  3801  Learning Rate:  0.0022370771856165604  Varinance:  0.12328010847223778 \n",
      "\n",
      "Epoch:  3802  Learning Rate:  0.0022348412265967826  Varinance:  0.1231078906794485 \n",
      "\n",
      "Epoch:  3803  Learning Rate:  0.0022326075024184187  Varinance:  0.12293591346860333 \n",
      "\n",
      "Epoch:  3804  Learning Rate:  0.0022303760108477433  Varinance:  0.12276417650361802 \n",
      "\n",
      "Epoch:  3805  Learning Rate:  0.002228146749653265  Varinance:  0.12259267944887778 \n",
      "\n",
      "Epoch:  3806  Learning Rate:  0.002225919716605721  Varinance:  0.12242142196923701 \n",
      "\n",
      "Epoch:  3807  Learning Rate:  0.00222369490947808  Varinance:  0.12225040373001789 \n",
      "\n",
      "Epoch:  3808  Learning Rate:  0.002221472326045534  Varinance:  0.12207962439701049 \n",
      "\n",
      "Epoch:  3809  Learning Rate:  0.0022192519640854987  Varinance:  0.12190908363647139 \n",
      "\n",
      "Epoch:  3810  Learning Rate:  0.0022170338213776115  Varinance:  0.12173878111512385 \n",
      "\n",
      "Epoch:  3811  Learning Rate:  0.0022148178957037317  Varinance:  0.12156871650015627 \n",
      "\n",
      "Epoch:  3812  Learning Rate:  0.0022126041848479316  Varinance:  0.12139888945922213 \n",
      "\n",
      "Epoch:  3813  Learning Rate:  0.0022103926865965017  Varinance:  0.12122929966043938 \n",
      "\n",
      "Epoch:  3814  Learning Rate:  0.0022081833987379405  Varinance:  0.12105994677238921 \n",
      "\n",
      "Epoch:  3815  Learning Rate:  0.0022059763190629637  Varinance:  0.12089083046411626 \n",
      "\n",
      "Epoch:  3816  Learning Rate:  0.00220377144536449  Varinance:  0.12072195040512701 \n",
      "\n",
      "Epoch:  3817  Learning Rate:  0.002201568775437645  Varinance:  0.12055330626539011 \n",
      "\n",
      "Epoch:  3818  Learning Rate:  0.0021993683070797576  Varinance:  0.12038489771533487 \n",
      "\n",
      "Epoch:  3819  Learning Rate:  0.002197170038090362  Varinance:  0.12021672442585106 \n",
      "\n",
      "Epoch:  3820  Learning Rate:  0.0021949739662711874  Varinance:  0.12004878606828849 \n",
      "\n",
      "Epoch:  3821  Learning Rate:  0.0021927800894261623  Varinance:  0.11988108231445567 \n",
      "\n",
      "Epoch:  3822  Learning Rate:  0.002190588405361408  Varinance:  0.11971361283661998 \n",
      "\n",
      "Epoch:  3823  Learning Rate:  0.0021883989118852425  Varinance:  0.11954637730750627 \n",
      "\n",
      "Epoch:  3824  Learning Rate:  0.0021862116068081714  Varinance:  0.1193793754002969 \n",
      "\n",
      "Epoch:  3825  Learning Rate:  0.0021840264879428896  Varinance:  0.11921260678863059 \n",
      "\n",
      "Epoch:  3826  Learning Rate:  0.0021818435531042765  Varinance:  0.11904607114660186 \n",
      "\n",
      "Epoch:  3827  Learning Rate:  0.002179662800109399  Varinance:  0.11887976814876093 \n",
      "\n",
      "Epoch:  3828  Learning Rate:  0.002177484226777504  Varinance:  0.11871369747011215 \n",
      "\n",
      "Epoch:  3829  Learning Rate:  0.0021753078309300167  Varinance:  0.11854785878611435 \n",
      "\n",
      "Epoch:  3830  Learning Rate:  0.0021731336103905405  Varinance:  0.11838225177267926 \n",
      "\n",
      "Epoch:  3831  Learning Rate:  0.002170961562984857  Varinance:  0.11821687610617186 \n",
      "\n",
      "Epoch:  3832  Learning Rate:  0.002168791686540917  Varinance:  0.11805173146340883 \n",
      "\n",
      "Epoch:  3833  Learning Rate:  0.0021666239788888453  Varinance:  0.11788681752165839 \n",
      "\n",
      "Epoch:  3834  Learning Rate:  0.0021644584378609313  Varinance:  0.11772213395863985 \n",
      "\n",
      "Epoch:  3835  Learning Rate:  0.0021622950612916365  Varinance:  0.11755768045252238 \n",
      "\n",
      "Epoch:  3836  Learning Rate:  0.0021601338470175835  Varinance:  0.11739345668192515 \n",
      "\n",
      "Epoch:  3837  Learning Rate:  0.0021579747928775574  Varinance:  0.11722946232591593 \n",
      "\n",
      "Epoch:  3838  Learning Rate:  0.002155817896712503  Varinance:  0.11706569706401086 \n",
      "\n",
      "Epoch:  3839  Learning Rate:  0.002153663156365526  Varinance:  0.11690216057617406 \n",
      "\n",
      "Epoch:  3840  Learning Rate:  0.0021515105696818847  Varinance:  0.11673885254281635 \n",
      "\n",
      "Epoch:  3841  Learning Rate:  0.0021493601345089924  Varinance:  0.1165757726447954 \n",
      "\n",
      "Epoch:  3842  Learning Rate:  0.0021472118486964126  Varinance:  0.11641292056341432 \n",
      "\n",
      "Epoch:  3843  Learning Rate:  0.0021450657100958617  Varinance:  0.11625029598042178 \n",
      "\n",
      "Epoch:  3844  Learning Rate:  0.0021429217165611996  Varinance:  0.11608789857801081 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3845  Learning Rate:  0.0021407798659484325  Varinance:  0.11592572803881833 \n",
      "\n",
      "Epoch:  3846  Learning Rate:  0.002138640156115709  Varinance:  0.11576378404592495 \n",
      "\n",
      "Epoch:  3847  Learning Rate:  0.0021365025849233205  Varinance:  0.11560206628285362 \n",
      "\n",
      "Epoch:  3848  Learning Rate:  0.002134367150233695  Varinance:  0.11544057443356973 \n",
      "\n",
      "Epoch:  3849  Learning Rate:  0.002132233849911398  Varinance:  0.1152793081824798 \n",
      "\n",
      "Epoch:  3850  Learning Rate:  0.002130102681823127  Varinance:  0.11511826721443168 \n",
      "\n",
      "Epoch:  3851  Learning Rate:  0.002127973643837717  Varinance:  0.11495745121471307 \n",
      "\n",
      "Epoch:  3852  Learning Rate:  0.0021258467338261277  Varinance:  0.11479685986905135 \n",
      "\n",
      "Epoch:  3853  Learning Rate:  0.0021237219496614496  Varinance:  0.11463649286361326 \n",
      "\n",
      "Epoch:  3854  Learning Rate:  0.0021215992892188967  Varinance:  0.11447634988500353 \n",
      "\n",
      "Epoch:  3855  Learning Rate:  0.002119478750375811  Varinance:  0.1143164306202651 \n",
      "\n",
      "Epoch:  3856  Learning Rate:  0.002117360331011653  Varinance:  0.11415673475687767 \n",
      "\n",
      "Epoch:  3857  Learning Rate:  0.002115244029008002  Varinance:  0.11399726198275795 \n",
      "\n",
      "Epoch:  3858  Learning Rate:  0.0021131298422485552  Varinance:  0.11383801198625833 \n",
      "\n",
      "Epoch:  3859  Learning Rate:  0.0021110177686191277  Varinance:  0.11367898445616648 \n",
      "\n",
      "Epoch:  3860  Learning Rate:  0.0021089078060076445  Varinance:  0.11352017908170527 \n",
      "\n",
      "Epoch:  3861  Learning Rate:  0.0021067999523041434  Varinance:  0.11336159555253116 \n",
      "\n",
      "Epoch:  3862  Learning Rate:  0.002104694205400769  Varinance:  0.11320323355873464 \n",
      "\n",
      "Epoch:  3863  Learning Rate:  0.0021025905631917767  Varinance:  0.11304509279083869 \n",
      "\n",
      "Epoch:  3864  Learning Rate:  0.0021004890235735226  Varinance:  0.11288717293979908 \n",
      "\n",
      "Epoch:  3865  Learning Rate:  0.002098389584444467  Varinance:  0.11272947369700291 \n",
      "\n",
      "Epoch:  3866  Learning Rate:  0.00209629224370517  Varinance:  0.11257199475426848 \n",
      "\n",
      "Epoch:  3867  Learning Rate:  0.002094196999258292  Varinance:  0.11241473580384485 \n",
      "\n",
      "Epoch:  3868  Learning Rate:  0.002092103849008588  Varinance:  0.11225769653841068 \n",
      "\n",
      "Epoch:  3869  Learning Rate:  0.0020900127908629073  Varinance:  0.11210087665107422 \n",
      "\n",
      "Epoch:  3870  Learning Rate:  0.002087923822730191  Varinance:  0.11194427583537227 \n",
      "\n",
      "Epoch:  3871  Learning Rate:  0.0020858369425214717  Varinance:  0.11178789378526967 \n",
      "\n",
      "Epoch:  3872  Learning Rate:  0.0020837521481498695  Varinance:  0.11163173019515905 \n",
      "\n",
      "Epoch:  3873  Learning Rate:  0.0020816694375305885  Varinance:  0.11147578475985968 \n",
      "\n",
      "Epoch:  3874  Learning Rate:  0.0020795888085809178  Varinance:  0.11132005717461745 \n",
      "\n",
      "Epoch:  3875  Learning Rate:  0.0020775102592202298  Varinance:  0.1111645471351036 \n",
      "\n",
      "Epoch:  3876  Learning Rate:  0.002075433787369974  Varinance:  0.11100925433741496 \n",
      "\n",
      "Epoch:  3877  Learning Rate:  0.002073359390953679  Varinance:  0.11085417847807252 \n",
      "\n",
      "Epoch:  3878  Learning Rate:  0.002071287067896948  Varinance:  0.11069931925402128 \n",
      "\n",
      "Epoch:  3879  Learning Rate:  0.0020692168161274556  Varinance:  0.1105446763626299 \n",
      "\n",
      "Epoch:  3880  Learning Rate:  0.0020671486335749533  Varinance:  0.11039024950168937 \n",
      "\n",
      "Epoch:  3881  Learning Rate:  0.0020650825181712565  Varinance:  0.11023603836941324 \n",
      "\n",
      "Epoch:  3882  Learning Rate:  0.0020630184678502505  Varinance:  0.11008204266443633 \n",
      "\n",
      "Epoch:  3883  Learning Rate:  0.002060956480547883  Varinance:  0.10992826208581474 \n",
      "\n",
      "Epoch:  3884  Learning Rate:  0.0020588965542021687  Varinance:  0.10977469633302478 \n",
      "\n",
      "Epoch:  3885  Learning Rate:  0.00205683868675318  Varinance:  0.10962134510596251 \n",
      "\n",
      "Epoch:  3886  Learning Rate:  0.00205478287614305  Varinance:  0.10946820810494359 \n",
      "\n",
      "Epoch:  3887  Learning Rate:  0.002052729120315966  Varinance:  0.10931528503070184 \n",
      "\n",
      "Epoch:  3888  Learning Rate:  0.0020506774172181744  Varinance:  0.1091625755843896 \n",
      "\n",
      "Epoch:  3889  Learning Rate:  0.0020486277647979705  Varinance:  0.10901007946757635 \n",
      "\n",
      "Epoch:  3890  Learning Rate:  0.002046580161005703  Varinance:  0.10885779638224871 \n",
      "\n",
      "Epoch:  3891  Learning Rate:  0.002044534603793765  Varinance:  0.10870572603080945 \n",
      "\n",
      "Epoch:  3892  Learning Rate:  0.0020424910911166034  Varinance:  0.10855386811607702 \n",
      "\n",
      "Epoch:  3893  Learning Rate:  0.002040449620930702  Varinance:  0.10840222234128534 \n",
      "\n",
      "Epoch:  3894  Learning Rate:  0.0020384101911945923  Varinance:  0.10825078841008245 \n",
      "\n",
      "Epoch:  3895  Learning Rate:  0.0020363727998688424  Varinance:  0.10809956602653086 \n",
      "\n",
      "Epoch:  3896  Learning Rate:  0.0020343374449160634  Varinance:  0.10794855489510602 \n",
      "\n",
      "Epoch:  3897  Learning Rate:  0.0020323041243008985  Varinance:  0.10779775472069669 \n",
      "\n",
      "Epoch:  3898  Learning Rate:  0.0020302728359900273  Varinance:  0.10764716520860354 \n",
      "\n",
      "Epoch:  3899  Learning Rate:  0.0020282435779521604  Varinance:  0.10749678606453891 \n",
      "\n",
      "Epoch:  3900  Learning Rate:  0.002026216348158041  Varinance:  0.10734661699462662 \n",
      "\n",
      "Epoch:  3901  Learning Rate:  0.002024191144580439  Varinance:  0.10719665770540054 \n",
      "\n",
      "Epoch:  3902  Learning Rate:  0.0020221679651941504  Varinance:  0.10704690790380497 \n",
      "\n",
      "Epoch:  3903  Learning Rate:  0.0020201468079759943  Varinance:  0.10689736729719322 \n",
      "\n",
      "Epoch:  3904  Learning Rate:  0.0020181276709048154  Varinance:  0.10674803559332754 \n",
      "\n",
      "Epoch:  3905  Learning Rate:  0.002016110551961476  Varinance:  0.1065989125003786 \n",
      "\n",
      "Epoch:  3906  Learning Rate:  0.002014095449128856  Varinance:  0.1064499977269244 \n",
      "\n",
      "Epoch:  3907  Learning Rate:  0.0020120823603918528  Varinance:  0.10630129098195043 \n",
      "\n",
      "Epoch:  3908  Learning Rate:  0.0020100712837373778  Varinance:  0.10615279197484838 \n",
      "\n",
      "Epoch:  3909  Learning Rate:  0.0020080622171543545  Varinance:  0.10600450041541622 \n",
      "\n",
      "Epoch:  3910  Learning Rate:  0.0020060551586337156  Varinance:  0.10585641601385702 \n",
      "\n",
      "Epoch:  3911  Learning Rate:  0.0020040501061684016  Varinance:  0.10570853848077882 \n",
      "\n",
      "Epoch:  3912  Learning Rate:  0.0020020470577533614  Varinance:  0.10556086752719405 \n",
      "\n",
      "Epoch:  3913  Learning Rate:  0.0020000460113855463  Varinance:  0.10541340286451858 \n",
      "\n",
      "Epoch:  3914  Learning Rate:  0.001998046965063909  Varinance:  0.10526614420457175 \n",
      "\n",
      "Epoch:  3915  Learning Rate:  0.0019960499167894024  Varinance:  0.1051190912595751 \n",
      "\n",
      "Epoch:  3916  Learning Rate:  0.00199405486456498  Varinance:  0.1049722437421526 \n",
      "\n",
      "Epoch:  3917  Learning Rate:  0.0019920618063955877  Varinance:  0.10482560136532931 \n",
      "\n",
      "Epoch:  3918  Learning Rate:  0.001990070740288168  Varinance:  0.10467916384253131 \n",
      "\n",
      "Epoch:  3919  Learning Rate:  0.001988081664251654  Varinance:  0.10453293088758515 \n",
      "\n",
      "Epoch:  3920  Learning Rate:  0.0019860945762969705  Varinance:  0.10438690221471693 \n",
      "\n",
      "Epoch:  3921  Learning Rate:  0.0019841094744370287  Varinance:  0.10424107753855219 \n",
      "\n",
      "Epoch:  3922  Learning Rate:  0.0019821263566867273  Varinance:  0.10409545657411488 \n",
      "\n",
      "Epoch:  3923  Learning Rate:  0.001980145221062946  Varinance:  0.10395003903682731 \n",
      "\n",
      "Epoch:  3924  Learning Rate:  0.0019781660655845525  Varinance:  0.10380482464250919 \n",
      "\n",
      "Epoch:  3925  Learning Rate:  0.0019761888882723886  Varinance:  0.10365981310737708 \n",
      "\n",
      "Epoch:  3926  Learning Rate:  0.0019742136871492783  Varinance:  0.10351500414804438 \n",
      "\n",
      "Epoch:  3927  Learning Rate:  0.0019722404602400185  Varinance:  0.10337039748151988 \n",
      "\n",
      "Epoch:  3928  Learning Rate:  0.0019702692055713843  Varinance:  0.10322599282520815 \n",
      "\n",
      "Epoch:  3929  Learning Rate:  0.0019682999211721197  Varinance:  0.10308178989690815 \n",
      "\n",
      "Epoch:  3930  Learning Rate:  0.0019663326050729404  Varinance:  0.10293778841481342 \n",
      "\n",
      "Epoch:  3931  Learning Rate:  0.0019643672553065292  Varinance:  0.10279398809751089 \n",
      "\n",
      "Epoch:  3932  Learning Rate:  0.0019624038699075376  Varinance:  0.10265038866398066 \n",
      "\n",
      "Epoch:  3933  Learning Rate:  0.00196044244691258  Varinance:  0.1025069898335956 \n",
      "\n",
      "Epoch:  3934  Learning Rate:  0.0019584829843602323  Varinance:  0.10236379132612032 \n",
      "\n",
      "Epoch:  3935  Learning Rate:  0.001956525480291031  Varinance:  0.10222079286171125 \n",
      "\n",
      "Epoch:  3936  Learning Rate:  0.0019545699327474745  Varinance:  0.10207799416091544 \n",
      "\n",
      "Epoch:  3937  Learning Rate:  0.0019526163397740135  Varinance:  0.10193539494467035 \n",
      "\n",
      "Epoch:  3938  Learning Rate:  0.0019506646994170548  Varinance:  0.10179299493430359 \n",
      "\n",
      "Epoch:  3939  Learning Rate:  0.0019487150097249571  Varinance:  0.10165079385153165 \n",
      "\n",
      "Epoch:  3940  Learning Rate:  0.0019467672687480328  Varinance:  0.10150879141846013 \n",
      "\n",
      "Epoch:  3941  Learning Rate:  0.0019448214745385391  Varinance:  0.10136698735758254 \n",
      "\n",
      "Epoch:  3942  Learning Rate:  0.001942877625150682  Varinance:  0.10122538139178035 \n",
      "\n",
      "Epoch:  3943  Learning Rate:  0.0019409357186406116  Varinance:  0.10108397324432193 \n",
      "\n",
      "Epoch:  3944  Learning Rate:  0.0019389957530664222  Varinance:  0.10094276263886218 \n",
      "\n",
      "Epoch:  3945  Learning Rate:  0.0019370577264881473  Varinance:  0.10080174929944238 \n",
      "\n",
      "Epoch:  3946  Learning Rate:  0.0019351216369677604  Varinance:  0.10066093295048892 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3947  Learning Rate:  0.0019331874825691707  Varinance:  0.1005203133168135 \n",
      "\n",
      "Epoch:  3948  Learning Rate:  0.0019312552613582256  Varinance:  0.10037989012361194 \n",
      "\n",
      "Epoch:  3949  Learning Rate:  0.0019293249714027027  Varinance:  0.10023966309646423 \n",
      "\n",
      "Epoch:  3950  Learning Rate:  0.0019273966107723123  Varinance:  0.10009963196133355 \n",
      "\n",
      "Epoch:  3951  Learning Rate:  0.0019254701775386922  Varinance:  0.09995979644456579 \n",
      "\n",
      "Epoch:  3952  Learning Rate:  0.001923545669775411  Varinance:  0.0998201562728895 \n",
      "\n",
      "Epoch:  3953  Learning Rate:  0.00192162308555796  Varinance:  0.09968071117341451 \n",
      "\n",
      "Epoch:  3954  Learning Rate:  0.0019197024229637542  Varinance:  0.09954146087363233 \n",
      "\n",
      "Epoch:  3955  Learning Rate:  0.001917783680072131  Varinance:  0.09940240510141471 \n",
      "\n",
      "Epoch:  3956  Learning Rate:  0.0019158668549643485  Varinance:  0.099263543585014 \n",
      "\n",
      "Epoch:  3957  Learning Rate:  0.0019139519457235807  Varinance:  0.09912487605306182 \n",
      "\n",
      "Epoch:  3958  Learning Rate:  0.0019120389504349176  Varinance:  0.09898640223456893 \n",
      "\n",
      "Epoch:  3959  Learning Rate:  0.0019101278671853638  Varinance:  0.0988481218589249 \n",
      "\n",
      "Epoch:  3960  Learning Rate:  0.0019082186940638372  Varinance:  0.09871003465589699 \n",
      "\n",
      "Epoch:  3961  Learning Rate:  0.0019063114291611638  Varinance:  0.0985721403556303 \n",
      "\n",
      "Epoch:  3962  Learning Rate:  0.0019044060705700782  Varinance:  0.09843443868864658 \n",
      "\n",
      "Epoch:  3963  Learning Rate:  0.001902502616385221  Varinance:  0.09829692938584436 \n",
      "\n",
      "Epoch:  3964  Learning Rate:  0.0019006010647031398  Varinance:  0.09815961217849785 \n",
      "\n",
      "Epoch:  3965  Learning Rate:  0.0018987014136222814  Varinance:  0.09802248679825665 \n",
      "\n",
      "Epoch:  3966  Learning Rate:  0.0018968036612429951  Varinance:  0.0978855529771455 \n",
      "\n",
      "Epoch:  3967  Learning Rate:  0.001894907805667527  Varinance:  0.09774881044756313 \n",
      "\n",
      "Epoch:  3968  Learning Rate:  0.0018930138450000235  Varinance:  0.0976122589422825 \n",
      "\n",
      "Epoch:  3969  Learning Rate:  0.0018911217773465228  Varinance:  0.09747589819444953 \n",
      "\n",
      "Epoch:  3970  Learning Rate:  0.0018892316008149571  Varinance:  0.09733972793758298 \n",
      "\n",
      "Epoch:  3971  Learning Rate:  0.0018873433135151486  Varinance:  0.09720374790557414 \n",
      "\n",
      "Epoch:  3972  Learning Rate:  0.001885456913558812  Varinance:  0.09706795783268568 \n",
      "\n",
      "Epoch:  3973  Learning Rate:  0.0018835723990595456  Varinance:  0.09693235745355183 \n",
      "\n",
      "Epoch:  3974  Learning Rate:  0.0018816897681328357  Varinance:  0.09679694650317722 \n",
      "\n",
      "Epoch:  3975  Learning Rate:  0.0018798090188960497  Varinance:  0.09666172471693697 \n",
      "\n",
      "Epoch:  3976  Learning Rate:  0.00187793014946844  Varinance:  0.09652669183057563 \n",
      "\n",
      "Epoch:  3977  Learning Rate:  0.0018760531579711363  Varinance:  0.09639184758020693 \n",
      "\n",
      "Epoch:  3978  Learning Rate:  0.0018741780425271472  Varinance:  0.09625719170231345 \n",
      "\n",
      "Epoch:  3979  Learning Rate:  0.0018723048012613556  Varinance:  0.09612272393374559 \n",
      "\n",
      "Epoch:  3980  Learning Rate:  0.0018704334323005223  Varinance:  0.09598844401172166 \n",
      "\n",
      "Epoch:  3981  Learning Rate:  0.0018685639337732774  Varinance:  0.09585435167382679 \n",
      "\n",
      "Epoch:  3982  Learning Rate:  0.0018666963038101215  Varinance:  0.09572044665801299 \n",
      "\n",
      "Epoch:  3983  Learning Rate:  0.0018648305405434243  Varinance:  0.09558672870259807 \n",
      "\n",
      "Epoch:  3984  Learning Rate:  0.0018629666421074243  Varinance:  0.09545319754626547 \n",
      "\n",
      "Epoch:  3985  Learning Rate:  0.0018611046066382211  Varinance:  0.09531985292806383 \n",
      "\n",
      "Epoch:  3986  Learning Rate:  0.0018592444322737798  Varinance:  0.0951866945874061 \n",
      "\n",
      "Epoch:  3987  Learning Rate:  0.001857386117153925  Varinance:  0.09505372226406951 \n",
      "\n",
      "Epoch:  3988  Learning Rate:  0.0018555296594203428  Varinance:  0.09492093569819454 \n",
      "\n",
      "Epoch:  3989  Learning Rate:  0.0018536750572165747  Varinance:  0.09478833463028498 \n",
      "\n",
      "Epoch:  3990  Learning Rate:  0.0018518223086880184  Varinance:  0.09465591880120688 \n",
      "\n",
      "Epoch:  3991  Learning Rate:  0.0018499714119819242  Varinance:  0.09452368795218832 \n",
      "\n",
      "Epoch:  3992  Learning Rate:  0.001848122365247397  Varinance:  0.09439164182481909 \n",
      "\n",
      "Epoch:  3993  Learning Rate:  0.001846275166635389  Varinance:  0.09425978016104966 \n",
      "\n",
      "Epoch:  3994  Learning Rate:  0.0018444298142987012  Varinance:  0.09412810270319129 \n",
      "\n",
      "Epoch:  3995  Learning Rate:  0.0018425863063919812  Varinance:  0.09399660919391493 \n",
      "\n",
      "Epoch:  3996  Learning Rate:  0.0018407446410717216  Varinance:  0.09386529937625131 \n",
      "\n",
      "Epoch:  3997  Learning Rate:  0.0018389048164962566  Varinance:  0.09373417299358988 \n",
      "\n",
      "Epoch:  3998  Learning Rate:  0.001837066830825761  Varinance:  0.09360322978967861 \n",
      "\n",
      "Epoch:  3999  Learning Rate:  0.0018352306822222492  Varinance:  0.09347246950862365 \n",
      "\n",
      "Epoch:  4000  Learning Rate:  0.0018333963688495728  Varinance:  0.09334189189488833 \n",
      "\n",
      "Epoch:  4001  Learning Rate:  0.001831563888873418  Varinance:  0.09321149669329322 \n",
      "\n",
      "Epoch:  4002  Learning Rate:  0.0018297332404613041  Varinance:  0.09308128364901513 \n",
      "\n",
      "Epoch:  4003  Learning Rate:  0.0018279044217825845  Varinance:  0.09295125250758711 \n",
      "\n",
      "Epoch:  4004  Learning Rate:  0.0018260774310084379  Varinance:  0.09282140301489746 \n",
      "\n",
      "Epoch:  4005  Learning Rate:  0.001824252266311876  Varinance:  0.09269173491718947 \n",
      "\n",
      "Epoch:  4006  Learning Rate:  0.0018224289258677304  Varinance:  0.09256224796106118 \n",
      "\n",
      "Epoch:  4007  Learning Rate:  0.001820607407852663  Varinance:  0.09243294189346425 \n",
      "\n",
      "Epoch:  4008  Learning Rate:  0.0018187877104451564  Varinance:  0.09230381646170421 \n",
      "\n",
      "Epoch:  4009  Learning Rate:  0.0018169698318255103  Varinance:  0.09217487141343936 \n",
      "\n",
      "Epoch:  4010  Learning Rate:  0.0018151537701758475  Varinance:  0.09204610649668044 \n",
      "\n",
      "Epoch:  4011  Learning Rate:  0.0018133395236801077  Varinance:  0.09191752145979053 \n",
      "\n",
      "Epoch:  4012  Learning Rate:  0.001811527090524041  Varinance:  0.09178911605148388 \n",
      "\n",
      "Epoch:  4013  Learning Rate:  0.0018097164688952176  Varinance:  0.09166089002082611 \n",
      "\n",
      "Epoch:  4014  Learning Rate:  0.0018079076569830121  Varinance:  0.09153284311723305 \n",
      "\n",
      "Epoch:  4015  Learning Rate:  0.0018061006529786145  Varinance:  0.09140497509047091 \n",
      "\n",
      "Epoch:  4016  Learning Rate:  0.0018042954550750218  Varinance:  0.09127728569065525 \n",
      "\n",
      "Epoch:  4017  Learning Rate:  0.001802492061467033  Varinance:  0.0911497746682507 \n",
      "\n",
      "Epoch:  4018  Learning Rate:  0.001800690470351256  Varinance:  0.09102244177407068 \n",
      "\n",
      "Epoch:  4019  Learning Rate:  0.0017988906799261007  Varinance:  0.09089528675927645 \n",
      "\n",
      "Epoch:  4020  Learning Rate:  0.0017970926883917736  Varinance:  0.0907683093753772 \n",
      "\n",
      "Epoch:  4021  Learning Rate:  0.0017952964939502868  Varinance:  0.09064150937422892 \n",
      "\n",
      "Epoch:  4022  Learning Rate:  0.0017935020948054413  Varinance:  0.09051488650803453 \n",
      "\n",
      "Epoch:  4023  Learning Rate:  0.0017917094891628405  Varinance:  0.09038844052934297 \n",
      "\n",
      "Epoch:  4024  Learning Rate:  0.0017899186752298795  Varinance:  0.09026217119104875 \n",
      "\n",
      "Epoch:  4025  Learning Rate:  0.0017881296512157416  Varinance:  0.09013607824639192 \n",
      "\n",
      "Epoch:  4026  Learning Rate:  0.0017863424153314036  Varinance:  0.09001016144895682 \n",
      "\n",
      "Epoch:  4027  Learning Rate:  0.0017845569657896317  Varinance:  0.08988442055267243 \n",
      "\n",
      "Epoch:  4028  Learning Rate:  0.0017827733008049724  Varinance:  0.08975885531181113 \n",
      "\n",
      "Epoch:  4029  Learning Rate:  0.001780991418593764  Varinance:  0.08963346548098888 \n",
      "\n",
      "Epoch:  4030  Learning Rate:  0.0017792113173741215  Varinance:  0.08950825081516421 \n",
      "\n",
      "Epoch:  4031  Learning Rate:  0.0017774329953659442  Varinance:  0.08938321106963797 \n",
      "\n",
      "Epoch:  4032  Learning Rate:  0.0017756564507909124  Varinance:  0.08925834600005307 \n",
      "\n",
      "Epoch:  4033  Learning Rate:  0.0017738816818724775  Varinance:  0.08913365536239345 \n",
      "\n",
      "Epoch:  4034  Learning Rate:  0.0017721086868358724  Varinance:  0.08900913891298427 \n",
      "\n",
      "Epoch:  4035  Learning Rate:  0.0017703374639081032  Varinance:  0.08888479640849077 \n",
      "\n",
      "Epoch:  4036  Learning Rate:  0.0017685680113179439  Varinance:  0.08876062760591844 \n",
      "\n",
      "Epoch:  4037  Learning Rate:  0.001766800327295945  Varinance:  0.088636632262612 \n",
      "\n",
      "Epoch:  4038  Learning Rate:  0.001765034410074419  Varinance:  0.08851281013625509 \n",
      "\n",
      "Epoch:  4039  Learning Rate:  0.00176327025788745  Varinance:  0.08838916098487018 \n",
      "\n",
      "Epoch:  4040  Learning Rate:  0.0017615078689708877  Varinance:  0.0882656845668174 \n",
      "\n",
      "Epoch:  4041  Learning Rate:  0.0017597472415623394  Varinance:  0.08814238064079478 \n",
      "\n",
      "Epoch:  4042  Learning Rate:  0.0017579883739011794  Varinance:  0.08801924896583718 \n",
      "\n",
      "Epoch:  4043  Learning Rate:  0.0017562312642285414  Varinance:  0.08789628930131613 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4044  Learning Rate:  0.001754475910787312  Varinance:  0.08777350140693947 \n",
      "\n",
      "Epoch:  4045  Learning Rate:  0.0017527223118221419  Varinance:  0.0876508850427505 \n",
      "\n",
      "Epoch:  4046  Learning Rate:  0.0017509704655794276  Varinance:  0.08752843996912794 \n",
      "\n",
      "Epoch:  4047  Learning Rate:  0.0017492203703073248  Varinance:  0.08740616594678503 \n",
      "\n",
      "Epoch:  4048  Learning Rate:  0.0017474720242557397  Varinance:  0.08728406273676954 \n",
      "\n",
      "Epoch:  4049  Learning Rate:  0.0017457254256763232  Varinance:  0.08716213010046282 \n",
      "\n",
      "Epoch:  4050  Learning Rate:  0.0017439805728224776  Varinance:  0.08704036779957959 \n",
      "\n",
      "Epoch:  4051  Learning Rate:  0.0017422374639493516  Varinance:  0.08691877559616767 \n",
      "\n",
      "Epoch:  4052  Learning Rate:  0.0017404960973138332  Varinance:  0.08679735325260697 \n",
      "\n",
      "Epoch:  4053  Learning Rate:  0.0017387564711745587  Varinance:  0.08667610053160962 \n",
      "\n",
      "Epoch:  4054  Learning Rate:  0.0017370185837918988  Varinance:  0.08655501719621901 \n",
      "\n",
      "Epoch:  4055  Learning Rate:  0.0017352824334279677  Varinance:  0.0864341030098098 \n",
      "\n",
      "Epoch:  4056  Learning Rate:  0.001733548018346616  Varinance:  0.08631335773608698 \n",
      "\n",
      "Epoch:  4057  Learning Rate:  0.0017318153368134255  Varinance:  0.08619278113908561 \n",
      "\n",
      "Epoch:  4058  Learning Rate:  0.0017300843870957162  Varinance:  0.08607237298317069 \n",
      "\n",
      "Epoch:  4059  Learning Rate:  0.0017283551674625395  Varinance:  0.08595213303303605 \n",
      "\n",
      "Epoch:  4060  Learning Rate:  0.001726627676184673  Varinance:  0.0858320610537045 \n",
      "\n",
      "Epoch:  4061  Learning Rate:  0.001724901911534628  Varinance:  0.08571215681052682 \n",
      "\n",
      "Epoch:  4062  Learning Rate:  0.0017231778717866368  Varinance:  0.08559242006918197 \n",
      "\n",
      "Epoch:  4063  Learning Rate:  0.0017214555552166608  Varinance:  0.08547285059567589 \n",
      "\n",
      "Epoch:  4064  Learning Rate:  0.0017197349601023853  Varinance:  0.0853534481563415 \n",
      "\n",
      "Epoch:  4065  Learning Rate:  0.0017180160847232115  Varinance:  0.0852342125178383 \n",
      "\n",
      "Epoch:  4066  Learning Rate:  0.0017162989273602656  Varinance:  0.08511514344715153 \n",
      "\n",
      "Epoch:  4067  Learning Rate:  0.0017145834862963918  Varinance:  0.08499624071159219 \n",
      "\n",
      "Epoch:  4068  Learning Rate:  0.0017128697598161454  Varinance:  0.08487750407879606 \n",
      "\n",
      "Epoch:  4069  Learning Rate:  0.0017111577462058031  Varinance:  0.0847589333167238 \n",
      "\n",
      "Epoch:  4070  Learning Rate:  0.001709447443753348  Varinance:  0.08464052819366003 \n",
      "\n",
      "Epoch:  4071  Learning Rate:  0.0017077388507484794  Varinance:  0.08452228847821304 \n",
      "\n",
      "Epoch:  4072  Learning Rate:  0.0017060319654826053  Varinance:  0.08440421393931463 \n",
      "\n",
      "Epoch:  4073  Learning Rate:  0.0017043267862488375  Varinance:  0.08428630434621903 \n",
      "\n",
      "Epoch:  4074  Learning Rate:  0.0017026233113419977  Varinance:  0.08416855946850317 \n",
      "\n",
      "Epoch:  4075  Learning Rate:  0.0017009215390586124  Varinance:  0.08405097907606562 \n",
      "\n",
      "Epoch:  4076  Learning Rate:  0.0016992214676969067  Varinance:  0.08393356293912642 \n",
      "\n",
      "Epoch:  4077  Learning Rate:  0.0016975230955568119  Varinance:  0.08381631082822677 \n",
      "\n",
      "Epoch:  4078  Learning Rate:  0.0016958264209399527  Varinance:  0.08369922251422818 \n",
      "\n",
      "Epoch:  4079  Learning Rate:  0.0016941314421496552  Varinance:  0.08358229776831254 \n",
      "\n",
      "Epoch:  4080  Learning Rate:  0.0016924381574909431  Varinance:  0.08346553636198108 \n",
      "\n",
      "Epoch:  4081  Learning Rate:  0.001690746565270528  Varinance:  0.08334893806705451 \n",
      "\n",
      "Epoch:  4082  Learning Rate:  0.0016890566637968185  Varinance:  0.08323250265567214 \n",
      "\n",
      "Epoch:  4083  Learning Rate:  0.0016873684513799157  Varinance:  0.08311622990029154 \n",
      "\n",
      "Epoch:  4084  Learning Rate:  0.001685681926331603  Varinance:  0.08300011957368836 \n",
      "\n",
      "Epoch:  4085  Learning Rate:  0.001683997086965359  Varinance:  0.08288417144895545 \n",
      "\n",
      "Epoch:  4086  Learning Rate:  0.0016823139315963404  Varinance:  0.08276838529950287 \n",
      "\n",
      "Epoch:  4087  Learning Rate:  0.0016806324585413937  Varinance:  0.08265276089905693 \n",
      "\n",
      "Epoch:  4088  Learning Rate:  0.0016789526661190471  Varinance:  0.08253729802166035 \n",
      "\n",
      "Epoch:  4089  Learning Rate:  0.0016772745526495048  Varinance:  0.0824219964416713 \n",
      "\n",
      "Epoch:  4090  Learning Rate:  0.0016755981164546553  Varinance:  0.0823068559337631 \n",
      "\n",
      "Epoch:  4091  Learning Rate:  0.0016739233558580634  Varinance:  0.08219187627292415 \n",
      "\n",
      "Epoch:  4092  Learning Rate:  0.0016722502691849652  Varinance:  0.08207705723445681 \n",
      "\n",
      "Epoch:  4093  Learning Rate:  0.001670578854762277  Varinance:  0.08196239859397766 \n",
      "\n",
      "Epoch:  4094  Learning Rate:  0.0016689091109185813  Varinance:  0.08184790012741645 \n",
      "\n",
      "Epoch:  4095  Learning Rate:  0.0016672410359841357  Varinance:  0.08173356161101626 \n",
      "\n",
      "Epoch:  4096  Learning Rate:  0.0016655746282908666  Varinance:  0.08161938282133248 \n",
      "\n",
      "Epoch:  4097  Learning Rate:  0.0016639098861723626  Varinance:  0.08150536353523266 \n",
      "\n",
      "Epoch:  4098  Learning Rate:  0.001662246807963884  Varinance:  0.0813915035298963 \n",
      "\n",
      "Epoch:  4099  Learning Rate:  0.0016605853920023531  Varinance:  0.08127780258281386 \n",
      "\n",
      "Epoch:  4100  Learning Rate:  0.001658925636626351  Varinance:  0.08116426047178693 \n",
      "\n",
      "Epoch:  4101  Learning Rate:  0.0016572675401761256  Varinance:  0.08105087697492724 \n",
      "\n",
      "Epoch:  4102  Learning Rate:  0.0016556111009935768  Varinance:  0.08093765187065674 \n",
      "\n",
      "Epoch:  4103  Learning Rate:  0.001653956317422267  Varinance:  0.08082458493770671 \n",
      "\n",
      "Epoch:  4104  Learning Rate:  0.0016523031878074136  Varinance:  0.08071167595511756 \n",
      "\n",
      "Epoch:  4105  Learning Rate:  0.0016506517104958847  Varinance:  0.08059892470223855 \n",
      "\n",
      "Epoch:  4106  Learning Rate:  0.0016490018838362036  Varinance:  0.08048633095872693 \n",
      "\n",
      "Epoch:  4107  Learning Rate:  0.0016473537061785452  Varinance:  0.08037389450454802 \n",
      "\n",
      "Epoch:  4108  Learning Rate:  0.0016457071758747288  Varinance:  0.08026161511997432 \n",
      "\n",
      "Epoch:  4109  Learning Rate:  0.001644062291278227  Varinance:  0.08014949258558526 \n",
      "\n",
      "Epoch:  4110  Learning Rate:  0.001642419050744152  Varinance:  0.08003752668226706 \n",
      "\n",
      "Epoch:  4111  Learning Rate:  0.0016407774526292645  Varinance:  0.07992571719121168 \n",
      "\n",
      "Epoch:  4112  Learning Rate:  0.0016391374952919677  Varinance:  0.07981406389391708 \n",
      "\n",
      "Epoch:  4113  Learning Rate:  0.001637499177092302  Varinance:  0.07970256657218616 \n",
      "\n",
      "Epoch:  4114  Learning Rate:  0.0016358624963919493  Varinance:  0.0795912250081269 \n",
      "\n",
      "Epoch:  4115  Learning Rate:  0.0016342274515542309  Varinance:  0.07948003898415149 \n",
      "\n",
      "Epoch:  4116  Learning Rate:  0.0016325940409440986  Varinance:  0.07936900828297606 \n",
      "\n",
      "Epoch:  4117  Learning Rate:  0.001630962262928145  Varinance:  0.07925813268762048 \n",
      "\n",
      "Epoch:  4118  Learning Rate:  0.0016293321158745884  Varinance:  0.0791474119814075 \n",
      "\n",
      "Epoch:  4119  Learning Rate:  0.0016277035981532837  Varinance:  0.07903684594796274 \n",
      "\n",
      "Epoch:  4120  Learning Rate:  0.0016260767081357143  Varinance:  0.07892643437121394 \n",
      "\n",
      "Epoch:  4121  Learning Rate:  0.001624451444194987  Varinance:  0.07881617703539086 \n",
      "\n",
      "Epoch:  4122  Learning Rate:  0.0016228278047058396  Varinance:  0.07870607372502451 \n",
      "\n",
      "Epoch:  4123  Learning Rate:  0.0016212057880446334  Varinance:  0.07859612422494693 \n",
      "\n",
      "Epoch:  4124  Learning Rate:  0.0016195853925893488  Varinance:  0.0784863283202909 \n",
      "\n",
      "Epoch:  4125  Learning Rate:  0.0016179666167195931  Varinance:  0.07837668579648911 \n",
      "\n",
      "Epoch:  4126  Learning Rate:  0.0016163494588165876  Varinance:  0.07826719643927427 \n",
      "\n",
      "Epoch:  4127  Learning Rate:  0.0016147339172631757  Varinance:  0.07815786003467813 \n",
      "\n",
      "Epoch:  4128  Learning Rate:  0.001613119990443817  Varinance:  0.07804867636903164 \n",
      "\n",
      "Epoch:  4129  Learning Rate:  0.0016115076767445815  Varinance:  0.07793964522896399 \n",
      "\n",
      "Epoch:  4130  Learning Rate:  0.001609896974553159  Varinance:  0.07783076640140249 \n",
      "\n",
      "Epoch:  4131  Learning Rate:  0.0016082878822588434  Varinance:  0.0777220396735723 \n",
      "\n",
      "Epoch:  4132  Learning Rate:  0.001606680398252544  Varinance:  0.0776134648329955 \n",
      "\n",
      "Epoch:  4133  Learning Rate:  0.0016050745209267787  Varinance:  0.07750504166749134 \n",
      "\n",
      "Epoch:  4134  Learning Rate:  0.001603470248675666  Varinance:  0.07739676996517512 \n",
      "\n",
      "Epoch:  4135  Learning Rate:  0.0016018675798949358  Varinance:  0.07728864951445849 \n",
      "\n",
      "Epoch:  4136  Learning Rate:  0.0016002665129819208  Varinance:  0.07718068010404844 \n",
      "\n",
      "Epoch:  4137  Learning Rate:  0.0015986670463355502  Varinance:  0.07707286152294711 \n",
      "\n",
      "Epoch:  4138  Learning Rate:  0.001597069178356361  Varinance:  0.07696519356045164 \n",
      "\n",
      "Epoch:  4139  Learning Rate:  0.0015954729074464814  Varinance:  0.0768576760061532 \n",
      "\n",
      "Epoch:  4140  Learning Rate:  0.0015938782320096424  Varinance:  0.07675030864993722 \n",
      "\n",
      "Epoch:  4141  Learning Rate:  0.00159228515045117  Varinance:  0.07664309128198239 \n",
      "\n",
      "Epoch:  4142  Learning Rate:  0.0015906936611779787  Varinance:  0.07653602369276054 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4143  Learning Rate:  0.0015891037625985816  Varinance:  0.07642910567303643 \n",
      "\n",
      "Epoch:  4144  Learning Rate:  0.0015875154531230806  Varinance:  0.07632233701386679 \n",
      "\n",
      "Epoch:  4145  Learning Rate:  0.0015859287311631638  Varinance:  0.07621571750660056 \n",
      "\n",
      "Epoch:  4146  Learning Rate:  0.0015843435951321116  Varinance:  0.07610924694287782 \n",
      "\n",
      "Epoch:  4147  Learning Rate:  0.0015827600434447854  Varinance:  0.07600292511463008 \n",
      "\n",
      "Epoch:  4148  Learning Rate:  0.0015811780745176342  Varinance:  0.07589675181407926 \n",
      "\n",
      "Epoch:  4149  Learning Rate:  0.0015795976867686913  Varinance:  0.07579072683373753 \n",
      "\n",
      "Epoch:  4150  Learning Rate:  0.0015780188786175646  Varinance:  0.07568484996640712 \n",
      "\n",
      "Epoch:  4151  Learning Rate:  0.0015764416484854488  Varinance:  0.07557912100517947 \n",
      "\n",
      "Epoch:  4152  Learning Rate:  0.0015748659947951136  Varinance:  0.07547353974343529 \n",
      "\n",
      "Epoch:  4153  Learning Rate:  0.0015732919159709034  Varinance:  0.07536810597484371 \n",
      "\n",
      "Epoch:  4154  Learning Rate:  0.0015717194104387417  Varinance:  0.0752628194933623 \n",
      "\n",
      "Epoch:  4155  Learning Rate:  0.0015701484766261196  Varinance:  0.07515768009323633 \n",
      "\n",
      "Epoch:  4156  Learning Rate:  0.0015685791129621054  Varinance:  0.07505268756899845 \n",
      "\n",
      "Epoch:  4157  Learning Rate:  0.001567011317877336  Varinance:  0.07494784171546859 \n",
      "\n",
      "Epoch:  4158  Learning Rate:  0.001565445089804014  Varinance:  0.07484314232775297 \n",
      "\n",
      "Epoch:  4159  Learning Rate:  0.001563880427175912  Varinance:  0.07473858920124438 \n",
      "\n",
      "Epoch:  4160  Learning Rate:  0.001562317328428369  Varinance:  0.07463418213162112 \n",
      "\n",
      "Epoch:  4161  Learning Rate:  0.0015607557919982832  Varinance:  0.07452992091484724 \n",
      "\n",
      "Epoch:  4162  Learning Rate:  0.0015591958163241207  Varinance:  0.07442580534717155 \n",
      "\n",
      "Epoch:  4163  Learning Rate:  0.0015576373998459033  Varinance:  0.07432183522512759 \n",
      "\n",
      "Epoch:  4164  Learning Rate:  0.0015560805410052155  Varinance:  0.0742180103455332 \n",
      "\n",
      "Epoch:  4165  Learning Rate:  0.0015545252382451998  Varinance:  0.07411433050548993 \n",
      "\n",
      "Epoch:  4166  Learning Rate:  0.0015529714900105504  Varinance:  0.07401079550238295 \n",
      "\n",
      "Epoch:  4167  Learning Rate:  0.0015514192947475203  Varinance:  0.07390740513388024 \n",
      "\n",
      "Epoch:  4168  Learning Rate:  0.0015498686509039159  Varinance:  0.07380415919793269 \n",
      "\n",
      "Epoch:  4169  Learning Rate:  0.00154831955692909  Varinance:  0.07370105749277325 \n",
      "\n",
      "Epoch:  4170  Learning Rate:  0.0015467720112739515  Varinance:  0.07359809981691674 \n",
      "\n",
      "Epoch:  4171  Learning Rate:  0.0015452260123909516  Varinance:  0.07349528596915962 \n",
      "\n",
      "Epoch:  4172  Learning Rate:  0.0015436815587340932  Varinance:  0.07339261574857918 \n",
      "\n",
      "Epoch:  4173  Learning Rate:  0.0015421386487589233  Varinance:  0.07329008895453365 \n",
      "\n",
      "Epoch:  4174  Learning Rate:  0.0015405972809225295  Varinance:  0.07318770538666136 \n",
      "\n",
      "Epoch:  4175  Learning Rate:  0.0015390574536835447  Varinance:  0.07308546484488052 \n",
      "\n",
      "Epoch:  4176  Learning Rate:  0.0015375191655021435  Varinance:  0.07298336712938905 \n",
      "\n",
      "Epoch:  4177  Learning Rate:  0.0015359824148400345  Varinance:  0.07288141204066376 \n",
      "\n",
      "Epoch:  4178  Learning Rate:  0.0015344472001604696  Varinance:  0.07277959937946037 \n",
      "\n",
      "Epoch:  4179  Learning Rate:  0.0015329135199282314  Varinance:  0.07267792894681274 \n",
      "\n",
      "Epoch:  4180  Learning Rate:  0.001531381372609641  Varinance:  0.07257640054403293 \n",
      "\n",
      "Epoch:  4181  Learning Rate:  0.001529850756672552  Varinance:  0.07247501397271035 \n",
      "\n",
      "Epoch:  4182  Learning Rate:  0.0015283216705863458  Varinance:  0.0723737690347116 \n",
      "\n",
      "Epoch:  4183  Learning Rate:  0.0015267941128219377  Varinance:  0.07227266553218022 \n",
      "\n",
      "Epoch:  4184  Learning Rate:  0.0015252680818517708  Varinance:  0.07217170326753594 \n",
      "\n",
      "Epoch:  4185  Learning Rate:  0.0015237435761498118  Varinance:  0.07207088204347474 \n",
      "\n",
      "Epoch:  4186  Learning Rate:  0.001522220594191557  Varinance:  0.07197020166296794 \n",
      "\n",
      "Epoch:  4187  Learning Rate:  0.001520699134454022  Varinance:  0.07186966192926242 \n",
      "\n",
      "Epoch:  4188  Learning Rate:  0.0015191791954157483  Varinance:  0.07176926264587967 \n",
      "\n",
      "Epoch:  4189  Learning Rate:  0.001517660775556798  Varinance:  0.07166900361661566 \n",
      "\n",
      "Epoch:  4190  Learning Rate:  0.0015161438733587481  Varinance:  0.07156888464554069 \n",
      "\n",
      "Epoch:  4191  Learning Rate:  0.001514628487304698  Varinance:  0.07146890553699844 \n",
      "\n",
      "Epoch:  4192  Learning Rate:  0.0015131146158792628  Varinance:  0.07136906609560624 \n",
      "\n",
      "Epoch:  4193  Learning Rate:  0.0015116022575685682  Varinance:  0.07126936612625404 \n",
      "\n",
      "Epoch:  4194  Learning Rate:  0.0015100914108602587  Varinance:  0.07116980543410467 \n",
      "\n",
      "Epoch:  4195  Learning Rate:  0.0015085820742434843  Varinance:  0.0710703838245929 \n",
      "\n",
      "Epoch:  4196  Learning Rate:  0.0015070742462089098  Varinance:  0.07097110110342532 \n",
      "\n",
      "Epoch:  4197  Learning Rate:  0.0015055679252487086  Varinance:  0.07087195707658014 \n",
      "\n",
      "Epoch:  4198  Learning Rate:  0.0015040631098565567  Varinance:  0.07077295155030633 \n",
      "\n",
      "Epoch:  4199  Learning Rate:  0.0015025597985276402  Varinance:  0.07067408433112383 \n",
      "\n",
      "Epoch:  4200  Learning Rate:  0.0015010579897586484  Varinance:  0.07057535522582255 \n",
      "\n",
      "Epoch:  4201  Learning Rate:  0.0014995576820477704  Varinance:  0.07047676404146258 \n",
      "\n",
      "Epoch:  4202  Learning Rate:  0.0014980588738947006  Varinance:  0.07037831058537339 \n",
      "\n",
      "Epoch:  4203  Learning Rate:  0.0014965615638006283  Varinance:  0.07027999466515353 \n",
      "\n",
      "Epoch:  4204  Learning Rate:  0.0014950657502682445  Varinance:  0.07018181608867059 \n",
      "\n",
      "Epoch:  4205  Learning Rate:  0.001493571431801737  Varinance:  0.07008377466406027 \n",
      "\n",
      "Epoch:  4206  Learning Rate:  0.0014920786069067844  Varinance:  0.06998587019972655 \n",
      "\n",
      "Epoch:  4207  Learning Rate:  0.0014905872740905628  Varinance:  0.06988810250434088 \n",
      "\n",
      "Epoch:  4208  Learning Rate:  0.001489097431861741  Varinance:  0.069790471386842 \n",
      "\n",
      "Epoch:  4209  Learning Rate:  0.0014876090787304737  Varinance:  0.06969297665643573 \n",
      "\n",
      "Epoch:  4210  Learning Rate:  0.0014861222132084107  Varinance:  0.06959561812259417 \n",
      "\n",
      "Epoch:  4211  Learning Rate:  0.0014846368338086832  Varinance:  0.06949839559505583 \n",
      "\n",
      "Epoch:  4212  Learning Rate:  0.0014831529390459135  Varinance:  0.06940130888382479 \n",
      "\n",
      "Epoch:  4213  Learning Rate:  0.0014816705274362074  Varinance:  0.06930435779917074 \n",
      "\n",
      "Epoch:  4214  Learning Rate:  0.0014801895974971512  Varinance:  0.06920754215162829 \n",
      "\n",
      "Epoch:  4215  Learning Rate:  0.0014787101477478156  Varinance:  0.06911086175199668 \n",
      "\n",
      "Epoch:  4216  Learning Rate:  0.0014772321767087524  Varinance:  0.06901431641133966 \n",
      "\n",
      "Epoch:  4217  Learning Rate:  0.0014757556829019876  Varinance:  0.0689179059409847 \n",
      "\n",
      "Epoch:  4218  Learning Rate:  0.00147428066485103  Varinance:  0.068821630152523 \n",
      "\n",
      "Epoch:  4219  Learning Rate:  0.001472807121080859  Varinance:  0.0687254888578088 \n",
      "\n",
      "Epoch:  4220  Learning Rate:  0.0014713350501179316  Varinance:  0.0686294818689594 \n",
      "\n",
      "Epoch:  4221  Learning Rate:  0.0014698644504901786  Varinance:  0.06853360899835434 \n",
      "\n",
      "Epoch:  4222  Learning Rate:  0.0014683953207269968  Varinance:  0.06843787005863529 \n",
      "\n",
      "Epoch:  4223  Learning Rate:  0.0014669276593592582  Varinance:  0.06834226486270586 \n",
      "\n",
      "Epoch:  4224  Learning Rate:  0.0014654614649193026  Varinance:  0.06824679322373076 \n",
      "\n",
      "Epoch:  4225  Learning Rate:  0.0014639967359409327  Varinance:  0.06815145495513589 \n",
      "\n",
      "Epoch:  4226  Learning Rate:  0.0014625334709594223  Varinance:  0.06805624987060764 \n",
      "\n",
      "Epoch:  4227  Learning Rate:  0.001461071668511503  Varinance:  0.06796117778409286 \n",
      "\n",
      "Epoch:  4228  Learning Rate:  0.0014596113271353743  Varinance:  0.06786623850979814 \n",
      "\n",
      "Epoch:  4229  Learning Rate:  0.0014581524453706955  Varinance:  0.06777143186218963 \n",
      "\n",
      "Epoch:  4230  Learning Rate:  0.0014566950217585824  Varinance:  0.06767675765599279 \n",
      "\n",
      "Epoch:  4231  Learning Rate:  0.0014552390548416125  Varinance:  0.06758221570619176 \n",
      "\n",
      "Epoch:  4232  Learning Rate:  0.00145378454316382  Varinance:  0.06748780582802928 \n",
      "\n",
      "Epoch:  4233  Learning Rate:  0.0014523314852706902  Varinance:  0.06739352783700599 \n",
      "\n",
      "Epoch:  4234  Learning Rate:  0.0014508798797091681  Varinance:  0.06729938154888054 \n",
      "\n",
      "Epoch:  4235  Learning Rate:  0.0014494297250276455  Varinance:  0.06720536677966873 \n",
      "\n",
      "Epoch:  4236  Learning Rate:  0.0014479810197759687  Varinance:  0.06711148334564342 \n",
      "\n",
      "Epoch:  4237  Learning Rate:  0.0014465337625054336  Varinance:  0.06701773106333425 \n",
      "\n",
      "Epoch:  4238  Learning Rate:  0.0014450879517687802  Varinance:  0.06692410974952699 \n",
      "\n",
      "Epoch:  4239  Learning Rate:  0.001443643586120199  Varinance:  0.06683061922126356 \n",
      "\n",
      "Epoch:  4240  Learning Rate:  0.0014422006641153255  Varinance:  0.06673725929584126 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4241  Learning Rate:  0.0014407591843112351  Varinance:  0.0666440297908127 \n",
      "\n",
      "Epoch:  4242  Learning Rate:  0.0014393191452664504  Varinance:  0.06655093052398546 \n",
      "\n",
      "Epoch:  4243  Learning Rate:  0.0014378805455409294  Varinance:  0.06645796131342142 \n",
      "\n",
      "Epoch:  4244  Learning Rate:  0.001436443383696074  Varinance:  0.06636512197743687 \n",
      "\n",
      "Epoch:  4245  Learning Rate:  0.001435007658294723  Varinance:  0.06627241233460167 \n",
      "\n",
      "Epoch:  4246  Learning Rate:  0.001433573367901149  Varinance:  0.06617983220373937 \n",
      "\n",
      "Epoch:  4247  Learning Rate:  0.0014321405110810622  Varinance:  0.06608738140392642 \n",
      "\n",
      "Epoch:  4248  Learning Rate:  0.0014307090864016068  Varinance:  0.06599505975449205 \n",
      "\n",
      "Epoch:  4249  Learning Rate:  0.0014292790924313563  Varinance:  0.06590286707501798 \n",
      "\n",
      "Epoch:  4250  Learning Rate:  0.001427850527740318  Varinance:  0.06581080318533784 \n",
      "\n",
      "Epoch:  4251  Learning Rate:  0.0014264233908999257  Varinance:  0.0657188679055371 \n",
      "\n",
      "Epoch:  4252  Learning Rate:  0.0014249976804830429  Varinance:  0.06562706105595235 \n",
      "\n",
      "Epoch:  4253  Learning Rate:  0.0014235733950639607  Varinance:  0.06553538245717143 \n",
      "\n",
      "Epoch:  4254  Learning Rate:  0.0014221505332183907  Varinance:  0.06544383193003257 \n",
      "\n",
      "Epoch:  4255  Learning Rate:  0.0014207290935234738  Varinance:  0.06535240929562437 \n",
      "\n",
      "Epoch:  4256  Learning Rate:  0.0014193090745577676  Varinance:  0.06526111437528545 \n",
      "\n",
      "Epoch:  4257  Learning Rate:  0.0014178904749012544  Varinance:  0.06516994699060383 \n",
      "\n",
      "Epoch:  4258  Learning Rate:  0.0014164732931353354  Varinance:  0.065078906963417 \n",
      "\n",
      "Epoch:  4259  Learning Rate:  0.0014150575278428263  Varinance:  0.06498799411581108 \n",
      "\n",
      "Epoch:  4260  Learning Rate:  0.0014136431776079629  Varinance:  0.06489720827012097 \n",
      "\n",
      "Epoch:  4261  Learning Rate:  0.0014122302410163963  Varinance:  0.06480654924892959 \n",
      "\n",
      "Epoch:  4262  Learning Rate:  0.001410818716655187  Varinance:  0.06471601687506773 \n",
      "\n",
      "Epoch:  4263  Learning Rate:  0.0014094086031128132  Varinance:  0.0646256109716138 \n",
      "\n",
      "Epoch:  4264  Learning Rate:  0.0014079998989791587  Varinance:  0.06453533136189318 \n",
      "\n",
      "Epoch:  4265  Learning Rate:  0.0014065926028455206  Varinance:  0.06444517786947827 \n",
      "\n",
      "Epoch:  4266  Learning Rate:  0.0014051867133046039  Varinance:  0.06435515031818767 \n",
      "\n",
      "Epoch:  4267  Learning Rate:  0.0014037822289505162  Varinance:  0.0642652485320864 \n",
      "\n",
      "Epoch:  4268  Learning Rate:  0.0014023791483787744  Varinance:  0.06417547233548503 \n",
      "\n",
      "Epoch:  4269  Learning Rate:  0.001400977470186299  Varinance:  0.06408582155293956 \n",
      "\n",
      "Epoch:  4270  Learning Rate:  0.0013995771929714096  Varinance:  0.06399629600925127 \n",
      "\n",
      "Epoch:  4271  Learning Rate:  0.001398178315333831  Varinance:  0.06390689552946596 \n",
      "\n",
      "Epoch:  4272  Learning Rate:  0.0013967808358746829  Varinance:  0.06381761993887407 \n",
      "\n",
      "Epoch:  4273  Learning Rate:  0.001395384753196487  Varinance:  0.06372846906300988 \n",
      "\n",
      "Epoch:  4274  Learning Rate:  0.0013939900659031617  Varinance:  0.06363944272765149 \n",
      "\n",
      "Epoch:  4275  Learning Rate:  0.0013925967726000175  Varinance:  0.06355054075882043 \n",
      "\n",
      "Epoch:  4276  Learning Rate:  0.001391204871893762  Varinance:  0.06346176298278113 \n",
      "\n",
      "Epoch:  4277  Learning Rate:  0.0013898143623924954  Varinance:  0.06337310922604092 \n",
      "\n",
      "Epoch:  4278  Learning Rate:  0.0013884252427057058  Varinance:  0.06328457931534928 \n",
      "\n",
      "Epoch:  4279  Learning Rate:  0.0013870375114442759  Varinance:  0.06319617307769794 \n",
      "\n",
      "Epoch:  4280  Learning Rate:  0.0013856511672204717  Varinance:  0.06310789034032013 \n",
      "\n",
      "Epoch:  4281  Learning Rate:  0.0013842662086479501  Varinance:  0.06301973093069042 \n",
      "\n",
      "Epoch:  4282  Learning Rate:  0.001382882634341754  Varinance:  0.06293169467652461 \n",
      "\n",
      "Epoch:  4283  Learning Rate:  0.0013815004429183061  Varinance:  0.06284378140577893 \n",
      "\n",
      "Epoch:  4284  Learning Rate:  0.0013801196329954162  Varinance:  0.0627559909466501 \n",
      "\n",
      "Epoch:  4285  Learning Rate:  0.0013787402031922758  Varinance:  0.06266832312757473 \n",
      "\n",
      "Epoch:  4286  Learning Rate:  0.0013773621521294519  Varinance:  0.06258077777722927 \n",
      "\n",
      "Epoch:  4287  Learning Rate:  0.0013759854784288963  Varinance:  0.06249335472452932 \n",
      "\n",
      "Epoch:  4288  Learning Rate:  0.0013746101807139326  Varinance:  0.06240605379862951 \n",
      "\n",
      "Epoch:  4289  Learning Rate:  0.0013732362576092641  Varinance:  0.06231887482892329 \n",
      "\n",
      "Epoch:  4290  Learning Rate:  0.001371863707740969  Varinance:  0.062231817645042195 \n",
      "\n",
      "Epoch:  4291  Learning Rate:  0.0013704925297364945  Varinance:  0.06214488207685603 \n",
      "\n",
      "Epoch:  4292  Learning Rate:  0.0013691227222246641  Varinance:  0.06205806795447199 \n",
      "\n",
      "Epoch:  4293  Learning Rate:  0.0013677542838356712  Varinance:  0.06197137510823487 \n",
      "\n",
      "Epoch:  4294  Learning Rate:  0.001366387213201075  Varinance:  0.06188480336872628 \n",
      "\n",
      "Epoch:  4295  Learning Rate:  0.0013650215089538072  Varinance:  0.0617983525667645 \n",
      "\n",
      "Epoch:  4296  Learning Rate:  0.0013636571697281605  Varinance:  0.06171202253340432 \n",
      "\n",
      "Epoch:  4297  Learning Rate:  0.0013622941941597974  Varinance:  0.061625813099936325 \n",
      "\n",
      "Epoch:  4298  Learning Rate:  0.0013609325808857432  Varinance:  0.06153972409788698 \n",
      "\n",
      "Epoch:  4299  Learning Rate:  0.0013595723285443819  Varinance:  0.06145375535901793 \n",
      "\n",
      "Epoch:  4300  Learning Rate:  0.0013582134357754626  Varinance:  0.06136790671532601 \n",
      "\n",
      "Epoch:  4301  Learning Rate:  0.0013568559012200935  Varinance:  0.06128217799904259 \n",
      "\n",
      "Epoch:  4302  Learning Rate:  0.0013554997235207375  Varinance:  0.06119656904263345 \n",
      "\n",
      "Epoch:  4303  Learning Rate:  0.001354144901321219  Varinance:  0.061111079678798516 \n",
      "\n",
      "Epoch:  4304  Learning Rate:  0.0013527914332667136  Varinance:  0.061025709740471265 \n",
      "\n",
      "Epoch:  4305  Learning Rate:  0.0013514393180037543  Varinance:  0.06094045906081874 \n",
      "\n",
      "Epoch:  4306  Learning Rate:  0.0013500885541802266  Varinance:  0.06085532747324088 \n",
      "\n",
      "Epoch:  4307  Learning Rate:  0.0013487391404453644  Varinance:  0.06077031481137036 \n",
      "\n",
      "Epoch:  4308  Learning Rate:  0.0013473910754497553  Varinance:  0.06068542090907247 \n",
      "\n",
      "Epoch:  4309  Learning Rate:  0.001346044357845335  Varinance:  0.06060064560044429 \n",
      "\n",
      "Epoch:  4310  Learning Rate:  0.0013446989862853833  Varinance:  0.060515988719814943 \n",
      "\n",
      "Epoch:  4311  Learning Rate:  0.0013433549594245315  Varinance:  0.06043145010174475 \n",
      "\n",
      "Epoch:  4312  Learning Rate:  0.0013420122759187497  Varinance:  0.06034702958102535 \n",
      "\n",
      "Epoch:  4313  Learning Rate:  0.0013406709344253556  Varinance:  0.06026272699267901 \n",
      "\n",
      "Epoch:  4314  Learning Rate:  0.0013393309336030088  Varinance:  0.06017854217195849 \n",
      "\n",
      "Epoch:  4315  Learning Rate:  0.001337992272111706  Varinance:  0.06009447495434684 \n",
      "\n",
      "Epoch:  4316  Learning Rate:  0.0013366549486127871  Varinance:  0.0600105251755567 \n",
      "\n",
      "Epoch:  4317  Learning Rate:  0.0013353189617689293  Varinance:  0.059926692671530474 \n",
      "\n",
      "Epoch:  4318  Learning Rate:  0.0013339843102441432  Varinance:  0.05984297727843949 \n",
      "\n",
      "Epoch:  4319  Learning Rate:  0.0013326509927037797  Varinance:  0.05975937883268419 \n",
      "\n",
      "Epoch:  4320  Learning Rate:  0.001331319007814519  Varinance:  0.059675897170893354 \n",
      "\n",
      "Epoch:  4321  Learning Rate:  0.0013299883542443767  Varinance:  0.05959253212992402 \n",
      "\n",
      "Epoch:  4322  Learning Rate:  0.001328659030662701  Varinance:  0.0595092835468613 \n",
      "\n",
      "Epoch:  4323  Learning Rate:  0.0013273310357401653  Varinance:  0.05942615125901765 \n",
      "\n",
      "Epoch:  4324  Learning Rate:  0.001326004368148776  Varinance:  0.059343135103933006 \n",
      "\n",
      "Epoch:  4325  Learning Rate:  0.0013246790265618667  Varinance:  0.05926023491937406 \n",
      "\n",
      "Epoch:  4326  Learning Rate:  0.0013233550096540929  Varinance:  0.05917745054333435 \n",
      "\n",
      "Epoch:  4327  Learning Rate:  0.0013220323161014403  Varinance:  0.05909478181403356 \n",
      "\n",
      "Epoch:  4328  Learning Rate:  0.0013207109445812128  Varinance:  0.0590122285699174 \n",
      "\n",
      "Epoch:  4329  Learning Rate:  0.00131939089377204  Varinance:  0.05892979064965738 \n",
      "\n",
      "Epoch:  4330  Learning Rate:  0.001318072162353872  Varinance:  0.0588474678921502 \n",
      "\n",
      "Epoch:  4331  Learning Rate:  0.0013167547490079753  Varinance:  0.05876526013651784 \n",
      "\n",
      "Epoch:  4332  Learning Rate:  0.001315438652416937  Varinance:  0.058683167222106765 \n",
      "\n",
      "Epoch:  4333  Learning Rate:  0.0013141238712646621  Varinance:  0.05860118898848813 \n",
      "\n",
      "Epoch:  4334  Learning Rate:  0.0013128104042363668  Varinance:  0.05851932527545699 \n",
      "\n",
      "Epoch:  4335  Learning Rate:  0.001311498250018586  Varinance:  0.05843757592303225 \n",
      "\n",
      "Epoch:  4336  Learning Rate:  0.0013101874072991638  Varinance:  0.05835594077145645 \n",
      "\n",
      "Epoch:  4337  Learning Rate:  0.0013088778747672577  Varinance:  0.05827441966119505 \n",
      "\n",
      "Epoch:  4338  Learning Rate:  0.0013075696511133368  Varinance:  0.05819301243293665 \n",
      "\n",
      "Epoch:  4339  Learning Rate:  0.0013062627350291748  Varinance:  0.05811171892759219 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4340  Learning Rate:  0.0013049571252078567  Varinance:  0.05803053898629489 \n",
      "\n",
      "Epoch:  4341  Learning Rate:  0.0013036528203437736  Varinance:  0.057949472450400046 \n",
      "\n",
      "Epoch:  4342  Learning Rate:  0.0013023498191326185  Varinance:  0.05786851916148436 \n",
      "\n",
      "Epoch:  4343  Learning Rate:  0.001301048120271392  Varinance:  0.05778767896134605 \n",
      "\n",
      "Epoch:  4344  Learning Rate:  0.0012997477224583932  Varinance:  0.05770695169200414 \n",
      "\n",
      "Epoch:  4345  Learning Rate:  0.0012984486243932253  Varinance:  0.057626337195698554 \n",
      "\n",
      "Epoch:  4346  Learning Rate:  0.0012971508247767909  Varinance:  0.05754583531488943 \n",
      "\n",
      "Epoch:  4347  Learning Rate:  0.0012958543223112883  Varinance:  0.057465445892257015 \n",
      "\n",
      "Epoch:  4348  Learning Rate:  0.001294559115700216  Varinance:  0.05738516877070144 \n",
      "\n",
      "Epoch:  4349  Learning Rate:  0.0012932652036483684  Varinance:  0.057305003793342135 \n",
      "\n",
      "Epoch:  4350  Learning Rate:  0.001291972584861831  Varinance:  0.05722495080351785 \n",
      "\n",
      "Epoch:  4351  Learning Rate:  0.0012906812580479874  Varinance:  0.05714500964478603 \n",
      "\n",
      "Epoch:  4352  Learning Rate:  0.0012893912219155083  Varinance:  0.0570651801609228 \n",
      "\n",
      "Epoch:  4353  Learning Rate:  0.0012881024751743584  Varinance:  0.056985462195922446 \n",
      "\n",
      "Epoch:  4354  Learning Rate:  0.0012868150165357922  Varinance:  0.056905855593997134 \n",
      "\n",
      "Epoch:  4355  Learning Rate:  0.0012855288447123489  Varinance:  0.05682636019957684 \n",
      "\n",
      "Epoch:  4356  Learning Rate:  0.0012842439584178572  Varinance:  0.05674697585730865 \n",
      "\n",
      "Epoch:  4357  Learning Rate:  0.001282960356367432  Varinance:  0.056667702412056896 \n",
      "\n",
      "Epoch:  4358  Learning Rate:  0.001281678037277469  Varinance:  0.05658853970890238 \n",
      "\n",
      "Epoch:  4359  Learning Rate:  0.0012803969998656513  Varinance:  0.056509487593142574 \n",
      "\n",
      "Epoch:  4360  Learning Rate:  0.001279117242850939  Varinance:  0.056430545910290876 \n",
      "\n",
      "Epoch:  4361  Learning Rate:  0.001277838764953576  Varinance:  0.05635171450607653 \n",
      "\n",
      "Epoch:  4362  Learning Rate:  0.0012765615648950858  Varinance:  0.0562729932264444 \n",
      "\n",
      "Epoch:  4363  Learning Rate:  0.0012752856413982658  Varinance:  0.0561943819175544 \n",
      "\n",
      "Epoch:  4364  Learning Rate:  0.0012740109931871932  Varinance:  0.05611588042578153 \n",
      "\n",
      "Epoch:  4365  Learning Rate:  0.0012727376189872212  Varinance:  0.056037488597715186 \n",
      "\n",
      "Epoch:  4366  Learning Rate:  0.0012714655175249732  Varinance:  0.05595920628015932 \n",
      "\n",
      "Epoch:  4367  Learning Rate:  0.0012701946875283499  Varinance:  0.05588103332013169 \n",
      "\n",
      "Epoch:  4368  Learning Rate:  0.0012689251277265186  Varinance:  0.05580296956486378 \n",
      "\n",
      "Epoch:  4369  Learning Rate:  0.0012676568368499208  Varinance:  0.05572501486180067 \n",
      "\n",
      "Epoch:  4370  Learning Rate:  0.0012663898136302666  Varinance:  0.05564716905860031 \n",
      "\n",
      "Epoch:  4371  Learning Rate:  0.0012651240568005307  Varinance:  0.05556943200313368 \n",
      "\n",
      "Epoch:  4372  Learning Rate:  0.0012638595650949567  Varinance:  0.05549180354348415 \n",
      "\n",
      "Epoch:  4373  Learning Rate:  0.0012625963372490546  Varinance:  0.055414283527947285 \n",
      "\n",
      "Epoch:  4374  Learning Rate:  0.0012613343719995935  Varinance:  0.05533687180503073 \n",
      "\n",
      "Epoch:  4375  Learning Rate:  0.0012600736680846106  Varinance:  0.055259568223453556 \n",
      "\n",
      "Epoch:  4376  Learning Rate:  0.0012588142242434  Varinance:  0.055182372632146405 \n",
      "\n",
      "Epoch:  4377  Learning Rate:  0.0012575560392165182  Varinance:  0.0551052848802507 \n",
      "\n",
      "Epoch:  4378  Learning Rate:  0.0012562991117457817  Varinance:  0.055028304817118845 \n",
      "\n",
      "Epoch:  4379  Learning Rate:  0.0012550434405742606  Varinance:  0.054951432292313536 \n",
      "\n",
      "Epoch:  4380  Learning Rate:  0.0012537890244462857  Varinance:  0.05487466715560762 \n",
      "\n",
      "Epoch:  4381  Learning Rate:  0.0012525358621074385  Varinance:  0.05479800925698396 \n",
      "\n",
      "Epoch:  4382  Learning Rate:  0.0012512839523045579  Varinance:  0.05472145844663478 \n",
      "\n",
      "Epoch:  4383  Learning Rate:  0.0012500332937857351  Varinance:  0.0546450145749618 \n",
      "\n",
      "Epoch:  4384  Learning Rate:  0.001248783885300309  Varinance:  0.0545686774925755 \n",
      "\n",
      "Epoch:  4385  Learning Rate:  0.0012475357255988724  Varinance:  0.05449244705029525 \n",
      "\n",
      "Epoch:  4386  Learning Rate:  0.0012462888134332662  Varinance:  0.0544163230991487 \n",
      "\n",
      "Epoch:  4387  Learning Rate:  0.0012450431475565765  Varinance:  0.05434030549037158 \n",
      "\n",
      "Epoch:  4388  Learning Rate:  0.001243798726723139  Varinance:  0.0542643940754076 \n",
      "\n",
      "Epoch:  4389  Learning Rate:  0.001242555549688531  Varinance:  0.0541885887059078 \n",
      "\n",
      "Epoch:  4390  Learning Rate:  0.001241313615209576  Varinance:  0.054112889233730636 \n",
      "\n",
      "Epoch:  4391  Learning Rate:  0.0012400729220443407  Varinance:  0.054037295510941344 \n",
      "\n",
      "Epoch:  4392  Learning Rate:  0.00123883346895213  Varinance:  0.05396180738981201 \n",
      "\n",
      "Epoch:  4393  Learning Rate:  0.0012375952546934913  Varinance:  0.05388642472282093 \n",
      "\n",
      "Epoch:  4394  Learning Rate:  0.0012363582780302114  Varinance:  0.05381114736265248 \n",
      "\n",
      "Epoch:  4395  Learning Rate:  0.0012351225377253115  Varinance:  0.053735975162197004 \n",
      "\n",
      "Epoch:  4396  Learning Rate:  0.0012338880325430534  Varinance:  0.05366090797455013 \n",
      "\n",
      "Epoch:  4397  Learning Rate:  0.0012326547612489297  Varinance:  0.05358594565301291 \n",
      "\n",
      "Epoch:  4398  Learning Rate:  0.0012314227226096698  Varinance:  0.05351108805109114 \n",
      "\n",
      "Epoch:  4399  Learning Rate:  0.0012301919153932363  Varinance:  0.05343633502249543 \n",
      "\n",
      "Epoch:  4400  Learning Rate:  0.0012289623383688196  Varinance:  0.053361686421140676 \n",
      "\n",
      "Epoch:  4401  Learning Rate:  0.0012277339903068436  Varinance:  0.053287142101145754 \n",
      "\n",
      "Epoch:  4402  Learning Rate:  0.0012265068699789617  Varinance:  0.05321270191683355 \n",
      "\n",
      "Epoch:  4403  Learning Rate:  0.0012252809761580505  Varinance:  0.05313836572273023 \n",
      "\n",
      "Epoch:  4404  Learning Rate:  0.0012240563076182187  Varinance:  0.05306413337356538 \n",
      "\n",
      "Epoch:  4405  Learning Rate:  0.0012228328631347955  Varinance:  0.05299000472427138 \n",
      "\n",
      "Epoch:  4406  Learning Rate:  0.0012216106414843374  Varinance:  0.052915979629983255 \n",
      "\n",
      "Epoch:  4407  Learning Rate:  0.0012203896414446237  Varinance:  0.052842057946038565 \n",
      "\n",
      "Epoch:  4408  Learning Rate:  0.001219169861794652  Varinance:  0.05276823952797675 \n",
      "\n",
      "Epoch:  4409  Learning Rate:  0.0012179513013146435  Varinance:  0.052694524231539246 \n",
      "\n",
      "Epoch:  4410  Learning Rate:  0.001216733958786039  Varinance:  0.05262091191266885 \n",
      "\n",
      "Epoch:  4411  Learning Rate:  0.0012155178329914937  Varinance:  0.05254740242750974 \n",
      "\n",
      "Epoch:  4412  Learning Rate:  0.0012143029227148837  Varinance:  0.05247399563240696 \n",
      "\n",
      "Epoch:  4413  Learning Rate:  0.0012130892267412964  Varinance:  0.05240069138390622 \n",
      "\n",
      "Epoch:  4414  Learning Rate:  0.0012118767438570371  Varinance:  0.05232748953875377 \n",
      "\n",
      "Epoch:  4415  Learning Rate:  0.0012106654728496237  Varinance:  0.05225438995389579 \n",
      "\n",
      "Epoch:  4416  Learning Rate:  0.001209455412507783  Varinance:  0.052181392486478476 \n",
      "\n",
      "Epoch:  4417  Learning Rate:  0.0012082465616214556  Varinance:  0.05210849699384744 \n",
      "\n",
      "Epoch:  4418  Learning Rate:  0.0012070389189817914  Varinance:  0.05203570333354771 \n",
      "\n",
      "Epoch:  4419  Learning Rate:  0.0012058324833811459  Varinance:  0.05196301136332322 \n",
      "\n",
      "Epoch:  4420  Learning Rate:  0.0012046272536130852  Varinance:  0.05189042094111662 \n",
      "\n",
      "Epoch:  4421  Learning Rate:  0.0012034232284723776  Varinance:  0.051817931925069144 \n",
      "\n",
      "Epoch:  4422  Learning Rate:  0.0012022204067549985  Varinance:  0.051745544173519994 \n",
      "\n",
      "Epoch:  4423  Learning Rate:  0.0012010187872581275  Varinance:  0.051673257545006485 \n",
      "\n",
      "Epoch:  4424  Learning Rate:  0.001199818368780143  Varinance:  0.051601071898263344 \n",
      "\n",
      "Epoch:  4425  Learning Rate:  0.001198619150120627  Varinance:  0.05152898709222284 \n",
      "\n",
      "Epoch:  4426  Learning Rate:  0.0011974211300803622  Varinance:  0.05145700298601415 \n",
      "\n",
      "Epoch:  4427  Learning Rate:  0.0011962243074613262  Varinance:  0.05138511943896327 \n",
      "\n",
      "Epoch:  4428  Learning Rate:  0.0011950286810666986  Varinance:  0.051313336310592816 \n",
      "\n",
      "Epoch:  4429  Learning Rate:  0.0011938342497008502  Varinance:  0.05124165346062149 \n",
      "\n",
      "Epoch:  4430  Learning Rate:  0.0011926410121693512  Varinance:  0.05117007074896413 \n",
      "\n",
      "Epoch:  4431  Learning Rate:  0.0011914489672789647  Varinance:  0.051098588035731106 \n",
      "\n",
      "Epoch:  4432  Learning Rate:  0.0011902581138376438  Varinance:  0.05102720518122836 \n",
      "\n",
      "Epoch:  4433  Learning Rate:  0.001189068450654536  Varinance:  0.05095592204595687 \n",
      "\n",
      "Epoch:  4434  Learning Rate:  0.0011878799765399788  Varinance:  0.05088473849061247 \n",
      "\n",
      "Epoch:  4435  Learning Rate:  0.0011866926903054962  Varinance:  0.050813654376085746 \n",
      "\n",
      "Epoch:  4436  Learning Rate:  0.0011855065907638038  Varinance:  0.05074266956346144 \n",
      "\n",
      "Epoch:  4437  Learning Rate:  0.0011843216767288  Varinance:  0.05067178391401851 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4438  Learning Rate:  0.0011831379470155716  Varinance:  0.050600997289229586 \n",
      "\n",
      "Epoch:  4439  Learning Rate:  0.0011819554004403898  Varinance:  0.05053030955076082 \n",
      "\n",
      "Epoch:  4440  Learning Rate:  0.0011807740358207059  Varinance:  0.05045972056047174 \n",
      "\n",
      "Epoch:  4441  Learning Rate:  0.0011795938519751563  Varinance:  0.050389230180414635 \n",
      "\n",
      "Epoch:  4442  Learning Rate:  0.0011784148477235578  Varinance:  0.05031883827283475 \n",
      "\n",
      "Epoch:  4443  Learning Rate:  0.0011772370218869044  Varinance:  0.05024854470016954 \n",
      "\n",
      "Epoch:  4444  Learning Rate:  0.001176060373287372  Varinance:  0.05017834932504883 \n",
      "\n",
      "Epoch:  4445  Learning Rate:  0.0011748849007483097  Varinance:  0.050108252010294195 \n",
      "\n",
      "Epoch:  4446  Learning Rate:  0.001173710603094246  Varinance:  0.050038252618918866 \n",
      "\n",
      "Epoch:  4447  Learning Rate:  0.0011725374791508845  Varinance:  0.04996835101412755 \n",
      "\n",
      "Epoch:  4448  Learning Rate:  0.0011713655277450989  Varinance:  0.049898547059315894 \n",
      "\n",
      "Epoch:  4449  Learning Rate:  0.0011701947477049384  Varinance:  0.04982884061807053 \n",
      "\n",
      "Epoch:  4450  Learning Rate:  0.0011690251378596243  Varinance:  0.049759231554168504 \n",
      "\n",
      "Epoch:  4451  Learning Rate:  0.0011678566970395442  Varinance:  0.04968971973157733 \n",
      "\n",
      "Epoch:  4452  Learning Rate:  0.0011666894240762598  Varinance:  0.0496203050144544 \n",
      "\n",
      "Epoch:  4453  Learning Rate:  0.0011655233178024956  Varinance:  0.04955098726714689 \n",
      "\n",
      "Epoch:  4454  Learning Rate:  0.0011643583770521461  Varinance:  0.04948176635419163 \n",
      "\n",
      "Epoch:  4455  Learning Rate:  0.0011631946006602722  Varinance:  0.04941264214031448 \n",
      "\n",
      "Epoch:  4456  Learning Rate:  0.0011620319874630946  Varinance:  0.049343614490430454 \n",
      "\n",
      "Epoch:  4457  Learning Rate:  0.001160870536298001  Varinance:  0.04927468326964312 \n",
      "\n",
      "Epoch:  4458  Learning Rate:  0.0011597102460035418  Varinance:  0.04920584834324464 \n",
      "\n",
      "Epoch:  4459  Learning Rate:  0.0011585511154194243  Varinance:  0.04913710957671524 \n",
      "\n",
      "Epoch:  4460  Learning Rate:  0.0011573931433865195  Varinance:  0.04906846683572308 \n",
      "\n",
      "Epoch:  4461  Learning Rate:  0.0011562363287468537  Varinance:  0.04899991998612406 \n",
      "\n",
      "Epoch:  4462  Learning Rate:  0.001155080670343613  Varinance:  0.04893146889396136 \n",
      "\n",
      "Epoch:  4463  Learning Rate:  0.0011539261670211398  Varinance:  0.04886311342546542 \n",
      "\n",
      "Epoch:  4464  Learning Rate:  0.0011527728176249287  Varinance:  0.048794853447053414 \n",
      "\n",
      "Epoch:  4465  Learning Rate:  0.0011516206210016315  Varinance:  0.048726688825329253 \n",
      "\n",
      "Epoch:  4466  Learning Rate:  0.0011504695759990524  Varinance:  0.04865861942708313 \n",
      "\n",
      "Epoch:  4467  Learning Rate:  0.001149319681466144  Varinance:  0.04859064511929127 \n",
      "\n",
      "Epoch:  4468  Learning Rate:  0.0011481709362530137  Varinance:  0.04852276576911589 \n",
      "\n",
      "Epoch:  4469  Learning Rate:  0.0011470233392109144  Varinance:  0.0484549812439046 \n",
      "\n",
      "Epoch:  4470  Learning Rate:  0.0011458768891922499  Varinance:  0.04838729141119046 \n",
      "\n",
      "Epoch:  4471  Learning Rate:  0.001144731585050571  Varinance:  0.04831969613869151 \n",
      "\n",
      "Epoch:  4472  Learning Rate:  0.0011435874256405718  Varinance:  0.04825219529431054 \n",
      "\n",
      "Epoch:  4473  Learning Rate:  0.0011424444098180934  Varinance:  0.048184788746135014 \n",
      "\n",
      "Epoch:  4474  Learning Rate:  0.001141302536440121  Varinance:  0.04811747636243651 \n",
      "\n",
      "Epoch:  4475  Learning Rate:  0.0011401618043647792  Varinance:  0.04805025801167078 \n",
      "\n",
      "Epoch:  4476  Learning Rate:  0.0011390222124513377  Varinance:  0.047983133562477194 \n",
      "\n",
      "Epoch:  4477  Learning Rate:  0.0011378837595602028  Varinance:  0.04791610288367877 \n",
      "\n",
      "Epoch:  4478  Learning Rate:  0.0011367464445529221  Varinance:  0.047849165844281664 \n",
      "\n",
      "Epoch:  4479  Learning Rate:  0.0011356102662921817  Varinance:  0.04778232231347502 \n",
      "\n",
      "Epoch:  4480  Learning Rate:  0.0011344752236418013  Varinance:  0.047715572160630854 \n",
      "\n",
      "Epoch:  4481  Learning Rate:  0.0011333413154667388  Varinance:  0.047648915255303494 \n",
      "\n",
      "Epoch:  4482  Learning Rate:  0.0011322085406330876  Varinance:  0.04758235146722965 \n",
      "\n",
      "Epoch:  4483  Learning Rate:  0.00113107689800807  Varinance:  0.04751588066632786 \n",
      "\n",
      "Epoch:  4484  Learning Rate:  0.0011299463864600458  Varinance:  0.047449502722698524 \n",
      "\n",
      "Epoch:  4485  Learning Rate:  0.0011288170048585012  Varinance:  0.04738321750662341 \n",
      "\n",
      "Epoch:  4486  Learning Rate:  0.0011276887520740558  Varinance:  0.04731702488856546 \n",
      "\n",
      "Epoch:  4487  Learning Rate:  0.0011265616269784573  Varinance:  0.04725092473916874 \n",
      "\n",
      "Epoch:  4488  Learning Rate:  0.0011254356284445784  Varinance:  0.04718491692925783 \n",
      "\n",
      "Epoch:  4489  Learning Rate:  0.0011243107553464222  Varinance:  0.04711900132983794 \n",
      "\n",
      "Epoch:  4490  Learning Rate:  0.0011231870065591158  Varinance:  0.0470531778120943 \n",
      "\n",
      "Epoch:  4491  Learning Rate:  0.0011220643809589084  Varinance:  0.04698744624739229 \n",
      "\n",
      "Epoch:  4492  Learning Rate:  0.0011209428774231766  Varinance:  0.046921806507276814 \n",
      "\n",
      "Epoch:  4493  Learning Rate:  0.0011198224948304146  Varinance:  0.04685625846347225 \n",
      "\n",
      "Epoch:  4494  Learning Rate:  0.001118703232060241  Varinance:  0.04679080198788232 \n",
      "\n",
      "Epoch:  4495  Learning Rate:  0.0011175850879933933  Varinance:  0.046725436952589475 \n",
      "\n",
      "Epoch:  4496  Learning Rate:  0.0011164680615117259  Varinance:  0.04666016322985507 \n",
      "\n",
      "Epoch:  4497  Learning Rate:  0.001115352151498213  Varinance:  0.046594980692118725 \n",
      "\n",
      "Epoch:  4498  Learning Rate:  0.0011142373568369458  Varinance:  0.04652988921199841 \n",
      "\n",
      "Epoch:  4499  Learning Rate:  0.001113123676413127  Varinance:  0.04646488866228995 \n",
      "\n",
      "Epoch:  4500  Learning Rate:  0.0011120111091130784  Varinance:  0.04639997891596684 \n",
      "\n",
      "Epoch:  4501  Learning Rate:  0.0011108996538242307  Varinance:  0.046335159846180154 \n",
      "\n",
      "Epoch:  4502  Learning Rate:  0.0011097893094351293  Varinance:  0.04627043132625802 \n",
      "\n",
      "Epoch:  4503  Learning Rate:  0.001108680074835431  Varinance:  0.046205793229705694 \n",
      "\n",
      "Epoch:  4504  Learning Rate:  0.0011075719489158987  Varinance:  0.04614124543020498 \n",
      "\n",
      "Epoch:  4505  Learning Rate:  0.0011064649305684087  Varinance:  0.046076787801614165 \n",
      "\n",
      "Epoch:  4506  Learning Rate:  0.0011053590186859404  Varinance:  0.04601242021796789 \n",
      "\n",
      "Epoch:  4507  Learning Rate:  0.0011042542121625831  Varinance:  0.04594814255347658 \n",
      "\n",
      "Epoch:  4508  Learning Rate:  0.0011031505098935306  Varinance:  0.045883954682526554 \n",
      "\n",
      "Epoch:  4509  Learning Rate:  0.0011020479107750794  Varinance:  0.04581985647967944 \n",
      "\n",
      "Epoch:  4510  Learning Rate:  0.0011009464137046304  Varinance:  0.04575584781967227 \n",
      "\n",
      "Epoch:  4511  Learning Rate:  0.0010998460175806882  Varinance:  0.04569192857741693 \n",
      "\n",
      "Epoch:  4512  Learning Rate:  0.001098746721302854  Varinance:  0.045628098628000074 \n",
      "\n",
      "Epoch:  4513  Learning Rate:  0.0010976485237718337  Varinance:  0.045564357846682946 \n",
      "\n",
      "Epoch:  4514  Learning Rate:  0.0010965514238894278  Varinance:  0.045500706108900904 \n",
      "\n",
      "Epoch:  4515  Learning Rate:  0.001095455420558537  Varinance:  0.04543714329026348 \n",
      "\n",
      "Epoch:  4516  Learning Rate:  0.0010943605126831591  Varinance:  0.0453736692665538 \n",
      "\n",
      "Epoch:  4517  Learning Rate:  0.001093266699168384  Varinance:  0.04531028391372868 \n",
      "\n",
      "Epoch:  4518  Learning Rate:  0.0010921739789203995  Varinance:  0.045246987107918114 \n",
      "\n",
      "Epoch:  4519  Learning Rate:  0.0010910823508464855  Varinance:  0.04518377872542513 \n",
      "\n",
      "Epoch:  4520  Learning Rate:  0.0010899918138550125  Varinance:  0.045120658642725665 \n",
      "\n",
      "Epoch:  4521  Learning Rate:  0.0010889023668554452  Varinance:  0.045057626736468064 \n",
      "\n",
      "Epoch:  4522  Learning Rate:  0.0010878140087583344  Varinance:  0.04499468288347316 \n",
      "\n",
      "Epoch:  4523  Learning Rate:  0.0010867267384753229  Varinance:  0.04493182696073369 \n",
      "\n",
      "Epoch:  4524  Learning Rate:  0.0010856405549191416  Varinance:  0.04486905884541438 \n",
      "\n",
      "Epoch:  4525  Learning Rate:  0.001084555457003605  Varinance:  0.044806378414851446 \n",
      "\n",
      "Epoch:  4526  Learning Rate:  0.0010834714436436152  Varinance:  0.04474378554655247 \n",
      "\n",
      "Epoch:  4527  Learning Rate:  0.0010823885137551607  Varinance:  0.04468128011819625 \n",
      "\n",
      "Epoch:  4528  Learning Rate:  0.0010813066662553091  Varinance:  0.04461886200763231 \n",
      "\n",
      "Epoch:  4529  Learning Rate:  0.0010802259000622146  Varinance:  0.04455653109288096 \n",
      "\n",
      "Epoch:  4530  Learning Rate:  0.0010791462140951095  Varinance:  0.0444942872521328 \n",
      "\n",
      "Epoch:  4531  Learning Rate:  0.0010780676072743084  Varinance:  0.0444321303637487 \n",
      "\n",
      "Epoch:  4532  Learning Rate:  0.0010769900785212053  Varinance:  0.04437006030625935 \n",
      "\n",
      "Epoch:  4533  Learning Rate:  0.0010759136267582696  Varinance:  0.04430807695836513 \n",
      "\n",
      "Epoch:  4534  Learning Rate:  0.00107483825090905  Varinance:  0.04424618019893601 \n",
      "\n",
      "Epoch:  4535  Learning Rate:  0.0010737639498981722  Varinance:  0.04418436990701099 \n",
      "\n",
      "Epoch:  4536  Learning Rate:  0.0010726907226513327  Varinance:  0.04412264596179821 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4537  Learning Rate:  0.0010716185680953061  Varinance:  0.044061008242674456 \n",
      "\n",
      "Epoch:  4538  Learning Rate:  0.001070547485157936  Varinance:  0.04399945662918498 \n",
      "\n",
      "Epoch:  4539  Learning Rate:  0.0010694774727681405  Varinance:  0.04393799100104346 \n",
      "\n",
      "Epoch:  4540  Learning Rate:  0.0010684085298559075  Varinance:  0.043876611238131434 \n",
      "\n",
      "Epoch:  4541  Learning Rate:  0.0010673406553522926  Varinance:  0.04381531722049841 \n",
      "\n",
      "Epoch:  4542  Learning Rate:  0.0010662738481894219  Varinance:  0.04375410882836128 \n",
      "\n",
      "Epoch:  4543  Learning Rate:  0.0010652081073004892  Varinance:  0.04369298594210446 \n",
      "\n",
      "Epoch:  4544  Learning Rate:  0.001064143431619752  Varinance:  0.04363194844227932 \n",
      "\n",
      "Epoch:  4545  Learning Rate:  0.0010630798200825355  Varinance:  0.04357099620960411 \n",
      "\n",
      "Epoch:  4546  Learning Rate:  0.001062017271625227  Varinance:  0.04351012912496383 \n",
      "\n",
      "Epoch:  4547  Learning Rate:  0.0010609557851852787  Varinance:  0.04344934706940971 \n",
      "\n",
      "Epoch:  4548  Learning Rate:  0.0010598953597012047  Varinance:  0.04338864992415931 \n",
      "\n",
      "Epoch:  4549  Learning Rate:  0.001058835994112578  Varinance:  0.04332803757059598 \n",
      "\n",
      "Epoch:  4550  Learning Rate:  0.0010577776873600335  Varinance:  0.04326750989026892 \n",
      "\n",
      "Epoch:  4551  Learning Rate:  0.0010567204383852655  Varinance:  0.04320706676489266 \n",
      "\n",
      "Epoch:  4552  Learning Rate:  0.0010556642461310231  Varinance:  0.043146708076347025 \n",
      "\n",
      "Epoch:  4553  Learning Rate:  0.0010546091095411157  Varinance:  0.04308643370667691 \n",
      "\n",
      "Epoch:  4554  Learning Rate:  0.0010535550275604046  Varinance:  0.043026243538091875 \n",
      "\n",
      "Epoch:  4555  Learning Rate:  0.0010525019991348091  Varinance:  0.04296613745296617 \n",
      "\n",
      "Epoch:  4556  Learning Rate:  0.0010514500232113014  Varinance:  0.042906115333838195 \n",
      "\n",
      "Epoch:  4557  Learning Rate:  0.0010503990987379035  Varinance:  0.0428461770634106 \n",
      "\n",
      "Epoch:  4558  Learning Rate:  0.0010493492246636917  Varinance:  0.042786322524549815 \n",
      "\n",
      "Epoch:  4559  Learning Rate:  0.001048300399938793  Varinance:  0.04272655160028584 \n",
      "\n",
      "Epoch:  4560  Learning Rate:  0.0010472526235143808  Varinance:  0.04266686417381225 \n",
      "\n",
      "Epoch:  4561  Learning Rate:  0.0010462058943426803  Varinance:  0.0426072601284856 \n",
      "\n",
      "Epoch:  4562  Learning Rate:  0.0010451602113769605  Varinance:  0.04254773934782556 \n",
      "\n",
      "Epoch:  4563  Learning Rate:  0.001044115573571539  Varinance:  0.04248830171551438 \n",
      "\n",
      "Epoch:  4564  Learning Rate:  0.001043071979881779  Varinance:  0.04242894711539692 \n",
      "\n",
      "Epoch:  4565  Learning Rate:  0.0010420294292640849  Varinance:  0.042369675431480236 \n",
      "\n",
      "Epoch:  4566  Learning Rate:  0.001040987920675907  Varinance:  0.042310486547933374 \n",
      "\n",
      "Epoch:  4567  Learning Rate:  0.0010399474530757376  Varinance:  0.04225138034908735 \n",
      "\n",
      "Epoch:  4568  Learning Rate:  0.0010389080254231066  Varinance:  0.04219235671943456 \n",
      "\n",
      "Epoch:  4569  Learning Rate:  0.0010378696366785888  Varinance:  0.042133415543628976 \n",
      "\n",
      "Epoch:  4570  Learning Rate:  0.001036832285803793  Varinance:  0.04207455670648554 \n",
      "\n",
      "Epoch:  4571  Learning Rate:  0.0010357959717613697  Varinance:  0.04201578009298013 \n",
      "\n",
      "Epoch:  4572  Learning Rate:  0.0010347606935150052  Varinance:  0.041957085588249406 \n",
      "\n",
      "Epoch:  4573  Learning Rate:  0.0010337264500294195  Varinance:  0.04189847307759034 \n",
      "\n",
      "Epoch:  4574  Learning Rate:  0.0010326932402703703  Varinance:  0.04183994244646031 \n",
      "\n",
      "Epoch:  4575  Learning Rate:  0.001031661063204648  Varinance:  0.04178149358047654 \n",
      "\n",
      "Epoch:  4576  Learning Rate:  0.0010306299178000741  Varinance:  0.04172312636541617 \n",
      "\n",
      "Epoch:  4577  Learning Rate:  0.0010295998030255048  Varinance:  0.041664840687215834 \n",
      "\n",
      "Epoch:  4578  Learning Rate:  0.0010285707178508234  Varinance:  0.04160663643197149 \n",
      "\n",
      "Epoch:  4579  Learning Rate:  0.0010275426612469454  Varinance:  0.04154851348593831 \n",
      "\n",
      "Epoch:  4580  Learning Rate:  0.0010265156321858154  Varinance:  0.04149047173553026 \n",
      "\n",
      "Epoch:  4581  Learning Rate:  0.0010254896296404022  Varinance:  0.04143251106732008 \n",
      "\n",
      "Epoch:  4582  Learning Rate:  0.001024464652584704  Varinance:  0.041374631368038844 \n",
      "\n",
      "Epoch:  4583  Learning Rate:  0.0010234406999937449  Varinance:  0.041316832524576005 \n",
      "\n",
      "Epoch:  4584  Learning Rate:  0.0010224177708435698  Varinance:  0.04125911442397891 \n",
      "\n",
      "Epoch:  4585  Learning Rate:  0.001021395864111252  Varinance:  0.041201476953452686 \n",
      "\n",
      "Epoch:  4586  Learning Rate:  0.0010203749787748822  Varinance:  0.041143920000360164 \n",
      "\n",
      "Epoch:  4587  Learning Rate:  0.0010193551138135766  Varinance:  0.04108644345222138 \n",
      "\n",
      "Epoch:  4588  Learning Rate:  0.0010183362682074704  Varinance:  0.0410290471967136 \n",
      "\n",
      "Epoch:  4589  Learning Rate:  0.0010173184409377164  Varinance:  0.04097173112167092 \n",
      "\n",
      "Epoch:  4590  Learning Rate:  0.0010163016309864882  Varinance:  0.04091449511508423 \n",
      "\n",
      "Epoch:  4591  Learning Rate:  0.0010152858373369763  Varinance:  0.040857339065100785 \n",
      "\n",
      "Epoch:  4592  Learning Rate:  0.0010142710589733856  Varinance:  0.040800262860024125 \n",
      "\n",
      "Epoch:  4593  Learning Rate:  0.0010132572948809396  Varinance:  0.0407432663883139 \n",
      "\n",
      "Epoch:  4594  Learning Rate:  0.0010122445440458715  Varinance:  0.04068634953858546 \n",
      "\n",
      "Epoch:  4595  Learning Rate:  0.001011232805455432  Varinance:  0.04062951219960988 \n",
      "\n",
      "Epoch:  4596  Learning Rate:  0.0010102220780978835  Varinance:  0.04057275426031347 \n",
      "\n",
      "Epoch:  4597  Learning Rate:  0.001009212360962496  Varinance:  0.040516075609777866 \n",
      "\n",
      "Epoch:  4598  Learning Rate:  0.0010082036530395537  Varinance:  0.040459476137239554 \n",
      "\n",
      "Epoch:  4599  Learning Rate:  0.001007195953320349  Varinance:  0.04040295573208973 \n",
      "\n",
      "Epoch:  4600  Learning Rate:  0.0010061892607971812  Varinance:  0.040346514283874216 \n",
      "\n",
      "Epoch:  4601  Learning Rate:  0.0010051835744633586  Varinance:  0.040290151682292996 \n",
      "\n",
      "Epoch:  4602  Learning Rate:  0.0010041788933131935  Varinance:  0.04023386781720029 \n",
      "\n",
      "Epoch:  4603  Learning Rate:  0.001003175216342005  Varinance:  0.040177662578604084 \n",
      "\n",
      "Epoch:  4604  Learning Rate:  0.0010021725425461178  Varinance:  0.04012153585666599 \n",
      "\n",
      "Epoch:  4605  Learning Rate:  0.0010011708709228555  Varinance:  0.04006548754170119 \n",
      "\n",
      "Epoch:  4606  Learning Rate:  0.0010001702004705478  Varinance:  0.04000951752417795 \n",
      "\n",
      "Epoch:  4607  Learning Rate:  0.0009991705301885246  Varinance:  0.03995362569471769 \n",
      "\n",
      "Epoch:  4608  Learning Rate:  0.000998171859077114  Varinance:  0.03989781194409447 \n",
      "\n",
      "Epoch:  4609  Learning Rate:  0.0009971741861376467  Varinance:  0.03984207616323509 \n",
      "\n",
      "Epoch:  4610  Learning Rate:  0.0009961775103724476  Varinance:  0.039786418243218616 \n",
      "\n",
      "Epoch:  4611  Learning Rate:  0.0009951818307848421  Varinance:  0.03973083807527627 \n",
      "\n",
      "Epoch:  4612  Learning Rate:  0.0009941871463791509  Varinance:  0.039675335550791314 \n",
      "\n",
      "Epoch:  4613  Learning Rate:  0.000993193456160688  Varinance:  0.039619910561298634 \n",
      "\n",
      "Epoch:  4614  Learning Rate:  0.0009922007591357644  Varinance:  0.03956456299848474 \n",
      "\n",
      "Epoch:  4615  Learning Rate:  0.0009912090543116832  Varinance:  0.039509292754187325 \n",
      "\n",
      "Epoch:  4616  Learning Rate:  0.0009902183406967381  Varinance:  0.03945409972039535 \n",
      "\n",
      "Epoch:  4617  Learning Rate:  0.0009892286173002172  Varinance:  0.03939898378924852 \n",
      "\n",
      "Epoch:  4618  Learning Rate:  0.000988239883132395  Varinance:  0.03934394485303722 \n",
      "\n",
      "Epoch:  4619  Learning Rate:  0.0009872521372045384  Varinance:  0.03928898280420244 \n",
      "\n",
      "Epoch:  4620  Learning Rate:  0.000986265378528902  Varinance:  0.039234097535335244 \n",
      "\n",
      "Epoch:  4621  Learning Rate:  0.0009852796061187257  Varinance:  0.03917928893917691 \n",
      "\n",
      "Epoch:  4622  Learning Rate:  0.0009842948189882374  Varinance:  0.039124556908618414 \n",
      "\n",
      "Epoch:  4623  Learning Rate:  0.0009833110161526512  Varinance:  0.03906990133670046 \n",
      "\n",
      "Epoch:  4624  Learning Rate:  0.000982328196628162  Varinance:  0.039015322116613134 \n",
      "\n",
      "Epoch:  4625  Learning Rate:  0.0009813463594319522  Varinance:  0.03896081914169566 \n",
      "\n",
      "Epoch:  4626  Learning Rate:  0.0009803655035821829  Varinance:  0.03890639230543641 \n",
      "\n",
      "Epoch:  4627  Learning Rate:  0.000979385628097999  Varinance:  0.03885204150147241 \n",
      "\n",
      "Epoch:  4628  Learning Rate:  0.0009784067319995253  Varinance:  0.038797766623589386 \n",
      "\n",
      "Epoch:  4629  Learning Rate:  0.0009774288143078644  Varinance:  0.038743567565721324 \n",
      "\n",
      "Epoch:  4630  Learning Rate:  0.0009764518740451001  Varinance:  0.03868944422195051 \n",
      "\n",
      "Epoch:  4631  Learning Rate:  0.0009754759102342903  Varinance:  0.03863539648650709 \n",
      "\n",
      "Epoch:  4632  Learning Rate:  0.0009745009218994721  Varinance:  0.03858142425376897 \n",
      "\n",
      "Epoch:  4633  Learning Rate:  0.000973526908065658  Varinance:  0.038527527418261695 \n",
      "\n",
      "Epoch:  4634  Learning Rate:  0.0009725538677588322  Varinance:  0.038473705874658044 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4635  Learning Rate:  0.0009715818000059549  Varinance:  0.03841995951777805 \n",
      "\n",
      "Epoch:  4636  Learning Rate:  0.0009706107038349598  Varinance:  0.038366288242588586 \n",
      "\n",
      "Epoch:  4637  Learning Rate:  0.0009696405782747483  Varinance:  0.03831269194420324 \n",
      "\n",
      "Epoch:  4638  Learning Rate:  0.000968671422355197  Varinance:  0.03825917051788224 \n",
      "\n",
      "Epoch:  4639  Learning Rate:  0.0009677032351071476  Varinance:  0.03820572385903199 \n",
      "\n",
      "Epoch:  4640  Learning Rate:  0.0009667360155624143  Varinance:  0.03815235186320513 \n",
      "\n",
      "Epoch:  4641  Learning Rate:  0.0009657697627537777  Varinance:  0.03809905442610009 \n",
      "\n",
      "Epoch:  4642  Learning Rate:  0.0009648044757149836  Varinance:  0.03804583144356115 \n",
      "\n",
      "Epoch:  4643  Learning Rate:  0.0009638401534807457  Varinance:  0.037992682811577964 \n",
      "\n",
      "Epoch:  4644  Learning Rate:  0.0009628767950867423  Varinance:  0.037939608426285486 \n",
      "\n",
      "Epoch:  4645  Learning Rate:  0.0009619143995696134  Varinance:  0.0378866081839639 \n",
      "\n",
      "Epoch:  4646  Learning Rate:  0.0009609529659669651  Varinance:  0.03783368198103812 \n",
      "\n",
      "Epoch:  4647  Learning Rate:  0.000959992493317362  Varinance:  0.03778082971407788 \n",
      "\n",
      "Epoch:  4648  Learning Rate:  0.0009590329806603323  Varinance:  0.037728051279797295 \n",
      "\n",
      "Epoch:  4649  Learning Rate:  0.0009580744270363639  Varinance:  0.03767534657505487 \n",
      "\n",
      "Epoch:  4650  Learning Rate:  0.0009571168314869016  Varinance:  0.03762271549685312 \n",
      "\n",
      "Epoch:  4651  Learning Rate:  0.0009561601930543505  Varinance:  0.03757015794233843 \n",
      "\n",
      "Epoch:  4652  Learning Rate:  0.0009552045107820731  Varinance:  0.037517673808800965 \n",
      "\n",
      "Epoch:  4653  Learning Rate:  0.000954249783714385  Varinance:  0.03746526299367425 \n",
      "\n",
      "Epoch:  4654  Learning Rate:  0.0009532960108965612  Varinance:  0.037412925394535224 \n",
      "\n",
      "Epoch:  4655  Learning Rate:  0.0009523431913748267  Varinance:  0.03736066090910376 \n",
      "\n",
      "Epoch:  4656  Learning Rate:  0.0009513913241963631  Varinance:  0.037308469435242755 \n",
      "\n",
      "Epoch:  4657  Learning Rate:  0.0009504404084093038  Varinance:  0.037256350870957695 \n",
      "\n",
      "Epoch:  4658  Learning Rate:  0.0009494904430627312  Varinance:  0.03720430511439654 \n",
      "\n",
      "Epoch:  4659  Learning Rate:  0.0009485414272066808  Varinance:  0.03715233206384964 \n",
      "\n",
      "Epoch:  4660  Learning Rate:  0.0009475933598921375  Varinance:  0.03710043161774927 \n",
      "\n",
      "Epoch:  4661  Learning Rate:  0.0009466462401710324  Varinance:  0.037048603674669764 \n",
      "\n",
      "Epoch:  4662  Learning Rate:  0.000945700067096247  Varinance:  0.036996848133327 \n",
      "\n",
      "Epoch:  4663  Learning Rate:  0.0009447548397216066  Varinance:  0.03694516489257845 \n",
      "\n",
      "Epoch:  4664  Learning Rate:  0.0009438105571018848  Varinance:  0.03689355385142282 \n",
      "\n",
      "Epoch:  4665  Learning Rate:  0.0009428672182927995  Varinance:  0.03684201490899988 \n",
      "\n",
      "Epoch:  4666  Learning Rate:  0.0009419248223510103  Varinance:  0.036790547964590414 \n",
      "\n",
      "Epoch:  4667  Learning Rate:  0.000940983368334122  Varinance:  0.03673915291761577 \n",
      "\n",
      "Epoch:  4668  Learning Rate:  0.0009400428553006812  Varinance:  0.036687829667637936 \n",
      "\n",
      "Epoch:  4669  Learning Rate:  0.0009391032823101732  Varinance:  0.0366365781143591 \n",
      "\n",
      "Epoch:  4670  Learning Rate:  0.0009381646484230267  Varinance:  0.03658539815762158 \n",
      "\n",
      "Epoch:  4671  Learning Rate:  0.0009372269527006058  Varinance:  0.03653428969740768 \n",
      "\n",
      "Epoch:  4672  Learning Rate:  0.0009362901942052157  Varinance:  0.03648325263383932 \n",
      "\n",
      "Epoch:  4673  Learning Rate:  0.0009353543720000989  Varinance:  0.036432286867178074 \n",
      "\n",
      "Epoch:  4674  Learning Rate:  0.000934419485149431  Varinance:  0.03638139229782471 \n",
      "\n",
      "Epoch:  4675  Learning Rate:  0.0009334855327183261  Varinance:  0.03633056882631926 \n",
      "\n",
      "Epoch:  4676  Learning Rate:  0.0009325525137728327  Varinance:  0.03627981635334059 \n",
      "\n",
      "Epoch:  4677  Learning Rate:  0.0009316204273799298  Varinance:  0.03622913477970635 \n",
      "\n",
      "Epoch:  4678  Learning Rate:  0.0009306892726075327  Varinance:  0.03617852400637282 \n",
      "\n",
      "Epoch:  4679  Learning Rate:  0.0009297590485244852  Varinance:  0.0361279839344345 \n",
      "\n",
      "Epoch:  4680  Learning Rate:  0.0009288297542005636  Varinance:  0.036077514465124194 \n",
      "\n",
      "Epoch:  4681  Learning Rate:  0.0009279013887064744  Varinance:  0.03602711549981256 \n",
      "\n",
      "Epoch:  4682  Learning Rate:  0.0009269739511138502  Varinance:  0.03597678694000816 \n",
      "\n",
      "Epoch:  4683  Learning Rate:  0.0009260474404952547  Varinance:  0.035926528687357046 \n",
      "\n",
      "Epoch:  4684  Learning Rate:  0.0009251218559241776  Varinance:  0.035876340643642674 \n",
      "\n",
      "Epoch:  4685  Learning Rate:  0.0009241971964750325  Varinance:  0.035826222710785796 \n",
      "\n",
      "Epoch:  4686  Learning Rate:  0.0009232734612231619  Varinance:  0.03577617479084405 \n",
      "\n",
      "Epoch:  4687  Learning Rate:  0.0009223506492448286  Varinance:  0.03572619678601202 \n",
      "\n",
      "Epoch:  4688  Learning Rate:  0.0009214287596172214  Varinance:  0.0356762885986208 \n",
      "\n",
      "Epoch:  4689  Learning Rate:  0.0009205077914184514  Varinance:  0.03562645013113805 \n",
      "\n",
      "Epoch:  4690  Learning Rate:  0.0009195877437275487  Varinance:  0.03557668128616758 \n",
      "\n",
      "Epoch:  4691  Learning Rate:  0.0009186686156244664  Varinance:  0.03552698196644925 \n",
      "\n",
      "Epoch:  4692  Learning Rate:  0.0009177504061900771  Varinance:  0.035477352074858914 \n",
      "\n",
      "Epoch:  4693  Learning Rate:  0.0009168331145061698  Varinance:  0.03542779151440795 \n",
      "\n",
      "Epoch:  4694  Learning Rate:  0.0009159167396554539  Varinance:  0.03537830018824336 \n",
      "\n",
      "Epoch:  4695  Learning Rate:  0.0009150012807215533  Varinance:  0.03532887799964731 \n",
      "\n",
      "Epoch:  4696  Learning Rate:  0.0009140867367890097  Varinance:  0.03527952485203723 \n",
      "\n",
      "Epoch:  4697  Learning Rate:  0.00091317310694328  Varinance:  0.03523024064896536 \n",
      "\n",
      "Epoch:  4698  Learning Rate:  0.0009122603902707324  Varinance:  0.03518102529411867 \n",
      "\n",
      "Epoch:  4699  Learning Rate:  0.0009113485858586511  Varinance:  0.03513187869131878 \n",
      "\n",
      "Epoch:  4700  Learning Rate:  0.0009104376927952324  Varinance:  0.03508280074452153 \n",
      "\n",
      "Epoch:  4701  Learning Rate:  0.0009095277101695816  Varinance:  0.03503379135781707 \n",
      "\n",
      "Epoch:  4702  Learning Rate:  0.0009086186370717176  Varinance:  0.03498485043542943 \n",
      "\n",
      "Epoch:  4703  Learning Rate:  0.0009077104725925654  Varinance:  0.03493597788171644 \n",
      "\n",
      "Epoch:  4704  Learning Rate:  0.0009068032158239616  Varinance:  0.034887173601169635 \n",
      "\n",
      "Epoch:  4705  Learning Rate:  0.0009058968658586501  Varinance:  0.03483843749841383 \n",
      "\n",
      "Epoch:  4706  Learning Rate:  0.0009049914217902789  Varinance:  0.03478976947820722 \n",
      "\n",
      "Epoch:  4707  Learning Rate:  0.000904086882713405  Varinance:  0.034741169445440916 \n",
      "\n",
      "Epoch:  4708  Learning Rate:  0.0009031832477234899  Varinance:  0.03469263730513901 \n",
      "\n",
      "Epoch:  4709  Learning Rate:  0.0009022805159168972  Varinance:  0.034644172962458204 \n",
      "\n",
      "Epoch:  4710  Learning Rate:  0.0009013786863908961  Varinance:  0.034595776322687656 \n",
      "\n",
      "Epoch:  4711  Learning Rate:  0.000900477758243656  Varinance:  0.034547447291248964 \n",
      "\n",
      "Epoch:  4712  Learning Rate:  0.000899577730574249  Varinance:  0.034499185773695686 \n",
      "\n",
      "Epoch:  4713  Learning Rate:  0.0008986786024826484  Varinance:  0.03445099167571347 \n",
      "\n",
      "Epoch:  4714  Learning Rate:  0.0008977803730697244  Varinance:  0.034402864903119584 \n",
      "\n",
      "Epoch:  4715  Learning Rate:  0.0008968830414372482  Varinance:  0.034354805361862986 \n",
      "\n",
      "Epoch:  4716  Learning Rate:  0.000895986606687889  Varinance:  0.034306812958023944 \n",
      "\n",
      "Epoch:  4717  Learning Rate:  0.0008950910679252104  Varinance:  0.03425888759781389 \n",
      "\n",
      "Epoch:  4718  Learning Rate:  0.0008941964242536751  Varinance:  0.034211029187575424 \n",
      "\n",
      "Epoch:  4719  Learning Rate:  0.0008933026747786377  Varinance:  0.03416323763378181 \n",
      "\n",
      "Epoch:  4720  Learning Rate:  0.0008924098186063496  Varinance:  0.03411551284303711 \n",
      "\n",
      "Epoch:  4721  Learning Rate:  0.0008915178548439554  Varinance:  0.03406785472207573 \n",
      "\n",
      "Epoch:  4722  Learning Rate:  0.0008906267825994894  Varinance:  0.03402026317776249 \n",
      "\n",
      "Epoch:  4723  Learning Rate:  0.0008897366009818801  Varinance:  0.03397273811709221 \n",
      "\n",
      "Epoch:  4724  Learning Rate:  0.0008888473091009469  Varinance:  0.03392527944718968 \n",
      "\n",
      "Epoch:  4725  Learning Rate:  0.0008879589060673959  Varinance:  0.03387788707530945 \n",
      "\n",
      "Epoch:  4726  Learning Rate:  0.0008870713909928259  Varinance:  0.03383056090883558 \n",
      "\n",
      "Epoch:  4727  Learning Rate:  0.0008861847629897199  Varinance:  0.0337833008552816 \n",
      "\n",
      "Epoch:  4728  Learning Rate:  0.0008852990211714509  Varinance:  0.033736106822290106 \n",
      "\n",
      "Epoch:  4729  Learning Rate:  0.0008844141646522776  Varinance:  0.03368897871763288 \n",
      "\n",
      "Epoch:  4730  Learning Rate:  0.0008835301925473419  Varinance:  0.03364191644921041 \n",
      "\n",
      "Epoch:  4731  Learning Rate:  0.0008826471039726723  Varinance:  0.033594919925051855 \n",
      "\n",
      "Epoch:  4732  Learning Rate:  0.0008817648980451809  Varinance:  0.033547989053314964 \n",
      "\n",
      "Epoch:  4733  Learning Rate:  0.0008808835738826605  Varinance:  0.033501123742285636 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4734  Learning Rate:  0.000880003130603788  Varinance:  0.03345432390037803 \n",
      "\n",
      "Epoch:  4735  Learning Rate:  0.0008791235673281188  Varinance:  0.03340758943613416 \n",
      "\n",
      "Epoch:  4736  Learning Rate:  0.00087824488317609  Varinance:  0.03336092025822378 \n",
      "\n",
      "Epoch:  4737  Learning Rate:  0.0008773670772690184  Varinance:  0.03331431627544435 \n",
      "\n",
      "Epoch:  4738  Learning Rate:  0.0008764901487290965  Varinance:  0.03326777739672059 \n",
      "\n",
      "Epoch:  4739  Learning Rate:  0.0008756140966793962  Varinance:  0.0332213035311046 \n",
      "\n",
      "Epoch:  4740  Learning Rate:  0.0008747389202438665  Varinance:  0.033174894587775366 \n",
      "\n",
      "Epoch:  4741  Learning Rate:  0.000873864618547329  Varinance:  0.03312855047603891 \n",
      "\n",
      "Epoch:  4742  Learning Rate:  0.0008729911907154838  Varinance:  0.03308227110532784 \n",
      "\n",
      "Epoch:  4743  Learning Rate:  0.0008721186358749012  Varinance:  0.03303605638520128 \n",
      "\n",
      "Epoch:  4744  Learning Rate:  0.0008712469531530272  Varinance:  0.032989906225344805 \n",
      "\n",
      "Epoch:  4745  Learning Rate:  0.0008703761416781796  Varinance:  0.03294382053557002 \n",
      "\n",
      "Epoch:  4746  Learning Rate:  0.0008695062005795456  Varinance:  0.03289779922581463 \n",
      "\n",
      "Epoch:  4747  Learning Rate:  0.0008686371289871846  Varinance:  0.03285184220614207 \n",
      "\n",
      "Epoch:  4748  Learning Rate:  0.0008677689260320258  Varinance:  0.032805949386741494 \n",
      "\n",
      "Epoch:  4749  Learning Rate:  0.0008669015908458644  Varinance:  0.032760120677927454 \n",
      "\n",
      "Epoch:  4750  Learning Rate:  0.0008660351225613669  Varinance:  0.032714355990139776 \n",
      "\n",
      "Epoch:  4751  Learning Rate:  0.0008651695203120635  Varinance:  0.032668655233943494 \n",
      "\n",
      "Epoch:  4752  Learning Rate:  0.0008643047832323524  Varinance:  0.03262301832002846 \n",
      "\n",
      "Epoch:  4753  Learning Rate:  0.0008634409104574973  Varinance:  0.032577445159209406 \n",
      "\n",
      "Epoch:  4754  Learning Rate:  0.000862577901123624  Varinance:  0.03253193566242553 \n",
      "\n",
      "Epoch:  4755  Learning Rate:  0.0008617157543677243  Varinance:  0.03248648974074058 \n",
      "\n",
      "Epoch:  4756  Learning Rate:  0.00086085446932765  Varinance:  0.03244110730534243 \n",
      "\n",
      "Epoch:  4757  Learning Rate:  0.0008599940451421168  Varinance:  0.03239578826754304 \n",
      "\n",
      "Epoch:  4758  Learning Rate:  0.0008591344809507013  Varinance:  0.03235053253877836 \n",
      "\n",
      "Epoch:  4759  Learning Rate:  0.0008582757758938376  Varinance:  0.0323053400306079 \n",
      "\n",
      "Epoch:  4760  Learning Rate:  0.0008574179291128212  Varinance:  0.03226021065471489 \n",
      "\n",
      "Epoch:  4761  Learning Rate:  0.0008565609397498061  Varinance:  0.03221514432290579 \n",
      "\n",
      "Epoch:  4762  Learning Rate:  0.0008557048069478013  Varinance:  0.032170140947110376 \n",
      "\n",
      "Epoch:  4763  Learning Rate:  0.0008548495298506758  Varinance:  0.032125200439381385 \n",
      "\n",
      "Epoch:  4764  Learning Rate:  0.0008539951076031503  Varinance:  0.0320803227118944 \n",
      "\n",
      "Epoch:  4765  Learning Rate:  0.0008531415393508038  Varinance:  0.03203550767694777 \n",
      "\n",
      "Epoch:  4766  Learning Rate:  0.0008522888242400684  Varinance:  0.03199075524696228 \n",
      "\n",
      "Epoch:  4767  Learning Rate:  0.0008514369614182276  Varinance:  0.03194606533448114 \n",
      "\n",
      "Epoch:  4768  Learning Rate:  0.0008505859500334191  Varinance:  0.03190143785216965 \n",
      "\n",
      "Epoch:  4769  Learning Rate:  0.0008497357892346323  Varinance:  0.03185687271281514 \n",
      "\n",
      "Epoch:  4770  Learning Rate:  0.0008488864781717048  Varinance:  0.03181236982932683 \n",
      "\n",
      "Epoch:  4771  Learning Rate:  0.0008480380159953269  Varinance:  0.0317679291147355 \n",
      "\n",
      "Epoch:  4772  Learning Rate:  0.0008471904018570349  Varinance:  0.03172355048219354 \n",
      "\n",
      "Epoch:  4773  Learning Rate:  0.0008463436349092154  Varinance:  0.03167923384497454 \n",
      "\n",
      "Epoch:  4774  Learning Rate:  0.0008454977143051022  Varinance:  0.03163497911647338 \n",
      "\n",
      "Epoch:  4775  Learning Rate:  0.0008446526391987729  Varinance:  0.0315907862102058 \n",
      "\n",
      "Epoch:  4776  Learning Rate:  0.0008438084087451531  Varinance:  0.03154665503980839 \n",
      "\n",
      "Epoch:  4777  Learning Rate:  0.0008429650221000132  Varinance:  0.03150258551903846 \n",
      "\n",
      "Epoch:  4778  Learning Rate:  0.0008421224784199648  Varinance:  0.03145857756177367 \n",
      "\n",
      "Epoch:  4779  Learning Rate:  0.0008412807768624659  Varinance:  0.03141463108201214 \n",
      "\n",
      "Epoch:  4780  Learning Rate:  0.0008404399165858131  Varinance:  0.03137074599387197 \n",
      "\n",
      "Epoch:  4781  Learning Rate:  0.0008395998967491471  Varinance:  0.03132692221159138 \n",
      "\n",
      "Epoch:  4782  Learning Rate:  0.0008387607165124484  Varinance:  0.03128315964952832 \n",
      "\n",
      "Epoch:  4783  Learning Rate:  0.0008379223750365355  Varinance:  0.031239458222160336 \n",
      "\n",
      "Epoch:  4784  Learning Rate:  0.0008370848714830672  Varinance:  0.031195817844084574 \n",
      "\n",
      "Epoch:  4785  Learning Rate:  0.0008362482050145412  Varinance:  0.031152238430017343 \n",
      "\n",
      "Epoch:  4786  Learning Rate:  0.000835412374794289  Varinance:  0.031108719894794225 \n",
      "\n",
      "Epoch:  4787  Learning Rate:  0.0008345773799864819  Varinance:  0.03106526215336964 \n",
      "\n",
      "Epoch:  4788  Learning Rate:  0.0008337432197561237  Varinance:  0.03102186512081694 \n",
      "\n",
      "Epoch:  4789  Learning Rate:  0.0008329098932690548  Varinance:  0.030978528712328025 \n",
      "\n",
      "Epoch:  4790  Learning Rate:  0.0008320773996919492  Varinance:  0.030935252843213284 \n",
      "\n",
      "Epoch:  4791  Learning Rate:  0.000831245738192312  Varinance:  0.030892037428901467 \n",
      "\n",
      "Epoch:  4792  Learning Rate:  0.000830414907938482  Varinance:  0.03084888238493939 \n",
      "\n",
      "Epoch:  4793  Learning Rate:  0.0008295849080996302  Varinance:  0.03080578762699194 \n",
      "\n",
      "Epoch:  4794  Learning Rate:  0.0008287557378457548  Varinance:  0.030762753070841704 \n",
      "\n",
      "Epoch:  4795  Learning Rate:  0.0008279273963476869  Varinance:  0.030719778632389037 \n",
      "\n",
      "Epoch:  4796  Learning Rate:  0.0008270998827770836  Varinance:  0.030676864227651687 \n",
      "\n",
      "Epoch:  4797  Learning Rate:  0.0008262731963064322  Varinance:  0.030634009772764738 \n",
      "\n",
      "Epoch:  4798  Learning Rate:  0.0008254473361090467  Varinance:  0.03059121518398048 \n",
      "\n",
      "Epoch:  4799  Learning Rate:  0.0008246223013590652  Varinance:  0.03054848037766813 \n",
      "\n",
      "Epoch:  4800  Learning Rate:  0.0008237980912314537  Varinance:  0.03050580527031381 \n",
      "\n",
      "Epoch:  4801  Learning Rate:  0.0008229747049020031  Varinance:  0.03046318977852024 \n",
      "\n",
      "Epoch:  4802  Learning Rate:  0.0008221521415473251  Varinance:  0.03042063381900663 \n",
      "\n",
      "Epoch:  4803  Learning Rate:  0.0008213304003448578  Varinance:  0.030378137308608638 \n",
      "\n",
      "Epoch:  4804  Learning Rate:  0.0008205094804728587  Varinance:  0.03033570016427796 \n",
      "\n",
      "Epoch:  4805  Learning Rate:  0.0008196893811104084  Varinance:  0.03029332230308243 \n",
      "\n",
      "Epoch:  4806  Learning Rate:  0.0008188701014374084  Varinance:  0.030251003642205638 \n",
      "\n",
      "Epoch:  4807  Learning Rate:  0.000818051640634577  Varinance:  0.030208744098946954 \n",
      "\n",
      "Epoch:  4808  Learning Rate:  0.0008172339978834548  Varinance:  0.03016654359072121 \n",
      "\n",
      "Epoch:  4809  Learning Rate:  0.000816417172366399  Varinance:  0.0301244020350586 \n",
      "\n",
      "Epoch:  4810  Learning Rate:  0.000815601163266583  Varinance:  0.03008231934960461 \n",
      "\n",
      "Epoch:  4811  Learning Rate:  0.0008147859697679989  Varinance:  0.03004029545211966 \n",
      "\n",
      "Epoch:  4812  Learning Rate:  0.0008139715910554518  Varinance:  0.029998330260479163 \n",
      "\n",
      "Epoch:  4813  Learning Rate:  0.0008131580263145637  Varinance:  0.029956423692673158 \n",
      "\n",
      "Epoch:  4814  Learning Rate:  0.0008123452747317702  Varinance:  0.029914575666806352 \n",
      "\n",
      "Epoch:  4815  Learning Rate:  0.0008115333354943185  Varinance:  0.029872786101097767 \n",
      "\n",
      "Epoch:  4816  Learning Rate:  0.00081072220779027  Varinance:  0.029831054913880682 \n",
      "\n",
      "Epoch:  4817  Learning Rate:  0.0008099118908084975  Varinance:  0.029789382023602547 \n",
      "\n",
      "Epoch:  4818  Learning Rate:  0.0008091023837386825  Varinance:  0.029747767348824606 \n",
      "\n",
      "Epoch:  4819  Learning Rate:  0.0008082936857713194  Varinance:  0.029706210808222 \n",
      "\n",
      "Epoch:  4820  Learning Rate:  0.0008074857960977087  Varinance:  0.029664712320583365 \n",
      "\n",
      "Epoch:  4821  Learning Rate:  0.0008066787139099614  Varinance:  0.029623271804810888 \n",
      "\n",
      "Epoch:  4822  Learning Rate:  0.0008058724384009961  Varinance:  0.029581889179919975 \n",
      "\n",
      "Epoch:  4823  Learning Rate:  0.0008050669687645356  Varinance:  0.02954056436503915 \n",
      "\n",
      "Epoch:  4824  Learning Rate:  0.0008042623041951109  Varinance:  0.029499297279410017 \n",
      "\n",
      "Epoch:  4825  Learning Rate:  0.0008034584438880581  Varinance:  0.029458087842386856 \n",
      "\n",
      "Epoch:  4826  Learning Rate:  0.0008026553870395154  Varinance:  0.029416935973436746 \n",
      "\n",
      "Epoch:  4827  Learning Rate:  0.0008018531328464274  Varinance:  0.029375841592139136 \n",
      "\n",
      "Epoch:  4828  Learning Rate:  0.0008010516805065384  Varinance:  0.029334804618185942 \n",
      "\n",
      "Epoch:  4829  Learning Rate:  0.0008002510292183966  Varinance:  0.029293824971381176 \n",
      "\n",
      "Epoch:  4830  Learning Rate:  0.0007994511781813513  Varinance:  0.02925290257164089 \n",
      "\n",
      "Epoch:  4831  Learning Rate:  0.0007986521265955503  Varinance:  0.02921203733899309 \n",
      "\n",
      "Epoch:  4832  Learning Rate:  0.0007978538736619422  Varinance:  0.02917122919357738 \n",
      "\n",
      "Epoch:  4833  Learning Rate:  0.0007970564185822753  Varinance:  0.029130478055645052 \n",
      "\n",
      "Epoch:  4834  Learning Rate:  0.0007962597605590925  Varinance:  0.02908978384555871 \n",
      "\n",
      "Epoch:  4835  Learning Rate:  0.0007954638987957372  Varinance:  0.029049146483792216 \n",
      "\n",
      "Epoch:  4836  Learning Rate:  0.0007946688324963465  Varinance:  0.029008565890930597 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4837  Learning Rate:  0.0007938745608658545  Varinance:  0.028968041987669727 \n",
      "\n",
      "Epoch:  4838  Learning Rate:  0.0007930810831099901  Varinance:  0.028927574694816365 \n",
      "\n",
      "Epoch:  4839  Learning Rate:  0.0007922883984352743  Varinance:  0.028887163933287795 \n",
      "\n",
      "Epoch:  4840  Learning Rate:  0.0007914965060490229  Varinance:  0.028846809624111874 \n",
      "\n",
      "Epoch:  4841  Learning Rate:  0.0007907054051593442  Varinance:  0.02880651168842672 \n",
      "\n",
      "Epoch:  4842  Learning Rate:  0.0007899150949751358  Varinance:  0.028766270047480592 \n",
      "\n",
      "Epoch:  4843  Learning Rate:  0.000789125574706089  Varinance:  0.02872608462263186 \n",
      "\n",
      "Epoch:  4844  Learning Rate:  0.000788336843562682  Varinance:  0.028685955335348654 \n",
      "\n",
      "Epoch:  4845  Learning Rate:  0.0007875489007561842  Varinance:  0.028645882107208896 \n",
      "\n",
      "Epoch:  4846  Learning Rate:  0.0007867617454986536  Varinance:  0.028605864859899965 \n",
      "\n",
      "Epoch:  4847  Learning Rate:  0.0007859753770029332  Varinance:  0.028565903515218743 \n",
      "\n",
      "Epoch:  4848  Learning Rate:  0.0007851897944826553  Varinance:  0.02852599799507129 \n",
      "\n",
      "Epoch:  4849  Learning Rate:  0.0007844049971522381  Varinance:  0.02848614822147275 \n",
      "\n",
      "Epoch:  4850  Learning Rate:  0.0007836209842268827  Varinance:  0.028446354116547282 \n",
      "\n",
      "Epoch:  4851  Learning Rate:  0.0007828377549225773  Varinance:  0.028406615602527746 \n",
      "\n",
      "Epoch:  4852  Learning Rate:  0.0007820553084560916  Varinance:  0.028366932601755736 \n",
      "\n",
      "Epoch:  4853  Learning Rate:  0.0007812736440449795  Varinance:  0.028327305036681225 \n",
      "\n",
      "Epoch:  4854  Learning Rate:  0.0007804927609075774  Varinance:  0.028287732829862626 \n",
      "\n",
      "Epoch:  4855  Learning Rate:  0.0007797126582630004  Varinance:  0.028248215903966464 \n",
      "\n",
      "Epoch:  4856  Learning Rate:  0.0007789333353311466  Varinance:  0.028208754181767284 \n",
      "\n",
      "Epoch:  4857  Learning Rate:  0.0007781547913326938  Varinance:  0.028169347586147607 \n",
      "\n",
      "Epoch:  4858  Learning Rate:  0.0007773770254890965  Varinance:  0.028129996040097555 \n",
      "\n",
      "Epoch:  4859  Learning Rate:  0.0007766000370225901  Varinance:  0.028090699466714962 \n",
      "\n",
      "Epoch:  4860  Learning Rate:  0.0007758238251561848  Varinance:  0.028051457789204967 \n",
      "\n",
      "Epoch:  4861  Learning Rate:  0.0007750483891136692  Varinance:  0.028012270930880104 \n",
      "\n",
      "Epoch:  4862  Learning Rate:  0.0007742737281196081  Varinance:  0.027973138815159955 \n",
      "\n",
      "Epoch:  4863  Learning Rate:  0.000773499841399339  Varinance:  0.027934061365571083 \n",
      "\n",
      "Epoch:  4864  Learning Rate:  0.0007727267281789756  Varinance:  0.02789503850574695 \n",
      "\n",
      "Epoch:  4865  Learning Rate:  0.0007719543876854055  Varinance:  0.027856070159427614 \n",
      "\n",
      "Epoch:  4866  Learning Rate:  0.0007711828191462869  Varinance:  0.027817156250459756 \n",
      "\n",
      "Epoch:  4867  Learning Rate:  0.0007704120217900522  Varinance:  0.027778296702796357 \n",
      "\n",
      "Epoch:  4868  Learning Rate:  0.0007696419948459029  Varinance:  0.027739491440496655 \n",
      "\n",
      "Epoch:  4869  Learning Rate:  0.0007688727375438126  Varinance:  0.027700740387726026 \n",
      "\n",
      "Epoch:  4870  Learning Rate:  0.0007681042491145245  Varinance:  0.027662043468755705 \n",
      "\n",
      "Epoch:  4871  Learning Rate:  0.000767336528789549  Varinance:  0.0276234006079628 \n",
      "\n",
      "Epoch:  4872  Learning Rate:  0.0007665695758011661  Varinance:  0.027584811729829968 \n",
      "\n",
      "Epoch:  4873  Learning Rate:  0.0007658033893824236  Varinance:  0.02754627675894547 \n",
      "\n",
      "Epoch:  4874  Learning Rate:  0.0007650379687671336  Varinance:  0.02750779562000282 \n",
      "\n",
      "Epoch:  4875  Learning Rate:  0.0007642733131898768  Varinance:  0.027469368237800745 \n",
      "\n",
      "Epoch:  4876  Learning Rate:  0.0007635094218859962  Varinance:  0.027430994537243108 \n",
      "\n",
      "Epoch:  4877  Learning Rate:  0.0007627462940916011  Varinance:  0.027392674443338566 \n",
      "\n",
      "Epoch:  4878  Learning Rate:  0.0007619839290435643  Varinance:  0.02735440788120063 \n",
      "\n",
      "Epoch:  4879  Learning Rate:  0.0007612223259795195  Varinance:  0.027316194776047353 \n",
      "\n",
      "Epoch:  4880  Learning Rate:  0.0007604614841378648  Varinance:  0.027278035053201317 \n",
      "\n",
      "Epoch:  4881  Learning Rate:  0.0007597014027577567  Varinance:  0.027239928638089377 \n",
      "\n",
      "Epoch:  4882  Learning Rate:  0.000758942081079115  Varinance:  0.02720187545624256 \n",
      "\n",
      "Epoch:  4883  Learning Rate:  0.000758183518342618  Varinance:  0.027163875433296 \n",
      "\n",
      "Epoch:  4884  Learning Rate:  0.000757425713789702  Varinance:  0.027125928494988606 \n",
      "\n",
      "Epoch:  4885  Learning Rate:  0.0007566686666625629  Varinance:  0.027088034567163126 \n",
      "\n",
      "Epoch:  4886  Learning Rate:  0.0007559123762041542  Varinance:  0.02705019357576583 \n",
      "\n",
      "Epoch:  4887  Learning Rate:  0.0007551568416581839  Varinance:  0.027012405446846494 \n",
      "\n",
      "Epoch:  4888  Learning Rate:  0.0007544020622691189  Varinance:  0.026974670106558168 \n",
      "\n",
      "Epoch:  4889  Learning Rate:  0.0007536480372821785  Varinance:  0.02693698748115704 \n",
      "\n",
      "Epoch:  4890  Learning Rate:  0.0007528947659433381  Varinance:  0.026899357497002382 \n",
      "\n",
      "Epoch:  4891  Learning Rate:  0.000752142247499327  Varinance:  0.026861780080556272 \n",
      "\n",
      "Epoch:  4892  Learning Rate:  0.0007513904811976255  Varinance:  0.026824255158383582 \n",
      "\n",
      "Epoch:  4893  Learning Rate:  0.0007506394662864678  Varinance:  0.026786782657151693 \n",
      "\n",
      "Epoch:  4894  Learning Rate:  0.0007498892020148395  Varinance:  0.026749362503630525 \n",
      "\n",
      "Epoch:  4895  Learning Rate:  0.0007491396876324752  Varinance:  0.026711994624692214 \n",
      "\n",
      "Epoch:  4896  Learning Rate:  0.0007483909223898615  Varinance:  0.026674678947311063 \n",
      "\n",
      "Epoch:  4897  Learning Rate:  0.0007476429055382319  Varinance:  0.026637415398563458 \n",
      "\n",
      "Epoch:  4898  Learning Rate:  0.0007468956363295702  Varinance:  0.026600203905627555 \n",
      "\n",
      "Epoch:  4899  Learning Rate:  0.0007461491140166078  Varinance:  0.026563044395783348 \n",
      "\n",
      "Epoch:  4900  Learning Rate:  0.0007454033378528208  Varinance:  0.026525936796412342 \n",
      "\n",
      "Epoch:  4901  Learning Rate:  0.0007446583070924339  Varinance:  0.026488881034997476 \n",
      "\n",
      "Epoch:  4902  Learning Rate:  0.0007439140209904167  Varinance:  0.02645187703912308 \n",
      "\n",
      "Epoch:  4903  Learning Rate:  0.0007431704788024818  Varinance:  0.026414924736474537 \n",
      "\n",
      "Epoch:  4904  Learning Rate:  0.0007424276797850884  Varinance:  0.02637802405483836 \n",
      "\n",
      "Epoch:  4905  Learning Rate:  0.000741685623195436  Varinance:  0.026341174922101834 \n",
      "\n",
      "Epoch:  4906  Learning Rate:  0.0007409443082914684  Varinance:  0.026304377266253086 \n",
      "\n",
      "Epoch:  4907  Learning Rate:  0.0007402037343318717  Varinance:  0.02626763101538077 \n",
      "\n",
      "Epoch:  4908  Learning Rate:  0.0007394639005760703  Varinance:  0.02623093609767399 \n",
      "\n",
      "Epoch:  4909  Learning Rate:  0.0007387248062842311  Varinance:  0.02619429244142224 \n",
      "\n",
      "Epoch:  4910  Learning Rate:  0.0007379864507172603  Varinance:  0.0261576999750151 \n",
      "\n",
      "Epoch:  4911  Learning Rate:  0.0007372488331368012  Varinance:  0.026121158626942288 \n",
      "\n",
      "Epoch:  4912  Learning Rate:  0.0007365119528052373  Varinance:  0.02608466832579331 \n",
      "\n",
      "Epoch:  4913  Learning Rate:  0.0007357758089856869  Varinance:  0.026048229000257536 \n",
      "\n",
      "Epoch:  4914  Learning Rate:  0.0007350404009420068  Varinance:  0.026011840579123875 \n",
      "\n",
      "Epoch:  4915  Learning Rate:  0.0007343057279387895  Varinance:  0.02597550299128072 \n",
      "\n",
      "Epoch:  4916  Learning Rate:  0.0007335717892413608  Varinance:  0.025939216165715887 \n",
      "\n",
      "Epoch:  4917  Learning Rate:  0.0007328385841157824  Varinance:  0.025902980031516268 \n",
      "\n",
      "Epoch:  4918  Learning Rate:  0.0007321061118288498  Varinance:  0.025866794517867938 \n",
      "\n",
      "Epoch:  4919  Learning Rate:  0.0007313743716480895  Varinance:  0.025830659554055797 \n",
      "\n",
      "Epoch:  4920  Learning Rate:  0.0007306433628417623  Varinance:  0.025794575069463626 \n",
      "\n",
      "Epoch:  4921  Learning Rate:  0.0007299130846788583  Varinance:  0.025758540993573775 \n",
      "\n",
      "Epoch:  4922  Learning Rate:  0.0007291835364290998  Varinance:  0.02572255725596712 \n",
      "\n",
      "Epoch:  4923  Learning Rate:  0.000728454717362939  Varinance:  0.02568662378632296 \n",
      "\n",
      "Epoch:  4924  Learning Rate:  0.0007277266267515558  Varinance:  0.025650740514418763 \n",
      "\n",
      "Epoch:  4925  Learning Rate:  0.00072699926386686  Varinance:  0.02561490737013015 \n",
      "\n",
      "Epoch:  4926  Learning Rate:  0.0007262726279814892  Varinance:  0.02557912428343066 \n",
      "\n",
      "Epoch:  4927  Learning Rate:  0.0007255467183688063  Varinance:  0.025543391184391702 \n",
      "\n",
      "Epoch:  4928  Learning Rate:  0.0007248215343029028  Varinance:  0.025507708003182323 \n",
      "\n",
      "Epoch:  4929  Learning Rate:  0.0007240970750585934  Varinance:  0.025472074670069136 \n",
      "\n",
      "Epoch:  4930  Learning Rate:  0.0007233733399114195  Varinance:  0.025436491115416213 \n",
      "\n",
      "Epoch:  4931  Learning Rate:  0.0007226503281376463  Varinance:  0.025400957269684825 \n",
      "\n",
      "Epoch:  4932  Learning Rate:  0.0007219280390142608  Varinance:  0.025365473063433484 \n",
      "\n",
      "Epoch:  4933  Learning Rate:  0.0007212064718189746  Varinance:  0.025330038427317625 \n",
      "\n",
      "Epoch:  4934  Learning Rate:  0.0007204856258302209  Varinance:  0.02529465329208956 \n",
      "\n",
      "Epoch:  4935  Learning Rate:  0.0007197655003271524  Varinance:  0.025259317588598423 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4936  Learning Rate:  0.0007190460945896448  Varinance:  0.025224031247789824 \n",
      "\n",
      "Epoch:  4937  Learning Rate:  0.0007183274078982913  Varinance:  0.025188794200705973 \n",
      "\n",
      "Epoch:  4938  Learning Rate:  0.0007176094395344053  Varinance:  0.02515360637848528 \n",
      "\n",
      "Epoch:  4939  Learning Rate:  0.0007168921887800194  Varinance:  0.025118467712362463 \n",
      "\n",
      "Epoch:  4940  Learning Rate:  0.0007161756549178814  Varinance:  0.025083378133668243 \n",
      "\n",
      "Epoch:  4941  Learning Rate:  0.000715459837231458  Varinance:  0.02504833757382925 \n",
      "\n",
      "Epoch:  4942  Learning Rate:  0.000714744735004932  Varinance:  0.02501334596436798 \n",
      "\n",
      "Epoch:  4943  Learning Rate:  0.0007140303475231999  Varinance:  0.02497840323690252 \n",
      "\n",
      "Epoch:  4944  Learning Rate:  0.0007133166740718757  Varinance:  0.02494350932314655 \n",
      "\n",
      "Epoch:  4945  Learning Rate:  0.0007126037139372842  Varinance:  0.024908664154909062 \n",
      "\n",
      "Epoch:  4946  Learning Rate:  0.000711891466406466  Varinance:  0.0248738676640944 \n",
      "\n",
      "Epoch:  4947  Learning Rate:  0.0007111799307671742  Varinance:  0.024839119782701962 \n",
      "\n",
      "Epoch:  4948  Learning Rate:  0.0007104691063078718  Varinance:  0.024804420442826148 \n",
      "\n",
      "Epoch:  4949  Learning Rate:  0.0007097589923177349  Varinance:  0.024769769576656286 \n",
      "\n",
      "Epoch:  4950  Learning Rate:  0.0007090495880866501  Varinance:  0.024735167116476334 \n",
      "\n",
      "Epoch:  4951  Learning Rate:  0.0007083408929052119  Varinance:  0.024700612994664944 \n",
      "\n",
      "Epoch:  4952  Learning Rate:  0.0007076329060647262  Varinance:  0.024666107143695153 \n",
      "\n",
      "Epoch:  4953  Learning Rate:  0.000706925626857205  Varinance:  0.024631649496134396 \n",
      "\n",
      "Epoch:  4954  Learning Rate:  0.0007062190545753695  Varinance:  0.02459723998464426 \n",
      "\n",
      "Epoch:  4955  Learning Rate:  0.000705513188512648  Varinance:  0.024562878541980398 \n",
      "\n",
      "Epoch:  4956  Learning Rate:  0.0007048080279631732  Varinance:  0.024528565100992467 \n",
      "\n",
      "Epoch:  4957  Learning Rate:  0.0007041035722217852  Varinance:  0.024494299594623833 \n",
      "\n",
      "Epoch:  4958  Learning Rate:  0.0007033998205840286  Varinance:  0.024460081955911647 \n",
      "\n",
      "Epoch:  4959  Learning Rate:  0.0007026967723461506  Varinance:  0.02442591211798649 \n",
      "\n",
      "Epoch:  4960  Learning Rate:  0.0007019944268051042  Varinance:  0.024391790014072465 \n",
      "\n",
      "Epoch:  4961  Learning Rate:  0.0007012927832585425  Varinance:  0.02435771557748689 \n",
      "\n",
      "Epoch:  4962  Learning Rate:  0.0007005918410048224  Varinance:  0.024323688741640228 \n",
      "\n",
      "Epoch:  4963  Learning Rate:  0.0006998915993430023  Varinance:  0.02428970944003603 \n",
      "\n",
      "Epoch:  4964  Learning Rate:  0.0006991920575728394  Varinance:  0.024255777606270672 \n",
      "\n",
      "Epoch:  4965  Learning Rate:  0.0006984932149947922  Varinance:  0.02422189317403335 \n",
      "\n",
      "Epoch:  4966  Learning Rate:  0.0006977950709100188  Varinance:  0.02418805607710586 \n",
      "\n",
      "Epoch:  4967  Learning Rate:  0.0006970976246203739  Varinance:  0.024154266249362476 \n",
      "\n",
      "Epoch:  4968  Learning Rate:  0.0006964008754284123  Varinance:  0.02412052362476993 \n",
      "\n",
      "Epoch:  4969  Learning Rate:  0.0006957048226373837  Varinance:  0.024086828137387106 \n",
      "\n",
      "Epoch:  4970  Learning Rate:  0.0006950094655512355  Varinance:  0.024053179721365097 \n",
      "\n",
      "Epoch:  4971  Learning Rate:  0.0006943148034746115  Varinance:  0.024019578310946886 \n",
      "\n",
      "Epoch:  4972  Learning Rate:  0.0006936208357128481  Varinance:  0.023986023840467413 \n",
      "\n",
      "Epoch:  4973  Learning Rate:  0.0006929275615719783  Varinance:  0.023952516244353288 \n",
      "\n",
      "Epoch:  4974  Learning Rate:  0.0006922349803587285  Varinance:  0.023919055457122713 \n",
      "\n",
      "Epoch:  4975  Learning Rate:  0.0006915430913805161  Varinance:  0.023885641413385436 \n",
      "\n",
      "Epoch:  4976  Learning Rate:  0.0006908518939454533  Varinance:  0.023852274047842473 \n",
      "\n",
      "Epoch:  4977  Learning Rate:  0.0006901613873623413  Varinance:  0.023818953295286136 \n",
      "\n",
      "Epoch:  4978  Learning Rate:  0.0006894715709406743  Varinance:  0.023785679090599754 \n",
      "\n",
      "Epoch:  4979  Learning Rate:  0.0006887824439906362  Varinance:  0.023752451368757692 \n",
      "\n",
      "Epoch:  4980  Learning Rate:  0.000688094005823099  Varinance:  0.023719270064825108 \n",
      "\n",
      "Epoch:  4981  Learning Rate:  0.0006874062557496248  Varinance:  0.023686135113957858 \n",
      "\n",
      "Epoch:  4982  Learning Rate:  0.0006867191930824644  Varinance:  0.023653046451402454 \n",
      "\n",
      "Epoch:  4983  Learning Rate:  0.0006860328171345536  Varinance:  0.023620004012495775 \n",
      "\n",
      "Epoch:  4984  Learning Rate:  0.0006853471272195179  Varinance:  0.023587007732665123 \n",
      "\n",
      "Epoch:  4985  Learning Rate:  0.0006846621226516658  Varinance:  0.023554057547427912 \n",
      "\n",
      "Epoch:  4986  Learning Rate:  0.0006839778027459934  Varinance:  0.02352115339239174 \n",
      "\n",
      "Epoch:  4987  Learning Rate:  0.0006832941668181813  Varinance:  0.02348829520325407 \n",
      "\n",
      "Epoch:  4988  Learning Rate:  0.0006826112141845924  Varinance:  0.02345548291580221 \n",
      "\n",
      "Epoch:  4989  Learning Rate:  0.0006819289441622746  Varinance:  0.023422716465913235 \n",
      "\n",
      "Epoch:  4990  Learning Rate:  0.0006812473560689584  Varinance:  0.023389995789553697 \n",
      "\n",
      "Epoch:  4991  Learning Rate:  0.0006805664492230543  Varinance:  0.023357320822779706 \n",
      "\n",
      "Epoch:  4992  Learning Rate:  0.0006798862229436569  Varinance:  0.023324691501736595 \n",
      "\n",
      "Epoch:  4993  Learning Rate:  0.0006792066765505385  Varinance:  0.023292107762658998 \n",
      "\n",
      "Epoch:  4994  Learning Rate:  0.0006785278093641531  Varinance:  0.023259569541870556 \n",
      "\n",
      "Epoch:  4995  Learning Rate:  0.0006778496207056344  Varinance:  0.023227076775783874 \n",
      "\n",
      "Epoch:  4996  Learning Rate:  0.0006771721098967922  Varinance:  0.023194629400900432 \n",
      "\n",
      "Epoch:  4997  Learning Rate:  0.0006764952762601163  Varinance:  0.023162227353810346 \n",
      "\n",
      "Epoch:  4998  Learning Rate:  0.0006758191191187737  Varinance:  0.023129870571192392 \n",
      "\n",
      "Epoch:  4999  Learning Rate:  0.0006751436377966059  Varinance:  0.023097558989813742 \n",
      "\n",
      "Epoch:  5000  Learning Rate:  0.0006744688316181327  Varinance:  0.023065292546529895 \n",
      "\n",
      "Epoch:  5001  Learning Rate:  0.0006737946999085467  Varinance:  0.023033071178284632 \n",
      "\n",
      "Epoch:  5002  Learning Rate:  0.0006731212419937169  Varinance:  0.02300089482210974 \n",
      "\n",
      "Epoch:  5003  Learning Rate:  0.0006724484572001857  Varinance:  0.02296876341512504 \n",
      "\n",
      "Epoch:  5004  Learning Rate:  0.0006717763448551672  Varinance:  0.02293667689453812 \n",
      "\n",
      "Epoch:  5005  Learning Rate:  0.0006711049042865501  Varinance:  0.022904635197644376 \n",
      "\n",
      "Epoch:  5006  Learning Rate:  0.0006704341348228926  Varinance:  0.022872638261826728 \n",
      "\n",
      "Epoch:  5007  Learning Rate:  0.0006697640357934257  Varinance:  0.022840686024555565 \n",
      "\n",
      "Epoch:  5008  Learning Rate:  0.0006690946065280512  Varinance:  0.022808778423388704 \n",
      "\n",
      "Epoch:  5009  Learning Rate:  0.0006684258463573382  Varinance:  0.022776915395971105 \n",
      "\n",
      "Epoch:  5010  Learning Rate:  0.0006677577546125275  Varinance:  0.022745096880034912 \n",
      "\n",
      "Epoch:  5011  Learning Rate:  0.0006670903306255273  Varinance:  0.022713322813399175 \n",
      "\n",
      "Epoch:  5012  Learning Rate:  0.0006664235737289131  Varinance:  0.02268159313396989 \n",
      "\n",
      "Epoch:  5013  Learning Rate:  0.0006657574832559285  Varinance:  0.022649907779739742 \n",
      "\n",
      "Epoch:  5014  Learning Rate:  0.0006650920585404822  Varinance:  0.022618266688788028 \n",
      "\n",
      "Epoch:  5015  Learning Rate:  0.0006644272989171497  Varinance:  0.02258666979928061 \n",
      "\n",
      "Epoch:  5016  Learning Rate:  0.0006637632037211722  Varinance:  0.02255511704946965 \n",
      "\n",
      "Epoch:  5017  Learning Rate:  0.0006630997722884531  Varinance:  0.022523608377693655 \n",
      "\n",
      "Epoch:  5018  Learning Rate:  0.0006624370039555617  Varinance:  0.022492143722377185 \n",
      "\n",
      "Epoch:  5019  Learning Rate:  0.0006617748980597298  Varinance:  0.022460723022030893 \n",
      "\n",
      "Epoch:  5020  Learning Rate:  0.0006611134539388507  Varinance:  0.022429346215251278 \n",
      "\n",
      "Epoch:  5021  Learning Rate:  0.0006604526709314811  Varinance:  0.022398013240720625 \n",
      "\n",
      "Epoch:  5022  Learning Rate:  0.000659792548376837  Varinance:  0.022366724037206923 \n",
      "\n",
      "Epoch:  5023  Learning Rate:  0.0006591330856147962  Varinance:  0.022335478543563624 \n",
      "\n",
      "Epoch:  5024  Learning Rate:  0.0006584742819858965  Varinance:  0.022304276698729683 \n",
      "\n",
      "Epoch:  5025  Learning Rate:  0.000657816136831333  Varinance:  0.02227311844172927 \n",
      "\n",
      "Epoch:  5026  Learning Rate:  0.0006571586494929613  Varinance:  0.022242003711671825 \n",
      "\n",
      "Epoch:  5027  Learning Rate:  0.0006565018193132944  Varinance:  0.02221093244775178 \n",
      "\n",
      "Epoch:  5028  Learning Rate:  0.0006558456456355009  Varinance:  0.02217990458924852 \n",
      "\n",
      "Epoch:  5029  Learning Rate:  0.0006551901278034083  Varinance:  0.0221489200755263 \n",
      "\n",
      "Epoch:  5030  Learning Rate:  0.0006545352651614975  Varinance:  0.022117978846034017 \n",
      "\n",
      "Epoch:  5031  Learning Rate:  0.0006538810570549064  Varinance:  0.022087080840305227 \n",
      "\n",
      "Epoch:  5032  Learning Rate:  0.0006532275028294275  Varinance:  0.02205622599795789 \n",
      "\n",
      "Epoch:  5033  Learning Rate:  0.0006525746018315052  Varinance:  0.02202541425869433 \n",
      "\n",
      "Epoch:  5034  Learning Rate:  0.0006519223534082392  Varinance:  0.02199464556230116 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5035  Learning Rate:  0.0006512707569073814  Varinance:  0.021963919848649028 \n",
      "\n",
      "Epoch:  5036  Learning Rate:  0.0006506198116773342  Varinance:  0.021933237057692657 \n",
      "\n",
      "Epoch:  5037  Learning Rate:  0.0006499695170671537  Varinance:  0.021902597129470576 \n",
      "\n",
      "Epoch:  5038  Learning Rate:  0.0006493198724265435  Varinance:  0.021872000004105156 \n",
      "\n",
      "Epoch:  5039  Learning Rate:  0.0006486708771058601  Varinance:  0.021841445621802357 \n",
      "\n",
      "Epoch:  5040  Learning Rate:  0.0006480225304561085  Varinance:  0.02181093392285166 \n",
      "\n",
      "Epoch:  5041  Learning Rate:  0.0006473748318289405  Varinance:  0.021780464847626038 \n",
      "\n",
      "Epoch:  5042  Learning Rate:  0.0006467277805766585  Varinance:  0.02175003833658166 \n",
      "\n",
      "Epoch:  5043  Learning Rate:  0.0006460813760522115  Varinance:  0.021719654330257968 \n",
      "\n",
      "Epoch:  5044  Learning Rate:  0.0006454356176091937  Varinance:  0.021689312769277387 \n",
      "\n",
      "Epoch:  5045  Learning Rate:  0.000644790504601848  Varinance:  0.02165901359434537 \n",
      "\n",
      "Epoch:  5046  Learning Rate:  0.00064414603638506  Varinance:  0.02162875674625013 \n",
      "\n",
      "Epoch:  5047  Learning Rate:  0.0006435022123143621  Varinance:  0.021598542165862624 \n",
      "\n",
      "Epoch:  5048  Learning Rate:  0.0006428590317459306  Varinance:  0.02156836979413644 \n",
      "\n",
      "Epoch:  5049  Learning Rate:  0.0006422164940365839  Varinance:  0.02153823957210759 \n",
      "\n",
      "Epoch:  5050  Learning Rate:  0.0006415745985437849  Varinance:  0.021508151440894532 \n",
      "\n",
      "Epoch:  5051  Learning Rate:  0.0006409333446256384  Varinance:  0.021478105341697907 \n",
      "\n",
      "Epoch:  5052  Learning Rate:  0.0006402927316408892  Varinance:  0.02144810121580055 \n",
      "\n",
      "Epoch:  5053  Learning Rate:  0.0006396527589489258  Varinance:  0.021418139004567294 \n",
      "\n",
      "Epoch:  5054  Learning Rate:  0.0006390134259097739  Varinance:  0.021388218649444873 \n",
      "\n",
      "Epoch:  5055  Learning Rate:  0.0006383747318841013  Varinance:  0.02135834009196186 \n",
      "\n",
      "Epoch:  5056  Learning Rate:  0.0006377366762332142  Varinance:  0.021328503273728462 \n",
      "\n",
      "Epoch:  5057  Learning Rate:  0.0006370992583190562  Varinance:  0.0212987081364365 \n",
      "\n",
      "Epoch:  5058  Learning Rate:  0.0006364624775042093  Varinance:  0.0212689546218592 \n",
      "\n",
      "Epoch:  5059  Learning Rate:  0.0006358263331518937  Varinance:  0.021239242671851188 \n",
      "\n",
      "Epoch:  5060  Learning Rate:  0.0006351908246259636  Varinance:  0.021209572228348267 \n",
      "\n",
      "Epoch:  5061  Learning Rate:  0.0006345559512909117  Varinance:  0.02117994323336735 \n",
      "\n",
      "Epoch:  5062  Learning Rate:  0.0006339217125118633  Varinance:  0.02115035562900641 \n",
      "\n",
      "Epoch:  5063  Learning Rate:  0.0006332881076545802  Varinance:  0.021120809357444228 \n",
      "\n",
      "Epoch:  5064  Learning Rate:  0.0006326551360854582  Varinance:  0.021091304360940442 \n",
      "\n",
      "Epoch:  5065  Learning Rate:  0.0006320227971715244  Varinance:  0.021061840581835287 \n",
      "\n",
      "Epoch:  5066  Learning Rate:  0.0006313910902804405  Varinance:  0.021032417962549548 \n",
      "\n",
      "Epoch:  5067  Learning Rate:  0.0006307600147804999  Varinance:  0.021003036445584506 \n",
      "\n",
      "Epoch:  5068  Learning Rate:  0.0006301295700406262  Varinance:  0.020973695973521684 \n",
      "\n",
      "Epoch:  5069  Learning Rate:  0.0006294997554303756  Varinance:  0.0209443964890229 \n",
      "\n",
      "Epoch:  5070  Learning Rate:  0.0006288705703199324  Varinance:  0.020915137934829988 \n",
      "\n",
      "Epoch:  5071  Learning Rate:  0.0006282420140801119  Varinance:  0.02088592025376485 \n",
      "\n",
      "Epoch:  5072  Learning Rate:  0.0006276140860823582  Varinance:  0.02085674338872921 \n",
      "\n",
      "Epoch:  5073  Learning Rate:  0.0006269867856987426  Varinance:  0.020827607282704545 \n",
      "\n",
      "Epoch:  5074  Learning Rate:  0.0006263601123019648  Varinance:  0.02079851187875205 \n",
      "\n",
      "Epoch:  5075  Learning Rate:  0.000625734065265352  Varinance:  0.02076945712001239 \n",
      "\n",
      "Epoch:  5076  Learning Rate:  0.0006251086439628562  Varinance:  0.02074044294970572 \n",
      "\n",
      "Epoch:  5077  Learning Rate:  0.0006244838477690569  Varinance:  0.020711469311131464 \n",
      "\n",
      "Epoch:  5078  Learning Rate:  0.0006238596760591568  Varinance:  0.020682536147668303 \n",
      "\n",
      "Epoch:  5079  Learning Rate:  0.0006232361282089849  Varinance:  0.020653643402773973 \n",
      "\n",
      "Epoch:  5080  Learning Rate:  0.0006226132035949937  Varinance:  0.0206247910199852 \n",
      "\n",
      "Epoch:  5081  Learning Rate:  0.0006219909015942574  Varinance:  0.020595978942917637 \n",
      "\n",
      "Epoch:  5082  Learning Rate:  0.0006213692215844744  Varinance:  0.020567207115265634 \n",
      "\n",
      "Epoch:  5083  Learning Rate:  0.0006207481629439655  Varinance:  0.020538475480802267 \n",
      "\n",
      "Epoch:  5084  Learning Rate:  0.0006201277250516706  Varinance:  0.020509783983379087 \n",
      "\n",
      "Epoch:  5085  Learning Rate:  0.000619507907287153  Varinance:  0.02048113256692616 \n",
      "\n",
      "Epoch:  5086  Learning Rate:  0.0006188887090305937  Varinance:  0.02045252117545182 \n",
      "\n",
      "Epoch:  5087  Learning Rate:  0.0006182701296627951  Varinance:  0.02042394975304262 \n",
      "\n",
      "Epoch:  5088  Learning Rate:  0.0006176521685651782  Varinance:  0.020395418243863282 \n",
      "\n",
      "Epoch:  5089  Learning Rate:  0.0006170348251197809  Varinance:  0.020366926592156442 \n",
      "\n",
      "Epoch:  5090  Learning Rate:  0.00061641809870926  Varinance:  0.020338474742242717 \n",
      "\n",
      "Epoch:  5091  Learning Rate:  0.0006158019887168897  Varinance:  0.020310062638520417 \n",
      "\n",
      "Epoch:  5092  Learning Rate:  0.000615186494526559  Varinance:  0.02028169022546561 \n",
      "\n",
      "Epoch:  5093  Learning Rate:  0.0006145716155227746  Varinance:  0.020253357447631872 \n",
      "\n",
      "Epoch:  5094  Learning Rate:  0.0006139573510906564  Varinance:  0.02022506424965023 \n",
      "\n",
      "Epoch:  5095  Learning Rate:  0.0006133437006159404  Varinance:  0.02019681057622912 \n",
      "\n",
      "Epoch:  5096  Learning Rate:  0.0006127306634849767  Varinance:  0.02016859637215414 \n",
      "\n",
      "Epoch:  5097  Learning Rate:  0.000612118239084727  Varinance:  0.0201404215822881 \n",
      "\n",
      "Epoch:  5098  Learning Rate:  0.0006115064268027674  Varinance:  0.020112286151570777 \n",
      "\n",
      "Epoch:  5099  Learning Rate:  0.000610895226027286  Varinance:  0.02008419002501886 \n",
      "\n",
      "Epoch:  5100  Learning Rate:  0.0006102846361470812  Varinance:  0.02005613314772591 \n",
      "\n",
      "Epoch:  5101  Learning Rate:  0.0006096746565515638  Varinance:  0.02002811546486212 \n",
      "\n",
      "Epoch:  5102  Learning Rate:  0.0006090652866307533  Varinance:  0.020000136921674336 \n",
      "\n",
      "Epoch:  5103  Learning Rate:  0.0006084565257752801  Varinance:  0.019972197463485834 \n",
      "\n",
      "Epoch:  5104  Learning Rate:  0.0006078483733763841  Varinance:  0.01994429703569634 \n",
      "\n",
      "Epoch:  5105  Learning Rate:  0.0006072408288259114  Varinance:  0.019916435583781793 \n",
      "\n",
      "Epoch:  5106  Learning Rate:  0.0006066338915163182  Varinance:  0.019888613053294316 \n",
      "\n",
      "Epoch:  5107  Learning Rate:  0.0006060275608406676  Varinance:  0.01986082938986213 \n",
      "\n",
      "Epoch:  5108  Learning Rate:  0.0006054218361926278  Varinance:  0.01983308453918935 \n",
      "\n",
      "Epoch:  5109  Learning Rate:  0.0006048167169664751  Varinance:  0.01980537844705601 \n",
      "\n",
      "Epoch:  5110  Learning Rate:  0.0006042122025570894  Varinance:  0.01977771105931781 \n",
      "\n",
      "Epoch:  5111  Learning Rate:  0.0006036082923599565  Varinance:  0.019750082321906174 \n",
      "\n",
      "Epoch:  5112  Learning Rate:  0.0006030049857711668  Varinance:  0.01972249218082799 \n",
      "\n",
      "Epoch:  5113  Learning Rate:  0.0006024022821874127  Varinance:  0.019694940582165577 \n",
      "\n",
      "Epoch:  5114  Learning Rate:  0.0006018001810059909  Varinance:  0.019667427472076626 \n",
      "\n",
      "Epoch:  5115  Learning Rate:  0.0006011986816248007  Varinance:  0.01963995279679398 \n",
      "\n",
      "Epoch:  5116  Learning Rate:  0.0006005977834423418  Varinance:  0.01961251650262566 \n",
      "\n",
      "Epoch:  5117  Learning Rate:  0.0005999974858577169  Varinance:  0.01958511853595461 \n",
      "\n",
      "Epoch:  5118  Learning Rate:  0.0005993977882706274  Varinance:  0.01955775884323877 \n",
      "\n",
      "Epoch:  5119  Learning Rate:  0.0005987986900813759  Varinance:  0.0195304373710108 \n",
      "\n",
      "Epoch:  5120  Learning Rate:  0.0005982001906908651  Varinance:  0.019503154065878065 \n",
      "\n",
      "Epoch:  5121  Learning Rate:  0.0005976022895005943  Varinance:  0.01947590887452257 \n",
      "\n",
      "Epoch:  5122  Learning Rate:  0.0005970049859126628  Varinance:  0.019448701743700725 \n",
      "\n",
      "Epoch:  5123  Learning Rate:  0.0005964082793297674  Varinance:  0.019421532620243397 \n",
      "\n",
      "Epoch:  5124  Learning Rate:  0.0005958121691552006  Varinance:  0.019394401451055657 \n",
      "\n",
      "Epoch:  5125  Learning Rate:  0.0005952166547928532  Varinance:  0.019367308183116813 \n",
      "\n",
      "Epoch:  5126  Learning Rate:  0.0005946217356472094  Varinance:  0.019340252763480198 \n",
      "\n",
      "Epoch:  5127  Learning Rate:  0.0005940274111233511  Varinance:  0.0193132351392731 \n",
      "\n",
      "Epoch:  5128  Learning Rate:  0.0005934336806269538  Varinance:  0.019286255257696724 \n",
      "\n",
      "Epoch:  5129  Learning Rate:  0.000592840543564286  Varinance:  0.019259313066025966 \n",
      "\n",
      "Epoch:  5130  Learning Rate:  0.0005922479993422118  Varinance:  0.019232408511609442 \n",
      "\n",
      "Epoch:  5131  Learning Rate:  0.0005916560473681857  Varinance:  0.01920554154186925 \n",
      "\n",
      "Epoch:  5132  Learning Rate:  0.0005910646870502563  Varinance:  0.019178712104301016 \n",
      "\n",
      "Epoch:  5133  Learning Rate:  0.0005904739177970638  Varinance:  0.019151920146473645 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5134  Learning Rate:  0.0005898837390178376  Varinance:  0.019125165616029307 \n",
      "\n",
      "Epoch:  5135  Learning Rate:  0.0005892941501223998  Varinance:  0.019098448460683346 \n",
      "\n",
      "Epoch:  5136  Learning Rate:  0.0005887051505211616  Varinance:  0.019071768628224087 \n",
      "\n",
      "Epoch:  5137  Learning Rate:  0.0005881167396251225  Varinance:  0.019045126066512857 \n",
      "\n",
      "Epoch:  5138  Learning Rate:  0.0005875289168458726  Varinance:  0.019018520723483777 \n",
      "\n",
      "Epoch:  5139  Learning Rate:  0.0005869416815955879  Varinance:  0.01899195254714369 \n",
      "\n",
      "Epoch:  5140  Learning Rate:  0.0005863550332870338  Varinance:  0.01896542148557213 \n",
      "\n",
      "Epoch:  5141  Learning Rate:  0.0005857689713335623  Varinance:  0.01893892748692109 \n",
      "\n",
      "Epoch:  5142  Learning Rate:  0.0005851834951491104  Varinance:  0.018912470499415062 \n",
      "\n",
      "Epoch:  5143  Learning Rate:  0.0005845986041482025  Varinance:  0.018886050471350804 \n",
      "\n",
      "Epoch:  5144  Learning Rate:  0.0005840142977459479  Varinance:  0.018859667351097356 \n",
      "\n",
      "Epoch:  5145  Learning Rate:  0.0005834305753580393  Varinance:  0.01883332108709585 \n",
      "\n",
      "Epoch:  5146  Learning Rate:  0.0005828474364007551  Varinance:  0.018807011627859437 \n",
      "\n",
      "Epoch:  5147  Learning Rate:  0.0005822648802909553  Varinance:  0.018780738921973236 \n",
      "\n",
      "Epoch:  5148  Learning Rate:  0.0005816829064460846  Varinance:  0.018754502918094135 \n",
      "\n",
      "Epoch:  5149  Learning Rate:  0.0005811015142841691  Varinance:  0.018728303564950803 \n",
      "\n",
      "Epoch:  5150  Learning Rate:  0.0005805207032238158  Varinance:  0.01870214081134348 \n",
      "\n",
      "Epoch:  5151  Learning Rate:  0.0005799404726842142  Varinance:  0.018676014606143987 \n",
      "\n",
      "Epoch:  5152  Learning Rate:  0.0005793608220851339  Varinance:  0.018649924898295516 \n",
      "\n",
      "Epoch:  5153  Learning Rate:  0.0005787817508469237  Varinance:  0.018623871636812592 \n",
      "\n",
      "Epoch:  5154  Learning Rate:  0.0005782032583905131  Varinance:  0.018597854770781016 \n",
      "\n",
      "Epoch:  5155  Learning Rate:  0.0005776253441374084  Varinance:  0.018571874249357637 \n",
      "\n",
      "Epoch:  5156  Learning Rate:  0.000577048007509696  Varinance:  0.018545930021770407 \n",
      "\n",
      "Epoch:  5157  Learning Rate:  0.0005764712479300398  Varinance:  0.018520022037318132 \n",
      "\n",
      "Epoch:  5158  Learning Rate:  0.000575895064821679  Varinance:  0.018494150245370513 \n",
      "\n",
      "Epoch:  5159  Learning Rate:  0.000575319457608431  Varinance:  0.01846831459536793 \n",
      "\n",
      "Epoch:  5160  Learning Rate:  0.0005747444257146891  Varinance:  0.0184425150368214 \n",
      "\n",
      "Epoch:  5161  Learning Rate:  0.0005741699685654203  Varinance:  0.01841675151931251 \n",
      "\n",
      "Epoch:  5162  Learning Rate:  0.0005735960855861684  Varinance:  0.018391023992493222 \n",
      "\n",
      "Epoch:  5163  Learning Rate:  0.0005730227762030494  Varinance:  0.01836533240608589 \n",
      "\n",
      "Epoch:  5164  Learning Rate:  0.0005724500398427543  Varinance:  0.018339676709883044 \n",
      "\n",
      "Epoch:  5165  Learning Rate:  0.0005718778759325472  Varinance:  0.018314056853747422 \n",
      "\n",
      "Epoch:  5166  Learning Rate:  0.0005713062839002634  Varinance:  0.01828847278761174 \n",
      "\n",
      "Epoch:  5167  Learning Rate:  0.000570735263174311  Varinance:  0.018262924461478668 \n",
      "\n",
      "Epoch:  5168  Learning Rate:  0.0005701648131836698  Varinance:  0.018237411825420765 \n",
      "\n",
      "Epoch:  5169  Learning Rate:  0.0005695949333578888  Varinance:  0.018211934829580268 \n",
      "\n",
      "Epoch:  5170  Learning Rate:  0.0005690256231270891  Varinance:  0.01818649342416914 \n",
      "\n",
      "Epoch:  5171  Learning Rate:  0.0005684568819219596  Varinance:  0.01816108755946883 \n",
      "\n",
      "Epoch:  5172  Learning Rate:  0.0005678887091737592  Varinance:  0.018135717185830257 \n",
      "\n",
      "Epoch:  5173  Learning Rate:  0.0005673211043143158  Varinance:  0.01811038225367374 \n",
      "\n",
      "Epoch:  5174  Learning Rate:  0.0005667540667760237  Varinance:  0.018085082713488795 \n",
      "\n",
      "Epoch:  5175  Learning Rate:  0.0005661875959918454  Varinance:  0.01805981851583417 \n",
      "\n",
      "Epoch:  5176  Learning Rate:  0.0005656216913953108  Varinance:  0.018034589611337606 \n",
      "\n",
      "Epoch:  5177  Learning Rate:  0.0005650563524205143  Varinance:  0.018009395950695894 \n",
      "\n",
      "Epoch:  5178  Learning Rate:  0.0005644915785021178  Varinance:  0.017984237484674644 \n",
      "\n",
      "Epoch:  5179  Learning Rate:  0.0005639273690753463  Varinance:  0.017959114164108243 \n",
      "\n",
      "Epoch:  5180  Learning Rate:  0.000563363723575991  Varinance:  0.01793402593989981 \n",
      "\n",
      "Epoch:  5181  Learning Rate:  0.0005628006414404066  Varinance:  0.017908972763020992 \n",
      "\n",
      "Epoch:  5182  Learning Rate:  0.0005622381221055101  Varinance:  0.017883954584511978 \n",
      "\n",
      "Epoch:  5183  Learning Rate:  0.0005616761650087825  Varinance:  0.017858971355481305 \n",
      "\n",
      "Epoch:  5184  Learning Rate:  0.0005611147695882673  Varinance:  0.01783402302710586 \n",
      "\n",
      "Epoch:  5185  Learning Rate:  0.0005605539352825678  Varinance:  0.017809109550630694 \n",
      "\n",
      "Epoch:  5186  Learning Rate:  0.0005599936615308509  Varinance:  0.017784230877368957 \n",
      "\n",
      "Epoch:  5187  Learning Rate:  0.0005594339477728417  Varinance:  0.017759386958701873 \n",
      "\n",
      "Epoch:  5188  Learning Rate:  0.0005588747934488268  Varinance:  0.01773457774607852 \n",
      "\n",
      "Epoch:  5189  Learning Rate:  0.0005583161979996525  Varinance:  0.01770980319101585 \n",
      "\n",
      "Epoch:  5190  Learning Rate:  0.0005577581608667223  Varinance:  0.01768506324509849 \n",
      "\n",
      "Epoch:  5191  Learning Rate:  0.0005572006814919993  Varinance:  0.01766035785997877 \n",
      "\n",
      "Epoch:  5192  Learning Rate:  0.0005566437593180049  Varinance:  0.0176356869873765 \n",
      "\n",
      "Epoch:  5193  Learning Rate:  0.0005560873937878154  Varinance:  0.01761105057907895 \n",
      "\n",
      "Epoch:  5194  Learning Rate:  0.0005555315843450668  Varinance:  0.017586448586940775 \n",
      "\n",
      "Epoch:  5195  Learning Rate:  0.0005549763304339483  Varinance:  0.017561880962883834 \n",
      "\n",
      "Epoch:  5196  Learning Rate:  0.0005544216314992064  Varinance:  0.017537347658897206 \n",
      "\n",
      "Epoch:  5197  Learning Rate:  0.0005538674869861427  Varinance:  0.01751284862703698 \n",
      "\n",
      "Epoch:  5198  Learning Rate:  0.0005533138963406117  Varinance:  0.017488383819426283 \n",
      "\n",
      "Epoch:  5199  Learning Rate:  0.0005527608590090232  Varinance:  0.01746395318825508 \n",
      "\n",
      "Epoch:  5200  Learning Rate:  0.0005522083744383402  Varinance:  0.017439556685780115 \n",
      "\n",
      "Epoch:  5201  Learning Rate:  0.0005516564420760772  Varinance:  0.017415194264324894 \n",
      "\n",
      "Epoch:  5202  Learning Rate:  0.0005511050613703027  Varinance:  0.017390865876279436 \n",
      "\n",
      "Epoch:  5203  Learning Rate:  0.000550554231769635  Varinance:  0.017366571474100357 \n",
      "\n",
      "Epoch:  5204  Learning Rate:  0.000550003952723245  Varinance:  0.017342311010310626 \n",
      "\n",
      "Epoch:  5205  Learning Rate:  0.000549454223680854  Varinance:  0.01731808443749954 \n",
      "\n",
      "Epoch:  5206  Learning Rate:  0.000548905044092732  Varinance:  0.017293891708322684 \n",
      "\n",
      "Epoch:  5207  Learning Rate:  0.0005483564134096998  Varinance:  0.017269732775501704 \n",
      "\n",
      "Epoch:  5208  Learning Rate:  0.0005478083310831272  Varinance:  0.01724560759182436 \n",
      "\n",
      "Epoch:  5209  Learning Rate:  0.0005472607965649308  Varinance:  0.017221516110144306 \n",
      "\n",
      "Epoch:  5210  Learning Rate:  0.0005467138093075772  Varinance:  0.01719745828338112 \n",
      "\n",
      "Epoch:  5211  Learning Rate:  0.0005461673687640779  Varinance:  0.0171734340645201 \n",
      "\n",
      "Epoch:  5212  Learning Rate:  0.0005456214743879928  Varinance:  0.017149443406612216 \n",
      "\n",
      "Epoch:  5213  Learning Rate:  0.0005450761256334282  Varinance:  0.01712548626277409 \n",
      "\n",
      "Epoch:  5214  Learning Rate:  0.0005445313219550341  Varinance:  0.017101562586187755 \n",
      "\n",
      "Epoch:  5215  Learning Rate:  0.0005439870628080073  Varinance:  0.017077672330100723 \n",
      "\n",
      "Epoch:  5216  Learning Rate:  0.0005434433476480891  Varinance:  0.01705381544782575 \n",
      "\n",
      "Epoch:  5217  Learning Rate:  0.0005429001759315634  Varinance:  0.01702999189274087 \n",
      "\n",
      "Epoch:  5218  Learning Rate:  0.0005423575471152594  Varinance:  0.017006201618289203 \n",
      "\n",
      "Epoch:  5219  Learning Rate:  0.0005418154606565473  Varinance:  0.01698244457797892 \n",
      "\n",
      "Epoch:  5220  Learning Rate:  0.0005412739160133407  Varinance:  0.016958720725383167 \n",
      "\n",
      "Epoch:  5221  Learning Rate:  0.000540732912644096  Varinance:  0.016935030014139894 \n",
      "\n",
      "Epoch:  5222  Learning Rate:  0.0005401924500078084  Varinance:  0.016911372397951877 \n",
      "\n",
      "Epoch:  5223  Learning Rate:  0.0005396525275640159  Varinance:  0.016887747830586504 \n",
      "\n",
      "Epoch:  5224  Learning Rate:  0.0005391131447727963  Varinance:  0.016864156265875816 \n",
      "\n",
      "Epoch:  5225  Learning Rate:  0.000538574301094766  Varinance:  0.016840597657716293 \n",
      "\n",
      "Epoch:  5226  Learning Rate:  0.0005380359959910821  Varinance:  0.016817071960068836 \n",
      "\n",
      "Epoch:  5227  Learning Rate:  0.0005374982289234387  Varinance:  0.016793579126958683 \n",
      "\n",
      "Epoch:  5228  Learning Rate:  0.0005369609993540687  Varinance:  0.01677011911247526 \n",
      "\n",
      "Epoch:  5229  Learning Rate:  0.0005364243067457436  Varinance:  0.016746691870772174 \n",
      "\n",
      "Epoch:  5230  Learning Rate:  0.0005358881505617693  Varinance:  0.016723297356067018 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5231  Learning Rate:  0.0005353525302659903  Varinance:  0.016699935522641415 \n",
      "\n",
      "Epoch:  5232  Learning Rate:  0.0005348174453227866  Varinance:  0.016676606324840797 \n",
      "\n",
      "Epoch:  5233  Learning Rate:  0.0005342828951970723  Varinance:  0.016653309717074374 \n",
      "\n",
      "Epoch:  5234  Learning Rate:  0.0005337488793542983  Varinance:  0.016630045653815095 \n",
      "\n",
      "Epoch:  5235  Learning Rate:  0.0005332153972604476  Varinance:  0.016606814089599447 \n",
      "\n",
      "Epoch:  5236  Learning Rate:  0.0005326824483820385  Varinance:  0.016583614979027488 \n",
      "\n",
      "Epoch:  5237  Learning Rate:  0.0005321500321861228  Varinance:  0.016560448276762646 \n",
      "\n",
      "Epoch:  5238  Learning Rate:  0.0005316181481402831  Varinance:  0.016537313937531696 \n",
      "\n",
      "Epoch:  5239  Learning Rate:  0.0005310867957126358  Varinance:  0.016514211916124698 \n",
      "\n",
      "Epoch:  5240  Learning Rate:  0.000530555974371829  Varinance:  0.016491142167394796 \n",
      "\n",
      "Epoch:  5241  Learning Rate:  0.0005300256835870402  Varinance:  0.016468104646258284 \n",
      "\n",
      "Epoch:  5242  Learning Rate:  0.0005294959228279798  Varinance:  0.016445099307694354 \n",
      "\n",
      "Epoch:  5243  Learning Rate:  0.0005289666915648857  Varinance:  0.016422126106745174 \n",
      "\n",
      "Epoch:  5244  Learning Rate:  0.0005284379892685273  Varinance:  0.01639918499851565 \n",
      "\n",
      "Epoch:  5245  Learning Rate:  0.0005279098154102028  Varinance:  0.01637627593817342 \n",
      "\n",
      "Epoch:  5246  Learning Rate:  0.0005273821694617371  Varinance:  0.016353398880948784 \n",
      "\n",
      "Epoch:  5247  Learning Rate:  0.0005268550508954849  Varinance:  0.016330553782134535 \n",
      "\n",
      "Epoch:  5248  Learning Rate:  0.000526328459184328  Varinance:  0.016307740597085972 \n",
      "\n",
      "Epoch:  5249  Learning Rate:  0.0005258023938016736  Varinance:  0.016284959281220708 \n",
      "\n",
      "Epoch:  5250  Learning Rate:  0.0005252768542214572  Varinance:  0.01626220979001869 \n",
      "\n",
      "Epoch:  5251  Learning Rate:  0.0005247518399181385  Varinance:  0.016239492079022012 \n",
      "\n",
      "Epoch:  5252  Learning Rate:  0.0005242273503667034  Varinance:  0.01621680610383488 \n",
      "\n",
      "Epoch:  5253  Learning Rate:  0.0005237033850426627  Varinance:  0.01619415182012356 \n",
      "\n",
      "Epoch:  5254  Learning Rate:  0.0005231799434220504  Varinance:  0.016171529183616196 \n",
      "\n",
      "Epoch:  5255  Learning Rate:  0.0005226570249814254  Varinance:  0.016148938150102828 \n",
      "\n",
      "Epoch:  5256  Learning Rate:  0.0005221346291978686  Varinance:  0.016126378675435205 \n",
      "\n",
      "Epoch:  5257  Learning Rate:  0.0005216127555489845  Varinance:  0.016103850715526804 \n",
      "\n",
      "Epoch:  5258  Learning Rate:  0.0005210914035128997  Varinance:  0.01608135422635265 \n",
      "\n",
      "Epoch:  5259  Learning Rate:  0.0005205705725682615  Varinance:  0.016058889163949267 \n",
      "\n",
      "Epoch:  5260  Learning Rate:  0.0005200502621942394  Varinance:  0.01603645548441463 \n",
      "\n",
      "Epoch:  5261  Learning Rate:  0.0005195304718705232  Varinance:  0.016014053143908 \n",
      "\n",
      "Epoch:  5262  Learning Rate:  0.0005190112010773216  Varinance:  0.015991682098649932 \n",
      "\n",
      "Epoch:  5263  Learning Rate:  0.0005184924492953649  Varinance:  0.015969342304922078 \n",
      "\n",
      "Epoch:  5264  Learning Rate:  0.0005179742160059002  Varinance:  0.015947033719067225 \n",
      "\n",
      "Epoch:  5265  Learning Rate:  0.0005174565006906946  Varinance:  0.01592475629748911 \n",
      "\n",
      "Epoch:  5266  Learning Rate:  0.0005169393028320333  Varinance:  0.01590250999665236 \n",
      "\n",
      "Epoch:  5267  Learning Rate:  0.0005164226219127176  Varinance:  0.01588029477308247 \n",
      "\n",
      "Epoch:  5268  Learning Rate:  0.0005159064574160667  Varinance:  0.015858110583365607 \n",
      "\n",
      "Epoch:  5269  Learning Rate:  0.0005153908088259167  Varinance:  0.015835957384148648 \n",
      "\n",
      "Epoch:  5270  Learning Rate:  0.000514875675626618  Varinance:  0.015813835132138986 \n",
      "\n",
      "Epoch:  5271  Learning Rate:  0.0005143610573030384  Varinance:  0.015791743784104494 \n",
      "\n",
      "Epoch:  5272  Learning Rate:  0.0005138469533405584  Varinance:  0.015769683296873473 \n",
      "\n",
      "Epoch:  5273  Learning Rate:  0.0005133333632250745  Varinance:  0.01574765362733449 \n",
      "\n",
      "Epoch:  5274  Learning Rate:  0.0005128202864429972  Varinance:  0.015725654732436384 \n",
      "\n",
      "Epoch:  5275  Learning Rate:  0.0005123077224812486  Varinance:  0.015703686569188084 \n",
      "\n",
      "Epoch:  5276  Learning Rate:  0.0005117956708272652  Varinance:  0.015681749094658626 \n",
      "\n",
      "Epoch:  5277  Learning Rate:  0.0005112841309689956  Varinance:  0.015659842265976984 \n",
      "\n",
      "Epoch:  5278  Learning Rate:  0.0005107731023948993  Varinance:  0.015637966040332015 \n",
      "\n",
      "Epoch:  5279  Learning Rate:  0.0005102625845939483  Varinance:  0.015616120374972432 \n",
      "\n",
      "Epoch:  5280  Learning Rate:  0.0005097525770556239  Varinance:  0.015594305227206604 \n",
      "\n",
      "Epoch:  5281  Learning Rate:  0.000509243079269919  Varinance:  0.015572520554402598 \n",
      "\n",
      "Epoch:  5282  Learning Rate:  0.0005087340907273364  Varinance:  0.015550766313987982 \n",
      "\n",
      "Epoch:  5283  Learning Rate:  0.0005082256109188865  Varinance:  0.015529042463449855 \n",
      "\n",
      "Epoch:  5284  Learning Rate:  0.0005077176393360898  Varinance:  0.015507348960334656 \n",
      "\n",
      "Epoch:  5285  Learning Rate:  0.0005072101754709752  Varinance:  0.015485685762248133 \n",
      "\n",
      "Epoch:  5286  Learning Rate:  0.0005067032188160778  Varinance:  0.015464052826855305 \n",
      "\n",
      "Epoch:  5287  Learning Rate:  0.000506196768864442  Varinance:  0.015442450111880265 \n",
      "\n",
      "Epoch:  5288  Learning Rate:  0.0005056908251096169  Varinance:  0.015420877575106226 \n",
      "\n",
      "Epoch:  5289  Learning Rate:  0.0005051853870456589  Varinance:  0.015399335174375322 \n",
      "\n",
      "Epoch:  5290  Learning Rate:  0.0005046804541671305  Varinance:  0.01537782286758863 \n",
      "\n",
      "Epoch:  5291  Learning Rate:  0.0005041760259690979  Varinance:  0.015356340612706 \n",
      "\n",
      "Epoch:  5292  Learning Rate:  0.0005036721019471333  Varinance:  0.01533488836774601 \n",
      "\n",
      "Epoch:  5293  Learning Rate:  0.0005031686815973131  Varinance:  0.015313466090785929 \n",
      "\n",
      "Epoch:  5294  Learning Rate:  0.0005026657644162159  Varinance:  0.015292073739961524 \n",
      "\n",
      "Epoch:  5295  Learning Rate:  0.0005021633499009254  Varinance:  0.015270711273467115 \n",
      "\n",
      "Epoch:  5296  Learning Rate:  0.0005016614375490263  Varinance:  0.01524937864955536 \n",
      "\n",
      "Epoch:  5297  Learning Rate:  0.0005011600268586065  Varinance:  0.015228075826537287 \n",
      "\n",
      "Epoch:  5298  Learning Rate:  0.0005006591173282559  Varinance:  0.015206802762782133 \n",
      "\n",
      "Epoch:  5299  Learning Rate:  0.0005001587084570639  Varinance:  0.015185559416717282 \n",
      "\n",
      "Epoch:  5300  Learning Rate:  0.0004996587997446219  Varinance:  0.015164345746828239 \n",
      "\n",
      "Epoch:  5301  Learning Rate:  0.0004991593906910217  Varinance:  0.015143161711658441 \n",
      "\n",
      "Epoch:  5302  Learning Rate:  0.0004986604807968534  Varinance:  0.015122007269809305 \n",
      "\n",
      "Epoch:  5303  Learning Rate:  0.000498162069563208  Varinance:  0.01510088237994003 \n",
      "\n",
      "Epoch:  5304  Learning Rate:  0.0004976641564916731  Varinance:  0.015079787000767573 \n",
      "\n",
      "Epoch:  5305  Learning Rate:  0.0004971667410843362  Varinance:  0.015058721091066592 \n",
      "\n",
      "Epoch:  5306  Learning Rate:  0.0004966698228437823  Varinance:  0.015037684609669298 \n",
      "\n",
      "Epoch:  5307  Learning Rate:  0.0004961734012730922  Varinance:  0.015016677515465452 \n",
      "\n",
      "Epoch:  5308  Learning Rate:  0.0004956774758758446  Varinance:  0.014995699767402193 \n",
      "\n",
      "Epoch:  5309  Learning Rate:  0.0004951820461561147  Varinance:  0.014974751324484068 \n",
      "\n",
      "Epoch:  5310  Learning Rate:  0.0004946871116184719  Varinance:  0.014953832145772847 \n",
      "\n",
      "Epoch:  5311  Learning Rate:  0.0004941926717679822  Varinance:  0.014932942190387497 \n",
      "\n",
      "Epoch:  5312  Learning Rate:  0.0004936987261102052  Varinance:  0.014912081417504126 \n",
      "\n",
      "Epoch:  5313  Learning Rate:  0.0004932052741511952  Varinance:  0.014891249786355823 \n",
      "\n",
      "Epoch:  5314  Learning Rate:  0.000492712315397501  Varinance:  0.014870447256232678 \n",
      "\n",
      "Epoch:  5315  Learning Rate:  0.0004922198493561629  Varinance:  0.014849673786481598 \n",
      "\n",
      "Epoch:  5316  Learning Rate:  0.0004917278755347151  Varinance:  0.014828929336506338 \n",
      "\n",
      "Epoch:  5317  Learning Rate:  0.0004912363934411843  Varinance:  0.014808213865767321 \n",
      "\n",
      "Epoch:  5318  Learning Rate:  0.0004907454025840874  Varinance:  0.014787527333781601 \n",
      "\n",
      "Epoch:  5319  Learning Rate:  0.0004902549024724343  Varinance:  0.01476686970012283 \n",
      "\n",
      "Epoch:  5320  Learning Rate:  0.0004897648926157242  Varinance:  0.014746240924421072 \n",
      "\n",
      "Epoch:  5321  Learning Rate:  0.0004892753725239476  Varinance:  0.01472564096636285 \n",
      "\n",
      "Epoch:  5322  Learning Rate:  0.0004887863417075847  Varinance:  0.014705069785690946 \n",
      "\n",
      "Epoch:  5323  Learning Rate:  0.0004882977996776038  Varinance:  0.014684527342204432 \n",
      "\n",
      "Epoch:  5324  Learning Rate:  0.00048780974594546324  Varinance:  0.0146640135957585 \n",
      "\n",
      "Epoch:  5325  Learning Rate:  0.00048732218002310964  Varinance:  0.014643528506264422 \n",
      "\n",
      "Epoch:  5326  Learning Rate:  0.0004868351014229763  Varinance:  0.014623072033689512 \n",
      "\n",
      "Epoch:  5327  Learning Rate:  0.00048634850965798546  Varinance:  0.014602644138056963 \n",
      "\n",
      "Epoch:  5328  Learning Rate:  0.00048586240424154436  Varinance:  0.014582244779445851 \n",
      "\n",
      "Epoch:  5329  Learning Rate:  0.0004853767846875479  Varinance:  0.014561873917990983 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5330  Learning Rate:  0.00048489165051037705  Varinance:  0.014541531513882901 \n",
      "\n",
      "Epoch:  5331  Learning Rate:  0.00048440700122489677  Varinance:  0.01452121752736772 \n",
      "\n",
      "Epoch:  5332  Learning Rate:  0.00048392283634645793  Varinance:  0.014500931918747096 \n",
      "\n",
      "Epoch:  5333  Learning Rate:  0.0004834391553908963  Varinance:  0.01448067464837817 \n",
      "\n",
      "Epoch:  5334  Learning Rate:  0.0004829559578745299  Varinance:  0.01446044567667342 \n",
      "\n",
      "Epoch:  5335  Learning Rate:  0.000482473243314162  Varinance:  0.014440244964100676 \n",
      "\n",
      "Epoch:  5336  Learning Rate:  0.0004819910112270772  Varinance:  0.014420072471182953 \n",
      "\n",
      "Epoch:  5337  Learning Rate:  0.00048150926113104385  Varinance:  0.014399928158498415 \n",
      "\n",
      "Epoch:  5338  Learning Rate:  0.0004810279925443121  Varinance:  0.014379811986680336 \n",
      "\n",
      "Epoch:  5339  Learning Rate:  0.00048054720498561264  Varinance:  0.01435972391641693 \n",
      "\n",
      "Epoch:  5340  Learning Rate:  0.00048006689797415816  Varinance:  0.014339663908451377 \n",
      "\n",
      "Epoch:  5341  Learning Rate:  0.00047958707102964214  Varinance:  0.014319631923581658 \n",
      "\n",
      "Epoch:  5342  Learning Rate:  0.0004791077236722366  Varinance:  0.014299627922660551 \n",
      "\n",
      "Epoch:  5343  Learning Rate:  0.00047862885542259516  Varinance:  0.014279651866595501 \n",
      "\n",
      "Epoch:  5344  Learning Rate:  0.0004781504658018486  Varinance:  0.014259703716348554 \n",
      "\n",
      "Epoch:  5345  Learning Rate:  0.0004776725543316077  Varinance:  0.014239783432936328 \n",
      "\n",
      "Epoch:  5346  Learning Rate:  0.00047719512053396126  Varinance:  0.01421989097742985 \n",
      "\n",
      "Epoch:  5347  Learning Rate:  0.0004767181639314748  Varinance:  0.014200026310954586 \n",
      "\n",
      "Epoch:  5348  Learning Rate:  0.000476241684047192  Varinance:  0.014180189394690247 \n",
      "\n",
      "Epoch:  5349  Learning Rate:  0.0004757656804046333  Varinance:  0.014160380189870838 \n",
      "\n",
      "Epoch:  5350  Learning Rate:  0.0004752901525277942  Varinance:  0.014140598657784463 \n",
      "\n",
      "Epoch:  5351  Learning Rate:  0.00047481509994114776  Varinance:  0.014120844759773328 \n",
      "\n",
      "Epoch:  5352  Learning Rate:  0.0004743405221696404  Varinance:  0.014101118457233658 \n",
      "\n",
      "Epoch:  5353  Learning Rate:  0.00047386641873869463  Varinance:  0.014081419711615572 \n",
      "\n",
      "Epoch:  5354  Learning Rate:  0.0004733927891742076  Varinance:  0.014061748484423079 \n",
      "\n",
      "Epoch:  5355  Learning Rate:  0.0004729196330025487  Varinance:  0.01404210473721393 \n",
      "\n",
      "Epoch:  5356  Learning Rate:  0.00047244694975056233  Varinance:  0.014022488431599615 \n",
      "\n",
      "Epoch:  5357  Learning Rate:  0.00047197473894556533  Varinance:  0.014002899529245222 \n",
      "\n",
      "Epoch:  5358  Learning Rate:  0.00047150300011534633  Varinance:  0.013983337991869387 \n",
      "\n",
      "Epoch:  5359  Learning Rate:  0.0004710317327881671  Varinance:  0.013963803781244266 \n",
      "\n",
      "Epoch:  5360  Learning Rate:  0.0004705609364927595  Varinance:  0.01394429685919536 \n",
      "\n",
      "Epoch:  5361  Learning Rate:  0.00047009061075832764  Varinance:  0.013924817187601558 \n",
      "\n",
      "Epoch:  5362  Learning Rate:  0.0004696207551145461  Varinance:  0.013905364728394948 \n",
      "\n",
      "Epoch:  5363  Learning Rate:  0.00046915136909155843  Varinance:  0.013885939443560844 \n",
      "\n",
      "Epoch:  5364  Learning Rate:  0.0004686824522199789  Varinance:  0.013866541295137633 \n",
      "\n",
      "Epoch:  5365  Learning Rate:  0.00046821400403089107  Varinance:  0.01384717024521673 \n",
      "\n",
      "Epoch:  5366  Learning Rate:  0.00046774602405584586  Varinance:  0.013827826255942548 \n",
      "\n",
      "Epoch:  5367  Learning Rate:  0.0004672785118268641  Varinance:  0.013808509289512329 \n",
      "\n",
      "Epoch:  5368  Learning Rate:  0.0004668114668764327  Varinance:  0.013789219308176176 \n",
      "\n",
      "Epoch:  5369  Learning Rate:  0.0004663448887375071  Varinance:  0.013769956274236887 \n",
      "\n",
      "Epoch:  5370  Learning Rate:  0.0004658787769435095  Varinance:  0.013750720150049927 \n",
      "\n",
      "Epoch:  5371  Learning Rate:  0.0004654131310283272  Varinance:  0.013731510898023382 \n",
      "\n",
      "Epoch:  5372  Learning Rate:  0.00046494795052631484  Varinance:  0.013712328480617802 \n",
      "\n",
      "Epoch:  5373  Learning Rate:  0.0004644832349722921  Varinance:  0.013693172860346235 \n",
      "\n",
      "Epoch:  5374  Learning Rate:  0.00046401898390154256  Varinance:  0.013674043999774038 \n",
      "\n",
      "Epoch:  5375  Learning Rate:  0.000463555196849816  Varinance:  0.013654941861518917 \n",
      "\n",
      "Epoch:  5376  Learning Rate:  0.0004630918733533246  Varinance:  0.013635866408250763 \n",
      "\n",
      "Epoch:  5377  Learning Rate:  0.00046262901294874515  Varinance:  0.013616817602691615 \n",
      "\n",
      "Epoch:  5378  Learning Rate:  0.00046216661517321756  Varinance:  0.013597795407615623 \n",
      "\n",
      "Epoch:  5379  Learning Rate:  0.0004617046795643433  Varinance:  0.013578799785848899 \n",
      "\n",
      "Epoch:  5380  Learning Rate:  0.0004612432056601874  Varinance:  0.01355983070026952 \n",
      "\n",
      "Epoch:  5381  Learning Rate:  0.0004607821929992752  Varinance:  0.013540888113807386 \n",
      "\n",
      "Epoch:  5382  Learning Rate:  0.00046032164112059447  Varinance:  0.013521971989444225 \n",
      "\n",
      "Epoch:  5383  Learning Rate:  0.0004598615495635936  Varinance:  0.013503082290213438 \n",
      "\n",
      "Epoch:  5384  Learning Rate:  0.0004594019178681802  Varinance:  0.013484218979200081 \n",
      "\n",
      "Epoch:  5385  Learning Rate:  0.00045894274557472294  Varinance:  0.013465382019540797 \n",
      "\n",
      "Epoch:  5386  Learning Rate:  0.0004584840322240499  Varinance:  0.01344657137442369 \n",
      "\n",
      "Epoch:  5387  Learning Rate:  0.00045802577735744695  Varinance:  0.013427787007088335 \n",
      "\n",
      "Epoch:  5388  Learning Rate:  0.00045756798051665987  Varinance:  0.013409028880825605 \n",
      "\n",
      "Epoch:  5389  Learning Rate:  0.00045711064124389104  Varinance:  0.01339029695897771 \n",
      "\n",
      "Epoch:  5390  Learning Rate:  0.00045665375908180155  Varinance:  0.013371591204938032 \n",
      "\n",
      "Epoch:  5391  Learning Rate:  0.00045619733357350967  Varinance:  0.013352911582151086 \n",
      "\n",
      "Epoch:  5392  Learning Rate:  0.0004557413642625888  Varinance:  0.013334258054112499 \n",
      "\n",
      "Epoch:  5393  Learning Rate:  0.00045528585069307034  Varinance:  0.01331563058436884 \n",
      "\n",
      "Epoch:  5394  Learning Rate:  0.00045483079240944083  Varinance:  0.013297029136517645 \n",
      "\n",
      "Epoch:  5395  Learning Rate:  0.00045437618895664134  Varinance:  0.013278453674207264 \n",
      "\n",
      "Epoch:  5396  Learning Rate:  0.00045392203988006897  Varinance:  0.01325990416113687 \n",
      "\n",
      "Epoch:  5397  Learning Rate:  0.0004534683447255739  Varinance:  0.013241380561056318 \n",
      "\n",
      "Epoch:  5398  Learning Rate:  0.0004530151030394614  Varinance:  0.013222882837766098 \n",
      "\n",
      "Epoch:  5399  Learning Rate:  0.00045256231436849006  Varinance:  0.013204410955117304 \n",
      "\n",
      "Epoch:  5400  Learning Rate:  0.00045210997825987037  Varinance:  0.013185964877011484 \n",
      "\n",
      "Epoch:  5401  Learning Rate:  0.0004516580942612666  Varinance:  0.013167544567400664 \n",
      "\n",
      "Epoch:  5402  Learning Rate:  0.0004512066619207953  Varinance:  0.013149149990287183 \n",
      "\n",
      "Epoch:  5403  Learning Rate:  0.00045075568078702295  Varinance:  0.01313078110972368 \n",
      "\n",
      "Epoch:  5404  Learning Rate:  0.00045030515040896937  Varinance:  0.013112437889813037 \n",
      "\n",
      "Epoch:  5405  Learning Rate:  0.0004498550703361034  Varinance:  0.013094120294708238 \n",
      "\n",
      "Epoch:  5406  Learning Rate:  0.0004494054401183452  Varinance:  0.013075828288612396 \n",
      "\n",
      "Epoch:  5407  Learning Rate:  0.00044895625930606493  Varinance:  0.013057561835778574 \n",
      "\n",
      "Epoch:  5408  Learning Rate:  0.0004485075274500811  Varinance:  0.01303932090050983 \n",
      "\n",
      "Epoch:  5409  Learning Rate:  0.00044805924410166195  Varinance:  0.01302110544715905 \n",
      "\n",
      "Epoch:  5410  Learning Rate:  0.00044761140881252473  Varinance:  0.01300291544012891 \n",
      "\n",
      "Epoch:  5411  Learning Rate:  0.0004471640211348332  Varinance:  0.012984750843871865 \n",
      "\n",
      "Epoch:  5412  Learning Rate:  0.0004467170806212005  Varinance:  0.012966611622889965 \n",
      "\n",
      "Epoch:  5413  Learning Rate:  0.0004462705868246852  Varinance:  0.01294849774173491 \n",
      "\n",
      "Epoch:  5414  Learning Rate:  0.000445824539298794  Varinance:  0.012930409165007865 \n",
      "\n",
      "Epoch:  5415  Learning Rate:  0.00044537893759747954  Varinance:  0.012912345857359495 \n",
      "\n",
      "Epoch:  5416  Learning Rate:  0.0004449337812751394  Varinance:  0.012894307783489814 \n",
      "\n",
      "Epoch:  5417  Learning Rate:  0.0004444890698866177  Varinance:  0.012876294908148142 \n",
      "\n",
      "Epoch:  5418  Learning Rate:  0.00044404480298720325  Varinance:  0.012858307196133085 \n",
      "\n",
      "Epoch:  5419  Learning Rate:  0.00044360098013262843  Varinance:  0.012840344612292373 \n",
      "\n",
      "Epoch:  5420  Learning Rate:  0.0004431576008790711  Varinance:  0.012822407121522893 \n",
      "\n",
      "Epoch:  5421  Learning Rate:  0.00044271466478315114  Varinance:  0.012804494688770521 \n",
      "\n",
      "Epoch:  5422  Learning Rate:  0.0004422721714019329  Varinance:  0.012786607279030148 \n",
      "\n",
      "Epoch:  5423  Learning Rate:  0.0004418301202929234  Varinance:  0.012768744857345532 \n",
      "\n",
      "Epoch:  5424  Learning Rate:  0.0004413885110140705  Varinance:  0.012750907388809263 \n",
      "\n",
      "Epoch:  5425  Learning Rate:  0.0004409473431237654  Varinance:  0.012733094838562728 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5426  Learning Rate:  0.00044050661618084063  Varinance:  0.012715307171795972 \n",
      "\n",
      "Epoch:  5427  Learning Rate:  0.00044006632974456826  Varinance:  0.012697544353747705 \n",
      "\n",
      "Epoch:  5428  Learning Rate:  0.0004396264833746628  Varinance:  0.012679806349705157 \n",
      "\n",
      "Epoch:  5429  Learning Rate:  0.00043918707663127685  Varinance:  0.012662093125004095 \n",
      "\n",
      "Epoch:  5430  Learning Rate:  0.00043874810907500427  Varinance:  0.012644404645028673 \n",
      "\n",
      "Epoch:  5431  Learning Rate:  0.00043830958026687763  Varinance:  0.012626740875211406 \n",
      "\n",
      "Epoch:  5432  Learning Rate:  0.0004378714897683674  Varinance:  0.01260910178103313 \n",
      "\n",
      "Epoch:  5433  Learning Rate:  0.00043743383714138343  Varinance:  0.012591487328022864 \n",
      "\n",
      "Epoch:  5434  Learning Rate:  0.00043699662194827347  Varinance:  0.012573897481757818 \n",
      "\n",
      "Epoch:  5435  Learning Rate:  0.0004365598437518214  Varinance:  0.012556332207863261 \n",
      "\n",
      "Epoch:  5436  Learning Rate:  0.0004361235021152499  Varinance:  0.012538791472012482 \n",
      "\n",
      "Epoch:  5437  Learning Rate:  0.0004356875966022165  Varinance:  0.012521275239926752 \n",
      "\n",
      "Epoch:  5438  Learning Rate:  0.000435252126776816  Varinance:  0.01250378347737519 \n",
      "\n",
      "Epoch:  5439  Learning Rate:  0.0004348170922035789  Varinance:  0.012486316150174776 \n",
      "\n",
      "Epoch:  5440  Learning Rate:  0.0004343824924474699  Varinance:  0.012468873224190196 \n",
      "\n",
      "Epoch:  5441  Learning Rate:  0.0004339483270738895  Varinance:  0.012451454665333872 \n",
      "\n",
      "Epoch:  5442  Learning Rate:  0.0004335145956486727  Varinance:  0.012434060439565802 \n",
      "\n",
      "Epoch:  5443  Learning Rate:  0.0004330812977380874  Varinance:  0.012416690512893546 \n",
      "\n",
      "Epoch:  5444  Learning Rate:  0.0004326484329088362  Varinance:  0.01239934485137218 \n",
      "\n",
      "Epoch:  5445  Learning Rate:  0.0004322160007280537  Varinance:  0.012382023421104156 \n",
      "\n",
      "Epoch:  5446  Learning Rate:  0.0004317840007633078  Varinance:  0.012364726188239321 \n",
      "\n",
      "Epoch:  5447  Learning Rate:  0.00043135243258259916  Varinance:  0.012347453118974772 \n",
      "\n",
      "Epoch:  5448  Learning Rate:  0.00043092129575435863  Varinance:  0.012330204179554861 \n",
      "\n",
      "Epoch:  5449  Learning Rate:  0.0004304905898474497  Varinance:  0.012312979336271073 \n",
      "\n",
      "Epoch:  5450  Learning Rate:  0.0004300603144311669  Varinance:  0.012295778555461979 \n",
      "\n",
      "Epoch:  5451  Learning Rate:  0.000429630469075234  Varinance:  0.0122786018035132 \n",
      "\n",
      "Epoch:  5452  Learning Rate:  0.00042920105334980633  Varinance:  0.012261449046857283 \n",
      "\n",
      "Epoch:  5453  Learning Rate:  0.0004287720668254675  Varinance:  0.012244320251973704 \n",
      "\n",
      "Epoch:  5454  Learning Rate:  0.00042834350907323114  Varinance:  0.012227215385388719 \n",
      "\n",
      "Epoch:  5455  Learning Rate:  0.0004279153796645399  Varinance:  0.012210134413675399 \n",
      "\n",
      "Epoch:  5456  Learning Rate:  0.0004274876781712636  Varinance:  0.01219307730345347 \n",
      "\n",
      "Epoch:  5457  Learning Rate:  0.0004270604041657012  Varinance:  0.012176044021389295 \n",
      "\n",
      "Epoch:  5458  Learning Rate:  0.00042663355722057886  Varinance:  0.012159034534195833 \n",
      "\n",
      "Epoch:  5459  Learning Rate:  0.00042620713690904894  Varinance:  0.012142048808632497 \n",
      "\n",
      "Epoch:  5460  Learning Rate:  0.00042578114280469176  Varinance:  0.012125086811505183 \n",
      "\n",
      "Epoch:  5461  Learning Rate:  0.0004253555744815126  Varinance:  0.012108148509666116 \n",
      "\n",
      "Epoch:  5462  Learning Rate:  0.0004249304315139433  Varinance:  0.012091233870013865 \n",
      "\n",
      "Epoch:  5463  Learning Rate:  0.00042450571347684134  Varinance:  0.012074342859493216 \n",
      "\n",
      "Epoch:  5464  Learning Rate:  0.0004240814199454878  Varinance:  0.012057475445095125 \n",
      "\n",
      "Epoch:  5465  Learning Rate:  0.0004236575504955896  Varinance:  0.012040631593856696 \n",
      "\n",
      "Epoch:  5466  Learning Rate:  0.0004232341047032775  Varinance:  0.012023811272861036 \n",
      "\n",
      "Epoch:  5467  Learning Rate:  0.0004228110821451051  Varinance:  0.012007014449237281 \n",
      "\n",
      "Epoch:  5468  Learning Rate:  0.0004223884823980504  Varinance:  0.011990241090160452 \n",
      "\n",
      "Epoch:  5469  Learning Rate:  0.00042196630503951293  Varinance:  0.011973491162851428 \n",
      "\n",
      "Epoch:  5470  Learning Rate:  0.0004215445496473156  Varinance:  0.011956764634576907 \n",
      "\n",
      "Epoch:  5471  Learning Rate:  0.00042112321579970355  Varinance:  0.011940061472649276 \n",
      "\n",
      "Epoch:  5472  Learning Rate:  0.0004207023030753419  Varinance:  0.011923381644426625 \n",
      "\n",
      "Epoch:  5473  Learning Rate:  0.0004202818110533184  Varinance:  0.011906725117312604 \n",
      "\n",
      "Epoch:  5474  Learning Rate:  0.0004198617393131414  Varinance:  0.01189009185875644 \n",
      "\n",
      "Epoch:  5475  Learning Rate:  0.0004194420874347384  Varinance:  0.011873481836252796 \n",
      "\n",
      "Epoch:  5476  Learning Rate:  0.00041902285499845795  Varinance:  0.01185689501734175 \n",
      "\n",
      "Epoch:  5477  Learning Rate:  0.0004186040415850672  Varinance:  0.011840331369608758 \n",
      "\n",
      "Epoch:  5478  Learning Rate:  0.0004181856467757529  Varinance:  0.011823790860684507 \n",
      "\n",
      "Epoch:  5479  Learning Rate:  0.0004177676701521206  Varinance:  0.011807273458244952 \n",
      "\n",
      "Epoch:  5480  Learning Rate:  0.0004173501112961929  Varinance:  0.011790779130011165 \n",
      "\n",
      "Epoch:  5481  Learning Rate:  0.0004169329697904112  Varinance:  0.011774307843749339 \n",
      "\n",
      "Epoch:  5482  Learning Rate:  0.0004165162452176344  Varinance:  0.011757859567270677 \n",
      "\n",
      "Epoch:  5483  Learning Rate:  0.00041609993716113723  Varinance:  0.011741434268431347 \n",
      "\n",
      "Epoch:  5484  Learning Rate:  0.00041568404520461234  Varinance:  0.011725031915132448 \n",
      "\n",
      "Epoch:  5485  Learning Rate:  0.00041526856893216673  Varinance:  0.011708652475319879 \n",
      "\n",
      "Epoch:  5486  Learning Rate:  0.0004148535079283248  Varinance:  0.011692295916984362 \n",
      "\n",
      "Epoch:  5487  Learning Rate:  0.0004144388617780257  Varinance:  0.011675962208161294 \n",
      "\n",
      "Epoch:  5488  Learning Rate:  0.0004140246300666226  Varinance:  0.011659651316930758 \n",
      "\n",
      "Epoch:  5489  Learning Rate:  0.000413610812379884  Varinance:  0.011643363211417408 \n",
      "\n",
      "Epoch:  5490  Learning Rate:  0.0004131974083039926  Varinance:  0.011627097859790419 \n",
      "\n",
      "Epoch:  5491  Learning Rate:  0.0004127844174255436  Varinance:  0.011610855230263471 \n",
      "\n",
      "Epoch:  5492  Learning Rate:  0.00041237183933154684  Varinance:  0.011594635291094597 \n",
      "\n",
      "Epoch:  5493  Learning Rate:  0.00041195967360942333  Varinance:  0.011578438010586223 \n",
      "\n",
      "Epoch:  5494  Learning Rate:  0.00041154791984700776  Varinance:  0.011562263357085007 \n",
      "\n",
      "Epoch:  5495  Learning Rate:  0.0004111365776325467  Varinance:  0.011546111298981874 \n",
      "\n",
      "Epoch:  5496  Learning Rate:  0.0004107256465546973  Varinance:  0.011529981804711868 \n",
      "\n",
      "Epoch:  5497  Learning Rate:  0.00041031512620252853  Varinance:  0.011513874842754132 \n",
      "\n",
      "Epoch:  5498  Learning Rate:  0.0004099050161655206  Varinance:  0.011497790381631871 \n",
      "\n",
      "Epoch:  5499  Learning Rate:  0.00040949531603356256  Varinance:  0.011481728389912224 \n",
      "\n",
      "Epoch:  5500  Learning Rate:  0.00040908602539695507  Varinance:  0.011465688836206276 \n",
      "\n",
      "Epoch:  5501  Learning Rate:  0.00040867714384640666  Varinance:  0.011449671689168939 \n",
      "\n",
      "Epoch:  5502  Learning Rate:  0.0004082686709730363  Varinance:  0.011433676917498905 \n",
      "\n",
      "Epoch:  5503  Learning Rate:  0.00040786060636837117  Varinance:  0.011417704489938627 \n",
      "\n",
      "Epoch:  5504  Learning Rate:  0.0004074529496243461  Varinance:  0.011401754375274182 \n",
      "\n",
      "Epoch:  5505  Learning Rate:  0.0004070457003333049  Varinance:  0.011385826542335293 \n",
      "\n",
      "Epoch:  5506  Learning Rate:  0.0004066388580879976  Varinance:  0.011369920959995186 \n",
      "\n",
      "Epoch:  5507  Learning Rate:  0.00040623242248158235  Varinance:  0.011354037597170609 \n",
      "\n",
      "Epoch:  5508  Learning Rate:  0.00040582639310762374  Varinance:  0.011338176422821707 \n",
      "\n",
      "Epoch:  5509  Learning Rate:  0.00040542076956009174  Varinance:  0.011322337405951979 \n",
      "\n",
      "Epoch:  5510  Learning Rate:  0.00040501555143336305  Varinance:  0.011306520515608258 \n",
      "\n",
      "Epoch:  5511  Learning Rate:  0.0004046107383222199  Varinance:  0.011290725720880582 \n",
      "\n",
      "Epoch:  5512  Learning Rate:  0.00040420632982184843  Varinance:  0.011274952990902204 \n",
      "\n",
      "Epoch:  5513  Learning Rate:  0.0004038023255278409  Varinance:  0.011259202294849455 \n",
      "\n",
      "Epoch:  5514  Learning Rate:  0.00040339872503619207  Varinance:  0.011243473601941773 \n",
      "\n",
      "Epoch:  5515  Learning Rate:  0.00040299552794330194  Varinance:  0.011227766881441557 \n",
      "\n",
      "Epoch:  5516  Learning Rate:  0.00040259273384597374  Varinance:  0.01121208210265415 \n",
      "\n",
      "Epoch:  5517  Learning Rate:  0.00040219034234141246  Varinance:  0.011196419234927807 \n",
      "\n",
      "Epoch:  5518  Learning Rate:  0.00040178835302722717  Varinance:  0.011180778247653556 \n",
      "\n",
      "Epoch:  5519  Learning Rate:  0.0004013867655014287  Varinance:  0.011165159110265228 \n",
      "\n",
      "Epoch:  5520  Learning Rate:  0.00040098557936242877  Varinance:  0.011149561792239311 \n",
      "\n",
      "Epoch:  5521  Learning Rate:  0.00040058479420904206  Varinance:  0.011133986263094978 \n",
      "\n",
      "Epoch:  5522  Learning Rate:  0.00040018440964048255  Varinance:  0.011118432492393951 \n",
      "\n",
      "Epoch:  5523  Learning Rate:  0.00039978442525636596  Varinance:  0.011102900449740468 \n",
      "\n",
      "Epoch:  5524  Learning Rate:  0.00039938484065670833  Varinance:  0.011087390104781268 \n",
      "\n",
      "Epoch:  5525  Learning Rate:  0.0003989856554419244  Varinance:  0.011071901427205443 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5526  Learning Rate:  0.0003985868692128291  Varinance:  0.011056434386744477 \n",
      "\n",
      "Epoch:  5527  Learning Rate:  0.0003981884815706365  Varinance:  0.01104098895317209 \n",
      "\n",
      "Epoch:  5528  Learning Rate:  0.0003977904921169584  Varinance:  0.011025565096304273 \n",
      "\n",
      "Epoch:  5529  Learning Rate:  0.00039739290045380586  Varinance:  0.01101016278599915 \n",
      "\n",
      "Epoch:  5530  Learning Rate:  0.0003969957061835865  Varinance:  0.01099478199215695 \n",
      "\n",
      "Epoch:  5531  Learning Rate:  0.0003965989089091065  Varinance:  0.010979422684719986 \n",
      "\n",
      "Epoch:  5532  Learning Rate:  0.0003962025082335688  Varinance:  0.010964084833672519 \n",
      "\n",
      "Epoch:  5533  Learning Rate:  0.00039580650376057195  Varinance:  0.010948768409040775 \n",
      "\n",
      "Epoch:  5534  Learning Rate:  0.0003954108950941118  Varinance:  0.010933473380892827 \n",
      "\n",
      "Epoch:  5535  Learning Rate:  0.0003950156818385802  Varinance:  0.010918199719338566 \n",
      "\n",
      "Epoch:  5536  Learning Rate:  0.00039462086359876285  Varinance:  0.01090294739452966 \n",
      "\n",
      "Epoch:  5537  Learning Rate:  0.00039422643997984245  Varinance:  0.01088771637665944 \n",
      "\n",
      "Epoch:  5538  Learning Rate:  0.0003938324105873944  Varinance:  0.01087250663596291 \n",
      "\n",
      "Epoch:  5539  Learning Rate:  0.0003934387750273899  Varinance:  0.010857318142716621 \n",
      "\n",
      "Epoch:  5540  Learning Rate:  0.0003930455329061935  Varinance:  0.010842150867238684 \n",
      "\n",
      "Epoch:  5541  Learning Rate:  0.00039265268383056244  Varinance:  0.010827004779888644 \n",
      "\n",
      "Epoch:  5542  Learning Rate:  0.00039226022740764785  Varinance:  0.010811879851067453 \n",
      "\n",
      "Epoch:  5543  Learning Rate:  0.00039186816324499377  Varinance:  0.010796776051217441 \n",
      "\n",
      "Epoch:  5544  Learning Rate:  0.00039147649095053525  Varinance:  0.010781693350822191 \n",
      "\n",
      "Epoch:  5545  Learning Rate:  0.0003910852101326006  Varinance:  0.010766631720406556 \n",
      "\n",
      "Epoch:  5546  Learning Rate:  0.0003906943203999084  Varinance:  0.010751591130536527 \n",
      "\n",
      "Epoch:  5547  Learning Rate:  0.0003903038213615691  Varinance:  0.010736571551819257 \n",
      "\n",
      "Epoch:  5548  Learning Rate:  0.00038991371262708406  Varinance:  0.010721572954902921 \n",
      "\n",
      "Epoch:  5549  Learning Rate:  0.00038952399380634377  Varinance:  0.010706595310476704 \n",
      "\n",
      "Epoch:  5550  Learning Rate:  0.0003891346645096298  Varinance:  0.010691638589270767 \n",
      "\n",
      "Epoch:  5551  Learning Rate:  0.00038874572434761305  Varinance:  0.010676702762056116 \n",
      "\n",
      "Epoch:  5552  Learning Rate:  0.0003883571729313527  Varinance:  0.010661787799644629 \n",
      "\n",
      "Epoch:  5553  Learning Rate:  0.000387969009872298  Varinance:  0.010646893672888926 \n",
      "\n",
      "Epoch:  5554  Learning Rate:  0.0003875812347822852  Varinance:  0.010632020352682375 \n",
      "\n",
      "Epoch:  5555  Learning Rate:  0.00038719384727353945  Varinance:  0.010617167809958982 \n",
      "\n",
      "Epoch:  5556  Learning Rate:  0.00038680684695867356  Varinance:  0.010602336015693355 \n",
      "\n",
      "Epoch:  5557  Learning Rate:  0.0003864202334506866  Varinance:  0.010587524940900678 \n",
      "\n",
      "Epoch:  5558  Learning Rate:  0.0003860340063629652  Varinance:  0.010572734556636586 \n",
      "\n",
      "Epoch:  5559  Learning Rate:  0.0003856481653092827  Varinance:  0.010557964833997189 \n",
      "\n",
      "Epoch:  5560  Learning Rate:  0.00038526270990379735  Varinance:  0.010543215744118938 \n",
      "\n",
      "Epoch:  5561  Learning Rate:  0.0003848776397610543  Varinance:  0.010528487258178636 \n",
      "\n",
      "Epoch:  5562  Learning Rate:  0.00038449295449598274  Varinance:  0.01051377934739333 \n",
      "\n",
      "Epoch:  5563  Learning Rate:  0.00038410865372389774  Varinance:  0.010499091983020277 \n",
      "\n",
      "Epoch:  5564  Learning Rate:  0.0003837247370604988  Varinance:  0.010484425136356904 \n",
      "\n",
      "Epoch:  5565  Learning Rate:  0.0003833412041218686  Varinance:  0.01046977877874071 \n",
      "\n",
      "Epoch:  5566  Learning Rate:  0.00038295805452447443  Varinance:  0.010455152881549261 \n",
      "\n",
      "Epoch:  5567  Learning Rate:  0.00038257528788516706  Varinance:  0.010440547416200086 \n",
      "\n",
      "Epoch:  5568  Learning Rate:  0.0003821929038211791  Varinance:  0.010425962354150643 \n",
      "\n",
      "Epoch:  5569  Learning Rate:  0.0003818109019501271  Varinance:  0.010411397666898288 \n",
      "\n",
      "Epoch:  5570  Learning Rate:  0.0003814292818900086  Varinance:  0.010396853325980154 \n",
      "\n",
      "Epoch:  5571  Learning Rate:  0.0003810480432592037  Varinance:  0.01038232930297318 \n",
      "\n",
      "Epoch:  5572  Learning Rate:  0.00038066718567647426  Varinance:  0.01036782556949397 \n",
      "\n",
      "Epoch:  5573  Learning Rate:  0.00038028670876096184  Varinance:  0.010353342097198816 \n",
      "\n",
      "Epoch:  5574  Learning Rate:  0.00037990661213218983  Varinance:  0.010338878857783576 \n",
      "\n",
      "Epoch:  5575  Learning Rate:  0.00037952689541006194  Varinance:  0.01032443582298365 \n",
      "\n",
      "Epoch:  5576  Learning Rate:  0.0003791475582148608  Varinance:  0.010310012964573949 \n",
      "\n",
      "Epoch:  5577  Learning Rate:  0.0003787686001672498  Varinance:  0.010295610254368778 \n",
      "\n",
      "Epoch:  5578  Learning Rate:  0.0003783900208882702  Varinance:  0.010281227664221852 \n",
      "\n",
      "Epoch:  5579  Learning Rate:  0.000378011819999343  Varinance:  0.010266865166026167 \n",
      "\n",
      "Epoch:  5580  Learning Rate:  0.0003776339971222676  Varinance:  0.010252522731714023 \n",
      "\n",
      "Epoch:  5581  Learning Rate:  0.00037725655187922057  Varinance:  0.010238200333256902 \n",
      "\n",
      "Epoch:  5582  Learning Rate:  0.00037687948389275675  Varinance:  0.010223897942665438 \n",
      "\n",
      "Epoch:  5583  Learning Rate:  0.0003765027927858086  Varinance:  0.010209615531989397 \n",
      "\n",
      "Epoch:  5584  Learning Rate:  0.00037612647818168426  Varinance:  0.010195353073317549 \n",
      "\n",
      "Epoch:  5585  Learning Rate:  0.00037575053970406985  Varinance:  0.010181110538777693 \n",
      "\n",
      "Epoch:  5586  Learning Rate:  0.00037537497697702607  Varinance:  0.01016688790053653 \n",
      "\n",
      "Epoch:  5587  Learning Rate:  0.00037499978962499055  Varinance:  0.01015268513079968 \n",
      "\n",
      "Epoch:  5588  Learning Rate:  0.0003746249772727762  Varinance:  0.010138502201811558 \n",
      "\n",
      "Epoch:  5589  Learning Rate:  0.0003742505395455701  Varinance:  0.010124339085855357 \n",
      "\n",
      "Epoch:  5590  Learning Rate:  0.00037387647606893464  Varinance:  0.010110195755253019 \n",
      "\n",
      "Epoch:  5591  Learning Rate:  0.00037350278646880675  Varinance:  0.010096072182365108 \n",
      "\n",
      "Epoch:  5592  Learning Rate:  0.00037312947037149613  Varinance:  0.010081968339590842 \n",
      "\n",
      "Epoch:  5593  Learning Rate:  0.00037275652740368737  Varinance:  0.010067884199367955 \n",
      "\n",
      "Epoch:  5594  Learning Rate:  0.0003723839571924367  Varinance:  0.010053819734172725 \n",
      "\n",
      "Epoch:  5595  Learning Rate:  0.00037201175936517426  Varinance:  0.01003977491651985 \n",
      "\n",
      "Epoch:  5596  Learning Rate:  0.0003716399335497025  Varinance:  0.010025749718962421 \n",
      "\n",
      "Epoch:  5597  Learning Rate:  0.00037126847937419497  Varinance:  0.010011744114091907 \n",
      "\n",
      "Epoch:  5598  Learning Rate:  0.0003708973964671977  Varinance:  0.009997758074538022 \n",
      "\n",
      "Epoch:  5599  Learning Rate:  0.00037052668445762814  Varinance:  0.009983791572968755 \n",
      "\n",
      "Epoch:  5600  Learning Rate:  0.0003701563429747736  Varinance:  0.009969844582090248 \n",
      "\n",
      "Epoch:  5601  Learning Rate:  0.00036978637164829324  Varinance:  0.009955917074646771 \n",
      "\n",
      "Epoch:  5602  Learning Rate:  0.000369416770108215  Varinance:  0.0099420090234207 \n",
      "\n",
      "Epoch:  5603  Learning Rate:  0.00036904753798493764  Varinance:  0.009928120401232391 \n",
      "\n",
      "Epoch:  5604  Learning Rate:  0.0003686786749092293  Varinance:  0.00991425118094021 \n",
      "\n",
      "Epoch:  5605  Learning Rate:  0.00036831018051222637  Varinance:  0.009900401335440404 \n",
      "\n",
      "Epoch:  5606  Learning Rate:  0.0003679420544254346  Varinance:  0.009886570837667112 \n",
      "\n",
      "Epoch:  5607  Learning Rate:  0.0003675742962807282  Varinance:  0.009872759660592263 \n",
      "\n",
      "Epoch:  5608  Learning Rate:  0.00036720690571034843  Varinance:  0.009858967777225539 \n",
      "\n",
      "Epoch:  5609  Learning Rate:  0.00036683988234690527  Varinance:  0.009845195160614354 \n",
      "\n",
      "Epoch:  5610  Learning Rate:  0.00036647322582337473  Varinance:  0.009831441783843743 \n",
      "\n",
      "Epoch:  5611  Learning Rate:  0.00036610693577310055  Varinance:  0.009817707620036366 \n",
      "\n",
      "Epoch:  5612  Learning Rate:  0.00036574101182979294  Varinance:  0.009803992642352405 \n",
      "\n",
      "Epoch:  5613  Learning Rate:  0.00036537545362752734  Varinance:  0.009790296823989564 \n",
      "\n",
      "Epoch:  5614  Learning Rate:  0.00036501026080074583  Varinance:  0.009776620138182965 \n",
      "\n",
      "Epoch:  5615  Learning Rate:  0.00036464543298425585  Varinance:  0.009762962558205121 \n",
      "\n",
      "Epoch:  5616  Learning Rate:  0.0003642809698132289  Varinance:  0.009749324057365904 \n",
      "\n",
      "Epoch:  5617  Learning Rate:  0.00036391687092320247  Varinance:  0.00973570460901244 \n",
      "\n",
      "Epoch:  5618  Learning Rate:  0.00036355313595007695  Varinance:  0.009722104186529126 \n",
      "\n",
      "Epoch:  5619  Learning Rate:  0.0003631897645301177  Varinance:  0.009708522763337496 \n",
      "\n",
      "Epoch:  5620  Learning Rate:  0.00036282675629995356  Varinance:  0.009694960312896257 \n",
      "\n",
      "Epoch:  5621  Learning Rate:  0.0003624641108965756  Varinance:  0.009681416808701162 \n",
      "\n",
      "Epoch:  5622  Learning Rate:  0.0003621018279573388  Varinance:  0.00966789222428499 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5623  Learning Rate:  0.0003617399071199604  Varinance:  0.009654386533217523 \n",
      "\n",
      "Epoch:  5624  Learning Rate:  0.00036137834802251893  Varinance:  0.009640899709105427 \n",
      "\n",
      "Epoch:  5625  Learning Rate:  0.000361017150303456  Varinance:  0.009627431725592274 \n",
      "\n",
      "Epoch:  5626  Learning Rate:  0.0003606563136015731  Varinance:  0.009613982556358421 \n",
      "\n",
      "Epoch:  5627  Learning Rate:  0.0003602958375560338  Varinance:  0.009600552175121025 \n",
      "\n",
      "Epoch:  5628  Learning Rate:  0.0003599357218063625  Varinance:  0.009587140555633939 \n",
      "\n",
      "Epoch:  5629  Learning Rate:  0.0003595759659924426  Varinance:  0.009573747671687671 \n",
      "\n",
      "Epoch:  5630  Learning Rate:  0.000359216569754519  Varinance:  0.00956037349710938 \n",
      "\n",
      "Epoch:  5631  Learning Rate:  0.00035885753273319477  Varinance:  0.009547018005762745 \n",
      "\n",
      "Epoch:  5632  Learning Rate:  0.0003584988545694332  Varinance:  0.009533681171547993 \n",
      "\n",
      "Epoch:  5633  Learning Rate:  0.00035814053490455637  Varinance:  0.009520362968401787 \n",
      "\n",
      "Epoch:  5634  Learning Rate:  0.00035778257338024396  Varinance:  0.009507063370297199 \n",
      "\n",
      "Epoch:  5635  Learning Rate:  0.0003574249696385348  Varinance:  0.009493782351243683 \n",
      "\n",
      "Epoch:  5636  Learning Rate:  0.0003570677233218253  Varinance:  0.009480519885286968 \n",
      "\n",
      "Epoch:  5637  Learning Rate:  0.0003567108340728686  Varinance:  0.00946727594650908 \n",
      "\n",
      "Epoch:  5638  Learning Rate:  0.000356354301534776  Varinance:  0.009454050509028207 \n",
      "\n",
      "Epoch:  5639  Learning Rate:  0.0003559981253510144  Varinance:  0.009440843546998735 \n",
      "\n",
      "Epoch:  5640  Learning Rate:  0.00035564230516540774  Varinance:  0.009427655034611127 \n",
      "\n",
      "Epoch:  5641  Learning Rate:  0.0003552868406221362  Varinance:  0.009414484946091898 \n",
      "\n",
      "Epoch:  5642  Learning Rate:  0.0003549317313657346  Varinance:  0.009401333255703598 \n",
      "\n",
      "Epoch:  5643  Learning Rate:  0.0003545769770410939  Varinance:  0.009388199937744698 \n",
      "\n",
      "Epoch:  5644  Learning Rate:  0.00035422257729346017  Varinance:  0.009375084966549601 \n",
      "\n",
      "Epoch:  5645  Learning Rate:  0.0003538685317684329  Varinance:  0.009361988316488537 \n",
      "\n",
      "Epoch:  5646  Learning Rate:  0.00035351484011196717  Varinance:  0.009348909961967565 \n",
      "\n",
      "Epoch:  5647  Learning Rate:  0.0003531615019703707  Varinance:  0.009335849877428484 \n",
      "\n",
      "Epoch:  5648  Learning Rate:  0.0003528085169903057  Varinance:  0.009322808037348786 \n",
      "\n",
      "Epoch:  5649  Learning Rate:  0.00035245588481878736  Varinance:  0.009309784416241647 \n",
      "\n",
      "Epoch:  5650  Learning Rate:  0.0003521036051031829  Varinance:  0.009296778988655816 \n",
      "\n",
      "Epoch:  5651  Learning Rate:  0.00035175167749121284  Varinance:  0.009283791729175626 \n",
      "\n",
      "Epoch:  5652  Learning Rate:  0.00035140010163094994  Varinance:  0.009270822612420881 \n",
      "\n",
      "Epoch:  5653  Learning Rate:  0.00035104887717081766  Varinance:  0.009257871613046878 \n",
      "\n",
      "Epoch:  5654  Learning Rate:  0.0003506980037595921  Varinance:  0.00924493870574429 \n",
      "\n",
      "Epoch:  5655  Learning Rate:  0.00035034748104639926  Varinance:  0.00923202386523915 \n",
      "\n",
      "Epoch:  5656  Learning Rate:  0.00034999730868071657  Varinance:  0.009219127066292819 \n",
      "\n",
      "Epoch:  5657  Learning Rate:  0.0003496474863123721  Varinance:  0.009206248283701885 \n",
      "\n",
      "Epoch:  5658  Learning Rate:  0.00034929801359154277  Varinance:  0.009193387492298175 \n",
      "\n",
      "Epoch:  5659  Learning Rate:  0.0003489488901687561  Varinance:  0.009180544666948644 \n",
      "\n",
      "Epoch:  5660  Learning Rate:  0.000348600115694889  Varinance:  0.009167719782555392 \n",
      "\n",
      "Epoch:  5661  Learning Rate:  0.00034825168982116633  Varinance:  0.009154912814055546 \n",
      "\n",
      "Epoch:  5662  Learning Rate:  0.00034790361219916283  Varinance:  0.00914212373642126 \n",
      "\n",
      "Epoch:  5663  Learning Rate:  0.00034755588248080023  Varinance:  0.00912935252465966 \n",
      "\n",
      "Epoch:  5664  Learning Rate:  0.00034720850031834903  Varinance:  0.009116599153812762 \n",
      "\n",
      "Epoch:  5665  Learning Rate:  0.00034686146536442745  Varinance:  0.009103863598957483 \n",
      "\n",
      "Epoch:  5666  Learning Rate:  0.0003465147772719998  Varinance:  0.009091145835205521 \n",
      "\n",
      "Epoch:  5667  Learning Rate:  0.0003461684356943783  Varinance:  0.009078445837703354 \n",
      "\n",
      "Epoch:  5668  Learning Rate:  0.0003458224402852216  Varinance:  0.009065763581632193 \n",
      "\n",
      "Epoch:  5669  Learning Rate:  0.0003454767906985337  Varinance:  0.009053099042207897 \n",
      "\n",
      "Epoch:  5670  Learning Rate:  0.00034513148658866567  Varinance:  0.009040452194680975 \n",
      "\n",
      "Epoch:  5671  Learning Rate:  0.0003447865276103127  Varinance:  0.00902782301433648 \n",
      "\n",
      "Epoch:  5672  Learning Rate:  0.00034444191341851597  Varinance:  0.00901521147649402 \n",
      "\n",
      "Epoch:  5673  Learning Rate:  0.00034409764366866174  Varinance:  0.009002617556507659 \n",
      "\n",
      "Epoch:  5674  Learning Rate:  0.0003437537180164795  Varinance:  0.008990041229765888 \n",
      "\n",
      "Epoch:  5675  Learning Rate:  0.00034341013611804397  Varinance:  0.008977482471691608 \n",
      "\n",
      "Epoch:  5676  Learning Rate:  0.00034306689762977346  Varinance:  0.008964941257742016 \n",
      "\n",
      "Epoch:  5677  Learning Rate:  0.00034272400220842884  Varinance:  0.008952417563408633 \n",
      "\n",
      "Epoch:  5678  Learning Rate:  0.00034238144951111534  Varinance:  0.008939911364217177 \n",
      "\n",
      "Epoch:  5679  Learning Rate:  0.00034203923919527956  Varinance:  0.008927422635727593 \n",
      "\n",
      "Epoch:  5680  Learning Rate:  0.00034169737091871145  Varinance:  0.008914951353533943 \n",
      "\n",
      "Epoch:  5681  Learning Rate:  0.00034135584433954305  Varinance:  0.008902497493264383 \n",
      "\n",
      "Epoch:  5682  Learning Rate:  0.0003410146591162472  Varinance:  0.00889006103058114 \n",
      "\n",
      "Epoch:  5683  Learning Rate:  0.0003406738149076388  Varinance:  0.00887764194118041 \n",
      "\n",
      "Epoch:  5684  Learning Rate:  0.00034033331137287405  Varinance:  0.008865240200792365 \n",
      "\n",
      "Epoch:  5685  Learning Rate:  0.00033999314817144866  Varinance:  0.008852855785181059 \n",
      "\n",
      "Epoch:  5686  Learning Rate:  0.0003396533249632001  Varinance:  0.008840488670144427 \n",
      "\n",
      "Epoch:  5687  Learning Rate:  0.00033931384140830457  Varinance:  0.008828138831514192 \n",
      "\n",
      "Epoch:  5688  Learning Rate:  0.0003389746971672787  Varinance:  0.00881580624515584 \n",
      "\n",
      "Epoch:  5689  Learning Rate:  0.00033863589190097853  Varinance:  0.0088034908869686 \n",
      "\n",
      "Epoch:  5690  Learning Rate:  0.0003382974252705982  Varinance:  0.008791192732885327 \n",
      "\n",
      "Epoch:  5691  Learning Rate:  0.00033795929693767126  Varinance:  0.008778911758872535 \n",
      "\n",
      "Epoch:  5692  Learning Rate:  0.0003376215065640698  Varinance:  0.00876664794093028 \n",
      "\n",
      "Epoch:  5693  Learning Rate:  0.0003372840538120027  Varinance:  0.00875440125509218 \n",
      "\n",
      "Epoch:  5694  Learning Rate:  0.0003369469383440178  Varinance:  0.0087421716774253 \n",
      "\n",
      "Epoch:  5695  Learning Rate:  0.00033661015982299906  Varinance:  0.008729959184030144 \n",
      "\n",
      "Epoch:  5696  Learning Rate:  0.00033627371791216824  Varinance:  0.00871776375104063 \n",
      "\n",
      "Epoch:  5697  Learning Rate:  0.0003359376122750836  Varinance:  0.00870558535462398 \n",
      "\n",
      "Epoch:  5698  Learning Rate:  0.0003356018425756389  Varinance:  0.00869342397098074 \n",
      "\n",
      "Epoch:  5699  Learning Rate:  0.0003352664084780648  Varinance:  0.008681279576344684 \n",
      "\n",
      "Epoch:  5700  Learning Rate:  0.00033493130964692736  Varinance:  0.008669152146982783 \n",
      "\n",
      "Epoch:  5701  Learning Rate:  0.0003345965457471272  Varinance:  0.008657041659195185 \n",
      "\n",
      "Epoch:  5702  Learning Rate:  0.000334262116443901  Varinance:  0.008644948089315117 \n",
      "\n",
      "Epoch:  5703  Learning Rate:  0.0003339280214028188  Varinance:  0.008632871413708897 \n",
      "\n",
      "Epoch:  5704  Learning Rate:  0.0003335942602897858  Varinance:  0.008620811608775831 \n",
      "\n",
      "Epoch:  5705  Learning Rate:  0.0003332608327710412  Varinance:  0.008608768650948218 \n",
      "\n",
      "Epoch:  5706  Learning Rate:  0.0003329277385131569  Varinance:  0.008596742516691265 \n",
      "\n",
      "Epoch:  5707  Learning Rate:  0.00033259497718303877  Varinance:  0.008584733182503053 \n",
      "\n",
      "Epoch:  5708  Learning Rate:  0.00033226254844792585  Varinance:  0.008572740624914513 \n",
      "\n",
      "Epoch:  5709  Learning Rate:  0.0003319304519753888  Varinance:  0.00856076482048934 \n",
      "\n",
      "Epoch:  5710  Learning Rate:  0.0003315986874333316  Varinance:  0.00854880574582399 \n",
      "\n",
      "Epoch:  5711  Learning Rate:  0.0003312672544899893  Varinance:  0.00853686337754759 \n",
      "\n",
      "Epoch:  5712  Learning Rate:  0.00033093615281392904  Varinance:  0.008524937692321941 \n",
      "\n",
      "Epoch:  5713  Learning Rate:  0.00033060538207404947  Varinance:  0.008513028666841423 \n",
      "\n",
      "Epoch:  5714  Learning Rate:  0.0003302749419395792  Varinance:  0.008501136277832978 \n",
      "\n",
      "Epoch:  5715  Learning Rate:  0.00032994483208007843  Varinance:  0.008489260502056077 \n",
      "\n",
      "Epoch:  5716  Learning Rate:  0.0003296150521654375  Varinance:  0.008477401316302632 \n",
      "\n",
      "Epoch:  5717  Learning Rate:  0.00032928560186587595  Varinance:  0.008465558697397 \n",
      "\n",
      "Epoch:  5718  Learning Rate:  0.00032895648085194397  Varinance:  0.008453732622195889 \n",
      "\n",
      "Epoch:  5719  Learning Rate:  0.00032862768879451997  Varinance:  0.008441923067588366 \n",
      "\n",
      "Epoch:  5720  Learning Rate:  0.0003282992253648121  Varinance:  0.008430130010495758 \n",
      "\n",
      "Epoch:  5721  Learning Rate:  0.00032797109023435735  Varinance:  0.008418353427871636 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5722  Learning Rate:  0.00032764328307501977  Varinance:  0.008406593296701783 \n",
      "\n",
      "Epoch:  5723  Learning Rate:  0.0003273158035589926  Varinance:  0.008394849594004107 \n",
      "\n",
      "Epoch:  5724  Learning Rate:  0.0003269886513587966  Varinance:  0.008383122296828645 \n",
      "\n",
      "Epoch:  5725  Learning Rate:  0.00032666182614727885  Varinance:  0.008371411382257493 \n",
      "\n",
      "Epoch:  5726  Learning Rate:  0.00032633532759761486  Varinance:  0.00835971682740473 \n",
      "\n",
      "Epoch:  5727  Learning Rate:  0.0003260091553833053  Varinance:  0.008348038609416438 \n",
      "\n",
      "Epoch:  5728  Learning Rate:  0.0003256833091781783  Varinance:  0.008336376705470635 \n",
      "\n",
      "Epoch:  5729  Learning Rate:  0.0003253577886563879  Varinance:  0.008324731092777168 \n",
      "\n",
      "Epoch:  5730  Learning Rate:  0.000325032593492413  Varinance:  0.008313101748577774 \n",
      "\n",
      "Epoch:  5731  Learning Rate:  0.00032470772336105864  Varinance:  0.00830148865014596 \n",
      "\n",
      "Epoch:  5732  Learning Rate:  0.00032438317793745505  Varinance:  0.008289891774787 \n",
      "\n",
      "Epoch:  5733  Learning Rate:  0.00032405895689705607  Varinance:  0.008278311099837829 \n",
      "\n",
      "Epoch:  5734  Learning Rate:  0.0003237350599156413  Varinance:  0.008266746602667078 \n",
      "\n",
      "Epoch:  5735  Learning Rate:  0.00032341148666931315  Varinance:  0.008255198260675 \n",
      "\n",
      "Epoch:  5736  Learning Rate:  0.00032308823683449865  Varinance:  0.008243666051293376 \n",
      "\n",
      "Epoch:  5737  Learning Rate:  0.0003227653100879482  Varinance:  0.008232149951985552 \n",
      "\n",
      "Epoch:  5738  Learning Rate:  0.0003224427061067344  Varinance:  0.008220649940246346 \n",
      "\n",
      "Epoch:  5739  Learning Rate:  0.00032212042456825356  Varinance:  0.00820916599360203 \n",
      "\n",
      "Epoch:  5740  Learning Rate:  0.0003217984651502244  Varinance:  0.008197698089610225 \n",
      "\n",
      "Epoch:  5741  Learning Rate:  0.000321476827530687  Varinance:  0.00818624620585995 \n",
      "\n",
      "Epoch:  5742  Learning Rate:  0.0003211555113880042  Varinance:  0.008174810319971526 \n",
      "\n",
      "Epoch:  5743  Learning Rate:  0.00032083451640085923  Varinance:  0.0081633904095965 \n",
      "\n",
      "Epoch:  5744  Learning Rate:  0.0003205138422482574  Varinance:  0.008151986452417678 \n",
      "\n",
      "Epoch:  5745  Learning Rate:  0.0003201934886095249  Varinance:  0.008140598426149032 \n",
      "\n",
      "Epoch:  5746  Learning Rate:  0.0003198734551643073  Varinance:  0.008129226308535678 \n",
      "\n",
      "Epoch:  5747  Learning Rate:  0.0003195537415925715  Varinance:  0.008117870077353777 \n",
      "\n",
      "Epoch:  5748  Learning Rate:  0.0003192343475746043  Varinance:  0.008106529710410582 \n",
      "\n",
      "Epoch:  5749  Learning Rate:  0.00031891527279101094  Varinance:  0.008095205185544348 \n",
      "\n",
      "Epoch:  5750  Learning Rate:  0.00031859651692271723  Varinance:  0.008083896480624247 \n",
      "\n",
      "Epoch:  5751  Learning Rate:  0.0003182780796509667  Varinance:  0.008072603573550406 \n",
      "\n",
      "Epoch:  5752  Learning Rate:  0.0003179599606573224  Varinance:  0.008061326442253826 \n",
      "\n",
      "Epoch:  5753  Learning Rate:  0.0003176421596236655  Varinance:  0.0080500650646963 \n",
      "\n",
      "Epoch:  5754  Learning Rate:  0.0003173246762321944  Varinance:  0.008038819418870443 \n",
      "\n",
      "Epoch:  5755  Learning Rate:  0.0003170075101654263  Varinance:  0.008027589482799602 \n",
      "\n",
      "Epoch:  5756  Learning Rate:  0.0003166906611061944  Varinance:  0.008016375234537843 \n",
      "\n",
      "Epoch:  5757  Learning Rate:  0.0003163741287376501  Varinance:  0.00800517665216984 \n",
      "\n",
      "Epoch:  5758  Learning Rate:  0.0003160579127432612  Varinance:  0.007993993713810928 \n",
      "\n",
      "Epoch:  5759  Learning Rate:  0.00031574201280681105  Varinance:  0.007982826397607017 \n",
      "\n",
      "Epoch:  5760  Learning Rate:  0.0003154264286124  Varinance:  0.007971674681734497 \n",
      "\n",
      "Epoch:  5761  Learning Rate:  0.0003151111598444442  Varinance:  0.007960538544400292 \n",
      "\n",
      "Epoch:  5762  Learning Rate:  0.00031479620618767414  Varinance:  0.007949417963841757 \n",
      "\n",
      "Epoch:  5763  Learning Rate:  0.0003144815673271368  Varinance:  0.007938312918326657 \n",
      "\n",
      "Epoch:  5764  Learning Rate:  0.0003141672429481927  Varinance:  0.007927223386153082 \n",
      "\n",
      "Epoch:  5765  Learning Rate:  0.0003138532327365178  Varinance:  0.007916149345649473 \n",
      "\n",
      "Epoch:  5766  Learning Rate:  0.000313539536378102  Varinance:  0.007905090775174551 \n",
      "\n",
      "Epoch:  5767  Learning Rate:  0.00031322615355924843  Varinance:  0.007894047653117225 \n",
      "\n",
      "Epoch:  5768  Learning Rate:  0.0003129130839665746  Varinance:  0.007883019957896633 \n",
      "\n",
      "Epoch:  5769  Learning Rate:  0.000312600327287011  Varinance:  0.007872007667962054 \n",
      "\n",
      "Epoch:  5770  Learning Rate:  0.00031228788320780053  Varinance:  0.007861010761792879 \n",
      "\n",
      "Epoch:  5771  Learning Rate:  0.0003119757514164995  Varinance:  0.007850029217898524 \n",
      "\n",
      "Epoch:  5772  Learning Rate:  0.0003116639316009756  Varinance:  0.00783906301481847 \n",
      "\n",
      "Epoch:  5773  Learning Rate:  0.00031135242344940936  Varinance:  0.007828112131122172 \n",
      "\n",
      "Epoch:  5774  Learning Rate:  0.00031104122665029274  Varinance:  0.00781717654540899 \n",
      "\n",
      "Epoch:  5775  Learning Rate:  0.00031073034089242844  Varinance:  0.00780625623630821 \n",
      "\n",
      "Epoch:  5776  Learning Rate:  0.00031041976586493087  Varinance:  0.007795351182478968 \n",
      "\n",
      "Epoch:  5777  Learning Rate:  0.0003101095012572254  Varinance:  0.0077844613626102226 \n",
      "\n",
      "Epoch:  5778  Learning Rate:  0.0003097995467590467  Varinance:  0.007773586755420662 \n",
      "\n",
      "Epoch:  5779  Learning Rate:  0.00030948990206044086  Varinance:  0.007762727339658743 \n",
      "\n",
      "Epoch:  5780  Learning Rate:  0.00030918056685176257  Varinance:  0.007751883094102612 \n",
      "\n",
      "Epoch:  5781  Learning Rate:  0.0003088715408236769  Varinance:  0.007741053997560022 \n",
      "\n",
      "Epoch:  5782  Learning Rate:  0.0003085628236671581  Varinance:  0.00773024002886837 \n",
      "\n",
      "Epoch:  5783  Learning Rate:  0.0003082544150734884  Varinance:  0.007719441166894599 \n",
      "\n",
      "Epoch:  5784  Learning Rate:  0.0003079463147342594  Varinance:  0.007708657390535197 \n",
      "\n",
      "Epoch:  5785  Learning Rate:  0.00030763852234137116  Varinance:  0.007697888678716082 \n",
      "\n",
      "Epoch:  5786  Learning Rate:  0.0003073310375870306  Varinance:  0.007687135010392659 \n",
      "\n",
      "Epoch:  5787  Learning Rate:  0.00030702386016375345  Varinance:  0.007676396364549731 \n",
      "\n",
      "Epoch:  5788  Learning Rate:  0.0003067169897643619  Varinance:  0.007665672720201412 \n",
      "\n",
      "Epoch:  5789  Learning Rate:  0.0003064104260819855  Varinance:  0.007654964056391181 \n",
      "\n",
      "Epoch:  5790  Learning Rate:  0.00030610416881006117  Varinance:  0.007644270352191775 \n",
      "\n",
      "Epoch:  5791  Learning Rate:  0.00030579821764233073  Varinance:  0.007633591586705178 \n",
      "\n",
      "Epoch:  5792  Learning Rate:  0.0003054925722728435  Varinance:  0.007622927739062529 \n",
      "\n",
      "Epoch:  5793  Learning Rate:  0.00030518723239595426  Varinance:  0.007612278788424161 \n",
      "\n",
      "Epoch:  5794  Learning Rate:  0.0003048821977063226  Varinance:  0.007601644713979519 \n",
      "\n",
      "Epoch:  5795  Learning Rate:  0.0003045774678989143  Varinance:  0.007591025494947078 \n",
      "\n",
      "Epoch:  5796  Learning Rate:  0.000304273042668999  Varinance:  0.007580421110574384 \n",
      "\n",
      "Epoch:  5797  Learning Rate:  0.00030396892171215175  Varinance:  0.007569831540137965 \n",
      "\n",
      "Epoch:  5798  Learning Rate:  0.0003036651047242518  Varinance:  0.0075592567629433035 \n",
      "\n",
      "Epoch:  5799  Learning Rate:  0.00030336159140148164  Varinance:  0.007548696758324759 \n",
      "\n",
      "Epoch:  5800  Learning Rate:  0.0003030583814403281  Varinance:  0.007538151505645597 \n",
      "\n",
      "Epoch:  5801  Learning Rate:  0.0003027554745375816  Varinance:  0.007527620984297912 \n",
      "\n",
      "Epoch:  5802  Learning Rate:  0.0003024528703903345  Varinance:  0.0075171051737025485 \n",
      "\n",
      "Epoch:  5803  Learning Rate:  0.0003021505686959833  Varinance:  0.007506604053309136 \n",
      "\n",
      "Epoch:  5804  Learning Rate:  0.00030184856915222574  Varinance:  0.007496117602596002 \n",
      "\n",
      "Epoch:  5805  Learning Rate:  0.00030154687145706247  Varinance:  0.0074856458010701555 \n",
      "\n",
      "Epoch:  5806  Learning Rate:  0.00030124547530879607  Varinance:  0.007475188628267189 \n",
      "\n",
      "Epoch:  5807  Learning Rate:  0.0003009443804060298  Varinance:  0.0074647460637513275 \n",
      "\n",
      "Epoch:  5808  Learning Rate:  0.00030064358644766903  Varinance:  0.007454318087115344 \n",
      "\n",
      "Epoch:  5809  Learning Rate:  0.00030034309313292  Varinance:  0.007443904677980477 \n",
      "\n",
      "Epoch:  5810  Learning Rate:  0.00030004290016128886  Varinance:  0.007433505815996476 \n",
      "\n",
      "Epoch:  5811  Learning Rate:  0.0002997430072325831  Varinance:  0.007423121480841503 \n",
      "\n",
      "Epoch:  5812  Learning Rate:  0.00029944341404690943  Varinance:  0.007412751652222124 \n",
      "\n",
      "Epoch:  5813  Learning Rate:  0.0002991441203046747  Varinance:  0.007402396309873214 \n",
      "\n",
      "Epoch:  5814  Learning Rate:  0.0002988451257065854  Varinance:  0.007392055433557998 \n",
      "\n",
      "Epoch:  5815  Learning Rate:  0.0002985464299536465  Varinance:  0.00738172900306797 \n",
      "\n",
      "Epoch:  5816  Learning Rate:  0.00029824803274716244  Varinance:  0.007371416998222816 \n",
      "\n",
      "Epoch:  5817  Learning Rate:  0.00029794993378873627  Varinance:  0.007361119398870451 \n",
      "\n",
      "Epoch:  5818  Learning Rate:  0.00029765213278026836  Varinance:  0.007350836184886942 \n",
      "\n",
      "Epoch:  5819  Learning Rate:  0.0002973546294239584  Varinance:  0.0073405673361764305 \n",
      "\n",
      "Epoch:  5820  Learning Rate:  0.0002970574234223023  Varinance:  0.007330312832671171 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5821  Learning Rate:  0.00029676051447809443  Varinance:  0.007320072654331437 \n",
      "\n",
      "Epoch:  5822  Learning Rate:  0.00029646390229442603  Varinance:  0.0073098467811455094 \n",
      "\n",
      "Epoch:  5823  Learning Rate:  0.00029616758657468434  Varinance:  0.0072996351931295875 \n",
      "\n",
      "Epoch:  5824  Learning Rate:  0.0002958715670225539  Varinance:  0.0072894378703278265 \n",
      "\n",
      "Epoch:  5825  Learning Rate:  0.00029557584334201543  Varinance:  0.007279254792812254 \n",
      "\n",
      "Epoch:  5826  Learning Rate:  0.0002952804152373447  Varinance:  0.007269085940682706 \n",
      "\n",
      "Epoch:  5827  Learning Rate:  0.000294985282413114  Varinance:  0.0072589312940668504 \n",
      "\n",
      "Epoch:  5828  Learning Rate:  0.0002946904445741901  Varinance:  0.007248790833120106 \n",
      "\n",
      "Epoch:  5829  Learning Rate:  0.0002943959014257353  Varinance:  0.0072386645380256255 \n",
      "\n",
      "Epoch:  5830  Learning Rate:  0.00029410165267320667  Varinance:  0.007228552388994207 \n",
      "\n",
      "Epoch:  5831  Learning Rate:  0.0002938076980223551  Varinance:  0.0072184543662643305 \n",
      "\n",
      "Epoch:  5832  Learning Rate:  0.0002935140371792259  Varinance:  0.007208370450102085 \n",
      "\n",
      "Epoch:  5833  Learning Rate:  0.00029322066985015865  Varinance:  0.007198300620801086 \n",
      "\n",
      "Epoch:  5834  Learning Rate:  0.0002929275957417854  Varinance:  0.00718824485868252 \n",
      "\n",
      "Epoch:  5835  Learning Rate:  0.00029263481456103256  Varinance:  0.007178203144095044 \n",
      "\n",
      "Epoch:  5836  Learning Rate:  0.00029234232601511845  Varinance:  0.00716817545741479 \n",
      "\n",
      "Epoch:  5837  Learning Rate:  0.0002920501298115547  Varinance:  0.007158161779045255 \n",
      "\n",
      "Epoch:  5838  Learning Rate:  0.00029175822565814536  Varinance:  0.00714816208941736 \n",
      "\n",
      "Epoch:  5839  Learning Rate:  0.0002914666132629857  Varinance:  0.007138176368989355 \n",
      "\n",
      "Epoch:  5840  Learning Rate:  0.0002911752923344637  Varinance:  0.007128204598246756 \n",
      "\n",
      "Epoch:  5841  Learning Rate:  0.00029088426258125843  Varinance:  0.007118246757702376 \n",
      "\n",
      "Epoch:  5842  Learning Rate:  0.00029059352371233973  Varinance:  0.00710830282789624 \n",
      "\n",
      "Epoch:  5843  Learning Rate:  0.00029030307543696934  Varinance:  0.007098372789395567 \n",
      "\n",
      "Epoch:  5844  Learning Rate:  0.0002900129174646982  Varinance:  0.007088456622794686 \n",
      "\n",
      "Epoch:  5845  Learning Rate:  0.0002897230495053687  Varinance:  0.0070785543087150795 \n",
      "\n",
      "Epoch:  5846  Learning Rate:  0.00028943347126911316  Varinance:  0.007068665827805291 \n",
      "\n",
      "Epoch:  5847  Learning Rate:  0.00028914418246635276  Varinance:  0.007058791160740865 \n",
      "\n",
      "Epoch:  5848  Learning Rate:  0.00028885518280779887  Varinance:  0.00704893028822438 \n",
      "\n",
      "Epoch:  5849  Learning Rate:  0.00028856647200445215  Varinance:  0.007039083190985357 \n",
      "\n",
      "Epoch:  5850  Learning Rate:  0.0002882780497676012  Varinance:  0.007029249849780251 \n",
      "\n",
      "Epoch:  5851  Learning Rate:  0.00028798991580882433  Varinance:  0.007019430245392361 \n",
      "\n",
      "Epoch:  5852  Learning Rate:  0.00028770206983998696  Varinance:  0.007009624358631871 \n",
      "\n",
      "Epoch:  5853  Learning Rate:  0.00028741451157324345  Varinance:  0.00699983217033577 \n",
      "\n",
      "Epoch:  5854  Learning Rate:  0.0002871272407210357  Varinance:  0.006990053661367779 \n",
      "\n",
      "Epoch:  5855  Learning Rate:  0.00028684025699609233  Varinance:  0.006980288812618393 \n",
      "\n",
      "Epoch:  5856  Learning Rate:  0.00028655356011142994  Varinance:  0.006970537605004786 \n",
      "\n",
      "Epoch:  5857  Learning Rate:  0.0002862671497803517  Varinance:  0.006960800019470803 \n",
      "\n",
      "Epoch:  5858  Learning Rate:  0.00028598102571644686  Varinance:  0.006951076036986876 \n",
      "\n",
      "Epoch:  5859  Learning Rate:  0.00028569518763359186  Varinance:  0.006941365638550051 \n",
      "\n",
      "Epoch:  5860  Learning Rate:  0.000285409635245948  Varinance:  0.006931668805183928 \n",
      "\n",
      "Epoch:  5861  Learning Rate:  0.00028512436826796326  Varinance:  0.006921985517938575 \n",
      "\n",
      "Epoch:  5862  Learning Rate:  0.0002848393864143707  Varinance:  0.0069123157578905685 \n",
      "\n",
      "Epoch:  5863  Learning Rate:  0.0002845546894001881  Varinance:  0.006902659506142912 \n",
      "\n",
      "Epoch:  5864  Learning Rate:  0.00028427027694071855  Varinance:  0.006893016743825015 \n",
      "\n",
      "Epoch:  5865  Learning Rate:  0.00028398614875154994  Varinance:  0.006883387452092615 \n",
      "\n",
      "Epoch:  5866  Learning Rate:  0.00028370230454855347  Varinance:  0.006873771612127811 \n",
      "\n",
      "Epoch:  5867  Learning Rate:  0.00028341874404788544  Varinance:  0.006864169205138988 \n",
      "\n",
      "Epoch:  5868  Learning Rate:  0.0002831354669659848  Varinance:  0.006854580212360746 \n",
      "\n",
      "Epoch:  5869  Learning Rate:  0.0002828524730195748  Varinance:  0.0068450046150539375 \n",
      "\n",
      "Epoch:  5870  Learning Rate:  0.0002825697619256616  Varinance:  0.00683544239450558 \n",
      "\n",
      "Epoch:  5871  Learning Rate:  0.00028228733340153364  Varinance:  0.006825893532028845 \n",
      "\n",
      "Epoch:  5872  Learning Rate:  0.0002820051871647626  Varinance:  0.00681635800896297 \n",
      "\n",
      "Epoch:  5873  Learning Rate:  0.0002817233229332024  Varinance:  0.006806835806673299 \n",
      "\n",
      "Epoch:  5874  Learning Rate:  0.00028144174042498847  Varinance:  0.006797326906551211 \n",
      "\n",
      "Epoch:  5875  Learning Rate:  0.00028116043935853856  Varinance:  0.006787831290014038 \n",
      "\n",
      "Epoch:  5876  Learning Rate:  0.0002808794194525513  Varinance:  0.006778348938505113 \n",
      "\n",
      "Epoch:  5877  Learning Rate:  0.00028059868042600684  Varinance:  0.006768879833493676 \n",
      "\n",
      "Epoch:  5878  Learning Rate:  0.0002803182219981664  Varinance:  0.0067594239564748694 \n",
      "\n",
      "Epoch:  5879  Learning Rate:  0.00028003804388857114  Varinance:  0.006749981288969649 \n",
      "\n",
      "Epoch:  5880  Learning Rate:  0.0002797581458170433  Varinance:  0.006740551812524817 \n",
      "\n",
      "Epoch:  5881  Learning Rate:  0.0002794785275036844  Varinance:  0.00673113550871296 \n",
      "\n",
      "Epoch:  5882  Learning Rate:  0.00027919918866887625  Varinance:  0.006721732359132367 \n",
      "\n",
      "Epoch:  5883  Learning Rate:  0.0002789201290332803  Varinance:  0.006712342345407075 \n",
      "\n",
      "Epoch:  5884  Learning Rate:  0.00027864134831783637  Varinance:  0.0067029654491867815 \n",
      "\n",
      "Epoch:  5885  Learning Rate:  0.00027836284624376396  Varinance:  0.0066936016521467944 \n",
      "\n",
      "Epoch:  5886  Learning Rate:  0.0002780846225325613  Varinance:  0.0066842509359880484 \n",
      "\n",
      "Epoch:  5887  Learning Rate:  0.0002778066769060041  Varinance:  0.006674913282437031 \n",
      "\n",
      "Epoch:  5888  Learning Rate:  0.0002775290090861471  Varinance:  0.006665588673245773 \n",
      "\n",
      "Epoch:  5889  Learning Rate:  0.0002772516187953221  Varinance:  0.006656277090191754 \n",
      "\n",
      "Epoch:  5890  Learning Rate:  0.0002769745057561391  Varinance:  0.006646978515077953 \n",
      "\n",
      "Epoch:  5891  Learning Rate:  0.0002766976696914851  Varinance:  0.006637692929732765 \n",
      "\n",
      "Epoch:  5892  Learning Rate:  0.0002764211103245236  Varinance:  0.006628420316009937 \n",
      "\n",
      "Epoch:  5893  Learning Rate:  0.0002761448273786955  Varinance:  0.006619160655788597 \n",
      "\n",
      "Epoch:  5894  Learning Rate:  0.000275868820577718  Varinance:  0.006609913930973181 \n",
      "\n",
      "Epoch:  5895  Learning Rate:  0.00027559308964558385  Varinance:  0.00660068012349341 \n",
      "\n",
      "Epoch:  5896  Learning Rate:  0.0002753176343065625  Varinance:  0.006591459215304218 \n",
      "\n",
      "Epoch:  5897  Learning Rate:  0.00027504245428519825  Varinance:  0.006582251188385782 \n",
      "\n",
      "Epoch:  5898  Learning Rate:  0.0002747675493063111  Varinance:  0.006573056024743448 \n",
      "\n",
      "Epoch:  5899  Learning Rate:  0.00027449291909499644  Varinance:  0.006563873706407668 \n",
      "\n",
      "Epoch:  5900  Learning Rate:  0.0002742185633766235  Varinance:  0.006554704215434031 \n",
      "\n",
      "Epoch:  5901  Learning Rate:  0.00027394448187683686  Varinance:  0.0065455475339031835 \n",
      "\n",
      "Epoch:  5902  Learning Rate:  0.0002736706743215551  Varinance:  0.006536403643920816 \n",
      "\n",
      "Epoch:  5903  Learning Rate:  0.00027339714043697025  Varinance:  0.006527272527617577 \n",
      "\n",
      "Epoch:  5904  Learning Rate:  0.00027312387994954883  Varinance:  0.00651815416714912 \n",
      "\n",
      "Epoch:  5905  Learning Rate:  0.0002728508925860299  Varinance:  0.006509048544696022 \n",
      "\n",
      "Epoch:  5906  Learning Rate:  0.0002725781780734263  Varinance:  0.006499955642463719 \n",
      "\n",
      "Epoch:  5907  Learning Rate:  0.0002723057361390237  Varinance:  0.00649087544268254 \n",
      "\n",
      "Epoch:  5908  Learning Rate:  0.0002720335665103797  Varinance:  0.006481807927607628 \n",
      "\n",
      "Epoch:  5909  Learning Rate:  0.0002717616689153249  Varinance:  0.006472753079518922 \n",
      "\n",
      "Epoch:  5910  Learning Rate:  0.00027149004308196187  Varinance:  0.0064637108807210865 \n",
      "\n",
      "Epoch:  5911  Learning Rate:  0.00027121868873866434  Varinance:  0.006454681313543537 \n",
      "\n",
      "Epoch:  5912  Learning Rate:  0.00027094760561407837  Varinance:  0.006445664360340372 \n",
      "\n",
      "Epoch:  5913  Learning Rate:  0.0002706767934371204  Varinance:  0.00643666000349031 \n",
      "\n",
      "Epoch:  5914  Learning Rate:  0.00027040625193697833  Varinance:  0.0064276682253967205 \n",
      "\n",
      "Epoch:  5915  Learning Rate:  0.00027013598084311106  Varinance:  0.006418689008487539 \n",
      "\n",
      "Epoch:  5916  Learning Rate:  0.0002698659798852469  Varinance:  0.006409722335215266 \n",
      "\n",
      "Epoch:  5917  Learning Rate:  0.0002695962487933851  Varinance:  0.006400768188056873 \n",
      "\n",
      "Epoch:  5918  Learning Rate:  0.00026932678729779474  Varinance:  0.006391826549513855 \n",
      "\n",
      "Epoch:  5919  Learning Rate:  0.00026905759512901395  Varinance:  0.006382897402112142 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5920  Learning Rate:  0.0002687886720178509  Varinance:  0.006373980728402046 \n",
      "\n",
      "Epoch:  5921  Learning Rate:  0.00026852001769538206  Varinance:  0.0063650765109582895 \n",
      "\n",
      "Epoch:  5922  Learning Rate:  0.0002682516318929533  Varinance:  0.006356184732379923 \n",
      "\n",
      "Epoch:  5923  Learning Rate:  0.000267983514342179  Varinance:  0.006347305375290321 \n",
      "\n",
      "Epoch:  5924  Learning Rate:  0.00026771566477494117  Varinance:  0.006338438422337093 \n",
      "\n",
      "Epoch:  5925  Learning Rate:  0.00026744808292339037  Varinance:  0.006329583856192132 \n",
      "\n",
      "Epoch:  5926  Learning Rate:  0.00026718076851994506  Varinance:  0.006320741659551527 \n",
      "\n",
      "Epoch:  5927  Learning Rate:  0.00026691372129729026  Varinance:  0.006311911815135512 \n",
      "\n",
      "Epoch:  5928  Learning Rate:  0.0002666469409883793  Varinance:  0.006303094305688496 \n",
      "\n",
      "Epoch:  5929  Learning Rate:  0.0002663804273264312  Varinance:  0.006294289113978978 \n",
      "\n",
      "Epoch:  5930  Learning Rate:  0.00026611418004493276  Varinance:  0.006285496222799539 \n",
      "\n",
      "Epoch:  5931  Learning Rate:  0.0002658481988776367  Varinance:  0.0062767156149667715 \n",
      "\n",
      "Epoch:  5932  Learning Rate:  0.0002655824835585615  Varinance:  0.006267947273321298 \n",
      "\n",
      "Epoch:  5933  Learning Rate:  0.0002653170338219919  Varinance:  0.006259191180727718 \n",
      "\n",
      "Epoch:  5934  Learning Rate:  0.0002650518494024785  Varinance:  0.00625044732007453 \n",
      "\n",
      "Epoch:  5935  Learning Rate:  0.0002647869300348364  Varinance:  0.006241715674274176 \n",
      "\n",
      "Epoch:  5936  Learning Rate:  0.0002645222754541466  Varinance:  0.006232996226262953 \n",
      "\n",
      "Epoch:  5937  Learning Rate:  0.00026425788539575406  Varinance:  0.006224288959001006 \n",
      "\n",
      "Epoch:  5938  Learning Rate:  0.0002639937595952689  Varinance:  0.006215593855472255 \n",
      "\n",
      "Epoch:  5939  Learning Rate:  0.0002637298977885657  Varinance:  0.00620691089868442 \n",
      "\n",
      "Epoch:  5940  Learning Rate:  0.00026346629971178194  Varinance:  0.006198240071668962 \n",
      "\n",
      "Epoch:  5941  Learning Rate:  0.00026320296510131986  Varinance:  0.006189581357481009 \n",
      "\n",
      "Epoch:  5942  Learning Rate:  0.000262939893693845  Varinance:  0.006180934739199399 \n",
      "\n",
      "Epoch:  5943  Learning Rate:  0.00026267708522628556  Varinance:  0.006172300199926591 \n",
      "\n",
      "Epoch:  5944  Learning Rate:  0.0002624145394358335  Varinance:  0.006163677722788667 \n",
      "\n",
      "Epoch:  5945  Learning Rate:  0.00026215225605994243  Varinance:  0.006155067290935241 \n",
      "\n",
      "Epoch:  5946  Learning Rate:  0.0002618902348363293  Varinance:  0.006146468887539506 \n",
      "\n",
      "Epoch:  5947  Learning Rate:  0.00026162847550297307  Varinance:  0.006137882495798158 \n",
      "\n",
      "Epoch:  5948  Learning Rate:  0.0002613669777981139  Varinance:  0.006129308098931332 \n",
      "\n",
      "Epoch:  5949  Learning Rate:  0.00026110574146025433  Varinance:  0.00612074568018264 \n",
      "\n",
      "Epoch:  5950  Learning Rate:  0.0002608447662281582  Varinance:  0.006112195222819102 \n",
      "\n",
      "Epoch:  5951  Learning Rate:  0.0002605840518408498  Varinance:  0.00610365671013108 \n",
      "\n",
      "Epoch:  5952  Learning Rate:  0.0002603235980376152  Varinance:  0.0060951301254323085 \n",
      "\n",
      "Epoch:  5953  Learning Rate:  0.00026006340455800016  Varinance:  0.006086615452059825 \n",
      "\n",
      "Epoch:  5954  Learning Rate:  0.00025980347114181124  Varinance:  0.006078112673373955 \n",
      "\n",
      "Epoch:  5955  Learning Rate:  0.0002595437975291154  Varinance:  0.006069621772758235 \n",
      "\n",
      "Epoch:  5956  Learning Rate:  0.0002592843834602385  Varinance:  0.006061142733619448 \n",
      "\n",
      "Epoch:  5957  Learning Rate:  0.0002590252286757666  Varinance:  0.006052675539387553 \n",
      "\n",
      "Epoch:  5958  Learning Rate:  0.00025876633291654525  Varinance:  0.006044220173515632 \n",
      "\n",
      "Epoch:  5959  Learning Rate:  0.0002585076959236781  Varinance:  0.006035776619479908 \n",
      "\n",
      "Epoch:  5960  Learning Rate:  0.0002582493174385287  Varinance:  0.00602734486077968 \n",
      "\n",
      "Epoch:  5961  Learning Rate:  0.000257991197202718  Varinance:  0.006018924880937305 \n",
      "\n",
      "Epoch:  5962  Learning Rate:  0.000257733334958126  Varinance:  0.006010516663498132 \n",
      "\n",
      "Epoch:  5963  Learning Rate:  0.0002574757304468907  Varinance:  0.006002120192030524 \n",
      "\n",
      "Epoch:  5964  Learning Rate:  0.00025721838341140704  Varinance:  0.0059937354501257995 \n",
      "\n",
      "Epoch:  5965  Learning Rate:  0.00025696129359432826  Varinance:  0.005985362421398168 \n",
      "\n",
      "Epoch:  5966  Learning Rate:  0.00025670446073856466  Varinance:  0.005977001089484763 \n",
      "\n",
      "Epoch:  5967  Learning Rate:  0.00025644788458728307  Varinance:  0.005968651438045561 \n",
      "\n",
      "Epoch:  5968  Learning Rate:  0.00025619156488390757  Varinance:  0.005960313450763381 \n",
      "\n",
      "Epoch:  5969  Learning Rate:  0.0002559355013721181  Varinance:  0.0059519871113437985 \n",
      "\n",
      "Epoch:  5970  Learning Rate:  0.0002556796937958513  Varinance:  0.00594367240351519 \n",
      "\n",
      "Epoch:  5971  Learning Rate:  0.00025542414189929985  Varinance:  0.005935369311028659 \n",
      "\n",
      "Epoch:  5972  Learning Rate:  0.0002551688454269114  Varinance:  0.005927077817657977 \n",
      "\n",
      "Epoch:  5973  Learning Rate:  0.00025491380412338957  Varinance:  0.005918797907199614 \n",
      "\n",
      "Epoch:  5974  Learning Rate:  0.0002546590177336934  Varinance:  0.005910529563472668 \n",
      "\n",
      "Epoch:  5975  Learning Rate:  0.0002544044860030359  Varinance:  0.005902272770318849 \n",
      "\n",
      "Epoch:  5976  Learning Rate:  0.00025415020867688585  Varinance:  0.005894027511602408 \n",
      "\n",
      "Epoch:  5977  Learning Rate:  0.00025389618550096543  Varinance:  0.005885793771210168 \n",
      "\n",
      "Epoch:  5978  Learning Rate:  0.0002536424162212517  Varinance:  0.0058775715330514665 \n",
      "\n",
      "Epoch:  5979  Learning Rate:  0.0002533889005839755  Varinance:  0.005869360781058082 \n",
      "\n",
      "Epoch:  5980  Learning Rate:  0.00025313563833562083  Varinance:  0.0058611614991842705 \n",
      "\n",
      "Epoch:  5981  Learning Rate:  0.00025288262922292556  Varinance:  0.005852973671406699 \n",
      "\n",
      "Epoch:  5982  Learning Rate:  0.0002526298729928808  Varinance:  0.005844797281724426 \n",
      "\n",
      "Epoch:  5983  Learning Rate:  0.0002523773693927299  Varinance:  0.005836632314158827 \n",
      "\n",
      "Epoch:  5984  Learning Rate:  0.0002521251181699696  Varinance:  0.005828478752753635 \n",
      "\n",
      "Epoch:  5985  Learning Rate:  0.0002518731190723483  Varinance:  0.005820336581574875 \n",
      "\n",
      "Epoch:  5986  Learning Rate:  0.00025162137184786705  Varinance:  0.0058122057847107955 \n",
      "\n",
      "Epoch:  5987  Learning Rate:  0.0002513698762447788  Varinance:  0.005804086346271905 \n",
      "\n",
      "Epoch:  5988  Learning Rate:  0.0002511186320115876  Varinance:  0.0057959782503909036 \n",
      "\n",
      "Epoch:  5989  Learning Rate:  0.00025086763889704925  Varinance:  0.00578788148122266 \n",
      "\n",
      "Epoch:  5990  Learning Rate:  0.000250616896650171  Varinance:  0.005779796022944153 \n",
      "\n",
      "Epoch:  5991  Learning Rate:  0.00025036640502021  Varinance:  0.005771721859754494 \n",
      "\n",
      "Epoch:  5992  Learning Rate:  0.00025011616375667515  Varinance:  0.005763658975874869 \n",
      "\n",
      "Epoch:  5993  Learning Rate:  0.00024986617260932463  Varinance:  0.005755607355548475 \n",
      "\n",
      "Epoch:  5994  Learning Rate:  0.0002496164313281676  Varinance:  0.005747566983040548 \n",
      "\n",
      "Epoch:  5995  Learning Rate:  0.00024936693966346287  Varinance:  0.005739537842638302 \n",
      "\n",
      "Epoch:  5996  Learning Rate:  0.0002491176973657184  Varinance:  0.005731519918650906 \n",
      "\n",
      "Epoch:  5997  Learning Rate:  0.0002488687041856921  Varinance:  0.005723513195409418 \n",
      "\n",
      "Epoch:  5998  Learning Rate:  0.00024861995987439085  Varinance:  0.005715517657266817 \n",
      "\n",
      "Epoch:  5999  Learning Rate:  0.00024837146418307  Varinance:  0.005707533288597941 \n",
      "\n",
      "Epoch:  6000  Learning Rate:  0.0002481232168632343  Varinance:  0.005699560073799425 \n",
      "\n",
      "Epoch:  6001  Learning Rate:  0.00024787521766663585  Varinance:  0.005691597997289729 \n",
      "\n",
      "Epoch:  6002  Learning Rate:  0.00024762746634527576  Varinance:  0.005683647043509072 \n",
      "\n",
      "Epoch:  6003  Learning Rate:  0.00024737996265140286  Varinance:  0.00567570719691942 \n",
      "\n",
      "Epoch:  6004  Learning Rate:  0.000247132706337513  Varinance:  0.005667778442004415 \n",
      "\n",
      "Epoch:  6005  Learning Rate:  0.0002468856971563503  Varinance:  0.005659860763269403 \n",
      "\n",
      "Epoch:  6006  Learning Rate:  0.00024663893486090514  Varinance:  0.005651954145241377 \n",
      "\n",
      "Epoch:  6007  Learning Rate:  0.00024639241920441533  Varinance:  0.005644058572468914 \n",
      "\n",
      "Epoch:  6008  Learning Rate:  0.00024614614994036555  Varinance:  0.0056361740295222075 \n",
      "\n",
      "Epoch:  6009  Learning Rate:  0.00024590012682248596  Varinance:  0.0056283005009929955 \n",
      "\n",
      "Epoch:  6010  Learning Rate:  0.00024565434960475366  Varinance:  0.0056204379714945496 \n",
      "\n",
      "Epoch:  6011  Learning Rate:  0.0002454088180413917  Varinance:  0.005612586425661608 \n",
      "\n",
      "Epoch:  6012  Learning Rate:  0.00024516353188686806  Varinance:  0.005604745848150402 \n",
      "\n",
      "Epoch:  6013  Learning Rate:  0.00024491849089589685  Varinance:  0.005596916223638598 \n",
      "\n",
      "Epoch:  6014  Learning Rate:  0.0002446736948234368  Varinance:  0.005589097536825237 \n",
      "\n",
      "Epoch:  6015  Learning Rate:  0.00024442914342469194  Varinance:  0.0055812897724307675 \n",
      "\n",
      "Epoch:  6016  Learning Rate:  0.00024418483645511106  Varinance:  0.005573492915196977 \n",
      "\n",
      "Epoch:  6017  Learning Rate:  0.0002439407736703868  Varinance:  0.005565706949886944 \n",
      "\n",
      "Epoch:  6018  Learning Rate:  0.00024369695482645657  Varinance:  0.005557931861285061 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6019  Learning Rate:  0.00024345337967950163  Varinance:  0.005550167634196964 \n",
      "\n",
      "Epoch:  6020  Learning Rate:  0.00024321004798594649  Varinance:  0.005542414253449527 \n",
      "\n",
      "Epoch:  6021  Learning Rate:  0.00024296695950245975  Varinance:  0.005534671703890789 \n",
      "\n",
      "Epoch:  6022  Learning Rate:  0.0002427241139859526  Varinance:  0.005526939970389987 \n",
      "\n",
      "Epoch:  6023  Learning Rate:  0.00024248151119357967  Varinance:  0.005519219037837494 \n",
      "\n",
      "Epoch:  6024  Learning Rate:  0.0002422391508827383  Varinance:  0.005511508891144762 \n",
      "\n",
      "Epoch:  6025  Learning Rate:  0.00024199703281106786  Varinance:  0.005503809515244349 \n",
      "\n",
      "Epoch:  6026  Learning Rate:  0.00024175515673645036  Varinance:  0.005496120895089854 \n",
      "\n",
      "Epoch:  6027  Learning Rate:  0.00024151352241700994  Varinance:  0.005488443015655904 \n",
      "\n",
      "Epoch:  6028  Learning Rate:  0.00024127212961111183  Varinance:  0.005480775861938085 \n",
      "\n",
      "Epoch:  6029  Learning Rate:  0.00024103097807736372  Varinance:  0.005473119418952977 \n",
      "\n",
      "Epoch:  6030  Learning Rate:  0.0002407900675746135  Varinance:  0.005465473671738086 \n",
      "\n",
      "Epoch:  6031  Learning Rate:  0.00024054939786195096  Varinance:  0.005457838605351796 \n",
      "\n",
      "Epoch:  6032  Learning Rate:  0.0002403089686987065  Varinance:  0.005450214204873389 \n",
      "\n",
      "Epoch:  6033  Learning Rate:  0.00024006877984445062  Varinance:  0.005442600455402983 \n",
      "\n",
      "Epoch:  6034  Learning Rate:  0.00023982883105899455  Varinance:  0.005434997342061521 \n",
      "\n",
      "Epoch:  6035  Learning Rate:  0.00023958912210238974  Varinance:  0.0054274048499907005 \n",
      "\n",
      "Epoch:  6036  Learning Rate:  0.0002393496527349268  Varinance:  0.005419822964353005 \n",
      "\n",
      "Epoch:  6037  Learning Rate:  0.00023911042271713672  Varinance:  0.005412251670331646 \n",
      "\n",
      "Epoch:  6038  Learning Rate:  0.00023887143180978908  Varinance:  0.005404690953130505 \n",
      "\n",
      "Epoch:  6039  Learning Rate:  0.00023863267977389317  Varinance:  0.0053971407979741615 \n",
      "\n",
      "Epoch:  6040  Learning Rate:  0.00023839416637069712  Varinance:  0.005389601190107825 \n",
      "\n",
      "Epoch:  6041  Learning Rate:  0.00023815589136168708  Varinance:  0.005382072114797329 \n",
      "\n",
      "Epoch:  6042  Learning Rate:  0.00023791785450858827  Varinance:  0.0053745535573290594 \n",
      "\n",
      "Epoch:  6043  Learning Rate:  0.00023768005557336404  Varinance:  0.005367045503009985 \n",
      "\n",
      "Epoch:  6044  Learning Rate:  0.00023744249431821492  Varinance:  0.005359547937167601 \n",
      "\n",
      "Epoch:  6045  Learning Rate:  0.00023720517050558013  Varinance:  0.005352060845149872 \n",
      "\n",
      "Epoch:  6046  Learning Rate:  0.0002369680838981354  Varinance:  0.005344584212325257 \n",
      "\n",
      "Epoch:  6047  Learning Rate:  0.00023673123425879434  Varinance:  0.005337118024082647 \n",
      "\n",
      "Epoch:  6048  Learning Rate:  0.00023649462135070747  Varinance:  0.005329662265831354 \n",
      "\n",
      "Epoch:  6049  Learning Rate:  0.0002362582449372614  Varinance:  0.005322216923001043 \n",
      "\n",
      "Epoch:  6050  Learning Rate:  0.00023602210478208005  Varinance:  0.005314781981041762 \n",
      "\n",
      "Epoch:  6051  Learning Rate:  0.0002357862006490233  Varinance:  0.005307357425423885 \n",
      "\n",
      "Epoch:  6052  Learning Rate:  0.00023555053230218663  Varinance:  0.005299943241638055 \n",
      "\n",
      "Epoch:  6053  Learning Rate:  0.0002353150995059021  Varinance:  0.005292539415195211 \n",
      "\n",
      "Epoch:  6054  Learning Rate:  0.0002350799020247365  Varinance:  0.0052851459316265266 \n",
      "\n",
      "Epoch:  6055  Learning Rate:  0.00023484493962349251  Varinance:  0.005277762776483394 \n",
      "\n",
      "Epoch:  6056  Learning Rate:  0.00023461021206720793  Varinance:  0.005270389935337363 \n",
      "\n",
      "Epoch:  6057  Learning Rate:  0.00023437571912115473  Varinance:  0.005263027393780165 \n",
      "\n",
      "Epoch:  6058  Learning Rate:  0.00023414146055084021  Varinance:  0.005255675137423664 \n",
      "\n",
      "Epoch:  6059  Learning Rate:  0.00023390743612200596  Varinance:  0.005248333151899792 \n",
      "\n",
      "Epoch:  6060  Learning Rate:  0.0002336736456006271  Varinance:  0.005241001422860582 \n",
      "\n",
      "Epoch:  6061  Learning Rate:  0.00023344008875291353  Varinance:  0.0052336799359780995 \n",
      "\n",
      "Epoch:  6062  Learning Rate:  0.00023320676534530798  Varinance:  0.0052263686769444375 \n",
      "\n",
      "Epoch:  6063  Learning Rate:  0.00023297367514448714  Varinance:  0.005219067631471647 \n",
      "\n",
      "Epoch:  6064  Learning Rate:  0.00023274081791736114  Varinance:  0.005211776785291767 \n",
      "\n",
      "Epoch:  6065  Learning Rate:  0.00023250819343107224  Varinance:  0.0052044961241567685 \n",
      "\n",
      "Epoch:  6066  Learning Rate:  0.00023227580145299609  Varinance:  0.005197225633838499 \n",
      "\n",
      "Epoch:  6067  Learning Rate:  0.000232043641750741  Varinance:  0.00518996530012871 \n",
      "\n",
      "Epoch:  6068  Learning Rate:  0.0002318117140921468  Varinance:  0.00518271510883899 \n",
      "\n",
      "Epoch:  6069  Learning Rate:  0.00023158001824528617  Varinance:  0.0051754750458007604 \n",
      "\n",
      "Epoch:  6070  Learning Rate:  0.0002313485539784629  Varinance:  0.005168245096865208 \n",
      "\n",
      "Epoch:  6071  Learning Rate:  0.00023111732106021292  Varinance:  0.005161025247903309 \n",
      "\n",
      "Epoch:  6072  Learning Rate:  0.00023088631925930345  Varinance:  0.005153815484805779 \n",
      "\n",
      "Epoch:  6073  Learning Rate:  0.00023065554834473227  Varinance:  0.00514661579348302 \n",
      "\n",
      "Epoch:  6074  Learning Rate:  0.00023042500808572865  Varinance:  0.005139426159865138 \n",
      "\n",
      "Epoch:  6075  Learning Rate:  0.0002301946982517525  Varinance:  0.00513224656990189 \n",
      "\n",
      "Epoch:  6076  Learning Rate:  0.00022996461861249362  Varinance:  0.005125077009562667 \n",
      "\n",
      "Epoch:  6077  Learning Rate:  0.00022973476893787268  Varinance:  0.005117917464836433 \n",
      "\n",
      "Epoch:  6078  Learning Rate:  0.00022950514899803965  Varinance:  0.005110767921731754 \n",
      "\n",
      "Epoch:  6079  Learning Rate:  0.00022927575856337477  Varinance:  0.005103628366276739 \n",
      "\n",
      "Epoch:  6080  Learning Rate:  0.00022904659740448772  Varinance:  0.005096498784518991 \n",
      "\n",
      "Epoch:  6081  Learning Rate:  0.00022881766529221693  Varinance:  0.005089379162525629 \n",
      "\n",
      "Epoch:  6082  Learning Rate:  0.00022858896199763054  Varinance:  0.005082269486383235 \n",
      "\n",
      "Epoch:  6083  Learning Rate:  0.00022836048729202543  Varinance:  0.005075169742197802 \n",
      "\n",
      "Epoch:  6084  Learning Rate:  0.0002281322409469264  Varinance:  0.005068079916094762 \n",
      "\n",
      "Epoch:  6085  Learning Rate:  0.0002279042227340875  Varinance:  0.005060999994218914 \n",
      "\n",
      "Epoch:  6086  Learning Rate:  0.00022767643242549017  Varinance:  0.0050539299627344264 \n",
      "\n",
      "Epoch:  6087  Learning Rate:  0.00022744886979334422  Varinance:  0.005046869807824766 \n",
      "\n",
      "Epoch:  6088  Learning Rate:  0.00022722153461008726  Varinance:  0.005039819515692729 \n",
      "\n",
      "Epoch:  6089  Learning Rate:  0.00022699442664838362  Varinance:  0.005032779072560387 \n",
      "\n",
      "Epoch:  6090  Learning Rate:  0.00022676754568112554  Varinance:  0.005025748464669029 \n",
      "\n",
      "Epoch:  6091  Learning Rate:  0.0002265408914814322  Varinance:  0.005018727678279194 \n",
      "\n",
      "Epoch:  6092  Learning Rate:  0.00022631446382264907  Varinance:  0.005011716699670602 \n",
      "\n",
      "Epoch:  6093  Learning Rate:  0.0002260882624783488  Varinance:  0.005004715515142154 \n",
      "\n",
      "Epoch:  6094  Learning Rate:  0.00022586228722232967  Varinance:  0.004997724111011859 \n",
      "\n",
      "Epoch:  6095  Learning Rate:  0.00022563653782861658  Varinance:  0.004990742473616868 \n",
      "\n",
      "Epoch:  6096  Learning Rate:  0.0002254110140714603  Varinance:  0.004983770589313421 \n",
      "\n",
      "Epoch:  6097  Learning Rate:  0.0002251857157253367  Varinance:  0.004976808444476787 \n",
      "\n",
      "Epoch:  6098  Learning Rate:  0.00022496064256494758  Varinance:  0.004969856025501297 \n",
      "\n",
      "Epoch:  6099  Learning Rate:  0.00022473579436521997  Varinance:  0.004962913318800281 \n",
      "\n",
      "Epoch:  6100  Learning Rate:  0.00022451117090130525  Varinance:  0.004955980310806053 \n",
      "\n",
      "Epoch:  6101  Learning Rate:  0.00022428677194858034  Varinance:  0.004949056987969862 \n",
      "\n",
      "Epoch:  6102  Learning Rate:  0.0002240625972826459  Varinance:  0.004942143336761901 \n",
      "\n",
      "Epoch:  6103  Learning Rate:  0.00022383864667932735  Varinance:  0.004935239343671273 \n",
      "\n",
      "Epoch:  6104  Learning Rate:  0.00022361491991467437  Varinance:  0.004928344995205922 \n",
      "\n",
      "Epoch:  6105  Learning Rate:  0.00022339141676495976  Varinance:  0.00492146027789267 \n",
      "\n",
      "Epoch:  6106  Learning Rate:  0.0002231681370066805  Varinance:  0.00491458517827715 \n",
      "\n",
      "Epoch:  6107  Learning Rate:  0.00022294508041655707  Varinance:  0.0049077196829238 \n",
      "\n",
      "Epoch:  6108  Learning Rate:  0.00022272224677153242  Varinance:  0.004900863778415799 \n",
      "\n",
      "Epoch:  6109  Learning Rate:  0.0002224996358487733  Varinance:  0.004894017451355098 \n",
      "\n",
      "Epoch:  6110  Learning Rate:  0.00022227724742566836  Varinance:  0.0048871806883623616 \n",
      "\n",
      "Epoch:  6111  Learning Rate:  0.0002220550812798294  Varinance:  0.0048803534760769205 \n",
      "\n",
      "Epoch:  6112  Learning Rate:  0.0002218331371890904  Varinance:  0.004873535801156796 \n",
      "\n",
      "Epoch:  6113  Learning Rate:  0.00022161141493150685  Varinance:  0.004866727650278638 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6114  Learning Rate:  0.00022138991428535676  Varinance:  0.004859929010137717 \n",
      "\n",
      "Epoch:  6115  Learning Rate:  0.00022116863502913954  Varinance:  0.004853139867447865 \n",
      "\n",
      "Epoch:  6116  Learning Rate:  0.00022094757694157563  Varinance:  0.004846360208941499 \n",
      "\n",
      "Epoch:  6117  Learning Rate:  0.00022072673980160726  Varinance:  0.004839590021369571 \n",
      "\n",
      "Epoch:  6118  Learning Rate:  0.00022050612338839692  Varinance:  0.004832829291501517 \n",
      "\n",
      "Epoch:  6119  Learning Rate:  0.0002202857274813283  Varinance:  0.004826078006125278 \n",
      "\n",
      "Epoch:  6120  Learning Rate:  0.0002200655518600057  Varinance:  0.004819336152047247 \n",
      "\n",
      "Epoch:  6121  Learning Rate:  0.00021984559630425313  Varinance:  0.004812603716092253 \n",
      "\n",
      "Epoch:  6122  Learning Rate:  0.0002196258605941152  Varinance:  0.004805880685103506 \n",
      "\n",
      "Epoch:  6123  Learning Rate:  0.0002194063445098563  Varinance:  0.004799167045942623 \n",
      "\n",
      "Epoch:  6124  Learning Rate:  0.00021918704783196005  Varinance:  0.004792462785489573 \n",
      "\n",
      "Epoch:  6125  Learning Rate:  0.0002189679703411301  Varinance:  0.004785767890642627 \n",
      "\n",
      "Epoch:  6126  Learning Rate:  0.00021874911181828853  Varinance:  0.004779082348318384 \n",
      "\n",
      "Epoch:  6127  Learning Rate:  0.000218530472044577  Varinance:  0.004772406145451716 \n",
      "\n",
      "Epoch:  6128  Learning Rate:  0.0002183120508013559  Varinance:  0.004765739268995749 \n",
      "\n",
      "Epoch:  6129  Learning Rate:  0.00021809384787020364  Varinance:  0.004759081705921813 \n",
      "\n",
      "Epoch:  6130  Learning Rate:  0.0002178758630329176  Varinance:  0.004752433443219463 \n",
      "\n",
      "Epoch:  6131  Learning Rate:  0.00021765809607151256  Varinance:  0.004745794467896429 \n",
      "\n",
      "Epoch:  6132  Learning Rate:  0.00021744054676822171  Varinance:  0.004739164766978565 \n",
      "\n",
      "Epoch:  6133  Learning Rate:  0.000217223214905496  Varinance:  0.004732544327509879 \n",
      "\n",
      "Epoch:  6134  Learning Rate:  0.00021700610026600302  Varinance:  0.00472593313655246 \n",
      "\n",
      "Epoch:  6135  Learning Rate:  0.00021678920263262848  Varinance:  0.00471933118118649 \n",
      "\n",
      "Epoch:  6136  Learning Rate:  0.00021657252178847476  Varinance:  0.004712738448510167 \n",
      "\n",
      "Epoch:  6137  Learning Rate:  0.0002163560575168607  Varinance:  0.004706154925639741 \n",
      "\n",
      "Epoch:  6138  Learning Rate:  0.0002161398096013224  Varinance:  0.004699580599709458 \n",
      "\n",
      "Epoch:  6139  Learning Rate:  0.0002159237778256115  Varinance:  0.004693015457871516 \n",
      "\n",
      "Epoch:  6140  Learning Rate:  0.00021570796197369646  Varinance:  0.00468645948729608 \n",
      "\n",
      "Epoch:  6141  Learning Rate:  0.0002154923618297615  Varinance:  0.004679912675171233 \n",
      "\n",
      "Epoch:  6142  Learning Rate:  0.00021527697717820617  Varinance:  0.004673375008702964 \n",
      "\n",
      "Epoch:  6143  Learning Rate:  0.00021506180780364598  Varinance:  0.004666846475115109 \n",
      "\n",
      "Epoch:  6144  Learning Rate:  0.00021484685349091164  Varinance:  0.004660327061649378 \n",
      "\n",
      "Epoch:  6145  Learning Rate:  0.00021463211402504855  Varinance:  0.004653816755565302 \n",
      "\n",
      "Epoch:  6146  Learning Rate:  0.00021441758919131756  Varinance:  0.004647315544140186 \n",
      "\n",
      "Epoch:  6147  Learning Rate:  0.00021420327877519344  Varinance:  0.004640823414669134 \n",
      "\n",
      "Epoch:  6148  Learning Rate:  0.00021398918256236595  Varinance:  0.004634340354464997 \n",
      "\n",
      "Epoch:  6149  Learning Rate:  0.00021377530033873905  Varinance:  0.004627866350858325 \n",
      "\n",
      "Epoch:  6150  Learning Rate:  0.00021356163189043006  Varinance:  0.004621401391197391 \n",
      "\n",
      "Epoch:  6151  Learning Rate:  0.00021334817700377083  Varinance:  0.0046149454628481345 \n",
      "\n",
      "Epoch:  6152  Learning Rate:  0.0002131349354653065  Varinance:  0.004608498553194153 \n",
      "\n",
      "Epoch:  6153  Learning Rate:  0.00021292190706179525  Varinance:  0.004602060649636643 \n",
      "\n",
      "Epoch:  6154  Learning Rate:  0.000212709091580209  Varinance:  0.004595631739594427 \n",
      "\n",
      "Epoch:  6155  Learning Rate:  0.00021249648880773185  Varinance:  0.0045892118105038985 \n",
      "\n",
      "Epoch:  6156  Learning Rate:  0.00021228409853176123  Varinance:  0.004582800849818985 \n",
      "\n",
      "Epoch:  6157  Learning Rate:  0.00021207192053990703  Varinance:  0.004576398845011159 \n",
      "\n",
      "Epoch:  6158  Learning Rate:  0.0002118599546199908  Varinance:  0.004570005783569388 \n",
      "\n",
      "Epoch:  6159  Learning Rate:  0.00021164820056004689  Varinance:  0.004563621653000126 \n",
      "\n",
      "Epoch:  6160  Learning Rate:  0.00021143665814832136  Varinance:  0.004557246440827255 \n",
      "\n",
      "Epoch:  6161  Learning Rate:  0.00021122532717327144  Varinance:  0.004550880134592111 \n",
      "\n",
      "Epoch:  6162  Learning Rate:  0.00021101420742356642  Varinance:  0.0045445227218534326 \n",
      "\n",
      "Epoch:  6163  Learning Rate:  0.00021080329868808625  Varinance:  0.004538174190187316 \n",
      "\n",
      "Epoch:  6164  Learning Rate:  0.00021059260075592234  Varinance:  0.004531834527187236 \n",
      "\n",
      "Epoch:  6165  Learning Rate:  0.00021038211341637694  Varinance:  0.004525503720463992 \n",
      "\n",
      "Epoch:  6166  Learning Rate:  0.00021017183645896228  Varinance:  0.004519181757645696 \n",
      "\n",
      "Epoch:  6167  Learning Rate:  0.00020996176967340158  Varinance:  0.004512868626377722 \n",
      "\n",
      "Epoch:  6168  Learning Rate:  0.00020975191284962824  Varinance:  0.004506564314322725 \n",
      "\n",
      "Epoch:  6169  Learning Rate:  0.00020954226577778504  Varinance:  0.004500268809160599 \n",
      "\n",
      "Epoch:  6170  Learning Rate:  0.00020933282824822527  Varinance:  0.0044939820985884175 \n",
      "\n",
      "Epoch:  6171  Learning Rate:  0.00020912360005151105  Varinance:  0.004487704170320471 \n",
      "\n",
      "Epoch:  6172  Learning Rate:  0.0002089145809784143  Varinance:  0.004481435012088201 \n",
      "\n",
      "Epoch:  6173  Learning Rate:  0.00020870577081991606  Varinance:  0.004475174611640199 \n",
      "\n",
      "Epoch:  6174  Learning Rate:  0.0002084971693672059  Varinance:  0.004468922956742143 \n",
      "\n",
      "Epoch:  6175  Learning Rate:  0.00020828877641168245  Varinance:  0.00446268003517683 \n",
      "\n",
      "Epoch:  6176  Learning Rate:  0.00020808059174495295  Varinance:  0.004456445834744122 \n",
      "\n",
      "Epoch:  6177  Learning Rate:  0.0002078726151588324  Varinance:  0.0044502203432609 \n",
      "\n",
      "Epoch:  6178  Learning Rate:  0.00020766484644534446  Varinance:  0.004444003548561089 \n",
      "\n",
      "Epoch:  6179  Learning Rate:  0.00020745728539672012  Varinance:  0.004437795438495602 \n",
      "\n",
      "Epoch:  6180  Learning Rate:  0.00020724993180539845  Varinance:  0.00443159600093233 \n",
      "\n",
      "Epoch:  6181  Learning Rate:  0.00020704278546402607  Varinance:  0.0044254052237560895 \n",
      "\n",
      "Epoch:  6182  Learning Rate:  0.00020683584616545622  Varinance:  0.004419223094868647 \n",
      "\n",
      "Epoch:  6183  Learning Rate:  0.00020662911370274972  Varinance:  0.004413049602188664 \n",
      "\n",
      "Epoch:  6184  Learning Rate:  0.00020642258786917434  Varinance:  0.0044068847336516635 \n",
      "\n",
      "Epoch:  6185  Learning Rate:  0.0002062162684582039  Varinance:  0.00440072847721004 \n",
      "\n",
      "Epoch:  6186  Learning Rate:  0.00020601015526351923  Varinance:  0.004394580820833013 \n",
      "\n",
      "Epoch:  6187  Learning Rate:  0.00020580424807900685  Varinance:  0.004388441752506614 \n",
      "\n",
      "Epoch:  6188  Learning Rate:  0.0002055985466987597  Varinance:  0.004382311260233636 \n",
      "\n",
      "Epoch:  6189  Learning Rate:  0.0002053930509170765  Varinance:  0.004376189332033654 \n",
      "\n",
      "Epoch:  6190  Learning Rate:  0.0002051877605284612  Varinance:  0.00437007595594298 \n",
      "\n",
      "Epoch:  6191  Learning Rate:  0.0002049826753276235  Varinance:  0.004363971120014616 \n",
      "\n",
      "Epoch:  6192  Learning Rate:  0.00020477779510947843  Varinance:  0.004357874812318275 \n",
      "\n",
      "Epoch:  6193  Learning Rate:  0.00020457311966914536  Varinance:  0.0043517870209403295 \n",
      "\n",
      "Epoch:  6194  Learning Rate:  0.00020436864880194914  Varinance:  0.004345707733983804 \n",
      "\n",
      "Epoch:  6195  Learning Rate:  0.0002041643823034186  Varinance:  0.0043396369395683155 \n",
      "\n",
      "Epoch:  6196  Learning Rate:  0.00020396031996928739  Varinance:  0.004333574625830106 \n",
      "\n",
      "Epoch:  6197  Learning Rate:  0.0002037564615954933  Varinance:  0.004327520780921986 \n",
      "\n",
      "Epoch:  6198  Learning Rate:  0.00020355280697817763  Varinance:  0.004321475393013296 \n",
      "\n",
      "Epoch:  6199  Learning Rate:  0.0002033493559136859  Varinance:  0.004315438450289925 \n",
      "\n",
      "Epoch:  6200  Learning Rate:  0.00020314610819856718  Varinance:  0.004309409940954258 \n",
      "\n",
      "Epoch:  6201  Learning Rate:  0.00020294306362957342  Varinance:  0.0043033898532251705 \n",
      "\n",
      "Epoch:  6202  Learning Rate:  0.00020274022200366038  Varinance:  0.00429737817533797 \n",
      "\n",
      "Epoch:  6203  Learning Rate:  0.00020253758311798606  Varinance:  0.004291374895544424 \n",
      "\n",
      "Epoch:  6204  Learning Rate:  0.00020233514676991176  Varinance:  0.004285380002112711 \n",
      "\n",
      "Epoch:  6205  Learning Rate:  0.00020213291275700122  Varinance:  0.0042793934833273775 \n",
      "\n",
      "Epoch:  6206  Learning Rate:  0.00020193088087702016  Varinance:  0.004273415327489354 \n",
      "\n",
      "Epoch:  6207  Learning Rate:  0.00020172905092793677  Varinance:  0.004267445522915912 \n",
      "\n",
      "Epoch:  6208  Learning Rate:  0.00020152742270792128  Varinance:  0.0042614840579406465 \n",
      "\n",
      "Epoch:  6209  Learning Rate:  0.00020132599601534515  Varinance:  0.004255530920913431 \n",
      "\n",
      "Epoch:  6210  Learning Rate:  0.00020112477064878197  Varinance:  0.004249586100200434 \n",
      "\n",
      "Epoch:  6211  Learning Rate:  0.00020092374640700604  Varinance:  0.004243649584184078 \n",
      "\n",
      "Epoch:  6212  Learning Rate:  0.00020072292308899323  Varinance:  0.004237721361262991 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6213  Learning Rate:  0.00020052230049392045  Varinance:  0.0042318014198520285 \n",
      "\n",
      "Epoch:  6214  Learning Rate:  0.0002003218784211647  Varinance:  0.004225889748382234 \n",
      "\n",
      "Epoch:  6215  Learning Rate:  0.000200121656670304  Varinance:  0.004219986335300787 \n",
      "\n",
      "Epoch:  6216  Learning Rate:  0.00019992163504111687  Varinance:  0.004214091169071029 \n",
      "\n",
      "Epoch:  6217  Learning Rate:  0.0001997218133335813  Varinance:  0.0042082042381724125 \n",
      "\n",
      "Epoch:  6218  Learning Rate:  0.00019952219134787582  Varinance:  0.0042023255311004854 \n",
      "\n",
      "Epoch:  6219  Learning Rate:  0.0001993227688843782  Varinance:  0.004196455036366851 \n",
      "\n",
      "Epoch:  6220  Learning Rate:  0.00019912354574366602  Varinance:  0.004190592742499181 \n",
      "\n",
      "Epoch:  6221  Learning Rate:  0.00019892452172651635  Varinance:  0.004184738638041173 \n",
      "\n",
      "Epoch:  6222  Learning Rate:  0.00019872569663390485  Varinance:  0.004178892711552507 \n",
      "\n",
      "Epoch:  6223  Learning Rate:  0.00019852707026700653  Varinance:  0.004173054951608867 \n",
      "\n",
      "Epoch:  6224  Learning Rate:  0.00019832864242719518  Varinance:  0.00416722534680189 \n",
      "\n",
      "Epoch:  6225  Learning Rate:  0.00019813041291604261  Varinance:  0.004161403885739156 \n",
      "\n",
      "Epoch:  6226  Learning Rate:  0.00019793238153531967  Varinance:  0.004155590557044137 \n",
      "\n",
      "Epoch:  6227  Learning Rate:  0.00019773454808699457  Varinance:  0.0041497853493562215 \n",
      "\n",
      "Epoch:  6228  Learning Rate:  0.00019753691237323404  Varinance:  0.004143988251330668 \n",
      "\n",
      "Epoch:  6229  Learning Rate:  0.00019733947419640248  Varinance:  0.004138199251638559 \n",
      "\n",
      "Epoch:  6230  Learning Rate:  0.00019714223335906144  Varinance:  0.004132418338966829 \n",
      "\n",
      "Epoch:  6231  Learning Rate:  0.00019694518966397015  Varinance:  0.0041266455020182065 \n",
      "\n",
      "Epoch:  6232  Learning Rate:  0.00019674834291408513  Varinance:  0.004120880729511209 \n",
      "\n",
      "Epoch:  6233  Learning Rate:  0.00019655169291255925  Varinance:  0.004115124010180092 \n",
      "\n",
      "Epoch:  6234  Learning Rate:  0.00019635523946274285  Varinance:  0.004109375332774873 \n",
      "\n",
      "Epoch:  6235  Learning Rate:  0.00019615898236818204  Varinance:  0.0041036346860612825 \n",
      "\n",
      "Epoch:  6236  Learning Rate:  0.00019596292143262003  Varinance:  0.004097902058820723 \n",
      "\n",
      "Epoch:  6237  Learning Rate:  0.00019576705645999587  Varinance:  0.004092177439850294 \n",
      "\n",
      "Epoch:  6238  Learning Rate:  0.0001955713872544444  Varinance:  0.004086460817962735 \n",
      "\n",
      "Epoch:  6239  Learning Rate:  0.0001953759136202964  Varinance:  0.004080752181986422 \n",
      "\n",
      "Epoch:  6240  Learning Rate:  0.0001951806353620785  Varinance:  0.0040750515207653125 \n",
      "\n",
      "Epoch:  6241  Learning Rate:  0.00019498555228451207  Varinance:  0.004069358823158976 \n",
      "\n",
      "Epoch:  6242  Learning Rate:  0.00019479066419251435  Varinance:  0.004063674078042542 \n",
      "\n",
      "Epoch:  6243  Learning Rate:  0.00019459597089119686  Varinance:  0.004057997274306657 \n",
      "\n",
      "Epoch:  6244  Learning Rate:  0.0001944014721858665  Varinance:  0.004052328400857512 \n",
      "\n",
      "Epoch:  6245  Learning Rate:  0.00019420716788202467  Varinance:  0.004046667446616786 \n",
      "\n",
      "Epoch:  6246  Learning Rate:  0.00019401305778536676  Varinance:  0.004041014400521645 \n",
      "\n",
      "Epoch:  6247  Learning Rate:  0.0001938191417017828  Varinance:  0.004035369251524683 \n",
      "\n",
      "Epoch:  6248  Learning Rate:  0.00019362541943735684  Varinance:  0.00402973198859395 \n",
      "\n",
      "Epoch:  6249  Learning Rate:  0.00019343189079836632  Varinance:  0.004024102600712908 \n",
      "\n",
      "Epoch:  6250  Learning Rate:  0.00019323855559128287  Varinance:  0.004018481076880388 \n",
      "\n",
      "Epoch:  6251  Learning Rate:  0.00019304541362277094  Varinance:  0.0040128674061106085 \n",
      "\n",
      "Epoch:  6252  Learning Rate:  0.00019285246469968873  Varinance:  0.00400726157743313 \n",
      "\n",
      "Epoch:  6253  Learning Rate:  0.00019265970862908746  Varinance:  0.004001663579892843 \n",
      "\n",
      "Epoch:  6254  Learning Rate:  0.00019246714521821068  Varinance:  0.0039960734025499215 \n",
      "\n",
      "Epoch:  6255  Learning Rate:  0.00019227477427449533  Varinance:  0.003990491034479844 \n",
      "\n",
      "Epoch:  6256  Learning Rate:  0.00019208259560557015  Varinance:  0.003984916464773351 \n",
      "\n",
      "Epoch:  6257  Learning Rate:  0.00019189060901925656  Varinance:  0.003979349682536397 \n",
      "\n",
      "Epoch:  6258  Learning Rate:  0.00019169881432356812  Varinance:  0.003973790676890183 \n",
      "\n",
      "Epoch:  6259  Learning Rate:  0.00019150721132670984  Varinance:  0.003968239436971094 \n",
      "\n",
      "Epoch:  6260  Learning Rate:  0.00019131579983707885  Varinance:  0.003962695951930704 \n",
      "\n",
      "Epoch:  6261  Learning Rate:  0.00019112457966326377  Varinance:  0.003957160210935715 \n",
      "\n",
      "Epoch:  6262  Learning Rate:  0.00019093355061404417  Varinance:  0.003951632203167986 \n",
      "\n",
      "Epoch:  6263  Learning Rate:  0.00019074271249839122  Varinance:  0.003946111917824491 \n",
      "\n",
      "Epoch:  6264  Learning Rate:  0.00019055206512546652  Varinance:  0.003940599344117272 \n",
      "\n",
      "Epoch:  6265  Learning Rate:  0.00019036160830462282  Varinance:  0.003935094471273462 \n",
      "\n",
      "Epoch:  6266  Learning Rate:  0.00019017134184540343  Varinance:  0.003929597288535239 \n",
      "\n",
      "Epoch:  6267  Learning Rate:  0.0001899812655575416  Varinance:  0.003924107785159811 \n",
      "\n",
      "Epoch:  6268  Learning Rate:  0.00018979137925096112  Varinance:  0.003918625950419378 \n",
      "\n",
      "Epoch:  6269  Learning Rate:  0.00018960168273577592  Varinance:  0.003913151773601145 \n",
      "\n",
      "Epoch:  6270  Learning Rate:  0.00018941217582228904  Varinance:  0.0039076852440072815 \n",
      "\n",
      "Epoch:  6271  Learning Rate:  0.000189222858320994  Varinance:  0.0039022263509548823 \n",
      "\n",
      "Epoch:  6272  Learning Rate:  0.00018903373004257283  Varinance:  0.003896775083775985 \n",
      "\n",
      "Epoch:  6273  Learning Rate:  0.00018884479079789746  Varinance:  0.0038913314318175245 \n",
      "\n",
      "Epoch:  6274  Learning Rate:  0.0001886560403980288  Varinance:  0.0038858953844413237 \n",
      "\n",
      "Epoch:  6275  Learning Rate:  0.00018846747865421609  Varinance:  0.003880466931024046 \n",
      "\n",
      "Epoch:  6276  Learning Rate:  0.00018827910537789775  Varinance:  0.003875046060957216 \n",
      "\n",
      "Epoch:  6277  Learning Rate:  0.00018809092038070063  Varinance:  0.0038696327636471776 \n",
      "\n",
      "Epoch:  6278  Learning Rate:  0.0001879029234744394  Varinance:  0.0038642270285150545 \n",
      "\n",
      "Epoch:  6279  Learning Rate:  0.00018771511447111748  Varinance:  0.003858828844996767 \n",
      "\n",
      "Epoch:  6280  Learning Rate:  0.0001875274931829255  Varinance:  0.003853438202542995 \n",
      "\n",
      "Epoch:  6281  Learning Rate:  0.00018734005942224233  Varinance:  0.0038480550906191337 \n",
      "\n",
      "Epoch:  6282  Learning Rate:  0.00018715281300163438  Varinance:  0.003842679498705315 \n",
      "\n",
      "Epoch:  6283  Learning Rate:  0.00018696575373385483  Varinance:  0.0038373114162963628 \n",
      "\n",
      "Epoch:  6284  Learning Rate:  0.0001867788814318446  Varinance:  0.0038319508329017786 \n",
      "\n",
      "Epoch:  6285  Learning Rate:  0.00018659219590873156  Varinance:  0.003826597738045701 \n",
      "\n",
      "Epoch:  6286  Learning Rate:  0.00018640569697782978  Varinance:  0.0038212521212669243 \n",
      "\n",
      "Epoch:  6287  Learning Rate:  0.0001862193844526407  Varinance:  0.0038159139721188544 \n",
      "\n",
      "Epoch:  6288  Learning Rate:  0.00018603325814685141  Varinance:  0.0038105832801694707 \n",
      "\n",
      "Epoch:  6289  Learning Rate:  0.0001858473178743358  Varinance:  0.0038052600350013477 \n",
      "\n",
      "Epoch:  6290  Learning Rate:  0.0001856615634491537  Varinance:  0.0037999442262116054 \n",
      "\n",
      "Epoch:  6291  Learning Rate:  0.00018547599468555034  Varinance:  0.0037946358434119013 \n",
      "\n",
      "Epoch:  6292  Learning Rate:  0.00018529061139795712  Varinance:  0.003789334876228387 \n",
      "\n",
      "Epoch:  6293  Learning Rate:  0.00018510541340099093  Varinance:  0.0037840413143017237 \n",
      "\n",
      "Epoch:  6294  Learning Rate:  0.00018492040050945339  Varinance:  0.003778755147287048 \n",
      "\n",
      "Epoch:  6295  Learning Rate:  0.00018473557253833193  Varinance:  0.003773476364853924 \n",
      "\n",
      "Epoch:  6296  Learning Rate:  0.00018455092930279825  Varinance:  0.0037682049566863684 \n",
      "\n",
      "Epoch:  6297  Learning Rate:  0.00018436647061820921  Varinance:  0.003762940912482804 \n",
      "\n",
      "Epoch:  6298  Learning Rate:  0.00018418219630010634  Varinance:  0.0037576842219560484 \n",
      "\n",
      "Epoch:  6299  Learning Rate:  0.00018399810616421497  Varinance:  0.0037524348748332716 \n",
      "\n",
      "Epoch:  6300  Learning Rate:  0.00018381420002644509  Varinance:  0.0037471928608560144 \n",
      "\n",
      "Epoch:  6301  Learning Rate:  0.00018363047770289073  Varinance:  0.0037419581697801475 \n",
      "\n",
      "Epoch:  6302  Learning Rate:  0.00018344693900982918  Varinance:  0.003736730791375833 \n",
      "\n",
      "Epoch:  6303  Learning Rate:  0.00018326358376372212  Varinance:  0.0037315107154275435 \n",
      "\n",
      "Epoch:  6304  Learning Rate:  0.0001830804117812139  Varinance:  0.003726297931734015 \n",
      "\n",
      "Epoch:  6305  Learning Rate:  0.00018289742287913276  Varinance:  0.003721092430108243 \n",
      "\n",
      "Epoch:  6306  Learning Rate:  0.0001827146168744899  Varinance:  0.0037158942003774308 \n",
      "\n",
      "Epoch:  6307  Learning Rate:  0.00018253199358447895  Varinance:  0.0037107032323830153 \n",
      "\n",
      "Epoch:  6308  Learning Rate:  0.00018234955282647681  Varinance:  0.003705519515980623 \n",
      "\n",
      "Epoch:  6309  Learning Rate:  0.00018216729441804285  Varinance:  0.0037003430410400334 \n",
      "\n",
      "Epoch:  6310  Learning Rate:  0.00018198521817691833  Varinance:  0.0036951737974451945 \n",
      "\n",
      "Epoch:  6311  Learning Rate:  0.0001818033239210273  Varinance:  0.003690011775094184 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6312  Learning Rate:  0.00018162161146847522  Varinance:  0.0036848569638991933 \n",
      "\n",
      "Epoch:  6313  Learning Rate:  0.0001814400806375497  Varinance:  0.0036797093537864894 \n",
      "\n",
      "Epoch:  6314  Learning Rate:  0.0001812587312467201  Varinance:  0.0036745689346964314 \n",
      "\n",
      "Epoch:  6315  Learning Rate:  0.0001810775631146367  Varinance:  0.00366943569658343 \n",
      "\n",
      "Epoch:  6316  Learning Rate:  0.00018089657606013153  Varinance:  0.00366430962941591 \n",
      "\n",
      "Epoch:  6317  Learning Rate:  0.00018071576990221763  Varinance:  0.0036591907231763314 \n",
      "\n",
      "Epoch:  6318  Learning Rate:  0.00018053514446008854  Varinance:  0.003654078967861139 \n",
      "\n",
      "Epoch:  6319  Learning Rate:  0.0001803546995531191  Varinance:  0.0036489743534807613 \n",
      "\n",
      "Epoch:  6320  Learning Rate:  0.0001801744350008641  Varinance:  0.00364387687005956 \n",
      "\n",
      "Epoch:  6321  Learning Rate:  0.00017999435062305912  Varinance:  0.0036387865076358537 \n",
      "\n",
      "Epoch:  6322  Learning Rate:  0.00017981444623961992  Varinance:  0.003633703256261876 \n",
      "\n",
      "Epoch:  6323  Learning Rate:  0.00017963472167064178  Varinance:  0.0036286271060037385 \n",
      "\n",
      "Epoch:  6324  Learning Rate:  0.00017945517673640028  Varinance:  0.003623558046941448 \n",
      "\n",
      "Epoch:  6325  Learning Rate:  0.0001792758112573506  Varinance:  0.003618496069168865 \n",
      "\n",
      "Epoch:  6326  Learning Rate:  0.00017909662505412698  Varinance:  0.003613441162793694 \n",
      "\n",
      "Epoch:  6327  Learning Rate:  0.0001789176179475435  Varinance:  0.003608393317937438 \n",
      "\n",
      "Epoch:  6328  Learning Rate:  0.00017873878975859273  Varinance:  0.0036033525247354216 \n",
      "\n",
      "Epoch:  6329  Learning Rate:  0.0001785601403084466  Varinance:  0.003598318773336747 \n",
      "\n",
      "Epoch:  6330  Learning Rate:  0.00017838166941845583  Varinance:  0.0035932920539042622 \n",
      "\n",
      "Epoch:  6331  Learning Rate:  0.00017820337691014918  Varinance:  0.0035882723566145734 \n",
      "\n",
      "Epoch:  6332  Learning Rate:  0.00017802526260523427  Varinance:  0.003583259671658005 \n",
      "\n",
      "Epoch:  6333  Learning Rate:  0.000177847326325597  Varinance:  0.0035782539892385908 \n",
      "\n",
      "Epoch:  6334  Learning Rate:  0.0001776695678933007  Varinance:  0.00357325529957403 \n",
      "\n",
      "Epoch:  6335  Learning Rate:  0.00017749198713058723  Varinance:  0.0035682635928957077 \n",
      "\n",
      "Epoch:  6336  Learning Rate:  0.00017731458385987554  Varinance:  0.0035632788594486533 \n",
      "\n",
      "Epoch:  6337  Learning Rate:  0.00017713735790376252  Varinance:  0.0035583010894915065 \n",
      "\n",
      "Epoch:  6338  Learning Rate:  0.0001769603090850223  Varinance:  0.003553330273296533 \n",
      "\n",
      "Epoch:  6339  Learning Rate:  0.00017678343722660574  Varinance:  0.0035483664011495817 \n",
      "\n",
      "Epoch:  6340  Learning Rate:  0.00017660674215164114  Varinance:  0.003543409463350079 \n",
      "\n",
      "Epoch:  6341  Learning Rate:  0.00017643022368343355  Varinance:  0.0035384594502109834 \n",
      "\n",
      "Epoch:  6342  Learning Rate:  0.00017625388164546423  Varinance:  0.0035335163520588027 \n",
      "\n",
      "Epoch:  6343  Learning Rate:  0.0001760777158613914  Varinance:  0.003528580159233562 \n",
      "\n",
      "Epoch:  6344  Learning Rate:  0.00017590172615504891  Varinance:  0.0035236508620887586 \n",
      "\n",
      "Epoch:  6345  Learning Rate:  0.00017572591235044725  Varinance:  0.003518728450991387 \n",
      "\n",
      "Epoch:  6346  Learning Rate:  0.00017555027427177273  Varinance:  0.0035138129163218967 \n",
      "\n",
      "Epoch:  6347  Learning Rate:  0.00017537481174338698  Varinance:  0.0035089042484741585 \n",
      "\n",
      "Epoch:  6348  Learning Rate:  0.0001751995245898276  Varinance:  0.0035040024378554793 \n",
      "\n",
      "Epoch:  6349  Learning Rate:  0.00017502441263580755  Varinance:  0.0034991074748865615 \n",
      "\n",
      "Epoch:  6350  Learning Rate:  0.00017484947570621452  Varinance:  0.003494219350001497 \n",
      "\n",
      "Epoch:  6351  Learning Rate:  0.00017467471362611197  Varinance:  0.0034893380536477193 \n",
      "\n",
      "Epoch:  6352  Learning Rate:  0.00017450012622073743  Varinance:  0.0034844635762860277 \n",
      "\n",
      "Epoch:  6353  Learning Rate:  0.00017432571331550366  Varinance:  0.003479595908390546 \n",
      "\n",
      "Epoch:  6354  Learning Rate:  0.0001741514747359979  Varinance:  0.003474735040448687 \n",
      "\n",
      "Epoch:  6355  Learning Rate:  0.0001739774103079812  Varinance:  0.003469880962961171 \n",
      "\n",
      "Epoch:  6356  Learning Rate:  0.00017380351985738936  Varinance:  0.003465033666441982 \n",
      "\n",
      "Epoch:  6357  Learning Rate:  0.00017362980321033197  Varinance:  0.0034601931414183626 \n",
      "\n",
      "Epoch:  6358  Learning Rate:  0.00017345626019309212  Varinance:  0.0034553593784307684 \n",
      "\n",
      "Epoch:  6359  Learning Rate:  0.00017328289063212705  Varinance:  0.0034505323680328907 \n",
      "\n",
      "Epoch:  6360  Learning Rate:  0.00017310969435406693  Varinance:  0.0034457121007916134 \n",
      "\n",
      "Epoch:  6361  Learning Rate:  0.00017293667118571557  Varinance:  0.003440898567286983 \n",
      "\n",
      "Epoch:  6362  Learning Rate:  0.00017276382095404998  Varinance:  0.003436091758112221 \n",
      "\n",
      "Epoch:  6363  Learning Rate:  0.00017259114348621958  Varinance:  0.0034312916638736864 \n",
      "\n",
      "Epoch:  6364  Learning Rate:  0.00017241863860954703  Varinance:  0.003426498275190866 \n",
      "\n",
      "Epoch:  6365  Learning Rate:  0.00017224630615152764  Varinance:  0.0034217115826963296 \n",
      "\n",
      "Epoch:  6366  Learning Rate:  0.0001720741459398286  Varinance:  0.0034169315770357558 \n",
      "\n",
      "Epoch:  6367  Learning Rate:  0.00017190215780228996  Varinance:  0.003412158248867888 \n",
      "\n",
      "Epoch:  6368  Learning Rate:  0.00017173034156692333  Varinance:  0.003407391588864502 \n",
      "\n",
      "Epoch:  6369  Learning Rate:  0.00017155869706191257  Varinance:  0.0034026315877104213 \n",
      "\n",
      "Epoch:  6370  Learning Rate:  0.00017138722411561328  Varinance:  0.0033978782361034797 \n",
      "\n",
      "Epoch:  6371  Learning Rate:  0.0001712159225565523  Varinance:  0.0033931315247545065 \n",
      "\n",
      "Epoch:  6372  Learning Rate:  0.0001710447922134281  Varinance:  0.003388391444387296 \n",
      "\n",
      "Epoch:  6373  Learning Rate:  0.00017087383291511054  Varinance:  0.003383657985738613 \n",
      "\n",
      "Epoch:  6374  Learning Rate:  0.00017070304449063998  Varinance:  0.0033789311395581675 \n",
      "\n",
      "Epoch:  6375  Learning Rate:  0.0001705324267692283  Varinance:  0.0033742108966085703 \n",
      "\n",
      "Epoch:  6376  Learning Rate:  0.0001703619795802574  Varinance:  0.0033694972476653565 \n",
      "\n",
      "Epoch:  6377  Learning Rate:  0.00017019170275328033  Varinance:  0.0033647901835169407 \n",
      "\n",
      "Epoch:  6378  Learning Rate:  0.0001700215961180203  Varinance:  0.0033600896949646125 \n",
      "\n",
      "Epoch:  6379  Learning Rate:  0.00016985165950437044  Varinance:  0.003355395772822492 \n",
      "\n",
      "Epoch:  6380  Learning Rate:  0.0001696818927423944  Varinance:  0.003350708407917552 \n",
      "\n",
      "Epoch:  6381  Learning Rate:  0.00016951229566232505  Varinance:  0.0033460275910895784 \n",
      "\n",
      "Epoch:  6382  Learning Rate:  0.00016934286809456553  Varinance:  0.0033413533131911354 \n",
      "\n",
      "Epoch:  6383  Learning Rate:  0.00016917360986968833  Varinance:  0.003336685565087584 \n",
      "\n",
      "Epoch:  6384  Learning Rate:  0.00016900452081843498  Varinance:  0.00333202433765704 \n",
      "\n",
      "Epoch:  6385  Learning Rate:  0.00016883560077171652  Varinance:  0.0033273696217903685 \n",
      "\n",
      "Epoch:  6386  Learning Rate:  0.00016866684956061306  Varinance:  0.003322721408391142 \n",
      "\n",
      "Epoch:  6387  Learning Rate:  0.00016849826701637304  Varinance:  0.003318079688375656 \n",
      "\n",
      "Epoch:  6388  Learning Rate:  0.00016832985297041424  Varinance:  0.0033134444526728996 \n",
      "\n",
      "Epoch:  6389  Learning Rate:  0.0001681616072543223  Varinance:  0.003308815692224512 \n",
      "\n",
      "Epoch:  6390  Learning Rate:  0.00016799352969985162  Varinance:  0.0033041933979848074 \n",
      "\n",
      "Epoch:  6391  Learning Rate:  0.00016782562013892478  Varinance:  0.0032995775609207284 \n",
      "\n",
      "Epoch:  6392  Learning Rate:  0.00016765787840363193  Varinance:  0.0032949681720118437 \n",
      "\n",
      "Epoch:  6393  Learning Rate:  0.00016749030432623146  Varinance:  0.0032903652222503045 \n",
      "\n",
      "Epoch:  6394  Learning Rate:  0.0001673228977391494  Varinance:  0.0032857687026408644 \n",
      "\n",
      "Epoch:  6395  Learning Rate:  0.00016715565847497888  Varinance:  0.0032811786042008426 \n",
      "\n",
      "Epoch:  6396  Learning Rate:  0.00016698858636648092  Varinance:  0.003276594917960089 \n",
      "\n",
      "Epoch:  6397  Learning Rate:  0.0001668216812465831  Varinance:  0.0032720176349610008 \n",
      "\n",
      "Epoch:  6398  Learning Rate:  0.00016665494294838042  Varinance:  0.0032674467462584862 \n",
      "\n",
      "Epoch:  6399  Learning Rate:  0.00016648837130513473  Varinance:  0.003262882242919952 \n",
      "\n",
      "Epoch:  6400  Learning Rate:  0.00016632196615027405  Varinance:  0.0032583241160252666 \n",
      "\n",
      "Epoch:  6401  Learning Rate:  0.0001661557273173934  Varinance:  0.003253772356666779 \n",
      "\n",
      "Epoch:  6402  Learning Rate:  0.00016598965464025407  Varinance:  0.003249226955949279 \n",
      "\n",
      "Epoch:  6403  Learning Rate:  0.00016582374795278304  Varinance:  0.0032446879049899667 \n",
      "\n",
      "Epoch:  6404  Learning Rate:  0.00016565800708907396  Varinance:  0.003240155194918469 \n",
      "\n",
      "Epoch:  6405  Learning Rate:  0.0001654924318833856  Varinance:  0.0032356288168767984 \n",
      "\n",
      "Epoch:  6406  Learning Rate:  0.00016532702217014292  Varinance:  0.003231108762019346 \n",
      "\n",
      "Epoch:  6407  Learning Rate:  0.00016516177778393634  Varinance:  0.003226595021512844 \n",
      "\n",
      "Epoch:  6408  Learning Rate:  0.00016499669855952115  Varinance:  0.0032220875865363826 \n",
      "\n",
      "Epoch:  6409  Learning Rate:  0.0001648317843318183  Varinance:  0.0032175864482813714 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6410  Learning Rate:  0.0001646670349359136  Varinance:  0.0032130915979515103 \n",
      "\n",
      "Epoch:  6411  Learning Rate:  0.00016450245020705747  Varinance:  0.0032086030267628046 \n",
      "\n",
      "Epoch:  6412  Learning Rate:  0.0001643380299806654  Varinance:  0.00320412072594353 \n",
      "\n",
      "Epoch:  6413  Learning Rate:  0.00016417377409231682  Varinance:  0.0031996446867341996 \n",
      "\n",
      "Epoch:  6414  Learning Rate:  0.00016400968237775606  Varinance:  0.0031951749003875773 \n",
      "\n",
      "Epoch:  6415  Learning Rate:  0.00016384575467289148  Varinance:  0.0031907113581686446 \n",
      "\n",
      "Epoch:  6416  Learning Rate:  0.00016368199081379504  Varinance:  0.0031862540513545898 \n",
      "\n",
      "Epoch:  6417  Learning Rate:  0.00016351839063670308  Varinance:  0.003181802971234768 \n",
      "\n",
      "Epoch:  6418  Learning Rate:  0.00016335495397801556  Varinance:  0.0031773581091107234 \n",
      "\n",
      "Epoch:  6419  Learning Rate:  0.00016319168067429544  Varinance:  0.003172919456296148 \n",
      "\n",
      "Epoch:  6420  Learning Rate:  0.00016302857056226975  Varinance:  0.003168487004116853 \n",
      "\n",
      "Epoch:  6421  Learning Rate:  0.00016286562347882808  Varinance:  0.0031640607439107843 \n",
      "\n",
      "Epoch:  6422  Learning Rate:  0.00016270283926102345  Varinance:  0.0031596406670279823 \n",
      "\n",
      "Epoch:  6423  Learning Rate:  0.0001625402177460718  Varinance:  0.0031552267648305757 \n",
      "\n",
      "Epoch:  6424  Learning Rate:  0.00016237775877135129  Varinance:  0.0031508190286927452 \n",
      "\n",
      "Epoch:  6425  Learning Rate:  0.00016221546217440306  Varinance:  0.003146417450000737 \n",
      "\n",
      "Epoch:  6426  Learning Rate:  0.00016205332779293069  Varinance:  0.00314202202015283 \n",
      "\n",
      "Epoch:  6427  Learning Rate:  0.00016189135546479947  Varinance:  0.0031376327305593027 \n",
      "\n",
      "Epoch:  6428  Learning Rate:  0.00016172954502803735  Varinance:  0.003133249572642453 \n",
      "\n",
      "Epoch:  6429  Learning Rate:  0.00016156789632083357  Varinance:  0.0031288725378365496 \n",
      "\n",
      "Epoch:  6430  Learning Rate:  0.00016140640918153957  Varinance:  0.0031245016175878394 \n",
      "\n",
      "Epoch:  6431  Learning Rate:  0.00016124508344866838  Varinance:  0.0031201368033544987 \n",
      "\n",
      "Epoch:  6432  Learning Rate:  0.00016108391896089391  Varinance:  0.0031157780866066516 \n",
      "\n",
      "Epoch:  6433  Learning Rate:  0.00016092291555705183  Varinance:  0.0031114254588263416 \n",
      "\n",
      "Epoch:  6434  Learning Rate:  0.00016076207307613887  Varinance:  0.0031070789115074924 \n",
      "\n",
      "Epoch:  6435  Learning Rate:  0.00016060139135731224  Varinance:  0.0031027384361559285 \n",
      "\n",
      "Epoch:  6436  Learning Rate:  0.0001604408702398905  Varinance:  0.0030984040242893335 \n",
      "\n",
      "Epoch:  6437  Learning Rate:  0.00016028050956335218  Varinance:  0.0030940756674372465 \n",
      "\n",
      "Epoch:  6438  Learning Rate:  0.00016012030916733684  Varinance:  0.003089753357141023 \n",
      "\n",
      "Epoch:  6439  Learning Rate:  0.00015996026889164412  Varinance:  0.003085437084953853 \n",
      "\n",
      "Epoch:  6440  Learning Rate:  0.00015980038857623348  Varinance:  0.0030811268424407235 \n",
      "\n",
      "Epoch:  6441  Learning Rate:  0.00015964066806122476  Varinance:  0.0030768226211783903 \n",
      "\n",
      "Epoch:  6442  Learning Rate:  0.00015948110718689751  Varinance:  0.003072524412755393 \n",
      "\n",
      "Epoch:  6443  Learning Rate:  0.0001593217057936906  Varinance:  0.0030682322087720137 \n",
      "\n",
      "Epoch:  6444  Learning Rate:  0.00015916246372220294  Varinance:  0.0030639460008402776 \n",
      "\n",
      "Epoch:  6445  Learning Rate:  0.00015900338081319208  Varinance:  0.0030596657805839087 \n",
      "\n",
      "Epoch:  6446  Learning Rate:  0.0001588444569075753  Varinance:  0.0030553915396383494 \n",
      "\n",
      "Epoch:  6447  Learning Rate:  0.00015868569184642882  Varinance:  0.0030511232696507273 \n",
      "\n",
      "Epoch:  6448  Learning Rate:  0.00015852708547098727  Varinance:  0.003046860962279822 \n",
      "\n",
      "Epoch:  6449  Learning Rate:  0.00015836863762264437  Varinance:  0.0030426046091960816 \n",
      "\n",
      "Epoch:  6450  Learning Rate:  0.00015821034814295245  Varinance:  0.003038354202081586 \n",
      "\n",
      "Epoch:  6451  Learning Rate:  0.0001580522168736217  Varinance:  0.0030341097326300388 \n",
      "\n",
      "Epoch:  6452  Learning Rate:  0.00015789424365652118  Varinance:  0.003029871192546732 \n",
      "\n",
      "Epoch:  6453  Learning Rate:  0.0001577364283336773  Varinance:  0.0030256385735485604 \n",
      "\n",
      "Epoch:  6454  Learning Rate:  0.0001575787707472749  Varinance:  0.0030214118673639913 \n",
      "\n",
      "Epoch:  6455  Learning Rate:  0.00015742127073965653  Varinance:  0.00301719106573303 \n",
      "\n",
      "Epoch:  6456  Learning Rate:  0.00015726392815332185  Varinance:  0.003012976160407236 \n",
      "\n",
      "Epoch:  6457  Learning Rate:  0.00015710674283092845  Varinance:  0.003008767143149688 \n",
      "\n",
      "Epoch:  6458  Learning Rate:  0.00015694971461529113  Varinance:  0.0030045640057349762 \n",
      "\n",
      "Epoch:  6459  Learning Rate:  0.00015679284334938132  Varinance:  0.0030003667399491635 \n",
      "\n",
      "Epoch:  6460  Learning Rate:  0.0001566361288763281  Varinance:  0.002996175337589806 \n",
      "\n",
      "Epoch:  6461  Learning Rate:  0.00015647957103941666  Varinance:  0.0029919897904659167 \n",
      "\n",
      "Epoch:  6462  Learning Rate:  0.00015632316968208933  Varinance:  0.002987810090397936 \n",
      "\n",
      "Epoch:  6463  Learning Rate:  0.0001561669246479448  Varinance:  0.0029836362292177464 \n",
      "\n",
      "Epoch:  6464  Learning Rate:  0.00015601083578073782  Varinance:  0.0029794681987686355 \n",
      "\n",
      "Epoch:  6465  Learning Rate:  0.0001558549029243796  Varinance:  0.002975305990905291 \n",
      "\n",
      "Epoch:  6466  Learning Rate:  0.00015569912592293747  Varinance:  0.0029711495974937634 \n",
      "\n",
      "Epoch:  6467  Learning Rate:  0.00015554350462063405  Varinance:  0.002966999010411482 \n",
      "\n",
      "Epoch:  6468  Learning Rate:  0.00015538803886184837  Varinance:  0.002962854221547223 \n",
      "\n",
      "Epoch:  6469  Learning Rate:  0.00015523272849111438  Varinance:  0.0029587152228010777 \n",
      "\n",
      "Epoch:  6470  Learning Rate:  0.0001550775733531218  Varinance:  0.0029545820060844687 \n",
      "\n",
      "Epoch:  6471  Learning Rate:  0.00015492257329271565  Varinance:  0.0029504545633201127 \n",
      "\n",
      "Epoch:  6472  Learning Rate:  0.00015476772815489553  Varinance:  0.002946332886442015 \n",
      "\n",
      "Epoch:  6473  Learning Rate:  0.00015461303778481649  Varinance:  0.002942216967395434 \n",
      "\n",
      "Epoch:  6474  Learning Rate:  0.00015445850202778827  Varinance:  0.0029381067981368945 \n",
      "\n",
      "Epoch:  6475  Learning Rate:  0.0001543041207292748  Varinance:  0.002934002370634159 \n",
      "\n",
      "Epoch:  6476  Learning Rate:  0.00015414989373489503  Varinance:  0.002929903676866195 \n",
      "\n",
      "Epoch:  6477  Learning Rate:  0.00015399582089042172  Varinance:  0.00292581070882319 \n",
      "\n",
      "Epoch:  6478  Learning Rate:  0.00015384190204178214  Varinance:  0.002921723458506521 \n",
      "\n",
      "Epoch:  6479  Learning Rate:  0.00015368813703505756  Varinance:  0.0029176419179287242 \n",
      "\n",
      "Epoch:  6480  Learning Rate:  0.0001535345257164827  Varinance:  0.0029135660791135085 \n",
      "\n",
      "Epoch:  6481  Learning Rate:  0.0001533810679324463  Varinance:  0.0029094959340957207 \n",
      "\n",
      "Epoch:  6482  Learning Rate:  0.0001532277635294908  Varinance:  0.0029054314749213392 \n",
      "\n",
      "Epoch:  6483  Learning Rate:  0.00015307461235431147  Varinance:  0.00290137269364744 \n",
      "\n",
      "Epoch:  6484  Learning Rate:  0.00015292161425375732  Varinance:  0.002897319582342208 \n",
      "\n",
      "Epoch:  6485  Learning Rate:  0.0001527687690748301  Varinance:  0.002893272133084911 \n",
      "\n",
      "Epoch:  6486  Learning Rate:  0.00015261607666468464  Varinance:  0.0028892303379658645 \n",
      "\n",
      "Epoch:  6487  Learning Rate:  0.00015246353687062872  Varinance:  0.0028851941890864503 \n",
      "\n",
      "Epoch:  6488  Learning Rate:  0.00015231114954012225  Varinance:  0.0028811636785590782 \n",
      "\n",
      "Epoch:  6489  Learning Rate:  0.000152158914520778  Varinance:  0.0028771387985071827 \n",
      "\n",
      "Epoch:  6490  Learning Rate:  0.0001520068316603611  Varinance:  0.002873119541065185 \n",
      "\n",
      "Epoch:  6491  Learning Rate:  0.00015185490080678836  Varinance:  0.0028691058983785116 \n",
      "\n",
      "Epoch:  6492  Learning Rate:  0.00015170312180812922  Varinance:  0.0028650978626035585 \n",
      "\n",
      "Epoch:  6493  Learning Rate:  0.00015155149451260444  Varinance:  0.0028610954259076665 \n",
      "\n",
      "Epoch:  6494  Learning Rate:  0.00015140001876858675  Varinance:  0.0028570985804691317 \n",
      "\n",
      "Epoch:  6495  Learning Rate:  0.0001512486944246006  Varinance:  0.0028531073184771736 \n",
      "\n",
      "Epoch:  6496  Learning Rate:  0.00015109752132932137  Varinance:  0.002849121632131926 \n",
      "\n",
      "Epoch:  6497  Learning Rate:  0.00015094649933157603  Varinance:  0.002845141513644405 \n",
      "\n",
      "Epoch:  6498  Learning Rate:  0.00015079562828034275  Varinance:  0.0028411669552365227 \n",
      "\n",
      "Epoch:  6499  Learning Rate:  0.00015064490802475016  Varinance:  0.0028371979491410566 \n",
      "\n",
      "Epoch:  6500  Learning Rate:  0.0001504943384140783  Varinance:  0.0028332344876016197 \n",
      "\n",
      "Epoch:  6501  Learning Rate:  0.00015034391929775724  Varinance:  0.0028292765628726755 \n",
      "\n",
      "Epoch:  6502  Learning Rate:  0.00015019365052536802  Varinance:  0.0028253241672195023 \n",
      "\n",
      "Epoch:  6503  Learning Rate:  0.000150043531946642  Varinance:  0.00282137729291819 \n",
      "\n",
      "Epoch:  6504  Learning Rate:  0.0001498935634114603  Varinance:  0.0028174359322556014 \n",
      "\n",
      "Epoch:  6505  Learning Rate:  0.00014974374476985457  Varinance:  0.00281350007752939 \n",
      "\n",
      "Epoch:  6506  Learning Rate:  0.00014959407587200602  Varinance:  0.002809569721047971 \n",
      "\n",
      "Epoch:  6507  Learning Rate:  0.0001494445565682458  Varinance:  0.0028056448551304863 \n",
      "\n",
      "Epoch:  6508  Learning Rate:  0.0001492951867090547  Varinance:  0.0028017254721068256 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6509  Learning Rate:  0.00014914596614506266  Varinance:  0.002797811564317587 \n",
      "\n",
      "Epoch:  6510  Learning Rate:  0.00014899689472704916  Varinance:  0.0027939031241140727 \n",
      "\n",
      "Epoch:  6511  Learning Rate:  0.00014884797230594295  Varinance:  0.002790000143858257 \n",
      "\n",
      "Epoch:  6512  Learning Rate:  0.0001486991987328213  Varinance:  0.0027861026159227975 \n",
      "\n",
      "Epoch:  6513  Learning Rate:  0.00014855057385891092  Varinance:  0.0027822105326910074 \n",
      "\n",
      "Epoch:  6514  Learning Rate:  0.00014840209753558666  Varinance:  0.002778323886556826 \n",
      "\n",
      "Epoch:  6515  Learning Rate:  0.0001482537696143723  Varinance:  0.002774442669924831 \n",
      "\n",
      "Epoch:  6516  Learning Rate:  0.00014810558994694  Varinance:  0.0027705668752102085 \n",
      "\n",
      "Epoch:  6517  Learning Rate:  0.0001479575583851099  Varinance:  0.0027666964948387424 \n",
      "\n",
      "Epoch:  6518  Learning Rate:  0.0001478096747808505  Varinance:  0.0027628315212467843 \n",
      "\n",
      "Epoch:  6519  Learning Rate:  0.00014766193898627835  Varinance:  0.002758971946881266 \n",
      "\n",
      "Epoch:  6520  Learning Rate:  0.00014751435085365734  Varinance:  0.002755117764199671 \n",
      "\n",
      "Epoch:  6521  Learning Rate:  0.00014736691023539963  Varinance:  0.0027512689656700056 \n",
      "\n",
      "Epoch:  6522  Learning Rate:  0.00014721961698406426  Varinance:  0.0027474255437708105 \n",
      "\n",
      "Epoch:  6523  Learning Rate:  0.00014707247095235818  Varinance:  0.002743587490991131 \n",
      "\n",
      "Epoch:  6524  Learning Rate:  0.00014692547199313542  Varinance:  0.0027397547998305083 \n",
      "\n",
      "Epoch:  6525  Learning Rate:  0.00014677861995939678  Varinance:  0.0027359274627989476 \n",
      "\n",
      "Epoch:  6526  Learning Rate:  0.00014663191470429032  Varinance:  0.0027321054724169314 \n",
      "\n",
      "Epoch:  6527  Learning Rate:  0.0001464853560811109  Varinance:  0.0027282888212153914 \n",
      "\n",
      "Epoch:  6528  Learning Rate:  0.00014633894394329967  Varinance:  0.0027244775017356785 \n",
      "\n",
      "Epoch:  6529  Learning Rate:  0.00014619267814444472  Varinance:  0.002720671506529578 \n",
      "\n",
      "Epoch:  6530  Learning Rate:  0.0001460465585382799  Varinance:  0.0027168708281592735 \n",
      "\n",
      "Epoch:  6531  Learning Rate:  0.00014590058497868585  Varinance:  0.002713075459197346 \n",
      "\n",
      "Epoch:  6532  Learning Rate:  0.00014575475731968903  Varinance:  0.0027092853922267373 \n",
      "\n",
      "Epoch:  6533  Learning Rate:  0.0001456090754154616  Varinance:  0.002705500619840764 \n",
      "\n",
      "Epoch:  6534  Learning Rate:  0.00014546353912032168  Varinance:  0.0027017211346430903 \n",
      "\n",
      "Epoch:  6535  Learning Rate:  0.00014531814828873312  Varinance:  0.002697946929247699 \n",
      "\n",
      "Epoch:  6536  Learning Rate:  0.00014517290277530484  Varinance:  0.0026941779962789043 \n",
      "\n",
      "Epoch:  6537  Learning Rate:  0.0001450278024347916  Varinance:  0.002690414328371319 \n",
      "\n",
      "Epoch:  6538  Learning Rate:  0.0001448828471220927  Varinance:  0.002686655918169851 \n",
      "\n",
      "Epoch:  6539  Learning Rate:  0.000144738036692253  Varinance:  0.0026829027583296678 \n",
      "\n",
      "Epoch:  6540  Learning Rate:  0.00014459337100046222  Varinance:  0.0026791548415162113 \n",
      "\n",
      "Epoch:  6541  Learning Rate:  0.00014444884990205434  Varinance:  0.002675412160405171 \n",
      "\n",
      "Epoch:  6542  Learning Rate:  0.00014430447325250838  Varinance:  0.0026716747076824528 \n",
      "\n",
      "Epoch:  6543  Learning Rate:  0.00014416024090744786  Varinance:  0.002667942476044195 \n",
      "\n",
      "Epoch:  6544  Learning Rate:  0.0001440161527226401  Varinance:  0.0026642154581967372 \n",
      "\n",
      "Epoch:  6545  Learning Rate:  0.00014387220855399723  Varinance:  0.0026604936468565973 \n",
      "\n",
      "Epoch:  6546  Learning Rate:  0.00014372840825757472  Varinance:  0.002656777034750479 \n",
      "\n",
      "Epoch:  6547  Learning Rate:  0.0001435847516895725  Varinance:  0.0026530656146152424 \n",
      "\n",
      "Epoch:  6548  Learning Rate:  0.00014344123870633403  Varinance:  0.0026493593791979005 \n",
      "\n",
      "Epoch:  6549  Learning Rate:  0.0001432978691643461  Varinance:  0.0026456583212555824 \n",
      "\n",
      "Epoch:  6550  Learning Rate:  0.0001431546429202393  Varinance:  0.00264196243355555 \n",
      "\n",
      "Epoch:  6551  Learning Rate:  0.00014301155983078746  Varinance:  0.002638271708875169 \n",
      "\n",
      "Epoch:  6552  Learning Rate:  0.00014286861975290724  Varinance:  0.0026345861400018807 \n",
      "\n",
      "Epoch:  6553  Learning Rate:  0.0001427258225436588  Varinance:  0.0026309057197332164 \n",
      "\n",
      "Epoch:  6554  Learning Rate:  0.00014258316806024468  Varinance:  0.0026272304408767643 \n",
      "\n",
      "Epoch:  6555  Learning Rate:  0.0001424406561600105  Varinance:  0.002623560296250165 \n",
      "\n",
      "Epoch:  6556  Learning Rate:  0.00014229828670044446  Varinance:  0.0026198952786810775 \n",
      "\n",
      "Epoch:  6557  Learning Rate:  0.00014215605953917686  Varinance:  0.002616235381007195 \n",
      "\n",
      "Epoch:  6558  Learning Rate:  0.00014201397453398064  Varinance:  0.0026125805960762172 \n",
      "\n",
      "Epoch:  6559  Learning Rate:  0.00014187203154277092  Varinance:  0.002608930916745819 \n",
      "\n",
      "Epoch:  6560  Learning Rate:  0.00014173023042360445  Varinance:  0.0026052863358836695 \n",
      "\n",
      "Epoch:  6561  Learning Rate:  0.00014158857103468034  Varinance:  0.0026016468463673946 \n",
      "\n",
      "Epoch:  6562  Learning Rate:  0.00014144705323433895  Varinance:  0.002598012441084576 \n",
      "\n",
      "Epoch:  6563  Learning Rate:  0.00014130567688106256  Varinance:  0.0025943831129327153 \n",
      "\n",
      "Epoch:  6564  Learning Rate:  0.00014116444183347494  Varinance:  0.002590758854819253 \n",
      "\n",
      "Epoch:  6565  Learning Rate:  0.00014102334795034082  Varinance:  0.002587139659661535 \n",
      "\n",
      "Epoch:  6566  Learning Rate:  0.00014088239509056638  Varinance:  0.002583525520386789 \n",
      "\n",
      "Epoch:  6567  Learning Rate:  0.00014074158311319892  Varinance:  0.0025799164299321357 \n",
      "\n",
      "Epoch:  6568  Learning Rate:  0.00014060091187742614  Varinance:  0.002576312381244559 \n",
      "\n",
      "Epoch:  6569  Learning Rate:  0.0001404603812425771  Varinance:  0.002572713367280899 \n",
      "\n",
      "Epoch:  6570  Learning Rate:  0.0001403199910681209  Varinance:  0.002569119381007823 \n",
      "\n",
      "Epoch:  6571  Learning Rate:  0.00014017974121366744  Varinance:  0.0025655304154018352 \n",
      "\n",
      "Epoch:  6572  Learning Rate:  0.00014003963153896702  Varinance:  0.0025619464634492523 \n",
      "\n",
      "Epoch:  6573  Learning Rate:  0.00013989966190390966  Varinance:  0.0025583675181461755 \n",
      "\n",
      "Epoch:  6574  Learning Rate:  0.0001397598321685259  Varinance:  0.002554793572498503 \n",
      "\n",
      "Epoch:  6575  Learning Rate:  0.00013962014219298603  Varinance:  0.0025512246195219007 \n",
      "\n",
      "Epoch:  6576  Learning Rate:  0.00013948059183759988  Varinance:  0.002547660652241793 \n",
      "\n",
      "Epoch:  6577  Learning Rate:  0.00013934118096281734  Varinance:  0.002544101663693337 \n",
      "\n",
      "Epoch:  6578  Learning Rate:  0.00013920190942922722  Varinance:  0.0025405476469214303 \n",
      "\n",
      "Epoch:  6579  Learning Rate:  0.00013906277709755814  Varinance:  0.0025369985949806884 \n",
      "\n",
      "Epoch:  6580  Learning Rate:  0.00013892378382867788  Varinance:  0.0025334545009354156 \n",
      "\n",
      "Epoch:  6581  Learning Rate:  0.0001387849294835929  Varinance:  0.0025299153578596176 \n",
      "\n",
      "Epoch:  6582  Learning Rate:  0.00013864621392344894  Varinance:  0.0025263811588369716 \n",
      "\n",
      "Epoch:  6583  Learning Rate:  0.0001385076370095306  Varinance:  0.002522851896960821 \n",
      "\n",
      "Epoch:  6584  Learning Rate:  0.0001383691986032607  Varinance:  0.0025193275653341445 \n",
      "\n",
      "Epoch:  6585  Learning Rate:  0.00013823089856620106  Varinance:  0.0025158081570695687 \n",
      "\n",
      "Epoch:  6586  Learning Rate:  0.00013809273676005136  Varinance:  0.0025122936652893415 \n",
      "\n",
      "Epoch:  6587  Learning Rate:  0.00013795471304664994  Varinance:  0.0025087840831253053 \n",
      "\n",
      "Epoch:  6588  Learning Rate:  0.00013781682728797318  Varinance:  0.0025052794037189107 \n",
      "\n",
      "Epoch:  6589  Learning Rate:  0.00013767907934613508  Varinance:  0.002501779620221186 \n",
      "\n",
      "Epoch:  6590  Learning Rate:  0.0001375414690833878  Varinance:  0.0024982847257927297 \n",
      "\n",
      "Epoch:  6591  Learning Rate:  0.00013740399636212117  Varinance:  0.002494794713603682 \n",
      "\n",
      "Epoch:  6592  Learning Rate:  0.00013726666104486225  Varinance:  0.002491309576833739 \n",
      "\n",
      "Epoch:  6593  Learning Rate:  0.00013712946299427594  Varinance:  0.0024878293086721226 \n",
      "\n",
      "Epoch:  6594  Learning Rate:  0.0001369924020731639  Varinance:  0.0024843539023175565 \n",
      "\n",
      "Epoch:  6595  Learning Rate:  0.00013685547814446536  Varinance:  0.0024808833509782796 \n",
      "\n",
      "Epoch:  6596  Learning Rate:  0.0001367186910712565  Varinance:  0.002477417647872013 \n",
      "\n",
      "Epoch:  6597  Learning Rate:  0.00013658204071675  Varinance:  0.0024739567862259574 \n",
      "\n",
      "Epoch:  6598  Learning Rate:  0.00013644552694429558  Varinance:  0.002470500759276761 \n",
      "\n",
      "Epoch:  6599  Learning Rate:  0.0001363091496173796  Varinance:  0.0024670495602705346 \n",
      "\n",
      "Epoch:  6600  Learning Rate:  0.00013617290859962446  Varinance:  0.002463603182462823 \n",
      "\n",
      "Epoch:  6601  Learning Rate:  0.0001360368037547894  Varinance:  0.00246016161911858 \n",
      "\n",
      "Epoch:  6602  Learning Rate:  0.0001359008349467693  Varinance:  0.0024567248635121806 \n",
      "\n",
      "Epoch:  6603  Learning Rate:  0.0001357650020395955  Varinance:  0.0024532929089273914 \n",
      "\n",
      "Epoch:  6604  Learning Rate:  0.00013562930489743514  Varinance:  0.0024498657486573656 \n",
      "\n",
      "Epoch:  6605  Learning Rate:  0.00013549374338459088  Varinance:  0.0024464433760046124 \n",
      "\n",
      "Epoch:  6606  Learning Rate:  0.00013535831736550128  Varinance:  0.002443025784281011 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6607  Learning Rate:  0.00013522302670474046  Varinance:  0.0024396129668077817 \n",
      "\n",
      "Epoch:  6608  Learning Rate:  0.00013508787126701748  Varinance:  0.002436204916915464 \n",
      "\n",
      "Epoch:  6609  Learning Rate:  0.00013495285091717717  Varinance:  0.002432801627943926 \n",
      "\n",
      "Epoch:  6610  Learning Rate:  0.0001348179655201989  Varinance:  0.002429403093242341 \n",
      "\n",
      "Epoch:  6611  Learning Rate:  0.00013468321494119735  Varinance:  0.0024260093061691584 \n",
      "\n",
      "Epoch:  6612  Learning Rate:  0.0001345485990454221  Varinance:  0.00242262026009212 \n",
      "\n",
      "Epoch:  6613  Learning Rate:  0.000134414117698257  Varinance:  0.0024192359483882282 \n",
      "\n",
      "Epoch:  6614  Learning Rate:  0.00013427977076522078  Varinance:  0.0024158563644437418 \n",
      "\n",
      "Epoch:  6615  Learning Rate:  0.00013414555811196667  Varinance:  0.0024124815016541446 \n",
      "\n",
      "Epoch:  6616  Learning Rate:  0.0001340114796042817  Varinance:  0.0024091113534241617 \n",
      "\n",
      "Epoch:  6617  Learning Rate:  0.00013387753510808763  Varinance:  0.0024057459131677297 \n",
      "\n",
      "Epoch:  6618  Learning Rate:  0.0001337437244894397  Varinance:  0.002402385174307974 \n",
      "\n",
      "Epoch:  6619  Learning Rate:  0.00013361004761452744  Varinance:  0.0023990291302772196 \n",
      "\n",
      "Epoch:  6620  Learning Rate:  0.000133476504349674  Varinance:  0.002395677774516963 \n",
      "\n",
      "Epoch:  6621  Learning Rate:  0.00013334309456133593  Varinance:  0.0023923311004778658 \n",
      "\n",
      "Epoch:  6622  Learning Rate:  0.00013320981811610353  Varinance:  0.002388989101619727 \n",
      "\n",
      "Epoch:  6623  Learning Rate:  0.0001330766748807005  Varinance:  0.0023856517714114945 \n",
      "\n",
      "Epoch:  6624  Learning Rate:  0.00013294366472198327  Varinance:  0.0023823191033312398 \n",
      "\n",
      "Epoch:  6625  Learning Rate:  0.00013281078750694198  Varinance:  0.002378991090866132 \n",
      "\n",
      "Epoch:  6626  Learning Rate:  0.00013267804310269915  Varinance:  0.0023756677275124523 \n",
      "\n",
      "Epoch:  6627  Learning Rate:  0.0001325454313765105  Varinance:  0.002372349006775562 \n",
      "\n",
      "Epoch:  6628  Learning Rate:  0.00013241295219576434  Varinance:  0.0023690349221699 \n",
      "\n",
      "Epoch:  6629  Learning Rate:  0.00013228060542798132  Varinance:  0.0023657254672189523 \n",
      "\n",
      "Epoch:  6630  Learning Rate:  0.00013214839094081486  Varinance:  0.0023624206354552653 \n",
      "\n",
      "Epoch:  6631  Learning Rate:  0.00013201630860205026  Varinance:  0.0023591204204204204 \n",
      "\n",
      "Epoch:  6632  Learning Rate:  0.00013188435827960526  Varinance:  0.0023558248156650076 \n",
      "\n",
      "Epoch:  6633  Learning Rate:  0.00013175253984152961  Varinance:  0.00235253381474864 \n",
      "\n",
      "Epoch:  6634  Learning Rate:  0.0001316208531560047  Varinance:  0.0023492474112399232 \n",
      "\n",
      "Epoch:  6635  Learning Rate:  0.00013148929809134391  Varinance:  0.0023459655987164516 \n",
      "\n",
      "Epoch:  6636  Learning Rate:  0.00013135787451599228  Varinance:  0.0023426883707647786 \n",
      "\n",
      "Epoch:  6637  Learning Rate:  0.00013122658229852601  Varinance:  0.0023394157209804295 \n",
      "\n",
      "Epoch:  6638  Learning Rate:  0.00013109542130765306  Varinance:  0.0023361476429678766 \n",
      "\n",
      "Epoch:  6639  Learning Rate:  0.00013096439141221223  Varinance:  0.0023328841303405135 \n",
      "\n",
      "Epoch:  6640  Learning Rate:  0.00013083349248117375  Varinance:  0.00232962517672067 \n",
      "\n",
      "Epoch:  6641  Learning Rate:  0.00013070272438363878  Varinance:  0.002326370775739578 \n",
      "\n",
      "Epoch:  6642  Learning Rate:  0.00013057208698883893  Varinance:  0.0023231209210373732 \n",
      "\n",
      "Epoch:  6643  Learning Rate:  0.00013044158016613697  Varinance:  0.0023198756062630618 \n",
      "\n",
      "Epoch:  6644  Learning Rate:  0.00013031120378502616  Varinance:  0.0023166348250745353 \n",
      "\n",
      "Epoch:  6645  Learning Rate:  0.00013018095771512986  Varinance:  0.002313398571138545 \n",
      "\n",
      "Epoch:  6646  Learning Rate:  0.00013005084182620227  Varinance:  0.0023101668381306766 \n",
      "\n",
      "Epoch:  6647  Learning Rate:  0.0001299208559881272  Varinance:  0.0023069396197353633 \n",
      "\n",
      "Epoch:  6648  Learning Rate:  0.00012979100007091897  Varinance:  0.0023037169096458564 \n",
      "\n",
      "Epoch:  6649  Learning Rate:  0.00012966127394472172  Varinance:  0.002300498701564223 \n",
      "\n",
      "Epoch:  6650  Learning Rate:  0.00012953167747980913  Varinance:  0.002297284989201314 \n",
      "\n",
      "Epoch:  6651  Learning Rate:  0.0001294022105465848  Varinance:  0.00229407576627678 \n",
      "\n",
      "Epoch:  6652  Learning Rate:  0.0001292728730155819  Varinance:  0.0022908710265190434 \n",
      "\n",
      "Epoch:  6653  Learning Rate:  0.0001291436647574627  Varinance:  0.0022876707636652767 \n",
      "\n",
      "Epoch:  6654  Learning Rate:  0.00012901458564301913  Varinance:  0.002284474971461414 \n",
      "\n",
      "Epoch:  6655  Learning Rate:  0.00012888563554317182  Varinance:  0.00228128364366212 \n",
      "\n",
      "Epoch:  6656  Learning Rate:  0.00012875681432897082  Varinance:  0.00227809677403079 \n",
      "\n",
      "Epoch:  6657  Learning Rate:  0.00012862812187159497  Varinance:  0.002274914356339518 \n",
      "\n",
      "Epoch:  6658  Learning Rate:  0.0001284995580423516  Varinance:  0.0022717363843691115 \n",
      "\n",
      "Epoch:  6659  Learning Rate:  0.000128371122712677  Varinance:  0.0022685628519090654 \n",
      "\n",
      "Epoch:  6660  Learning Rate:  0.0001282428157541359  Varinance:  0.0022653937527575387 \n",
      "\n",
      "Epoch:  6661  Learning Rate:  0.00012811463703842115  Varinance:  0.002262229080721365 \n",
      "\n",
      "Epoch:  6662  Learning Rate:  0.0001279865864373542  Varinance:  0.0022590688296160277 \n",
      "\n",
      "Epoch:  6663  Learning Rate:  0.00012785866382288426  Varinance:  0.0022559129932656513 \n",
      "\n",
      "Epoch:  6664  Learning Rate:  0.0001277308690670888  Varinance:  0.0022527615655029776 \n",
      "\n",
      "Epoch:  6665  Learning Rate:  0.00012760320204217314  Varinance:  0.0022496145401693745 \n",
      "\n",
      "Epoch:  6666  Learning Rate:  0.00012747566262047007  Varinance:  0.002246471911114815 \n",
      "\n",
      "Epoch:  6667  Learning Rate:  0.00012734825067444025  Varinance:  0.0022433336721978496 \n",
      "\n",
      "Epoch:  6668  Learning Rate:  0.0001272209660766718  Varinance:  0.002240199817285621 \n",
      "\n",
      "Epoch:  6669  Learning Rate:  0.00012709380869987993  Varinance:  0.002237070340253836 \n",
      "\n",
      "Epoch:  6670  Learning Rate:  0.00012696677841690746  Varinance:  0.0022339452349867594 \n",
      "\n",
      "Epoch:  6671  Learning Rate:  0.00012683987510072388  Varinance:  0.002230824495377188 \n",
      "\n",
      "Epoch:  6672  Learning Rate:  0.00012671309862442596  Varinance:  0.002227708115326463 \n",
      "\n",
      "Epoch:  6673  Learning Rate:  0.00012658644886123736  Varinance:  0.0022245960887444438 \n",
      "\n",
      "Epoch:  6674  Learning Rate:  0.00012645992568450807  Varinance:  0.002221488409549487 \n",
      "\n",
      "Epoch:  6675  Learning Rate:  0.00012633352896771494  Varinance:  0.0022183850716684557 \n",
      "\n",
      "Epoch:  6676  Learning Rate:  0.00012620725858446147  Varinance:  0.0022152860690366973 \n",
      "\n",
      "Epoch:  6677  Learning Rate:  0.00012608111440847697  Varinance:  0.0022121913955980197 \n",
      "\n",
      "Epoch:  6678  Learning Rate:  0.0001259550963136175  Varinance:  0.0022091010453047025 \n",
      "\n",
      "Epoch:  6679  Learning Rate:  0.00012582920417386474  Varinance:  0.002206015012117471 \n",
      "\n",
      "Epoch:  6680  Learning Rate:  0.00012570343786332664  Varinance:  0.002202933290005489 \n",
      "\n",
      "Epoch:  6681  Learning Rate:  0.00012557779725623695  Varinance:  0.002199855872946336 \n",
      "\n",
      "Epoch:  6682  Learning Rate:  0.0001254522822269549  Varinance:  0.0021967827549260136 \n",
      "\n",
      "Epoch:  6683  Learning Rate:  0.00012532689264996554  Varinance:  0.0021937139299389266 \n",
      "\n",
      "Epoch:  6684  Learning Rate:  0.00012520162839987936  Varinance:  0.0021906493919878565 \n",
      "\n",
      "Epoch:  6685  Learning Rate:  0.00012507648935143191  Varinance:  0.002187589135083976 \n",
      "\n",
      "Epoch:  6686  Learning Rate:  0.00012495147537948436  Varinance:  0.0021845331532468187 \n",
      "\n",
      "Epoch:  6687  Learning Rate:  0.00012482658635902248  Varinance:  0.0021814814405042766 \n",
      "\n",
      "Epoch:  6688  Learning Rate:  0.0001247018221651574  Varinance:  0.0021784339908925737 \n",
      "\n",
      "Epoch:  6689  Learning Rate:  0.00012457718267312493  Varinance:  0.0021753907984562765 \n",
      "\n",
      "Epoch:  6690  Learning Rate:  0.00012445266775828543  Varinance:  0.00217235185724827 \n",
      "\n",
      "Epoch:  6691  Learning Rate:  0.00012432827729612404  Varinance:  0.0021693171613297374 \n",
      "\n",
      "Epoch:  6692  Learning Rate:  0.00012420401116225042  Varinance:  0.0021662867047701683 \n",
      "\n",
      "Epoch:  6693  Learning Rate:  0.00012407986923239822  Varinance:  0.0021632604816473333 \n",
      "\n",
      "Epoch:  6694  Learning Rate:  0.00012395585138242572  Varinance:  0.002160238486047281 \n",
      "\n",
      "Epoch:  6695  Learning Rate:  0.0001238319574883148  Varinance:  0.0021572207120643086 \n",
      "\n",
      "Epoch:  6696  Learning Rate:  0.00012370818742617169  Varinance:  0.0021542071538009763 \n",
      "\n",
      "Epoch:  6697  Learning Rate:  0.00012358454107222643  Varinance:  0.0021511978053680818 \n",
      "\n",
      "Epoch:  6698  Learning Rate:  0.0001234610183028324  Varinance:  0.002148192660884639 \n",
      "\n",
      "Epoch:  6699  Learning Rate:  0.00012333761899446699  Varinance:  0.0021451917144778877 \n",
      "\n",
      "Epoch:  6700  Learning Rate:  0.00012321434302373096  Varinance:  0.0021421949602832687 \n",
      "\n",
      "Epoch:  6701  Learning Rate:  0.0001230911902673481  Varinance:  0.00213920239244442 \n",
      "\n",
      "Epoch:  6702  Learning Rate:  0.0001229681606021659  Varinance:  0.0021362140051131477 \n",
      "\n",
      "Epoch:  6703  Learning Rate:  0.0001228452539051544  Varinance:  0.0021332297924494398 \n",
      "\n",
      "Epoch:  6704  Learning Rate:  0.00012272247005340708  Varinance:  0.002130249748621443 \n",
      "\n",
      "Epoch:  6705  Learning Rate:  0.00012259980892414014  Varinance:  0.0021272738678054385 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6706  Learning Rate:  0.00012247727039469222  Varinance:  0.002124302144185855 \n",
      "\n",
      "Epoch:  6707  Learning Rate:  0.0001223548543425249  Varinance:  0.0021213345719552417 \n",
      "\n",
      "Epoch:  6708  Learning Rate:  0.00012223256064522226  Varinance:  0.002118371145314264 \n",
      "\n",
      "Epoch:  6709  Learning Rate:  0.0001221103891804903  Varinance:  0.0021154118584716776 \n",
      "\n",
      "Epoch:  6710  Learning Rate:  0.00012198833982615783  Varinance:  0.00211245670564434 \n",
      "\n",
      "Epoch:  6711  Learning Rate:  0.00012186641246017523  Varinance:  0.0021095056810571866 \n",
      "\n",
      "Epoch:  6712  Learning Rate:  0.00012174460696061525  Varinance:  0.00210655877894321 \n",
      "\n",
      "Epoch:  6713  Learning Rate:  0.0001216229232056725  Varinance:  0.00210361599354347 \n",
      "\n",
      "Epoch:  6714  Learning Rate:  0.00012150136107366295  Varinance:  0.0021006773191070676 \n",
      "\n",
      "Epoch:  6715  Learning Rate:  0.00012137992044302462  Varinance:  0.0020977427498911415 \n",
      "\n",
      "Epoch:  6716  Learning Rate:  0.00012125860119231697  Varinance:  0.002094812280160842 \n",
      "\n",
      "Epoch:  6717  Learning Rate:  0.00012113740320022049  Varinance:  0.002091885904189341 \n",
      "\n",
      "Epoch:  6718  Learning Rate:  0.00012101632634553741  Varinance:  0.0020889636162578122 \n",
      "\n",
      "Epoch:  6719  Learning Rate:  0.00012089537050719066  Varinance:  0.002086045410655405 \n",
      "\n",
      "Epoch:  6720  Learning Rate:  0.00012077453556422447  Varinance:  0.00208313128167926 \n",
      "\n",
      "Epoch:  6721  Learning Rate:  0.00012065382139580405  Varinance:  0.002080221223634479 \n",
      "\n",
      "Epoch:  6722  Learning Rate:  0.00012053322788121496  Varinance:  0.0020773152308341245 \n",
      "\n",
      "Epoch:  6723  Learning Rate:  0.0001204127548998638  Varinance:  0.0020744132975991907 \n",
      "\n",
      "Epoch:  6724  Learning Rate:  0.00012029240233127766  Varinance:  0.002071515418258618 \n",
      "\n",
      "Epoch:  6725  Learning Rate:  0.0001201721700551038  Varinance:  0.0020686215871492685 \n",
      "\n",
      "Epoch:  6726  Learning Rate:  0.00012005205795111009  Varinance:  0.002065731798615903 \n",
      "\n",
      "Epoch:  6727  Learning Rate:  0.00011993206589918425  Varinance:  0.002062846047011196 \n",
      "\n",
      "Epoch:  6728  Learning Rate:  0.00011981219377933429  Varinance:  0.0020599643266957053 \n",
      "\n",
      "Epoch:  6729  Learning Rate:  0.0001196924414716882  Varinance:  0.0020570866320378713 \n",
      "\n",
      "Epoch:  6730  Learning Rate:  0.00011957280885649346  Varinance:  0.002054212957413991 \n",
      "\n",
      "Epoch:  6731  Learning Rate:  0.00011945329581411753  Varinance:  0.002051343297208228 \n",
      "\n",
      "Epoch:  6732  Learning Rate:  0.00011933390222504748  Varinance:  0.0020484776458125904 \n",
      "\n",
      "Epoch:  6733  Learning Rate:  0.0001192146279698895  Varinance:  0.00204561599762691 \n",
      "\n",
      "Epoch:  6734  Learning Rate:  0.00011909547292936953  Varinance:  0.002042758347058854 \n",
      "\n",
      "Epoch:  6735  Learning Rate:  0.0001189764369843323  Varinance:  0.002039904688523896 \n",
      "\n",
      "Epoch:  6736  Learning Rate:  0.00011885752001574196  Varinance:  0.0020370550164453163 \n",
      "\n",
      "Epoch:  6737  Learning Rate:  0.00011873872190468166  Varinance:  0.002034209325254174 \n",
      "\n",
      "Epoch:  6738  Learning Rate:  0.00011862004253235305  Varinance:  0.0020313676093893186 \n",
      "\n",
      "Epoch:  6739  Learning Rate:  0.00011850148178007686  Varinance:  0.0020285298632973688 \n",
      "\n",
      "Epoch:  6740  Learning Rate:  0.00011838303952929242  Varinance:  0.002025696081432691 \n",
      "\n",
      "Epoch:  6741  Learning Rate:  0.00011826471566155728  Varinance:  0.002022866258257407 \n",
      "\n",
      "Epoch:  6742  Learning Rate:  0.00011814651005854776  Varinance:  0.0020200403882413777 \n",
      "\n",
      "Epoch:  6743  Learning Rate:  0.00011802842260205803  Varinance:  0.002017218465862177 \n",
      "\n",
      "Epoch:  6744  Learning Rate:  0.00011791045317400075  Varinance:  0.0020144004856051043 \n",
      "\n",
      "Epoch:  6745  Learning Rate:  0.00011779260165640658  Varinance:  0.0020115864419631593 \n",
      "\n",
      "Epoch:  6746  Learning Rate:  0.00011767486793142378  Varinance:  0.002008776329437039 \n",
      "\n",
      "Epoch:  6747  Learning Rate:  0.00011755725188131871  Varinance:  0.002005970142535112 \n",
      "\n",
      "Epoch:  6748  Learning Rate:  0.00011743975338847541  Varinance:  0.002003167875773428 \n",
      "\n",
      "Epoch:  6749  Learning Rate:  0.00011732237233539518  Varinance:  0.002000369523675698 \n",
      "\n",
      "Epoch:  6750  Learning Rate:  0.00011720510860469718  Varinance:  0.0019975750807732736 \n",
      "\n",
      "Epoch:  6751  Learning Rate:  0.00011708796207911745  Varinance:  0.0019947845416051556 \n",
      "\n",
      "Epoch:  6752  Learning Rate:  0.00011697093264150954  Varinance:  0.00199199790071797 \n",
      "\n",
      "Epoch:  6753  Learning Rate:  0.00011685402017484414  Varinance:  0.001989215152665965 \n",
      "\n",
      "Epoch:  6754  Learning Rate:  0.00011673722456220855  Varinance:  0.0019864362920109855 \n",
      "\n",
      "Epoch:  6755  Learning Rate:  0.00011662054568680734  Varinance:  0.0019836613133224837 \n",
      "\n",
      "Epoch:  6756  Learning Rate:  0.00011650398343196144  Varinance:  0.0019808902111774986 \n",
      "\n",
      "Epoch:  6757  Learning Rate:  0.00011638753768110867  Varinance:  0.0019781229801606333 \n",
      "\n",
      "Epoch:  6758  Learning Rate:  0.0001162712083178034  Varinance:  0.001975359614864068 \n",
      "\n",
      "Epoch:  6759  Learning Rate:  0.000116154995225716  Varinance:  0.001972600109887533 \n",
      "\n",
      "Epoch:  6760  Learning Rate:  0.00011603889828863356  Varinance:  0.001969844459838305 \n",
      "\n",
      "Epoch:  6761  Learning Rate:  0.00011592291739045914  Varinance:  0.0019670926593311866 \n",
      "\n",
      "Epoch:  6762  Learning Rate:  0.0001158070524152117  Varinance:  0.0019643447029885114 \n",
      "\n",
      "Epoch:  6763  Learning Rate:  0.00011569130324702641  Varinance:  0.0019616005854401256 \n",
      "\n",
      "Epoch:  6764  Learning Rate:  0.00011557566977015389  Varinance:  0.001958860301323368 \n",
      "\n",
      "Epoch:  6765  Learning Rate:  0.00011546015186896079  Varinance:  0.001956123845283078 \n",
      "\n",
      "Epoch:  6766  Learning Rate:  0.00011534474942792929  Varinance:  0.0019533912119715736 \n",
      "\n",
      "Epoch:  6767  Learning Rate:  0.00011522946233165669  Varinance:  0.001950662396048646 \n",
      "\n",
      "Epoch:  6768  Learning Rate:  0.00011511429046485605  Varinance:  0.0019479373921815367 \n",
      "\n",
      "Epoch:  6769  Learning Rate:  0.00011499923371235558  Varinance:  0.0019452161950449464 \n",
      "\n",
      "Epoch:  6770  Learning Rate:  0.0001148842919590983  Varinance:  0.0019424987993210156 \n",
      "\n",
      "Epoch:  6771  Learning Rate:  0.00011476946509014264  Varinance:  0.0019397851996993038 \n",
      "\n",
      "Epoch:  6772  Learning Rate:  0.00011465475299066155  Varinance:  0.001937075390876798 \n",
      "\n",
      "Epoch:  6773  Learning Rate:  0.00011454015554594299  Varinance:  0.0019343693675578913 \n",
      "\n",
      "Epoch:  6774  Learning Rate:  0.00011442567264138963  Varinance:  0.0019316671244543773 \n",
      "\n",
      "Epoch:  6775  Learning Rate:  0.00011431130416251834  Varinance:  0.0019289686562854265 \n",
      "\n",
      "Epoch:  6776  Learning Rate:  0.00011419704999496076  Varinance:  0.0019262739577775973 \n",
      "\n",
      "Epoch:  6777  Learning Rate:  0.00011408291002446276  Varinance:  0.0019235830236648147 \n",
      "\n",
      "Epoch:  6778  Learning Rate:  0.0001139688841368842  Varinance:  0.0019208958486883496 \n",
      "\n",
      "Epoch:  6779  Learning Rate:  0.00011385497221819939  Varinance:  0.0019182124275968304 \n",
      "\n",
      "Epoch:  6780  Learning Rate:  0.00011374117415449618  Varinance:  0.0019155327551462172 \n",
      "\n",
      "Epoch:  6781  Learning Rate:  0.00011362748983197659  Varinance:  0.0019128568260997991 \n",
      "\n",
      "Epoch:  6782  Learning Rate:  0.00011351391913695641  Varinance:  0.0019101846352281714 \n",
      "\n",
      "Epoch:  6783  Learning Rate:  0.00011340046195586473  Varinance:  0.0019075161773092438 \n",
      "\n",
      "Epoch:  6784  Learning Rate:  0.00011328711817524444  Varinance:  0.001904851447128222 \n",
      "\n",
      "Epoch:  6785  Learning Rate:  0.0001131738876817519  Varinance:  0.0019021904394775858 \n",
      "\n",
      "Epoch:  6786  Learning Rate:  0.00011306077036215634  Varinance:  0.0018995331491571002 \n",
      "\n",
      "Epoch:  6787  Learning Rate:  0.00011294776610334067  Varinance:  0.0018968795709737917 \n",
      "\n",
      "Epoch:  6788  Learning Rate:  0.00011283487479230043  Varinance:  0.0018942296997419429 \n",
      "\n",
      "Epoch:  6789  Learning Rate:  0.00011272209631614439  Varinance:  0.0018915835302830727 \n",
      "\n",
      "Epoch:  6790  Learning Rate:  0.00011260943056209414  Varinance:  0.001888941057425943 \n",
      "\n",
      "Epoch:  6791  Learning Rate:  0.00011249687741748374  Varinance:  0.00188630227600654 \n",
      "\n",
      "Epoch:  6792  Learning Rate:  0.00011238443676976013  Varinance:  0.001883667180868054 \n",
      "\n",
      "Epoch:  6793  Learning Rate:  0.00011227210850648276  Varinance:  0.0018810357668608887 \n",
      "\n",
      "Epoch:  6794  Learning Rate:  0.00011215989251532316  Varinance:  0.001878408028842639 \n",
      "\n",
      "Epoch:  6795  Learning Rate:  0.00011204778868406549  Varinance:  0.001875783961678086 \n",
      "\n",
      "Epoch:  6796  Learning Rate:  0.00011193579690060578  Varinance:  0.0018731635602391753 \n",
      "\n",
      "Epoch:  6797  Learning Rate:  0.00011182391705295228  Varinance:  0.0018705468194050259 \n",
      "\n",
      "Epoch:  6798  Learning Rate:  0.00011171214902922526  Varinance:  0.00186793373406191 \n",
      "\n",
      "Epoch:  6799  Learning Rate:  0.00011160049271765649  Varinance:  0.0018653242991032348 \n",
      "\n",
      "Epoch:  6800  Learning Rate:  0.0001114889480065897  Varinance:  0.0018627185094295495 \n",
      "\n",
      "Epoch:  6801  Learning Rate:  0.00011137751478448033  Varinance:  0.0018601163599485247 \n",
      "\n",
      "Epoch:  6802  Learning Rate:  0.00011126619293989494  Varinance:  0.0018575178455749488 \n",
      "\n",
      "Epoch:  6803  Learning Rate:  0.00011115498236151183  Varinance:  0.0018549229612307024 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6804  Learning Rate:  0.00011104388293812027  Varinance:  0.0018523317018447713 \n",
      "\n",
      "Epoch:  6805  Learning Rate:  0.00011093289455862089  Varinance:  0.0018497440623532245 \n",
      "\n",
      "Epoch:  6806  Learning Rate:  0.00011082201711202541  Varinance:  0.0018471600376991964 \n",
      "\n",
      "Epoch:  6807  Learning Rate:  0.00011071125048745618  Varinance:  0.0018445796228328943 \n",
      "\n",
      "Epoch:  6808  Learning Rate:  0.00011060059457414668  Varinance:  0.0018420028127115802 \n",
      "\n",
      "Epoch:  6809  Learning Rate:  0.00011049004926144104  Varinance:  0.0018394296022995512 \n",
      "\n",
      "Epoch:  6810  Learning Rate:  0.00011037961443879381  Varinance:  0.001836859986568149 \n",
      "\n",
      "Epoch:  6811  Learning Rate:  0.00011026928999577027  Varinance:  0.0018342939604957355 \n",
      "\n",
      "Epoch:  6812  Learning Rate:  0.00011015907582204587  Varinance:  0.0018317315190676928 \n",
      "\n",
      "Epoch:  6813  Learning Rate:  0.00011004897180740643  Varinance:  0.0018291726572763972 \n",
      "\n",
      "Epoch:  6814  Learning Rate:  0.00010993897784174807  Varinance:  0.0018266173701212305 \n",
      "\n",
      "Epoch:  6815  Learning Rate:  0.00010982909381507663  Varinance:  0.001824065652608561 \n",
      "\n",
      "Epoch:  6816  Learning Rate:  0.00010971931961750814  Varinance:  0.0018215174997517221 \n",
      "\n",
      "Epoch:  6817  Learning Rate:  0.00010960965513926852  Varinance:  0.001818972906571023 \n",
      "\n",
      "Epoch:  6818  Learning Rate:  0.00010950010027069308  Varinance:  0.0018164318680937272 \n",
      "\n",
      "Epoch:  6819  Learning Rate:  0.00010939065490222713  Varinance:  0.0018138943793540477 \n",
      "\n",
      "Epoch:  6820  Learning Rate:  0.00010928131892442509  Varinance:  0.001811360435393124 \n",
      "\n",
      "Epoch:  6821  Learning Rate:  0.00010917209222795108  Varinance:  0.001808830031259034 \n",
      "\n",
      "Epoch:  6822  Learning Rate:  0.00010906297470357852  Varinance:  0.0018063031620067721 \n",
      "\n",
      "Epoch:  6823  Learning Rate:  0.00010895396624218962  Varinance:  0.0018037798226982322 \n",
      "\n",
      "Epoch:  6824  Learning Rate:  0.00010884506673477608  Varinance:  0.0018012600084022145 \n",
      "\n",
      "Epoch:  6825  Learning Rate:  0.00010873627607243843  Varinance:  0.0017987437141944067 \n",
      "\n",
      "Epoch:  6826  Learning Rate:  0.00010862759414638579  Varinance:  0.0017962309351573775 \n",
      "\n",
      "Epoch:  6827  Learning Rate:  0.00010851902084793646  Varinance:  0.0017937216663805554 \n",
      "\n",
      "Epoch:  6828  Learning Rate:  0.00010841055606851694  Varinance:  0.001791215902960239 \n",
      "\n",
      "Epoch:  6829  Learning Rate:  0.00010830219969966251  Varinance:  0.0017887136399995774 \n",
      "\n",
      "Epoch:  6830  Learning Rate:  0.00010819395163301691  Varinance:  0.0017862148726085496 \n",
      "\n",
      "Epoch:  6831  Learning Rate:  0.00010808581176033185  Varinance:  0.0017837195959039766 \n",
      "\n",
      "Epoch:  6832  Learning Rate:  0.00010797777997346755  Varinance:  0.001781227805009497 \n",
      "\n",
      "Epoch:  6833  Learning Rate:  0.00010786985616439233  Varinance:  0.0017787394950555657 \n",
      "\n",
      "Epoch:  6834  Learning Rate:  0.00010776204022518218  Varinance:  0.001776254661179429 \n",
      "\n",
      "Epoch:  6835  Learning Rate:  0.00010765433204802133  Varinance:  0.001773773298525138 \n",
      "\n",
      "Epoch:  6836  Learning Rate:  0.00010754673152520139  Varinance:  0.0017712954022435252 \n",
      "\n",
      "Epoch:  6837  Learning Rate:  0.00010743923854912193  Varinance:  0.0017688209674921896 \n",
      "\n",
      "Epoch:  6838  Learning Rate:  0.0001073318530122901  Varinance:  0.0017663499894355022 \n",
      "\n",
      "Epoch:  6839  Learning Rate:  0.0001072245748073201  Varinance:  0.0017638824632445888 \n",
      "\n",
      "Epoch:  6840  Learning Rate:  0.00010711740382693383  Varinance:  0.0017614183840973215 \n",
      "\n",
      "Epoch:  6841  Learning Rate:  0.00010701033996396045  Varinance:  0.0017589577471783006 \n",
      "\n",
      "Epoch:  6842  Learning Rate:  0.00010690338311133583  Varinance:  0.0017565005476788624 \n",
      "\n",
      "Epoch:  6843  Learning Rate:  0.00010679653316210333  Varinance:  0.0017540467807970613 \n",
      "\n",
      "Epoch:  6844  Learning Rate:  0.0001066897900094128  Varinance:  0.00175159644173765 \n",
      "\n",
      "Epoch:  6845  Learning Rate:  0.00010658315354652117  Varinance:  0.001749149525712089 \n",
      "\n",
      "Epoch:  6846  Learning Rate:  0.00010647662366679205  Varinance:  0.001746706027938526 \n",
      "\n",
      "Epoch:  6847  Learning Rate:  0.0001063702002636954  Varinance:  0.0017442659436417906 \n",
      "\n",
      "Epoch:  6848  Learning Rate:  0.00010626388323080788  Varinance:  0.0017418292680533754 \n",
      "\n",
      "Epoch:  6849  Learning Rate:  0.00010615767246181252  Varinance:  0.0017393959964114426 \n",
      "\n",
      "Epoch:  6850  Learning Rate:  0.00010605156785049837  Varinance:  0.0017369661239608068 \n",
      "\n",
      "Epoch:  6851  Learning Rate:  0.00010594556929076102  Varinance:  0.0017345396459529162 \n",
      "\n",
      "Epoch:  6852  Learning Rate:  0.00010583967667660168  Varinance:  0.0017321165576458613 \n",
      "\n",
      "Epoch:  6853  Learning Rate:  0.00010573388990212784  Varinance:  0.0017296968543043553 \n",
      "\n",
      "Epoch:  6854  Learning Rate:  0.00010562820886155281  Varinance:  0.0017272805311997277 \n",
      "\n",
      "Epoch:  6855  Learning Rate:  0.00010552263344919536  Varinance:  0.0017248675836099053 \n",
      "\n",
      "Epoch:  6856  Learning Rate:  0.00010541716355948015  Varinance:  0.0017224580068194212 \n",
      "\n",
      "Epoch:  6857  Learning Rate:  0.00010531179908693738  Varinance:  0.0017200517961193952 \n",
      "\n",
      "Epoch:  6858  Learning Rate:  0.00010520653992620235  Varinance:  0.001717648946807516 \n",
      "\n",
      "Epoch:  6859  Learning Rate:  0.00010510138597201614  Varinance:  0.0017152494541880507 \n",
      "\n",
      "Epoch:  6860  Learning Rate:  0.00010499633711922455  Varinance:  0.0017128533135718233 \n",
      "\n",
      "Epoch:  6861  Learning Rate:  0.00010489139326277882  Varinance:  0.0017104605202762106 \n",
      "\n",
      "Epoch:  6862  Learning Rate:  0.00010478655429773522  Varinance:  0.0017080710696251227 \n",
      "\n",
      "Epoch:  6863  Learning Rate:  0.00010468182011925453  Varinance:  0.001705684956949011 \n",
      "\n",
      "Epoch:  6864  Learning Rate:  0.0001045771906226027  Varinance:  0.0017033021775848492 \n",
      "\n",
      "Epoch:  6865  Learning Rate:  0.00010447266570315028  Varinance:  0.0017009227268761176 \n",
      "\n",
      "Epoch:  6866  Learning Rate:  0.0001043682452563722  Varinance:  0.0016985466001728086 \n",
      "\n",
      "Epoch:  6867  Learning Rate:  0.00010426392917784816  Varinance:  0.0016961737928314089 \n",
      "\n",
      "Epoch:  6868  Learning Rate:  0.00010415971736326188  Varinance:  0.0016938043002148943 \n",
      "\n",
      "Epoch:  6869  Learning Rate:  0.00010405560970840164  Varinance:  0.00169143811769271 \n",
      "\n",
      "Epoch:  6870  Learning Rate:  0.0001039516061091599  Varinance:  0.0016890752406407783 \n",
      "\n",
      "Epoch:  6871  Learning Rate:  0.00010384770646153282  Varinance:  0.001686715664441482 \n",
      "\n",
      "Epoch:  6872  Learning Rate:  0.00010374391066162086  Varinance:  0.0016843593844836442 \n",
      "\n",
      "Epoch:  6873  Learning Rate:  0.00010364021860562829  Varinance:  0.0016820063961625396 \n",
      "\n",
      "Epoch:  6874  Learning Rate:  0.00010353663018986288  Varinance:  0.0016796566948798727 \n",
      "\n",
      "Epoch:  6875  Learning Rate:  0.00010343314531073638  Varinance:  0.0016773102760437736 \n",
      "\n",
      "Epoch:  6876  Learning Rate:  0.0001033297638647637  Varinance:  0.0016749671350687796 \n",
      "\n",
      "Epoch:  6877  Learning Rate:  0.00010322648574856353  Varinance:  0.0016726272673758412 \n",
      "\n",
      "Epoch:  6878  Learning Rate:  0.00010312331085885777  Varinance:  0.0016702906683923068 \n",
      "\n",
      "Epoch:  6879  Learning Rate:  0.0001030202390924714  Varinance:  0.001667957333551903 \n",
      "\n",
      "Epoch:  6880  Learning Rate:  0.00010291727034633277  Varinance:  0.0016656272582947444 \n",
      "\n",
      "Epoch:  6881  Learning Rate:  0.00010281440451747298  Varinance:  0.0016633004380673156 \n",
      "\n",
      "Epoch:  6882  Learning Rate:  0.00010271164150302628  Varinance:  0.0016609768683224534 \n",
      "\n",
      "Epoch:  6883  Learning Rate:  0.00010260898120022975  Varinance:  0.0016586565445193564 \n",
      "\n",
      "Epoch:  6884  Learning Rate:  0.00010250642350642285  Varinance:  0.0016563394621235625 \n",
      "\n",
      "Epoch:  6885  Learning Rate:  0.00010240396831904802  Varinance:  0.001654025616606947 \n",
      "\n",
      "Epoch:  6886  Learning Rate:  0.00010230161553565013  Varinance:  0.001651715003447703 \n",
      "\n",
      "Epoch:  6887  Learning Rate:  0.0001021993650538762  Varinance:  0.0016494076181303489 \n",
      "\n",
      "Epoch:  6888  Learning Rate:  0.00010209721677147594  Varinance:  0.0016471034561457102 \n",
      "\n",
      "Epoch:  6889  Learning Rate:  0.00010199517058630085  Varinance:  0.0016448025129909038 \n",
      "\n",
      "Epoch:  6890  Learning Rate:  0.00010189322639630487  Varinance:  0.0016425047841693448 \n",
      "\n",
      "Epoch:  6891  Learning Rate:  0.00010179138409954388  Varinance:  0.0016402102651907286 \n",
      "\n",
      "Epoch:  6892  Learning Rate:  0.00010168964359417534  Varinance:  0.0016379189515710242 \n",
      "\n",
      "Epoch:  6893  Learning Rate:  0.00010158800477845888  Varinance:  0.0016356308388324565 \n",
      "\n",
      "Epoch:  6894  Learning Rate:  0.00010148646755075579  Varinance:  0.0016333459225035159 \n",
      "\n",
      "Epoch:  6895  Learning Rate:  0.00010138503180952858  Varinance:  0.0016310641981189377 \n",
      "\n",
      "Epoch:  6896  Learning Rate:  0.00010128369745334174  Varinance:  0.0016287856612196869 \n",
      "\n",
      "Epoch:  6897  Learning Rate:  0.00010118246438086069  Varinance:  0.0016265103073529658 \n",
      "\n",
      "Epoch:  6898  Learning Rate:  0.00010108133249085245  Varinance:  0.0016242381320721955 \n",
      "\n",
      "Epoch:  6899  Learning Rate:  0.00010098030168218523  Varinance:  0.00162196913093701 \n",
      "\n",
      "Epoch:  6900  Learning Rate:  0.00010087937185382801  Varinance:  0.0016197032995132388 \n",
      "\n",
      "Epoch:  6901  Learning Rate:  0.00010077854290485106  Varinance:  0.0016174406333729144 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6902  Learning Rate:  0.00010067781473442547  Varinance:  0.0016151811280942542 \n",
      "\n",
      "Epoch:  6903  Learning Rate:  0.00010057718724182294  Varinance:  0.0016129247792616448 \n",
      "\n",
      "Epoch:  6904  Learning Rate:  0.00010047666032641611  Varinance:  0.0016106715824656499 \n",
      "\n",
      "Epoch:  6905  Learning Rate:  0.00010037623388767792  Varinance:  0.0016084215333029895 \n",
      "\n",
      "Epoch:  6906  Learning Rate:  0.00010027590782518196  Varinance:  0.0016061746273765385 \n",
      "\n",
      "Epoch:  6907  Learning Rate:  0.00010017568203860228  Varinance:  0.0016039308602953043 \n",
      "\n",
      "Epoch:  6908  Learning Rate:  0.00010007555642771289  Varinance:  0.0016016902276744399 \n",
      "\n",
      "Epoch:  6909  Learning Rate:  9.997553089238826e-05  Varinance:  0.00159945272513522 \n",
      "\n",
      "Epoch:  6910  Learning Rate:  9.987560533260293e-05  Varinance:  0.001597218348305031 \n",
      "\n",
      "Epoch:  6911  Learning Rate:  9.977577964843121e-05  Varinance:  0.001594987092817374 \n",
      "\n",
      "Epoch:  6912  Learning Rate:  9.96760537400475e-05  Varinance:  0.0015927589543118484 \n",
      "\n",
      "Epoch:  6913  Learning Rate:  9.957642750772578e-05  Varinance:  0.0015905339284341467 \n",
      "\n",
      "Epoch:  6914  Learning Rate:  9.947690085183984e-05  Varinance:  0.0015883120108360369 \n",
      "\n",
      "Epoch:  6915  Learning Rate:  9.937747367286315e-05  Varinance:  0.0015860931971753685 \n",
      "\n",
      "Epoch:  6916  Learning Rate:  9.927814587136831e-05  Varinance:  0.0015838774831160576 \n",
      "\n",
      "Epoch:  6917  Learning Rate:  9.917891734802762e-05  Varinance:  0.0015816648643280698 \n",
      "\n",
      "Epoch:  6918  Learning Rate:  9.907978800361263e-05  Varinance:  0.0015794553364874263 \n",
      "\n",
      "Epoch:  6919  Learning Rate:  9.898075773899383e-05  Varinance:  0.0015772488952761879 \n",
      "\n",
      "Epoch:  6920  Learning Rate:  9.888182645514108e-05  Varinance:  0.0015750455363824488 \n",
      "\n",
      "Epoch:  6921  Learning Rate:  9.878299405312295e-05  Varinance:  0.0015728452555003186 \n",
      "\n",
      "Epoch:  6922  Learning Rate:  9.86842604341071e-05  Varinance:  0.0015706480483299312 \n",
      "\n",
      "Epoch:  6923  Learning Rate:  9.858562549936001e-05  Varinance:  0.0015684539105774264 \n",
      "\n",
      "Epoch:  6924  Learning Rate:  9.848708915024653e-05  Varinance:  0.0015662628379549345 \n",
      "\n",
      "Epoch:  6925  Learning Rate:  9.838865128823041e-05  Varinance:  0.0015640748261805842 \n",
      "\n",
      "Epoch:  6926  Learning Rate:  9.829031181487387e-05  Varinance:  0.0015618898709784822 \n",
      "\n",
      "Epoch:  6927  Learning Rate:  9.819207063183726e-05  Varinance:  0.0015597079680787125 \n",
      "\n",
      "Epoch:  6928  Learning Rate:  9.809392764087953e-05  Varinance:  0.001557529113217315 \n",
      "\n",
      "Epoch:  6929  Learning Rate:  9.799588274385753e-05  Varinance:  0.0015553533021362943 \n",
      "\n",
      "Epoch:  6930  Learning Rate:  9.789793584272645e-05  Varinance:  0.0015531805305836039 \n",
      "\n",
      "Epoch:  6931  Learning Rate:  9.780008683953946e-05  Varinance:  0.0015510107943131283 \n",
      "\n",
      "Epoch:  6932  Learning Rate:  9.770233563644737e-05  Varinance:  0.0015488440890846925 \n",
      "\n",
      "Epoch:  6933  Learning Rate:  9.760468213569905e-05  Varinance:  0.0015466804106640415 \n",
      "\n",
      "Epoch:  6934  Learning Rate:  9.75071262396411e-05  Varinance:  0.0015445197548228388 \n",
      "\n",
      "Epoch:  6935  Learning Rate:  9.740966785071741e-05  Varinance:  0.0015423621173386464 \n",
      "\n",
      "Epoch:  6936  Learning Rate:  9.73123068714698e-05  Varinance:  0.0015402074939949327 \n",
      "\n",
      "Epoch:  6937  Learning Rate:  9.721504320453706e-05  Varinance:  0.0015380558805810566 \n",
      "\n",
      "Epoch:  6938  Learning Rate:  9.711787675265564e-05  Varinance:  0.001535907272892251 \n",
      "\n",
      "Epoch:  6939  Learning Rate:  9.702080741865914e-05  Varinance:  0.0015337616667296312 \n",
      "\n",
      "Epoch:  6940  Learning Rate:  9.692383510547806e-05  Varinance:  0.0015316190579001744 \n",
      "\n",
      "Epoch:  6941  Learning Rate:  9.682695971614017e-05  Varinance:  0.0015294794422167197 \n",
      "\n",
      "Epoch:  6942  Learning Rate:  9.673018115377016e-05  Varinance:  0.0015273428154979455 \n",
      "\n",
      "Epoch:  6943  Learning Rate:  9.663349932158926e-05  Varinance:  0.0015252091735683813 \n",
      "\n",
      "Epoch:  6944  Learning Rate:  9.653691412291583e-05  Varinance:  0.001523078512258388 \n",
      "\n",
      "Epoch:  6945  Learning Rate:  9.644042546116447e-05  Varinance:  0.0015209508274041442 \n",
      "\n",
      "Epoch:  6946  Learning Rate:  9.634403323984662e-05  Varinance:  0.0015188261148476531 \n",
      "\n",
      "Epoch:  6947  Learning Rate:  9.624773736257013e-05  Varinance:  0.0015167043704367262 \n",
      "\n",
      "Epoch:  6948  Learning Rate:  9.615153773303892e-05  Varinance:  0.0015145855900249676 \n",
      "\n",
      "Epoch:  6949  Learning Rate:  9.605543425505347e-05  Varinance:  0.0015124697694717817 \n",
      "\n",
      "Epoch:  6950  Learning Rate:  9.595942683251036e-05  Varinance:  0.0015103569046423548 \n",
      "\n",
      "Epoch:  6951  Learning Rate:  9.5863515369402e-05  Varinance:  0.0015082469914076517 \n",
      "\n",
      "Epoch:  6952  Learning Rate:  9.576769976981708e-05  Varinance:  0.0015061400256443972 \n",
      "\n",
      "Epoch:  6953  Learning Rate:  9.567197993793981e-05  Varinance:  0.0015040360032350848 \n",
      "\n",
      "Epoch:  6954  Learning Rate:  9.557635577805046e-05  Varinance:  0.0015019349200679594 \n",
      "\n",
      "Epoch:  6955  Learning Rate:  9.548082719452495e-05  Varinance:  0.001499836772037002 \n",
      "\n",
      "Epoch:  6956  Learning Rate:  9.538539409183449e-05  Varinance:  0.0014977415550419377 \n",
      "\n",
      "Epoch:  6957  Learning Rate:  9.529005637454608e-05  Varinance:  0.0014956492649882169 \n",
      "\n",
      "Epoch:  6958  Learning Rate:  9.519481394732207e-05  Varinance:  0.0014935598977870123 \n",
      "\n",
      "Epoch:  6959  Learning Rate:  9.509966671491985e-05  Varinance:  0.0014914734493552007 \n",
      "\n",
      "Epoch:  6960  Learning Rate:  9.500461458219236e-05  Varinance:  0.001489389915615371 \n",
      "\n",
      "Epoch:  6961  Learning Rate:  9.490965745408727e-05  Varinance:  0.0014873092924958082 \n",
      "\n",
      "Epoch:  6962  Learning Rate:  9.481479523564756e-05  Varinance:  0.001485231575930477 \n",
      "\n",
      "Epoch:  6963  Learning Rate:  9.472002783201107e-05  Varinance:  0.0014831567618590303 \n",
      "\n",
      "Epoch:  6964  Learning Rate:  9.462535514841023e-05  Varinance:  0.0014810848462267904 \n",
      "\n",
      "Epoch:  6965  Learning Rate:  9.453077709017241e-05  Varinance:  0.001479015824984747 \n",
      "\n",
      "Epoch:  6966  Learning Rate:  9.443629356271964e-05  Varinance:  0.0014769496940895373 \n",
      "\n",
      "Epoch:  6967  Learning Rate:  9.434190447156822e-05  Varinance:  0.0014748864495034556 \n",
      "\n",
      "Epoch:  6968  Learning Rate:  9.42476097223292e-05  Varinance:  0.0014728260871944368 \n",
      "\n",
      "Epoch:  6969  Learning Rate:  9.41534092207077e-05  Varinance:  0.0014707686031360397 \n",
      "\n",
      "Epoch:  6970  Learning Rate:  9.405930287250327e-05  Varinance:  0.0014687139933074566 \n",
      "\n",
      "Epoch:  6971  Learning Rate:  9.396529058360962e-05  Varinance:  0.0014666622536934938 \n",
      "\n",
      "Epoch:  6972  Learning Rate:  9.38713722600143e-05  Varinance:  0.0014646133802845693 \n",
      "\n",
      "Epoch:  6973  Learning Rate:  9.377754780779908e-05  Varinance:  0.0014625673690766942 \n",
      "\n",
      "Epoch:  6974  Learning Rate:  9.368381713313953e-05  Varinance:  0.0014605242160714814 \n",
      "\n",
      "Epoch:  6975  Learning Rate:  9.359018014230487e-05  Varinance:  0.0014584839172761292 \n",
      "\n",
      "Epoch:  6976  Learning Rate:  9.349663674165822e-05  Varinance:  0.0014564464687034059 \n",
      "\n",
      "Epoch:  6977  Learning Rate:  9.340318683765602e-05  Varinance:  0.0014544118663716572 \n",
      "\n",
      "Epoch:  6978  Learning Rate:  9.330983033684846e-05  Varinance:  0.0014523801063047893 \n",
      "\n",
      "Epoch:  6979  Learning Rate:  9.321656714587907e-05  Varinance:  0.0014503511845322646 \n",
      "\n",
      "Epoch:  6980  Learning Rate:  9.312339717148453e-05  Varinance:  0.0014483250970890845 \n",
      "\n",
      "Epoch:  6981  Learning Rate:  9.303032032049491e-05  Varinance:  0.001446301840015798 \n",
      "\n",
      "Epoch:  6982  Learning Rate:  9.293733649983344e-05  Varinance:  0.0014442814093584841 \n",
      "\n",
      "Epoch:  6983  Learning Rate:  9.284444561651615e-05  Varinance:  0.0014422638011687388 \n",
      "\n",
      "Epoch:  6984  Learning Rate:  9.275164757765228e-05  Varinance:  0.0014402490115036804 \n",
      "\n",
      "Epoch:  6985  Learning Rate:  9.265894229044365e-05  Varinance:  0.0014382370364259332 \n",
      "\n",
      "Epoch:  6986  Learning Rate:  9.256632966218501e-05  Varinance:  0.001436227872003625 \n",
      "\n",
      "Epoch:  6987  Learning Rate:  9.247380960026386e-05  Varinance:  0.0014342215143103674 \n",
      "\n",
      "Epoch:  6988  Learning Rate:  9.238138201215991e-05  Varinance:  0.0014322179594252657 \n",
      "\n",
      "Epoch:  6989  Learning Rate:  9.228904680544568e-05  Varinance:  0.0014302172034329022 \n",
      "\n",
      "Epoch:  6990  Learning Rate:  9.219680388778602e-05  Varinance:  0.0014282192424233209 \n",
      "\n",
      "Epoch:  6991  Learning Rate:  9.210465316693785e-05  Varinance:  0.0014262240724920363 \n",
      "\n",
      "Epoch:  6992  Learning Rate:  9.201259455075062e-05  Varinance:  0.0014242316897400142 \n",
      "\n",
      "Epoch:  6993  Learning Rate:  9.192062794716551e-05  Varinance:  0.0014222420902736696 \n",
      "\n",
      "Epoch:  6994  Learning Rate:  9.1828753264216e-05  Varinance:  0.0014202552702048499 \n",
      "\n",
      "Epoch:  6995  Learning Rate:  9.173697041002751e-05  Varinance:  0.0014182712256508405 \n",
      "\n",
      "Epoch:  6996  Learning Rate:  9.164527929281698e-05  Varinance:  0.0014162899527343522 \n",
      "\n",
      "Epoch:  6997  Learning Rate:  9.15536798208934e-05  Varinance:  0.0014143114475835032 \n",
      "\n",
      "Epoch:  6998  Learning Rate:  9.146217190265731e-05  Varinance:  0.0014123357063318292 \n",
      "\n",
      "Epoch:  6999  Learning Rate:  9.137075544660069e-05  Varinance:  0.0014103627251182646 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7000  Learning Rate:  9.127943036130722e-05  Varinance:  0.0014083925000871395 \n",
      "\n",
      "Epoch:  7001  Learning Rate:  9.118819655545162e-05  Varinance:  0.0014064250273881631 \n",
      "\n",
      "Epoch:  7002  Learning Rate:  9.109705393780019e-05  Varinance:  0.0014044603031764307 \n",
      "\n",
      "Epoch:  7003  Learning Rate:  9.100600241721036e-05  Varinance:  0.0014024983236124093 \n",
      "\n",
      "Epoch:  7004  Learning Rate:  9.091504190263046e-05  Varinance:  0.001400539084861921 \n",
      "\n",
      "Epoch:  7005  Learning Rate:  9.08241723031001e-05  Varinance:  0.001398582583096153 \n",
      "\n",
      "Epoch:  7006  Learning Rate:  9.073339352774954e-05  Varinance:  0.0013966288144916373 \n",
      "\n",
      "Epoch:  7007  Learning Rate:  9.064270548580009e-05  Varinance:  0.0013946777752302506 \n",
      "\n",
      "Epoch:  7008  Learning Rate:  9.055210808656373e-05  Varinance:  0.0013927294614991955 \n",
      "\n",
      "Epoch:  7009  Learning Rate:  9.046160123944293e-05  Varinance:  0.001390783869491009 \n",
      "\n",
      "Epoch:  7010  Learning Rate:  9.037118485393092e-05  Varinance:  0.0013888409954035457 \n",
      "\n",
      "Epoch:  7011  Learning Rate:  9.028085883961136e-05  Varinance:  0.0013869008354399654 \n",
      "\n",
      "Epoch:  7012  Learning Rate:  9.019062310615809e-05  Varinance:  0.0013849633858087392 \n",
      "\n",
      "Epoch:  7013  Learning Rate:  9.010047756333552e-05  Varinance:  0.0013830286427236344 \n",
      "\n",
      "Epoch:  7014  Learning Rate:  9.001042212099795e-05  Varinance:  0.0013810966024037003 \n",
      "\n",
      "Epoch:  7015  Learning Rate:  8.992045668908998e-05  Varinance:  0.0013791672610732756 \n",
      "\n",
      "Epoch:  7016  Learning Rate:  8.98305811776463e-05  Varinance:  0.001377240614961971 \n",
      "\n",
      "Epoch:  7017  Learning Rate:  8.974079549679119e-05  Varinance:  0.001375316660304666 \n",
      "\n",
      "Epoch:  7018  Learning Rate:  8.965109955673905e-05  Varinance:  0.001373395393341494 \n",
      "\n",
      "Epoch:  7019  Learning Rate:  8.956149326779403e-05  Varinance:  0.0013714768103178468 \n",
      "\n",
      "Epoch:  7020  Learning Rate:  8.947197654034965e-05  Varinance:  0.0013695609074843617 \n",
      "\n",
      "Epoch:  7021  Learning Rate:  8.938254928488936e-05  Varinance:  0.0013676476810969064 \n",
      "\n",
      "Epoch:  7022  Learning Rate:  8.929321141198571e-05  Varinance:  0.0013657371274165865 \n",
      "\n",
      "Epoch:  7023  Learning Rate:  8.920396283230091e-05  Varinance:  0.001363829242709728 \n",
      "\n",
      "Epoch:  7024  Learning Rate:  8.911480345658647e-05  Varinance:  0.001361924023247875 \n",
      "\n",
      "Epoch:  7025  Learning Rate:  8.902573319568283e-05  Varinance:  0.0013600214653077735 \n",
      "\n",
      "Epoch:  7026  Learning Rate:  8.89367519605198e-05  Varinance:  0.0013581215651713775 \n",
      "\n",
      "Epoch:  7027  Learning Rate:  8.884785966211622e-05  Varinance:  0.0013562243191258348 \n",
      "\n",
      "Epoch:  7028  Learning Rate:  8.875905621157963e-05  Varinance:  0.001354329723463473 \n",
      "\n",
      "Epoch:  7029  Learning Rate:  8.867034152010674e-05  Varinance:  0.0013524377744818063 \n",
      "\n",
      "Epoch:  7030  Learning Rate:  8.858171549898266e-05  Varinance:  0.0013505484684835194 \n",
      "\n",
      "Epoch:  7031  Learning Rate:  8.849317805958146e-05  Varinance:  0.0013486618017764628 \n",
      "\n",
      "Epoch:  7032  Learning Rate:  8.84047291133658e-05  Varinance:  0.0013467777706736394 \n",
      "\n",
      "Epoch:  7033  Learning Rate:  8.831636857188651e-05  Varinance:  0.0013448963714932087 \n",
      "\n",
      "Epoch:  7034  Learning Rate:  8.822809634678317e-05  Varinance:  0.0013430176005584742 \n",
      "\n",
      "Epoch:  7035  Learning Rate:  8.81399123497836e-05  Varinance:  0.001341141454197868 \n",
      "\n",
      "Epoch:  7036  Learning Rate:  8.805181649270365e-05  Varinance:  0.0013392679287449589 \n",
      "\n",
      "Epoch:  7037  Learning Rate:  8.796380868744761e-05  Varinance:  0.0013373970205384344 \n",
      "\n",
      "Epoch:  7038  Learning Rate:  8.787588884600749e-05  Varinance:  0.0013355287259221 \n",
      "\n",
      "Epoch:  7039  Learning Rate:  8.778805688046356e-05  Varinance:  0.0013336630412448616 \n",
      "\n",
      "Epoch:  7040  Learning Rate:  8.770031270298391e-05  Varinance:  0.0013317999628607315 \n",
      "\n",
      "Epoch:  7041  Learning Rate:  8.761265622582417e-05  Varinance:  0.0013299394871288167 \n",
      "\n",
      "Epoch:  7042  Learning Rate:  8.752508736132798e-05  Varinance:  0.0013280816104133026 \n",
      "\n",
      "Epoch:  7043  Learning Rate:  8.743760602192651e-05  Varinance:  0.001326226329083461 \n",
      "\n",
      "Epoch:  7044  Learning Rate:  8.735021212013828e-05  Varinance:  0.001324373639513633 \n",
      "\n",
      "Epoch:  7045  Learning Rate:  8.726290556856952e-05  Varinance:  0.0013225235380832274 \n",
      "\n",
      "Epoch:  7046  Learning Rate:  8.717568627991352e-05  Varinance:  0.0013206760211767034 \n",
      "\n",
      "Epoch:  7047  Learning Rate:  8.708855416695107e-05  Varinance:  0.0013188310851835786 \n",
      "\n",
      "Epoch:  7048  Learning Rate:  8.700150914255012e-05  Varinance:  0.0013169887264984138 \n",
      "\n",
      "Epoch:  7049  Learning Rate:  8.691455111966548e-05  Varinance:  0.0013151489415207992 \n",
      "\n",
      "Epoch:  7050  Learning Rate:  8.68276800113392e-05  Varinance:  0.0013133117266553623 \n",
      "\n",
      "Epoch:  7051  Learning Rate:  8.674089573070025e-05  Varinance:  0.0013114770783117507 \n",
      "\n",
      "Epoch:  7052  Learning Rate:  8.66541981909642e-05  Varinance:  0.0013096449929046296 \n",
      "\n",
      "Epoch:  7053  Learning Rate:  8.656758730543362e-05  Varinance:  0.0013078154668536661 \n",
      "\n",
      "Epoch:  7054  Learning Rate:  8.648106298749748e-05  Varinance:  0.001305988496583536 \n",
      "\n",
      "Epoch:  7055  Learning Rate:  8.639462515063154e-05  Varinance:  0.001304164078523909 \n",
      "\n",
      "Epoch:  7056  Learning Rate:  8.630827370839802e-05  Varinance:  0.0013023422091094363 \n",
      "\n",
      "Epoch:  7057  Learning Rate:  8.622200857444534e-05  Varinance:  0.0013005228847797563 \n",
      "\n",
      "Epoch:  7058  Learning Rate:  8.61358296625084e-05  Varinance:  0.0012987061019794788 \n",
      "\n",
      "Epoch:  7059  Learning Rate:  8.60497368864084e-05  Varinance:  0.0012968918571581826 \n",
      "\n",
      "Epoch:  7060  Learning Rate:  8.596373016005236e-05  Varinance:  0.0012950801467703998 \n",
      "\n",
      "Epoch:  7061  Learning Rate:  8.587780939743372e-05  Varinance:  0.0012932709672756216 \n",
      "\n",
      "Epoch:  7062  Learning Rate:  8.579197451263158e-05  Varinance:  0.0012914643151382857 \n",
      "\n",
      "Epoch:  7063  Learning Rate:  8.570622541981108e-05  Varinance:  0.0012896601868277616 \n",
      "\n",
      "Epoch:  7064  Learning Rate:  8.562056203322324e-05  Varinance:  0.0012878585788183585 \n",
      "\n",
      "Epoch:  7065  Learning Rate:  8.553498426720447e-05  Varinance:  0.001286059487589308 \n",
      "\n",
      "Epoch:  7066  Learning Rate:  8.54494920361771e-05  Varinance:  0.0012842629096247618 \n",
      "\n",
      "Epoch:  7067  Learning Rate:  8.536408525464897e-05  Varinance:  0.0012824688414137778 \n",
      "\n",
      "Epoch:  7068  Learning Rate:  8.527876383721313e-05  Varinance:  0.001280677279450324 \n",
      "\n",
      "Epoch:  7069  Learning Rate:  8.51935276985483e-05  Varinance:  0.0012788882202332673 \n",
      "\n",
      "Epoch:  7070  Learning Rate:  8.51083767534182e-05  Varinance:  0.0012771016602663578 \n",
      "\n",
      "Epoch:  7071  Learning Rate:  8.502331091667195e-05  Varinance:  0.0012753175960582368 \n",
      "\n",
      "Epoch:  7072  Learning Rate:  8.493833010324378e-05  Varinance:  0.0012735360241224209 \n",
      "\n",
      "Epoch:  7073  Learning Rate:  8.48534342281527e-05  Varinance:  0.0012717569409772993 \n",
      "\n",
      "Epoch:  7074  Learning Rate:  8.476862320650294e-05  Varinance:  0.0012699803431461183 \n",
      "\n",
      "Epoch:  7075  Learning Rate:  8.468389695348351e-05  Varinance:  0.0012682062271569875 \n",
      "\n",
      "Epoch:  7076  Learning Rate:  8.459925538436801e-05  Varinance:  0.0012664345895428667 \n",
      "\n",
      "Epoch:  7077  Learning Rate:  8.451469841451503e-05  Varinance:  0.0012646654268415522 \n",
      "\n",
      "Epoch:  7078  Learning Rate:  8.443022595936743e-05  Varinance:  0.0012628987355956843 \n",
      "\n",
      "Epoch:  7079  Learning Rate:  8.434583793445283e-05  Varinance:  0.0012611345123527324 \n",
      "\n",
      "Epoch:  7080  Learning Rate:  8.426153425538327e-05  Varinance:  0.001259372753664982 \n",
      "\n",
      "Epoch:  7081  Learning Rate:  8.41773148378549e-05  Varinance:  0.0012576134560895423 \n",
      "\n",
      "Epoch:  7082  Learning Rate:  8.40931795976484e-05  Varinance:  0.00125585661618833 \n",
      "\n",
      "Epoch:  7083  Learning Rate:  8.400912845062857e-05  Varinance:  0.001254102230528066 \n",
      "\n",
      "Epoch:  7084  Learning Rate:  8.392516131274412e-05  Varinance:  0.0012523502956802613 \n",
      "\n",
      "Epoch:  7085  Learning Rate:  8.384127810002804e-05  Varinance:  0.001250600808221223 \n",
      "\n",
      "Epoch:  7086  Learning Rate:  8.375747872859697e-05  Varinance:  0.0012488537647320412 \n",
      "\n",
      "Epoch:  7087  Learning Rate:  8.367376311465164e-05  Varinance:  0.0012471091617985747 \n",
      "\n",
      "Epoch:  7088  Learning Rate:  8.359013117447644e-05  Varinance:  0.001245366996011459 \n",
      "\n",
      "Epoch:  7089  Learning Rate:  8.350658282443933e-05  Varinance:  0.0012436272639660903 \n",
      "\n",
      "Epoch:  7090  Learning Rate:  8.342311798099198e-05  Varinance:  0.0012418899622626224 \n",
      "\n",
      "Epoch:  7091  Learning Rate:  8.333973656066964e-05  Varinance:  0.0012401550875059525 \n",
      "\n",
      "Epoch:  7092  Learning Rate:  8.325643848009074e-05  Varinance:  0.0012384226363057267 \n",
      "\n",
      "Epoch:  7093  Learning Rate:  8.317322365595733e-05  Varinance:  0.0012366926052763284 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7094  Learning Rate:  8.309009200505443e-05  Varinance:  0.0012349649910368629 \n",
      "\n",
      "Epoch:  7095  Learning Rate:  8.300704344425048e-05  Varinance:  0.001233239790211165 \n",
      "\n",
      "Epoch:  7096  Learning Rate:  8.292407789049694e-05  Varinance:  0.001231516999427785 \n",
      "\n",
      "Epoch:  7097  Learning Rate:  8.284119526082812e-05  Varinance:  0.001229796615319984 \n",
      "\n",
      "Epoch:  7098  Learning Rate:  8.275839547236149e-05  Varinance:  0.0012280786345257196 \n",
      "\n",
      "Epoch:  7099  Learning Rate:  8.267567844229729e-05  Varinance:  0.0012263630536876533 \n",
      "\n",
      "Epoch:  7100  Learning Rate:  8.259304408791836e-05  Varinance:  0.0012246498694531368 \n",
      "\n",
      "Epoch:  7101  Learning Rate:  8.251049232659046e-05  Varinance:  0.0012229390784741982 \n",
      "\n",
      "Epoch:  7102  Learning Rate:  8.24280230757617e-05  Varinance:  0.0012212306774075491 \n",
      "\n",
      "Epoch:  7103  Learning Rate:  8.234563625296287e-05  Varinance:  0.0012195246629145699 \n",
      "\n",
      "Epoch:  7104  Learning Rate:  8.226333177580725e-05  Varinance:  0.0012178210316613068 \n",
      "\n",
      "Epoch:  7105  Learning Rate:  8.218110956199017e-05  Varinance:  0.001216119780318457 \n",
      "\n",
      "Epoch:  7106  Learning Rate:  8.209896952928949e-05  Varinance:  0.001214420905561375 \n",
      "\n",
      "Epoch:  7107  Learning Rate:  8.201691159556528e-05  Varinance:  0.0012127244040700596 \n",
      "\n",
      "Epoch:  7108  Learning Rate:  8.193493567875941e-05  Varinance:  0.0012110302725291417 \n",
      "\n",
      "Epoch:  7109  Learning Rate:  8.185304169689613e-05  Varinance:  0.0012093385076278888 \n",
      "\n",
      "Epoch:  7110  Learning Rate:  8.17712295680813e-05  Varinance:  0.0012076491060601926 \n",
      "\n",
      "Epoch:  7111  Learning Rate:  8.168949921050283e-05  Varinance:  0.0012059620645245648 \n",
      "\n",
      "Epoch:  7112  Learning Rate:  8.160785054243047e-05  Varinance:  0.0012042773797241231 \n",
      "\n",
      "Epoch:  7113  Learning Rate:  8.152628348221538e-05  Varinance:  0.0012025950483665964 \n",
      "\n",
      "Epoch:  7114  Learning Rate:  8.144479794829056e-05  Varinance:  0.0012009150671643135 \n",
      "\n",
      "Epoch:  7115  Learning Rate:  8.136339385917054e-05  Varinance:  0.0011992374328341896 \n",
      "\n",
      "Epoch:  7116  Learning Rate:  8.12820711334511e-05  Varinance:  0.0011975621420977326 \n",
      "\n",
      "Epoch:  7117  Learning Rate:  8.120082968980964e-05  Varinance:  0.0011958891916810275 \n",
      "\n",
      "Epoch:  7118  Learning Rate:  8.111966944700455e-05  Varinance:  0.0011942185783147359 \n",
      "\n",
      "Epoch:  7119  Learning Rate:  8.103859032387568e-05  Varinance:  0.0011925502987340797 \n",
      "\n",
      "Epoch:  7120  Learning Rate:  8.095759223934396e-05  Varinance:  0.0011908843496788483 \n",
      "\n",
      "Epoch:  7121  Learning Rate:  8.087667511241114e-05  Varinance:  0.0011892207278933846 \n",
      "\n",
      "Epoch:  7122  Learning Rate:  8.079583886216018e-05  Varinance:  0.0011875594301265745 \n",
      "\n",
      "Epoch:  7123  Learning Rate:  8.071508340775491e-05  Varinance:  0.0011859004531318508 \n",
      "\n",
      "Epoch:  7124  Learning Rate:  8.063440866843967e-05  Varinance:  0.0011842437936671801 \n",
      "\n",
      "Epoch:  7125  Learning Rate:  8.05538145635399e-05  Varinance:  0.0011825894484950598 \n",
      "\n",
      "Epoch:  7126  Learning Rate:  8.047330101246134e-05  Varinance:  0.0011809374143825042 \n",
      "\n",
      "Epoch:  7127  Learning Rate:  8.039286793469048e-05  Varinance:  0.0011792876881010497 \n",
      "\n",
      "Epoch:  7128  Learning Rate:  8.031251524979435e-05  Varinance:  0.0011776402664267426 \n",
      "\n",
      "Epoch:  7129  Learning Rate:  8.023224287742007e-05  Varinance:  0.0011759951461401276 \n",
      "\n",
      "Epoch:  7130  Learning Rate:  8.015205073729544e-05  Varinance:  0.0011743523240262523 \n",
      "\n",
      "Epoch:  7131  Learning Rate:  8.007193874922815e-05  Varinance:  0.0011727117968746534 \n",
      "\n",
      "Epoch:  7132  Learning Rate:  7.999190683310629e-05  Varinance:  0.001171073561479355 \n",
      "\n",
      "Epoch:  7133  Learning Rate:  7.9911954908898e-05  Varinance:  0.0011694376146388534 \n",
      "\n",
      "Epoch:  7134  Learning Rate:  7.983208289665119e-05  Varinance:  0.0011678039531561235 \n",
      "\n",
      "Epoch:  7135  Learning Rate:  7.975229071649393e-05  Varinance:  0.0011661725738386064 \n",
      "\n",
      "Epoch:  7136  Learning Rate:  7.967257828863412e-05  Varinance:  0.0011645434734981967 \n",
      "\n",
      "Epoch:  7137  Learning Rate:  7.959294553335915e-05  Varinance:  0.001162916648951249 \n",
      "\n",
      "Epoch:  7138  Learning Rate:  7.951339237103643e-05  Varinance:  0.0011612920970185633 \n",
      "\n",
      "Epoch:  7139  Learning Rate:  7.943391872211264e-05  Varinance:  0.0011596698145253828 \n",
      "\n",
      "Epoch:  7140  Learning Rate:  7.935452450711419e-05  Varinance:  0.0011580497983013794 \n",
      "\n",
      "Epoch:  7141  Learning Rate:  7.927520964664691e-05  Varinance:  0.0011564320451806604 \n",
      "\n",
      "Epoch:  7142  Learning Rate:  7.919597406139584e-05  Varinance:  0.0011548165520017559 \n",
      "\n",
      "Epoch:  7143  Learning Rate:  7.911681767212541e-05  Varinance:  0.001153203315607605 \n",
      "\n",
      "Epoch:  7144  Learning Rate:  7.903774039967932e-05  Varinance:  0.0011515923328455648 \n",
      "\n",
      "Epoch:  7145  Learning Rate:  7.895874216498015e-05  Varinance:  0.001149983600567396 \n",
      "\n",
      "Epoch:  7146  Learning Rate:  7.88798228890298e-05  Varinance:  0.0011483771156292507 \n",
      "\n",
      "Epoch:  7147  Learning Rate:  7.880098249290884e-05  Varinance:  0.0011467728748916792 \n",
      "\n",
      "Epoch:  7148  Learning Rate:  7.872222089777694e-05  Varinance:  0.0011451708752196158 \n",
      "\n",
      "Epoch:  7149  Learning Rate:  7.864353802487256e-05  Varinance:  0.0011435711134823764 \n",
      "\n",
      "Epoch:  7150  Learning Rate:  7.856493379551269e-05  Varinance:  0.0011419735865536434 \n",
      "\n",
      "Epoch:  7151  Learning Rate:  7.848640813109316e-05  Varinance:  0.0011403782913114737 \n",
      "\n",
      "Epoch:  7152  Learning Rate:  7.840796095308838e-05  Varinance:  0.001138785224638285 \n",
      "\n",
      "Epoch:  7153  Learning Rate:  7.832959218305102e-05  Varinance:  0.001137194383420844 \n",
      "\n",
      "Epoch:  7154  Learning Rate:  7.825130174261244e-05  Varinance:  0.0011356057645502725 \n",
      "\n",
      "Epoch:  7155  Learning Rate:  7.817308955348205e-05  Varinance:  0.0011340193649220336 \n",
      "\n",
      "Epoch:  7156  Learning Rate:  7.809495553744772e-05  Varinance:  0.0011324351814359288 \n",
      "\n",
      "Epoch:  7157  Learning Rate:  7.801689961637551e-05  Varinance:  0.001130853210996085 \n",
      "\n",
      "Epoch:  7158  Learning Rate:  7.793892171220935e-05  Varinance:  0.0011292734505109596 \n",
      "\n",
      "Epoch:  7159  Learning Rate:  7.78610217469714e-05  Varinance:  0.0011276958968933289 \n",
      "\n",
      "Epoch:  7160  Learning Rate:  7.778319964276175e-05  Varinance:  0.0011261205470602762 \n",
      "\n",
      "Epoch:  7161  Learning Rate:  7.770545532175816e-05  Varinance:  0.0011245473979331966 \n",
      "\n",
      "Epoch:  7162  Learning Rate:  7.762778870621643e-05  Varinance:  0.0011229764464377849 \n",
      "\n",
      "Epoch:  7163  Learning Rate:  7.75501997184698e-05  Varinance:  0.001121407689504032 \n",
      "\n",
      "Epoch:  7164  Learning Rate:  7.747268828092937e-05  Varinance:  0.0011198411240662123 \n",
      "\n",
      "Epoch:  7165  Learning Rate:  7.739525431608374e-05  Varinance:  0.0011182767470628875 \n",
      "\n",
      "Epoch:  7166  Learning Rate:  7.731789774649881e-05  Varinance:  0.0011167145554368972 \n",
      "\n",
      "Epoch:  7167  Learning Rate:  7.724061849481805e-05  Varinance:  0.0011151545461353454 \n",
      "\n",
      "Epoch:  7168  Learning Rate:  7.71634164837623e-05  Varinance:  0.001113596716109607 \n",
      "\n",
      "Epoch:  7169  Learning Rate:  7.70862916361294e-05  Varinance:  0.0011120410623153133 \n",
      "\n",
      "Epoch:  7170  Learning Rate:  7.700924387479463e-05  Varinance:  0.0011104875817123509 \n",
      "\n",
      "Epoch:  7171  Learning Rate:  7.693227312271009e-05  Varinance:  0.0011089362712648468 \n",
      "\n",
      "Epoch:  7172  Learning Rate:  7.685537930290508e-05  Varinance:  0.001107387127941176 \n",
      "\n",
      "Epoch:  7173  Learning Rate:  7.677856233848584e-05  Varinance:  0.0011058401487139473 \n",
      "\n",
      "Epoch:  7174  Learning Rate:  7.670182215263526e-05  Varinance:  0.0011042953305599936 \n",
      "\n",
      "Epoch:  7175  Learning Rate:  7.662515866861323e-05  Varinance:  0.001102752670460377 \n",
      "\n",
      "Epoch:  7176  Learning Rate:  7.654857180975634e-05  Varinance:  0.0011012121654003745 \n",
      "\n",
      "Epoch:  7177  Learning Rate:  7.647206149947756e-05  Varinance:  0.0010996738123694762 \n",
      "\n",
      "Epoch:  7178  Learning Rate:  7.63956276612667e-05  Varinance:  0.0010981376083613731 \n",
      "\n",
      "Epoch:  7179  Learning Rate:  7.631927021868982e-05  Varinance:  0.001096603550373961 \n",
      "\n",
      "Epoch:  7180  Learning Rate:  7.624298909538951e-05  Varinance:  0.001095071635409329 \n",
      "\n",
      "Epoch:  7181  Learning Rate:  7.616678421508473e-05  Varinance:  0.00109354186047375 \n",
      "\n",
      "Epoch:  7182  Learning Rate:  7.609065550157043e-05  Varinance:  0.0010920142225776825 \n",
      "\n",
      "Epoch:  7183  Learning Rate:  7.601460287871798e-05  Varinance:  0.0010904887187357618 \n",
      "\n",
      "Epoch:  7184  Learning Rate:  7.593862627047481e-05  Varinance:  0.0010889653459667935 \n",
      "\n",
      "Epoch:  7185  Learning Rate:  7.586272560086417e-05  Varinance:  0.001087444101293744 \n",
      "\n",
      "Epoch:  7186  Learning Rate:  7.578690079398553e-05  Varinance:  0.001085924981743742 \n",
      "\n",
      "Epoch:  7187  Learning Rate:  7.571115177401392e-05  Varinance:  0.001084407984348071 \n",
      "\n",
      "Epoch:  7188  Learning Rate:  7.56354784652004e-05  Varinance:  0.0010828931061421555 \n",
      "\n",
      "Epoch:  7189  Learning Rate:  7.555988079187172e-05  Varinance:  0.001081380344165567 \n",
      "\n",
      "Epoch:  7190  Learning Rate:  7.548435867843004e-05  Varinance:  0.0010798696954620105 \n",
      "\n",
      "Epoch:  7191  Learning Rate:  7.540891204935334e-05  Varinance:  0.001078361157079323 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7192  Learning Rate:  7.533354082919505e-05  Varinance:  0.0010768547260694597 \n",
      "\n",
      "Epoch:  7193  Learning Rate:  7.525824494258378e-05  Varinance:  0.0010753503994884999 \n",
      "\n",
      "Epoch:  7194  Learning Rate:  7.518302431422381e-05  Varinance:  0.0010738481743966359 \n",
      "\n",
      "Epoch:  7195  Learning Rate:  7.510787886889435e-05  Varinance:  0.00107234804785816 \n",
      "\n",
      "Epoch:  7196  Learning Rate:  7.503280853145001e-05  Varinance:  0.0010708500169414725 \n",
      "\n",
      "Epoch:  7197  Learning Rate:  7.495781322682053e-05  Varinance:  0.001069354078719066 \n",
      "\n",
      "Epoch:  7198  Learning Rate:  7.488289288001045e-05  Varinance:  0.0010678602302675257 \n",
      "\n",
      "Epoch:  7199  Learning Rate:  7.48080474160995e-05  Varinance:  0.0010663684686675143 \n",
      "\n",
      "Epoch:  7200  Learning Rate:  7.473327676024225e-05  Varinance:  0.0010648787910037788 \n",
      "\n",
      "Epoch:  7201  Learning Rate:  7.465858083766792e-05  Varinance:  0.0010633911943651383 \n",
      "\n",
      "Epoch:  7202  Learning Rate:  7.458395957368074e-05  Varinance:  0.001061905675844473 \n",
      "\n",
      "Epoch:  7203  Learning Rate:  7.450941289365926e-05  Varinance:  0.0010604222325387306 \n",
      "\n",
      "Epoch:  7204  Learning Rate:  7.443494072305689e-05  Varinance:  0.0010589408615489118 \n",
      "\n",
      "Epoch:  7205  Learning Rate:  7.436054298740151e-05  Varinance:  0.001057461559980069 \n",
      "\n",
      "Epoch:  7206  Learning Rate:  7.428621961229527e-05  Varinance:  0.001055984324941293 \n",
      "\n",
      "Epoch:  7207  Learning Rate:  7.42119705234148e-05  Varinance:  0.0010545091535457197 \n",
      "\n",
      "Epoch:  7208  Learning Rate:  7.413779564651113e-05  Varinance:  0.001053036042910516 \n",
      "\n",
      "Epoch:  7209  Learning Rate:  7.40636949074092e-05  Varinance:  0.0010515649901568721 \n",
      "\n",
      "Epoch:  7210  Learning Rate:  7.398966823200841e-05  Varinance:  0.0010500959924100049 \n",
      "\n",
      "Epoch:  7211  Learning Rate:  7.391571554628198e-05  Varinance:  0.0010486290467991463 \n",
      "\n",
      "Epoch:  7212  Learning Rate:  7.384183677627723e-05  Varinance:  0.0010471641504575348 \n",
      "\n",
      "Epoch:  7213  Learning Rate:  7.37680318481155e-05  Varinance:  0.0010457013005224177 \n",
      "\n",
      "Epoch:  7214  Learning Rate:  7.369430068799167e-05  Varinance:  0.00104424049413504 \n",
      "\n",
      "Epoch:  7215  Learning Rate:  7.362064322217468e-05  Varinance:  0.0010427817284406427 \n",
      "\n",
      "Epoch:  7216  Learning Rate:  7.354705937700713e-05  Varinance:  0.0010413250005884486 \n",
      "\n",
      "Epoch:  7217  Learning Rate:  7.3473549078905e-05  Varinance:  0.0010398703077316685 \n",
      "\n",
      "Epoch:  7218  Learning Rate:  7.340011225435815e-05  Varinance:  0.0010384176470274906 \n",
      "\n",
      "Epoch:  7219  Learning Rate:  7.332674882992959e-05  Varinance:  0.001036967015637068 \n",
      "\n",
      "Epoch:  7220  Learning Rate:  7.325345873225598e-05  Varinance:  0.0010355184107255263 \n",
      "\n",
      "Epoch:  7221  Learning Rate:  7.318024188804729e-05  Varinance:  0.0010340718294619483 \n",
      "\n",
      "Epoch:  7222  Learning Rate:  7.31070982240865e-05  Varinance:  0.0010326272690193738 \n",
      "\n",
      "Epoch:  7223  Learning Rate:  7.303402766723002e-05  Varinance:  0.0010311847265747865 \n",
      "\n",
      "Epoch:  7224  Learning Rate:  7.296103014440738e-05  Varinance:  0.0010297441993091189 \n",
      "\n",
      "Epoch:  7225  Learning Rate:  7.288810558262088e-05  Varinance:  0.0010283056844072418 \n",
      "\n",
      "Epoch:  7226  Learning Rate:  7.28152539089461e-05  Varinance:  0.0010268691790579528 \n",
      "\n",
      "Epoch:  7227  Learning Rate:  7.274247505053125e-05  Varinance:  0.0010254346804539829 \n",
      "\n",
      "Epoch:  7228  Learning Rate:  7.26697689345975e-05  Varinance:  0.001024002185791982 \n",
      "\n",
      "Epoch:  7229  Learning Rate:  7.259713548843881e-05  Varinance:  0.001022571692272519 \n",
      "\n",
      "Epoch:  7230  Learning Rate:  7.25245746394216e-05  Varinance:  0.0010211431971000667 \n",
      "\n",
      "Epoch:  7231  Learning Rate:  7.245208631498507e-05  Varinance:  0.00101971669748301 \n",
      "\n",
      "Epoch:  7232  Learning Rate:  7.237967044264094e-05  Varinance:  0.0010182921906336329 \n",
      "\n",
      "Epoch:  7233  Learning Rate:  7.230732694997324e-05  Varinance:  0.001016869673768108 \n",
      "\n",
      "Epoch:  7234  Learning Rate:  7.223505576463857e-05  Varinance:  0.001015449144106503 \n",
      "\n",
      "Epoch:  7235  Learning Rate:  7.216285681436562e-05  Varinance:  0.0010140305988727669 \n",
      "\n",
      "Epoch:  7236  Learning Rate:  7.209073002695551e-05  Varinance:  0.0010126140352947283 \n",
      "\n",
      "Epoch:  7237  Learning Rate:  7.201867533028148e-05  Varinance:  0.001011199450604083 \n",
      "\n",
      "Epoch:  7238  Learning Rate:  7.194669265228873e-05  Varinance:  0.0010097868420364002 \n",
      "\n",
      "Epoch:  7239  Learning Rate:  7.187478192099462e-05  Varinance:  0.0010083762068311098 \n",
      "\n",
      "Epoch:  7240  Learning Rate:  7.18029430644885e-05  Varinance:  0.0010069675422314938 \n",
      "\n",
      "Epoch:  7241  Learning Rate:  7.173117601093136e-05  Varinance:  0.0010055608454846898 \n",
      "\n",
      "Epoch:  7242  Learning Rate:  7.165948068855626e-05  Varinance:  0.0010041561138416795 \n",
      "\n",
      "Epoch:  7243  Learning Rate:  7.158785702566776e-05  Varinance:  0.0010027533445572869 \n",
      "\n",
      "Epoch:  7244  Learning Rate:  7.151630495064225e-05  Varinance:  0.0010013525348901652 \n",
      "\n",
      "Epoch:  7245  Learning Rate:  7.144482439192772e-05  Varinance:  0.0009999536821028029 \n",
      "\n",
      "Epoch:  7246  Learning Rate:  7.137341527804346e-05  Varinance:  0.000998556783461512 \n",
      "\n",
      "Epoch:  7247  Learning Rate:  7.130207753758044e-05  Varinance:  0.0009971618362364194 \n",
      "\n",
      "Epoch:  7248  Learning Rate:  7.123081109920095e-05  Varinance:  0.000995768837701469 \n",
      "\n",
      "Epoch:  7249  Learning Rate:  7.115961589163846e-05  Varinance:  0.0009943777851344128 \n",
      "\n",
      "Epoch:  7250  Learning Rate:  7.108849184369783e-05  Varinance:  0.0009929886758168064 \n",
      "\n",
      "Epoch:  7251  Learning Rate:  7.101743888425491e-05  Varinance:  0.000991601507033998 \n",
      "\n",
      "Epoch:  7252  Learning Rate:  7.094645694225679e-05  Varinance:  0.0009902162760751336 \n",
      "\n",
      "Epoch:  7253  Learning Rate:  7.087554594672159e-05  Varinance:  0.0009888329802331458 \n",
      "\n",
      "Epoch:  7254  Learning Rate:  7.080470582673818e-05  Varinance:  0.0009874516168047436 \n",
      "\n",
      "Epoch:  7255  Learning Rate:  7.073393651146656e-05  Varinance:  0.000986072183090418 \n",
      "\n",
      "Epoch:  7256  Learning Rate:  7.066323793013727e-05  Varinance:  0.0009846946763944293 \n",
      "\n",
      "Epoch:  7257  Learning Rate:  7.059261001205184e-05  Varinance:  0.0009833190940248052 \n",
      "\n",
      "Epoch:  7258  Learning Rate:  7.052205268658232e-05  Varinance:  0.0009819454332933284 \n",
      "\n",
      "Epoch:  7259  Learning Rate:  7.045156588317133e-05  Varinance:  0.000980573691515543 \n",
      "\n",
      "Epoch:  7260  Learning Rate:  7.038114953133208e-05  Varinance:  0.0009792038660107428 \n",
      "\n",
      "Epoch:  7261  Learning Rate:  7.031080356064829e-05  Varinance:  0.0009778359541019607 \n",
      "\n",
      "Epoch:  7262  Learning Rate:  7.024052790077387e-05  Varinance:  0.0009764699531159756 \n",
      "\n",
      "Epoch:  7263  Learning Rate:  7.017032248143324e-05  Varinance:  0.000975105860383298 \n",
      "\n",
      "Epoch:  7264  Learning Rate:  7.01001872324209e-05  Varinance:  0.00097374367323817 \n",
      "\n",
      "Epoch:  7265  Learning Rate:  7.003012208360163e-05  Varinance:  0.0009723833890185519 \n",
      "\n",
      "Epoch:  7266  Learning Rate:  6.996012696491035e-05  Varinance:  0.0009710250050661282 \n",
      "\n",
      "Epoch:  7267  Learning Rate:  6.989020180635178e-05  Varinance:  0.0009696685187262973 \n",
      "\n",
      "Epoch:  7268  Learning Rate:  6.982034653800086e-05  Varinance:  0.0009683139273481602 \n",
      "\n",
      "Epoch:  7269  Learning Rate:  6.975056109000234e-05  Varinance:  0.0009669612282845269 \n",
      "\n",
      "Epoch:  7270  Learning Rate:  6.968084539257068e-05  Varinance:  0.0009656104188919034 \n",
      "\n",
      "Epoch:  7271  Learning Rate:  6.961119937599027e-05  Varinance:  0.0009642614965304898 \n",
      "\n",
      "Epoch:  7272  Learning Rate:  6.954162297061497e-05  Varinance:  0.0009629144585641698 \n",
      "\n",
      "Epoch:  7273  Learning Rate:  6.947211610686845e-05  Varinance:  0.0009615693023605139 \n",
      "\n",
      "Epoch:  7274  Learning Rate:  6.940267871524388e-05  Varinance:  0.0009602260252907704 \n",
      "\n",
      "Epoch:  7275  Learning Rate:  6.933331072630376e-05  Varinance:  0.0009588846247298548 \n",
      "\n",
      "Epoch:  7276  Learning Rate:  6.926401207068013e-05  Varinance:  0.0009575450980563548 \n",
      "\n",
      "Epoch:  7277  Learning Rate:  6.91947826790744e-05  Varinance:  0.0009562074426525199 \n",
      "\n",
      "Epoch:  7278  Learning Rate:  6.912562248225706e-05  Varinance:  0.0009548716559042515 \n",
      "\n",
      "Epoch:  7279  Learning Rate:  6.905653141106803e-05  Varinance:  0.0009535377352011083 \n",
      "\n",
      "Epoch:  7280  Learning Rate:  6.89875093964161e-05  Varinance:  0.0009522056779362934 \n",
      "\n",
      "Epoch:  7281  Learning Rate:  6.891855636927931e-05  Varinance:  0.0009508754815066535 \n",
      "\n",
      "Epoch:  7282  Learning Rate:  6.88496722607047e-05  Varinance:  0.0009495471433126666 \n",
      "\n",
      "Epoch:  7283  Learning Rate:  6.878085700180802e-05  Varinance:  0.0009482206607584475 \n",
      "\n",
      "Epoch:  7284  Learning Rate:  6.871211052377408e-05  Varinance:  0.0009468960312517369 \n",
      "\n",
      "Epoch:  7285  Learning Rate:  6.864343275785646e-05  Varinance:  0.0009455732522038922 \n",
      "\n",
      "Epoch:  7286  Learning Rate:  6.857482363537724e-05  Varinance:  0.0009442523210298918 \n",
      "\n",
      "Epoch:  7287  Learning Rate:  6.850628308772744e-05  Varinance:  0.0009429332351483237 \n",
      "\n",
      "Epoch:  7288  Learning Rate:  6.843781104636636e-05  Varinance:  0.0009416159919813836 \n",
      "\n",
      "Epoch:  7289  Learning Rate:  6.836940744282206e-05  Varinance:  0.0009403005889548634 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7290  Learning Rate:  6.830107220869094e-05  Varinance:  0.0009389870234981562 \n",
      "\n",
      "Epoch:  7291  Learning Rate:  6.823280527563766e-05  Varinance:  0.0009376752930442457 \n",
      "\n",
      "Epoch:  7292  Learning Rate:  6.816460657539535e-05  Varinance:  0.0009363653950296971 \n",
      "\n",
      "Epoch:  7293  Learning Rate:  6.809647603976534e-05  Varinance:  0.0009350573268946616 \n",
      "\n",
      "Epoch:  7294  Learning Rate:  6.8028413600617e-05  Varinance:  0.0009337510860828643 \n",
      "\n",
      "Epoch:  7295  Learning Rate:  6.7960419189888e-05  Varinance:  0.0009324466700416034 \n",
      "\n",
      "Epoch:  7296  Learning Rate:  6.789249273958377e-05  Varinance:  0.000931144076221738 \n",
      "\n",
      "Epoch:  7297  Learning Rate:  6.782463418177795e-05  Varinance:  0.0009298433020776933 \n",
      "\n",
      "Epoch:  7298  Learning Rate:  6.775684344861203e-05  Varinance:  0.0009285443450674505 \n",
      "\n",
      "Epoch:  7299  Learning Rate:  6.768912047229511e-05  Varinance:  0.0009272472026525369 \n",
      "\n",
      "Epoch:  7300  Learning Rate:  6.762146518510434e-05  Varinance:  0.0009259518722980313 \n",
      "\n",
      "Epoch:  7301  Learning Rate:  6.755387751938444e-05  Varinance:  0.0009246583514725512 \n",
      "\n",
      "Epoch:  7302  Learning Rate:  6.748635740754762e-05  Varinance:  0.0009233666376482531 \n",
      "\n",
      "Epoch:  7303  Learning Rate:  6.74189047820739e-05  Varinance:  0.0009220767283008189 \n",
      "\n",
      "Epoch:  7304  Learning Rate:  6.735151957551053e-05  Varinance:  0.0009207886209094625 \n",
      "\n",
      "Epoch:  7305  Learning Rate:  6.728420172047233e-05  Varinance:  0.0009195023129569187 \n",
      "\n",
      "Epoch:  7306  Learning Rate:  6.721695114964153e-05  Varinance:  0.0009182178019294341 \n",
      "\n",
      "Epoch:  7307  Learning Rate:  6.714976779576741e-05  Varinance:  0.0009169350853167723 \n",
      "\n",
      "Epoch:  7308  Learning Rate:  6.708265159166668e-05  Varinance:  0.0009156541606122012 \n",
      "\n",
      "Epoch:  7309  Learning Rate:  6.70156024702232e-05  Varinance:  0.0009143750253124924 \n",
      "\n",
      "Epoch:  7310  Learning Rate:  6.694862036438772e-05  Varinance:  0.00091309767691791 \n",
      "\n",
      "Epoch:  7311  Learning Rate:  6.688170520717824e-05  Varinance:  0.0009118221129322148 \n",
      "\n",
      "Epoch:  7312  Learning Rate:  6.681485693167948e-05  Varinance:  0.0009105483308626539 \n",
      "\n",
      "Epoch:  7313  Learning Rate:  6.674807547104321e-05  Varinance:  0.0009092763282199533 \n",
      "\n",
      "Epoch:  7314  Learning Rate:  6.668136075848805e-05  Varinance:  0.0009080061025183198 \n",
      "\n",
      "Epoch:  7315  Learning Rate:  6.661471272729913e-05  Varinance:  0.0009067376512754326 \n",
      "\n",
      "Epoch:  7316  Learning Rate:  6.65481313108285e-05  Varinance:  0.0009054709720124389 \n",
      "\n",
      "Epoch:  7317  Learning Rate:  6.648161644249478e-05  Varinance:  0.0009042060622539448 \n",
      "\n",
      "Epoch:  7318  Learning Rate:  6.641516805578299e-05  Varinance:  0.0009029429195280189 \n",
      "\n",
      "Epoch:  7319  Learning Rate:  6.634878608424484e-05  Varinance:  0.0009016815413661834 \n",
      "\n",
      "Epoch:  7320  Learning Rate:  6.628247046149825e-05  Varinance:  0.0009004219253034031 \n",
      "\n",
      "Epoch:  7321  Learning Rate:  6.621622112122764e-05  Varinance:  0.0008991640688780923 \n",
      "\n",
      "Epoch:  7322  Learning Rate:  6.615003799718374e-05  Varinance:  0.0008979079696321018 \n",
      "\n",
      "Epoch:  7323  Learning Rate:  6.608392102318327e-05  Varinance:  0.000896653625110718 \n",
      "\n",
      "Epoch:  7324  Learning Rate:  6.601787013310935e-05  Varinance:  0.000895401032862652 \n",
      "\n",
      "Epoch:  7325  Learning Rate:  6.59518852609111e-05  Varinance:  0.0008941501904400435 \n",
      "\n",
      "Epoch:  7326  Learning Rate:  6.588596634060358e-05  Varinance:  0.0008929010953984524 \n",
      "\n",
      "Epoch:  7327  Learning Rate:  6.582011330626792e-05  Varinance:  0.000891653745296848 \n",
      "\n",
      "Epoch:  7328  Learning Rate:  6.575432609205102e-05  Varinance:  0.0008904081376976148 \n",
      "\n",
      "Epoch:  7329  Learning Rate:  6.568860463216566e-05  Varinance:  0.0008891642701665408 \n",
      "\n",
      "Epoch:  7330  Learning Rate:  6.56229488608905e-05  Varinance:  0.0008879221402728163 \n",
      "\n",
      "Epoch:  7331  Learning Rate:  6.555735871256959e-05  Varinance:  0.0008866817455890223 \n",
      "\n",
      "Epoch:  7332  Learning Rate:  6.549183412161286e-05  Varinance:  0.0008854430836911357 \n",
      "\n",
      "Epoch:  7333  Learning Rate:  6.542637502249577e-05  Varinance:  0.00088420615215852 \n",
      "\n",
      "Epoch:  7334  Learning Rate:  6.536098134975909e-05  Varinance:  0.0008829709485739146 \n",
      "\n",
      "Epoch:  7335  Learning Rate:  6.529565303800927e-05  Varinance:  0.0008817374705234414 \n",
      "\n",
      "Epoch:  7336  Learning Rate:  6.523039002191786e-05  Varinance:  0.0008805057155965926 \n",
      "\n",
      "Epoch:  7337  Learning Rate:  6.516519223622193e-05  Varinance:  0.0008792756813862289 \n",
      "\n",
      "Epoch:  7338  Learning Rate:  6.51000596157237e-05  Varinance:  0.0008780473654885696 \n",
      "\n",
      "Epoch:  7339  Learning Rate:  6.503499209529047e-05  Varinance:  0.0008768207655031961 \n",
      "\n",
      "Epoch:  7340  Learning Rate:  6.496998960985475e-05  Varinance:  0.0008755958790330438 \n",
      "\n",
      "Epoch:  7341  Learning Rate:  6.490505209441411e-05  Varinance:  0.0008743727036843911 \n",
      "\n",
      "Epoch:  7342  Learning Rate:  6.48401794840309e-05  Varinance:  0.0008731512370668661 \n",
      "\n",
      "Epoch:  7343  Learning Rate:  6.477537171383266e-05  Varinance:  0.0008719314767934352 \n",
      "\n",
      "Epoch:  7344  Learning Rate:  6.471062871901147e-05  Varinance:  0.0008707134204803952 \n",
      "\n",
      "Epoch:  7345  Learning Rate:  6.464595043482439e-05  Varinance:  0.0008694970657473776 \n",
      "\n",
      "Epoch:  7346  Learning Rate:  6.458133679659318e-05  Varinance:  0.0008682824102173375 \n",
      "\n",
      "Epoch:  7347  Learning Rate:  6.45167877397041e-05  Varinance:  0.0008670694515165518 \n",
      "\n",
      "Epoch:  7348  Learning Rate:  6.445230319960815e-05  Varinance:  0.0008658581872746096 \n",
      "\n",
      "Epoch:  7349  Learning Rate:  6.43878831118208e-05  Varinance:  0.0008646486151244153 \n",
      "\n",
      "Epoch:  7350  Learning Rate:  6.432352741192188e-05  Varinance:  0.0008634407327021802 \n",
      "\n",
      "Epoch:  7351  Learning Rate:  6.425923603555579e-05  Varinance:  0.0008622345376474134 \n",
      "\n",
      "Epoch:  7352  Learning Rate:  6.419500891843105e-05  Varinance:  0.000861030027602926 \n",
      "\n",
      "Epoch:  7353  Learning Rate:  6.413084599632055e-05  Varinance:  0.0008598272002148202 \n",
      "\n",
      "Epoch:  7354  Learning Rate:  6.406674720506146e-05  Varinance:  0.0008586260531324882 \n",
      "\n",
      "Epoch:  7355  Learning Rate:  6.400271248055486e-05  Varinance:  0.0008574265840086016 \n",
      "\n",
      "Epoch:  7356  Learning Rate:  6.393874175876607e-05  Varinance:  0.0008562287904991154 \n",
      "\n",
      "Epoch:  7357  Learning Rate:  6.387483497572442e-05  Varinance:  0.0008550326702632591 \n",
      "\n",
      "Epoch:  7358  Learning Rate:  6.381099206752302e-05  Varinance:  0.000853838220963528 \n",
      "\n",
      "Epoch:  7359  Learning Rate:  6.374721297031906e-05  Varinance:  0.0008526454402656867 \n",
      "\n",
      "Epoch:  7360  Learning Rate:  6.368349762033332e-05  Varinance:  0.0008514543258387598 \n",
      "\n",
      "Epoch:  7361  Learning Rate:  6.361984595385052e-05  Varinance:  0.0008502648753550293 \n",
      "\n",
      "Epoch:  7362  Learning Rate:  6.355625790721902e-05  Varinance:  0.0008490770864900246 \n",
      "\n",
      "Epoch:  7363  Learning Rate:  6.349273341685068e-05  Varinance:  0.0008478909569225269 \n",
      "\n",
      "Epoch:  7364  Learning Rate:  6.342927241922103e-05  Varinance:  0.0008467064843345597 \n",
      "\n",
      "Epoch:  7365  Learning Rate:  6.336587485086916e-05  Varinance:  0.0008455236664113802 \n",
      "\n",
      "Epoch:  7366  Learning Rate:  6.330254064839735e-05  Varinance:  0.0008443425008414842 \n",
      "\n",
      "Epoch:  7367  Learning Rate:  6.323926974847152e-05  Varinance:  0.0008431629853165943 \n",
      "\n",
      "Epoch:  7368  Learning Rate:  6.317606208782066e-05  Varinance:  0.00084198511753166 \n",
      "\n",
      "Epoch:  7369  Learning Rate:  6.311291760323716e-05  Varinance:  0.0008408088951848459 \n",
      "\n",
      "Epoch:  7370  Learning Rate:  6.304983623157656e-05  Varinance:  0.0008396343159775366 \n",
      "\n",
      "Epoch:  7371  Learning Rate:  6.29868179097574e-05  Varinance:  0.0008384613776143281 \n",
      "\n",
      "Epoch:  7372  Learning Rate:  6.29238625747614e-05  Varinance:  0.0008372900778030183 \n",
      "\n",
      "Epoch:  7373  Learning Rate:  6.286097016363327e-05  Varinance:  0.0008361204142546116 \n",
      "\n",
      "Epoch:  7374  Learning Rate:  6.279814061348049e-05  Varinance:  0.000834952384683309 \n",
      "\n",
      "Epoch:  7375  Learning Rate:  6.273537386147361e-05  Varinance:  0.0008337859868065055 \n",
      "\n",
      "Epoch:  7376  Learning Rate:  6.267266984484575e-05  Varinance:  0.0008326212183447808 \n",
      "\n",
      "Epoch:  7377  Learning Rate:  6.261002850089298e-05  Varinance:  0.0008314580770219036 \n",
      "\n",
      "Epoch:  7378  Learning Rate:  6.254744976697397e-05  Varinance:  0.0008302965605648218 \n",
      "\n",
      "Epoch:  7379  Learning Rate:  6.248493358050989e-05  Varinance:  0.0008291366667036544 \n",
      "\n",
      "Epoch:  7380  Learning Rate:  6.242247987898466e-05  Varinance:  0.0008279783931716959 \n",
      "\n",
      "Epoch:  7381  Learning Rate:  6.236008859994444e-05  Varinance:  0.000826821737705406 \n",
      "\n",
      "Epoch:  7382  Learning Rate:  6.229775968099803e-05  Varinance:  0.0008256666980444074 \n",
      "\n",
      "Epoch:  7383  Learning Rate:  6.223549305981654e-05  Varinance:  0.0008245132719314766 \n",
      "\n",
      "Epoch:  7384  Learning Rate:  6.217328867413325e-05  Varinance:  0.0008233614571125477 \n",
      "\n",
      "Epoch:  7385  Learning Rate:  6.211114646174382e-05  Varinance:  0.0008222112513367032 \n",
      "\n",
      "Epoch:  7386  Learning Rate:  6.204906636050605e-05  Varinance:  0.0008210626523561662 \n",
      "\n",
      "Epoch:  7387  Learning Rate:  6.198704830833979e-05  Varinance:  0.0008199156579263037 \n",
      "\n",
      "Epoch:  7388  Learning Rate:  6.192509224322704e-05  Varinance:  0.0008187702658056174 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7389  Learning Rate:  6.186319810321165e-05  Varinance:  0.000817626473755742 \n",
      "\n",
      "Epoch:  7390  Learning Rate:  6.180136582639951e-05  Varinance:  0.0008164842795414336 \n",
      "\n",
      "Epoch:  7391  Learning Rate:  6.17395953509584e-05  Varinance:  0.0008153436809305763 \n",
      "\n",
      "Epoch:  7392  Learning Rate:  6.167788661511773e-05  Varinance:  0.0008142046756941721 \n",
      "\n",
      "Epoch:  7393  Learning Rate:  6.161623955716883e-05  Varinance:  0.0008130672616063322 \n",
      "\n",
      "Epoch:  7394  Learning Rate:  6.155465411546466e-05  Varinance:  0.0008119314364442819 \n",
      "\n",
      "Epoch:  7395  Learning Rate:  6.14931302284197e-05  Varinance:  0.00081079719798835 \n",
      "\n",
      "Epoch:  7396  Learning Rate:  6.143166783451013e-05  Varinance:  0.0008096645440219679 \n",
      "\n",
      "Epoch:  7397  Learning Rate:  6.137026687227347e-05  Varinance:  0.0008085334723316588 \n",
      "\n",
      "Epoch:  7398  Learning Rate:  6.130892728030879e-05  Varinance:  0.0008074039807070422 \n",
      "\n",
      "Epoch:  7399  Learning Rate:  6.124764899727655e-05  Varinance:  0.000806276066940826 \n",
      "\n",
      "Epoch:  7400  Learning Rate:  6.118643196189836e-05  Varinance:  0.0008051497288287968 \n",
      "\n",
      "Epoch:  7401  Learning Rate:  6.112527611295723e-05  Varinance:  0.0008040249641698251 \n",
      "\n",
      "Epoch:  7402  Learning Rate:  6.106418138929738e-05  Varinance:  0.0008029017707658545 \n",
      "\n",
      "Epoch:  7403  Learning Rate:  6.100314772982393e-05  Varinance:  0.0008017801464219008 \n",
      "\n",
      "Epoch:  7404  Learning Rate:  6.094217507350335e-05  Varinance:  0.0008006600889460416 \n",
      "\n",
      "Epoch:  7405  Learning Rate:  6.088126335936288e-05  Varinance:  0.0007995415961494216 \n",
      "\n",
      "Epoch:  7406  Learning Rate:  6.082041252649083e-05  Varinance:  0.0007984246658462421 \n",
      "\n",
      "Epoch:  7407  Learning Rate:  6.075962251403643e-05  Varinance:  0.0007973092958537545 \n",
      "\n",
      "Epoch:  7408  Learning Rate:  6.0698893261209555e-05  Varinance:  0.0007961954839922632 \n",
      "\n",
      "Epoch:  7409  Learning Rate:  6.0638224707281006e-05  Varinance:  0.000795083228085118 \n",
      "\n",
      "Epoch:  7410  Learning Rate:  6.057761679158227e-05  Varinance:  0.0007939725259587046 \n",
      "\n",
      "Epoch:  7411  Learning Rate:  6.051706945350532e-05  Varinance:  0.0007928633754424498 \n",
      "\n",
      "Epoch:  7412  Learning Rate:  6.045658263250292e-05  Varinance:  0.000791755774368811 \n",
      "\n",
      "Epoch:  7413  Learning Rate:  6.0396156268088126e-05  Varinance:  0.0007906497205732752 \n",
      "\n",
      "Epoch:  7414  Learning Rate:  6.0335790299834646e-05  Varinance:  0.0007895452118943486 \n",
      "\n",
      "Epoch:  7415  Learning Rate:  6.0275484667376546e-05  Varinance:  0.0007884422461735617 \n",
      "\n",
      "Epoch:  7416  Learning Rate:  6.021523931040808e-05  Varinance:  0.0007873408212554596 \n",
      "\n",
      "Epoch:  7417  Learning Rate:  6.015505416868394e-05  Varinance:  0.0007862409349875946 \n",
      "\n",
      "Epoch:  7418  Learning Rate:  6.009492918201904e-05  Varinance:  0.0007851425852205304 \n",
      "\n",
      "Epoch:  7419  Learning Rate:  6.0034864290288266e-05  Varinance:  0.0007840457698078316 \n",
      "\n",
      "Epoch:  7420  Learning Rate:  5.9974859433426856e-05  Varinance:  0.0007829504866060625 \n",
      "\n",
      "Epoch:  7421  Learning Rate:  5.991491455142981e-05  Varinance:  0.0007818567334747781 \n",
      "\n",
      "Epoch:  7422  Learning Rate:  5.985502958435231e-05  Varinance:  0.0007807645082765274 \n",
      "\n",
      "Epoch:  7423  Learning Rate:  5.979520447230944e-05  Varinance:  0.0007796738088768453 \n",
      "\n",
      "Epoch:  7424  Learning Rate:  5.973543915547597e-05  Varinance:  0.0007785846331442444 \n",
      "\n",
      "Epoch:  7425  Learning Rate:  5.967573357408663e-05  Varinance:  0.000777496978950219 \n",
      "\n",
      "Epoch:  7426  Learning Rate:  5.96160876684359e-05  Varinance:  0.0007764108441692353 \n",
      "\n",
      "Epoch:  7427  Learning Rate:  5.955650137887775e-05  Varinance:  0.0007753262266787305 \n",
      "\n",
      "Epoch:  7428  Learning Rate:  5.949697464582599e-05  Varinance:  0.0007742431243591027 \n",
      "\n",
      "Epoch:  7429  Learning Rate:  5.9437507409753774e-05  Varinance:  0.0007731615350937151 \n",
      "\n",
      "Epoch:  7430  Learning Rate:  5.937809961119394e-05  Varinance:  0.0007720814567688877 \n",
      "\n",
      "Epoch:  7431  Learning Rate:  5.9318751190738706e-05  Varinance:  0.0007710028872738891 \n",
      "\n",
      "Epoch:  7432  Learning Rate:  5.9259462089039555e-05  Varinance:  0.0007699258245009409 \n",
      "\n",
      "Epoch:  7433  Learning Rate:  5.920023224680743e-05  Varinance:  0.0007688502663452076 \n",
      "\n",
      "Epoch:  7434  Learning Rate:  5.914106160481254e-05  Varinance:  0.0007677762107047952 \n",
      "\n",
      "Epoch:  7435  Learning Rate:  5.908195010388413e-05  Varinance:  0.0007667036554807424 \n",
      "\n",
      "Epoch:  7436  Learning Rate:  5.90228976849108e-05  Varinance:  0.0007656325985770239 \n",
      "\n",
      "Epoch:  7437  Learning Rate:  5.896390428884002e-05  Varinance:  0.0007645630379005425 \n",
      "\n",
      "Epoch:  7438  Learning Rate:  5.8904969856678445e-05  Varinance:  0.0007634949713611205 \n",
      "\n",
      "Epoch:  7439  Learning Rate:  5.8846094329491686e-05  Varinance:  0.0007624283968715051 \n",
      "\n",
      "Epoch:  7440  Learning Rate:  5.878727764840411e-05  Varinance:  0.000761363312347357 \n",
      "\n",
      "Epoch:  7441  Learning Rate:  5.872851975459908e-05  Varinance:  0.0007602997157072506 \n",
      "\n",
      "Epoch:  7442  Learning Rate:  5.866982058931875e-05  Varinance:  0.0007592376048726636 \n",
      "\n",
      "Epoch:  7443  Learning Rate:  5.861118009386385e-05  Varinance:  0.0007581769777679818 \n",
      "\n",
      "Epoch:  7444  Learning Rate:  5.855259820959398e-05  Varinance:  0.0007571178323204898 \n",
      "\n",
      "Epoch:  7445  Learning Rate:  5.849407487792714e-05  Varinance:  0.0007560601664603645 \n",
      "\n",
      "Epoch:  7446  Learning Rate:  5.843561004034005e-05  Varinance:  0.0007550039781206777 \n",
      "\n",
      "Epoch:  7447  Learning Rate:  5.837720363836794e-05  Varinance:  0.0007539492652373878 \n",
      "\n",
      "Epoch:  7448  Learning Rate:  5.8318855613604265e-05  Varinance:  0.0007528960257493372 \n",
      "\n",
      "Epoch:  7449  Learning Rate:  5.826056590770107e-05  Varinance:  0.0007518442575982444 \n",
      "\n",
      "Epoch:  7450  Learning Rate:  5.820233446236868e-05  Varinance:  0.0007507939587287068 \n",
      "\n",
      "Epoch:  7451  Learning Rate:  5.814416121937556e-05  Varinance:  0.0007497451270881934 \n",
      "\n",
      "Epoch:  7452  Learning Rate:  5.808604612054856e-05  Varinance:  0.0007486977606270362 \n",
      "\n",
      "Epoch:  7453  Learning Rate:  5.802798910777246e-05  Varinance:  0.0007476518572984347 \n",
      "\n",
      "Epoch:  7454  Learning Rate:  5.796999012299032e-05  Varinance:  0.0007466074150584464 \n",
      "\n",
      "Epoch:  7455  Learning Rate:  5.7912049108203165e-05  Varinance:  0.0007455644318659855 \n",
      "\n",
      "Epoch:  7456  Learning Rate:  5.78541660054699e-05  Varinance:  0.0007445229056828133 \n",
      "\n",
      "Epoch:  7457  Learning Rate:  5.7796340756907466e-05  Varinance:  0.0007434828344735425 \n",
      "\n",
      "Epoch:  7458  Learning Rate:  5.7738573304690656e-05  Varinance:  0.0007424442162056293 \n",
      "\n",
      "Epoch:  7459  Learning Rate:  5.76808635910519e-05  Varinance:  0.0007414070488493653 \n",
      "\n",
      "Epoch:  7460  Learning Rate:  5.76232115582816e-05  Varinance:  0.0007403713303778814 \n",
      "\n",
      "Epoch:  7461  Learning Rate:  5.756561714872761e-05  Varinance:  0.0007393370587671385 \n",
      "\n",
      "Epoch:  7462  Learning Rate:  5.750808030479557e-05  Varinance:  0.0007383042319959267 \n",
      "\n",
      "Epoch:  7463  Learning Rate:  5.745060096894868e-05  Varinance:  0.0007372728480458553 \n",
      "\n",
      "Epoch:  7464  Learning Rate:  5.739317908370748e-05  Varinance:  0.0007362429049013576 \n",
      "\n",
      "Epoch:  7465  Learning Rate:  5.733581459165016e-05  Varinance:  0.0007352144005496822 \n",
      "\n",
      "Epoch:  7466  Learning Rate:  5.7278507435412255e-05  Varinance:  0.0007341873329808857 \n",
      "\n",
      "Epoch:  7467  Learning Rate:  5.722125755768651e-05  Varinance:  0.0007331617001878363 \n",
      "\n",
      "Epoch:  7468  Learning Rate:  5.7164064901223146e-05  Varinance:  0.000732137500166205 \n",
      "\n",
      "Epoch:  7469  Learning Rate:  5.710692940882939e-05  Varinance:  0.0007311147309144638 \n",
      "\n",
      "Epoch:  7470  Learning Rate:  5.70498510233698e-05  Varinance:  0.0007300933904338771 \n",
      "\n",
      "Epoch:  7471  Learning Rate:  5.699282968776604e-05  Varinance:  0.0007290734767285053 \n",
      "\n",
      "Epoch:  7472  Learning Rate:  5.693586534499668e-05  Varinance:  0.0007280549878051969 \n",
      "\n",
      "Epoch:  7473  Learning Rate:  5.687895793809739e-05  Varinance:  0.0007270379216735811 \n",
      "\n",
      "Epoch:  7474  Learning Rate:  5.6822107410160836e-05  Varinance:  0.0007260222763460713 \n",
      "\n",
      "Epoch:  7475  Learning Rate:  5.6765313704336375e-05  Varinance:  0.0007250080498378577 \n",
      "\n",
      "Epoch:  7476  Learning Rate:  5.67085767638304e-05  Varinance:  0.0007239952401668989 \n",
      "\n",
      "Epoch:  7477  Learning Rate:  5.665189653190587e-05  Varinance:  0.0007229838453539267 \n",
      "\n",
      "Epoch:  7478  Learning Rate:  5.659527295188259e-05  Varinance:  0.000721973863422436 \n",
      "\n",
      "Epoch:  7479  Learning Rate:  5.653870596713703e-05  Varinance:  0.0007209652923986849 \n",
      "\n",
      "Epoch:  7480  Learning Rate:  5.648219552110208e-05  Varinance:  0.0007199581303116841 \n",
      "\n",
      "Epoch:  7481  Learning Rate:  5.6425741557267384e-05  Varinance:  0.0007189523751932022 \n",
      "\n",
      "Epoch:  7482  Learning Rate:  5.6369344019178984e-05  Varinance:  0.0007179480250777568 \n",
      "\n",
      "Epoch:  7483  Learning Rate:  5.631300285043926e-05  Varinance:  0.0007169450780026075 \n",
      "\n",
      "Epoch:  7484  Learning Rate:  5.625671799470712e-05  Varinance:  0.0007159435320077599 \n",
      "\n",
      "Epoch:  7485  Learning Rate:  5.6200489395697616e-05  Varinance:  0.0007149433851359562 \n",
      "\n",
      "Epoch:  7486  Learning Rate:  5.614431699718219e-05  Varinance:  0.0007139446354326739 \n",
      "\n",
      "Epoch:  7487  Learning Rate:  5.608820074298849e-05  Varinance:  0.0007129472809461175 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7488  Learning Rate:  5.603214057700015e-05  Varinance:  0.0007119513197272214 \n",
      "\n",
      "Epoch:  7489  Learning Rate:  5.5976136443157066e-05  Varinance:  0.000710956749829643 \n",
      "\n",
      "Epoch:  7490  Learning Rate:  5.592018828545514e-05  Varinance:  0.0007099635693097551 \n",
      "\n",
      "Epoch:  7491  Learning Rate:  5.586429604794611e-05  Varinance:  0.0007089717762266492 \n",
      "\n",
      "Epoch:  7492  Learning Rate:  5.580845967473783e-05  Varinance:  0.0007079813686421268 \n",
      "\n",
      "Epoch:  7493  Learning Rate:  5.575267910999382e-05  Varinance:  0.0007069923446206986 \n",
      "\n",
      "Epoch:  7494  Learning Rate:  5.569695429793357e-05  Varinance:  0.000706004702229575 \n",
      "\n",
      "Epoch:  7495  Learning Rate:  5.564128518283231e-05  Varinance:  0.0007050184395386705 \n",
      "\n",
      "Epoch:  7496  Learning Rate:  5.558567170902083e-05  Varinance:  0.0007040335546205958 \n",
      "\n",
      "Epoch:  7497  Learning Rate:  5.5530113820885685e-05  Varinance:  0.0007030500455506499 \n",
      "\n",
      "Epoch:  7498  Learning Rate:  5.547461146286903e-05  Varinance:  0.000702067910406825 \n",
      "\n",
      "Epoch:  7499  Learning Rate:  5.5419164579468413e-05  Varinance:  0.0007010871472697966 \n",
      "\n",
      "Epoch:  7500  Learning Rate:  5.536377311523705e-05  Varinance:  0.0007001077542229227 \n",
      "\n",
      "Epoch:  7501  Learning Rate:  5.530843701478336e-05  Varinance:  0.000699129729352235 \n",
      "\n",
      "Epoch:  7502  Learning Rate:  5.52531562227713e-05  Varinance:  0.0006981530707464427 \n",
      "\n",
      "Epoch:  7503  Learning Rate:  5.519793068392011e-05  Varinance:  0.0006971777764969253 \n",
      "\n",
      "Epoch:  7504  Learning Rate:  5.5142760343004156e-05  Varinance:  0.0006962038446977246 \n",
      "\n",
      "Epoch:  7505  Learning Rate:  5.50876451448532e-05  Varinance:  0.0006952312734455486 \n",
      "\n",
      "Epoch:  7506  Learning Rate:  5.503258503435192e-05  Varinance:  0.0006942600608397627 \n",
      "\n",
      "Epoch:  7507  Learning Rate:  5.497757995644025e-05  Varinance:  0.0006932902049823895 \n",
      "\n",
      "Epoch:  7508  Learning Rate:  5.492262985611319e-05  Varinance:  0.0006923217039780982 \n",
      "\n",
      "Epoch:  7509  Learning Rate:  5.4867734678420505e-05  Varinance:  0.0006913545559342104 \n",
      "\n",
      "Epoch:  7510  Learning Rate:  5.4812894368467066e-05  Varinance:  0.0006903887589606908 \n",
      "\n",
      "Epoch:  7511  Learning Rate:  5.475810887141261e-05  Varinance:  0.0006894243111701414 \n",
      "\n",
      "Epoch:  7512  Learning Rate:  5.470337813247155e-05  Varinance:  0.0006884612106778043 \n",
      "\n",
      "Epoch:  7513  Learning Rate:  5.4648702096913217e-05  Varinance:  0.0006874994556015533 \n",
      "\n",
      "Epoch:  7514  Learning Rate:  5.45940807100615e-05  Varinance:  0.0006865390440618924 \n",
      "\n",
      "Epoch:  7515  Learning Rate:  5.4539513917295026e-05  Varinance:  0.000685579974181948 \n",
      "\n",
      "Epoch:  7516  Learning Rate:  5.448500166404708e-05  Varinance:  0.0006846222440874719 \n",
      "\n",
      "Epoch:  7517  Learning Rate:  5.443054389580528e-05  Varinance:  0.000683665851906834 \n",
      "\n",
      "Epoch:  7518  Learning Rate:  5.437614055811192e-05  Varinance:  0.0006827107957710155 \n",
      "\n",
      "Epoch:  7519  Learning Rate:  5.432179159656369e-05  Varinance:  0.0006817570738136116 \n",
      "\n",
      "Epoch:  7520  Learning Rate:  5.426749695681153e-05  Varinance:  0.0006808046841708244 \n",
      "\n",
      "Epoch:  7521  Learning Rate:  5.421325658456091e-05  Varinance:  0.00067985362498146 \n",
      "\n",
      "Epoch:  7522  Learning Rate:  5.415907042557135e-05  Varinance:  0.0006789038943869217 \n",
      "\n",
      "Epoch:  7523  Learning Rate:  5.4104938425656704e-05  Varinance:  0.0006779554905312122 \n",
      "\n",
      "Epoch:  7524  Learning Rate:  5.405086053068505e-05  Varinance:  0.0006770084115609273 \n",
      "\n",
      "Epoch:  7525  Learning Rate:  5.3996836686578394e-05  Varinance:  0.000676062655625248 \n",
      "\n",
      "Epoch:  7526  Learning Rate:  5.3942866839312915e-05  Varinance:  0.000675118220875945 \n",
      "\n",
      "Epoch:  7527  Learning Rate:  5.3888950934918824e-05  Varinance:  0.0006741751054673692 \n",
      "\n",
      "Epoch:  7528  Learning Rate:  5.383508891948011e-05  Varinance:  0.0006732333075564509 \n",
      "\n",
      "Epoch:  7529  Learning Rate:  5.378128073913485e-05  Varinance:  0.0006722928253026922 \n",
      "\n",
      "Epoch:  7530  Learning Rate:  5.372752634007476e-05  Varinance:  0.0006713536568681692 \n",
      "\n",
      "Epoch:  7531  Learning Rate:  5.367382566854548e-05  Varinance:  0.0006704158004175258 \n",
      "\n",
      "Epoch:  7532  Learning Rate:  5.3620178670846406e-05  Varinance:  0.0006694792541179664 \n",
      "\n",
      "Epoch:  7533  Learning Rate:  5.356658529333041e-05  Varinance:  0.0006685440161392591 \n",
      "\n",
      "Epoch:  7534  Learning Rate:  5.351304548240417e-05  Varinance:  0.0006676100846537274 \n",
      "\n",
      "Epoch:  7535  Learning Rate:  5.345955918452793e-05  Varinance:  0.0006666774578362497 \n",
      "\n",
      "Epoch:  7536  Learning Rate:  5.340612634621528e-05  Varinance:  0.0006657461338642498 \n",
      "\n",
      "Epoch:  7537  Learning Rate:  5.335274691403347e-05  Varinance:  0.000664816110917702 \n",
      "\n",
      "Epoch:  7538  Learning Rate:  5.329942083460298e-05  Varinance:  0.0006638873871791223 \n",
      "\n",
      "Epoch:  7539  Learning Rate:  5.3246148054597755e-05  Varinance:  0.0006629599608335624 \n",
      "\n",
      "Epoch:  7540  Learning Rate:  5.319292852074507e-05  Varinance:  0.0006620338300686132 \n",
      "\n",
      "Epoch:  7541  Learning Rate:  5.3139762179825296e-05  Varinance:  0.0006611089930743971 \n",
      "\n",
      "Epoch:  7542  Learning Rate:  5.3086648978672125e-05  Varinance:  0.0006601854480435615 \n",
      "\n",
      "Epoch:  7543  Learning Rate:  5.303358886417241e-05  Varinance:  0.000659263193171282 \n",
      "\n",
      "Epoch:  7544  Learning Rate:  5.298058178326593e-05  Varinance:  0.0006583422266552546 \n",
      "\n",
      "Epoch:  7545  Learning Rate:  5.29276276829457e-05  Varinance:  0.0006574225466956938 \n",
      "\n",
      "Epoch:  7546  Learning Rate:  5.287472651025751e-05  Varinance:  0.000656504151495325 \n",
      "\n",
      "Epoch:  7547  Learning Rate:  5.282187821230023e-05  Varinance:  0.0006555870392593882 \n",
      "\n",
      "Epoch:  7548  Learning Rate:  5.276908273622562e-05  Varinance:  0.0006546712081956299 \n",
      "\n",
      "Epoch:  7549  Learning Rate:  5.2716340029238105e-05  Varinance:  0.0006537566565142974 \n",
      "\n",
      "Epoch:  7550  Learning Rate:  5.266365003859501e-05  Varinance:  0.0006528433824281415 \n",
      "\n",
      "Epoch:  7551  Learning Rate:  5.261101271160638e-05  Varinance:  0.0006519313841524085 \n",
      "\n",
      "Epoch:  7552  Learning Rate:  5.2558427995634804e-05  Varinance:  0.0006510206599048392 \n",
      "\n",
      "Epoch:  7553  Learning Rate:  5.2505895838095646e-05  Varinance:  0.0006501112079056607 \n",
      "\n",
      "Epoch:  7554  Learning Rate:  5.245341618645665e-05  Varinance:  0.0006492030263775895 \n",
      "\n",
      "Epoch:  7555  Learning Rate:  5.240098898823822e-05  Varinance:  0.0006482961135458255 \n",
      "\n",
      "Epoch:  7556  Learning Rate:  5.234861419101319e-05  Varinance:  0.0006473904676380441 \n",
      "\n",
      "Epoch:  7557  Learning Rate:  5.229629174240666e-05  Varinance:  0.0006464860868844002 \n",
      "\n",
      "Epoch:  7558  Learning Rate:  5.224402159009624e-05  Varinance:  0.0006455829695175196 \n",
      "\n",
      "Epoch:  7559  Learning Rate:  5.2191803681811804e-05  Varinance:  0.0006446811137724985 \n",
      "\n",
      "Epoch:  7560  Learning Rate:  5.213963796533536e-05  Varinance:  0.0006437805178868951 \n",
      "\n",
      "Epoch:  7561  Learning Rate:  5.208752438850127e-05  Varinance:  0.0006428811801007332 \n",
      "\n",
      "Epoch:  7562  Learning Rate:  5.203546289919586e-05  Varinance:  0.0006419830986564952 \n",
      "\n",
      "Epoch:  7563  Learning Rate:  5.1983453445357685e-05  Varinance:  0.0006410862717991151 \n",
      "\n",
      "Epoch:  7564  Learning Rate:  5.193149597497734e-05  Varinance:  0.0006401906977759822 \n",
      "\n",
      "Epoch:  7565  Learning Rate:  5.187959043609725e-05  Varinance:  0.0006392963748369329 \n",
      "\n",
      "Epoch:  7566  Learning Rate:  5.182773677681191e-05  Varinance:  0.0006384033012342497 \n",
      "\n",
      "Epoch:  7567  Learning Rate:  5.177593494526772e-05  Varinance:  0.0006375114752226535 \n",
      "\n",
      "Epoch:  7568  Learning Rate:  5.172418488966274e-05  Varinance:  0.0006366208950593063 \n",
      "\n",
      "Epoch:  7569  Learning Rate:  5.167248655824701e-05  Varinance:  0.0006357315590038047 \n",
      "\n",
      "Epoch:  7570  Learning Rate:  5.16208398993221e-05  Varinance:  0.0006348434653181736 \n",
      "\n",
      "Epoch:  7571  Learning Rate:  5.1569244861241384e-05  Varinance:  0.0006339566122668688 \n",
      "\n",
      "Epoch:  7572  Learning Rate:  5.151770139240988e-05  Varinance:  0.0006330709981167696 \n",
      "\n",
      "Epoch:  7573  Learning Rate:  5.1466209441284e-05  Varinance:  0.0006321866211371778 \n",
      "\n",
      "Epoch:  7574  Learning Rate:  5.141476895637187e-05  Varinance:  0.000631303479599809 \n",
      "\n",
      "Epoch:  7575  Learning Rate:  5.1363379886233015e-05  Varinance:  0.0006304215717787972 \n",
      "\n",
      "Epoch:  7576  Learning Rate:  5.1312042179478286e-05  Varinance:  0.0006295408959506866 \n",
      "\n",
      "Epoch:  7577  Learning Rate:  5.1260755784770055e-05  Varinance:  0.0006286614503944262 \n",
      "\n",
      "Epoch:  7578  Learning Rate:  5.120952065082184e-05  Varinance:  0.0006277832333913724 \n",
      "\n",
      "Epoch:  7579  Learning Rate:  5.1158336726398535e-05  Varinance:  0.0006269062432252815 \n",
      "\n",
      "Epoch:  7580  Learning Rate:  5.110720396031627e-05  Varinance:  0.0006260304781823083 \n",
      "\n",
      "Epoch:  7581  Learning Rate:  5.105612230144218e-05  Varinance:  0.0006251559365509984 \n",
      "\n",
      "Epoch:  7582  Learning Rate:  5.100509169869464e-05  Varinance:  0.0006242826166222917 \n",
      "\n",
      "Epoch:  7583  Learning Rate:  5.09541121010431e-05  Varinance:  0.0006234105166895156 \n",
      "\n",
      "Epoch:  7584  Learning Rate:  5.090318345750786e-05  Varinance:  0.0006225396350483783 \n",
      "\n",
      "Epoch:  7585  Learning Rate:  5.0852305717160375e-05  Varinance:  0.0006216699699969721 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7586  Learning Rate:  5.0801478829122786e-05  Varinance:  0.0006208015198357657 \n",
      "\n",
      "Epoch:  7587  Learning Rate:  5.0750702742568266e-05  Varinance:  0.0006199342828676032 \n",
      "\n",
      "Epoch:  7588  Learning Rate:  5.0699977406720755e-05  Varinance:  0.0006190682573976962 \n",
      "\n",
      "Epoch:  7589  Learning Rate:  5.064930277085484e-05  Varinance:  0.0006182034417336271 \n",
      "\n",
      "Epoch:  7590  Learning Rate:  5.0598678784295914e-05  Varinance:  0.0006173398341853425 \n",
      "\n",
      "Epoch:  7591  Learning Rate:  5.054810539642003e-05  Varinance:  0.0006164774330651469 \n",
      "\n",
      "Epoch:  7592  Learning Rate:  5.049758255665372e-05  Varinance:  0.0006156162366877055 \n",
      "\n",
      "Epoch:  7593  Learning Rate:  5.0447110214474205e-05  Varinance:  0.0006147562433700368 \n",
      "\n",
      "Epoch:  7594  Learning Rate:  5.039668831940907e-05  Varinance:  0.0006138974514315115 \n",
      "\n",
      "Epoch:  7595  Learning Rate:  5.034631682103646e-05  Varinance:  0.0006130398591938449 \n",
      "\n",
      "Epoch:  7596  Learning Rate:  5.029599566898491e-05  Varinance:  0.0006121834649810998 \n",
      "\n",
      "Epoch:  7597  Learning Rate:  5.024572481293317e-05  Varinance:  0.0006113282671196805 \n",
      "\n",
      "Epoch:  7598  Learning Rate:  5.019550420261043e-05  Varinance:  0.0006104742639383258 \n",
      "\n",
      "Epoch:  7599  Learning Rate:  5.0145333787796124e-05  Varinance:  0.0006096214537681126 \n",
      "\n",
      "Epoch:  7600  Learning Rate:  5.009521351831974e-05  Varinance:  0.000608769834942448 \n",
      "\n",
      "Epoch:  7601  Learning Rate:  5.004514334406109e-05  Varinance:  0.0006079194057970684 \n",
      "\n",
      "Epoch:  7602  Learning Rate:  4.9995123214949906e-05  Varinance:  0.000607070164670032 \n",
      "\n",
      "Epoch:  7603  Learning Rate:  4.994515308096611e-05  Varinance:  0.0006062221099017216 \n",
      "\n",
      "Epoch:  7604  Learning Rate:  4.989523289213961e-05  Varinance:  0.0006053752398348388 \n",
      "\n",
      "Epoch:  7605  Learning Rate:  4.9845362598550104e-05  Varinance:  0.000604529552814397 \n",
      "\n",
      "Epoch:  7606  Learning Rate:  4.979554215032735e-05  Varinance:  0.0006036850471877247 \n",
      "\n",
      "Epoch:  7607  Learning Rate:  4.974577149765094e-05  Varinance:  0.0006028417213044591 \n",
      "\n",
      "Epoch:  7608  Learning Rate:  4.969605059075013e-05  Varinance:  0.0006019995735165399 \n",
      "\n",
      "Epoch:  7609  Learning Rate:  4.96463793799041e-05  Varinance:  0.000601158602178212 \n",
      "\n",
      "Epoch:  7610  Learning Rate:  4.959675781544155e-05  Varinance:  0.0006003188056460186 \n",
      "\n",
      "Epoch:  7611  Learning Rate:  4.9547185847740936e-05  Varinance:  0.0005994801822787996 \n",
      "\n",
      "Epoch:  7612  Learning Rate:  4.949766342723034e-05  Varinance:  0.0005986427304376842 \n",
      "\n",
      "Epoch:  7613  Learning Rate:  4.944819050438726e-05  Varinance:  0.0005978064484860948 \n",
      "\n",
      "Epoch:  7614  Learning Rate:  4.93987670297388e-05  Varinance:  0.0005969713347897395 \n",
      "\n",
      "Epoch:  7615  Learning Rate:  4.934939295386154e-05  Varinance:  0.0005961373877166064 \n",
      "\n",
      "Epoch:  7616  Learning Rate:  4.93000682273813e-05  Varinance:  0.0005953046056369667 \n",
      "\n",
      "Epoch:  7617  Learning Rate:  4.925079280097343e-05  Varinance:  0.0005944729869233672 \n",
      "\n",
      "Epoch:  7618  Learning Rate:  4.9201566625362426e-05  Varinance:  0.0005936425299506293 \n",
      "\n",
      "Epoch:  7619  Learning Rate:  4.9152389651322146e-05  Varinance:  0.0005928132330958413 \n",
      "\n",
      "Epoch:  7620  Learning Rate:  4.9103261829675665e-05  Varinance:  0.0005919850947383623 \n",
      "\n",
      "Epoch:  7621  Learning Rate:  4.9054183111295056e-05  Varinance:  0.0005911581132598149 \n",
      "\n",
      "Epoch:  7622  Learning Rate:  4.9005153447101646e-05  Varinance:  0.0005903322870440794 \n",
      "\n",
      "Epoch:  7623  Learning Rate:  4.8956172788065814e-05  Varinance:  0.0005895076144772974 \n",
      "\n",
      "Epoch:  7624  Learning Rate:  4.89072410852068e-05  Varinance:  0.0005886840939478633 \n",
      "\n",
      "Epoch:  7625  Learning Rate:  4.8858358289592994e-05  Varinance:  0.0005878617238464243 \n",
      "\n",
      "Epoch:  7626  Learning Rate:  4.88095243523415e-05  Varinance:  0.0005870405025658725 \n",
      "\n",
      "Epoch:  7627  Learning Rate:  4.8760739224618435e-05  Varinance:  0.0005862204285013485 \n",
      "\n",
      "Epoch:  7628  Learning Rate:  4.87120028576387e-05  Varinance:  0.0005854015000502342 \n",
      "\n",
      "Epoch:  7629  Learning Rate:  4.866331520266583e-05  Varinance:  0.000584583715612148 \n",
      "\n",
      "Epoch:  7630  Learning Rate:  4.861467621101227e-05  Varinance:  0.0005837670735889465 \n",
      "\n",
      "Epoch:  7631  Learning Rate:  4.856608583403893e-05  Varinance:  0.0005829515723847179 \n",
      "\n",
      "Epoch:  7632  Learning Rate:  4.8517544023155466e-05  Varinance:  0.0005821372104057811 \n",
      "\n",
      "Epoch:  7633  Learning Rate:  4.846905072982011e-05  Varinance:  0.0005813239860606778 \n",
      "\n",
      "Epoch:  7634  Learning Rate:  4.842060590553948e-05  Varinance:  0.0005805118977601766 \n",
      "\n",
      "Epoch:  7635  Learning Rate:  4.837220950186879e-05  Varinance:  0.0005797009439172656 \n",
      "\n",
      "Epoch:  7636  Learning Rate:  4.832386147041168e-05  Varinance:  0.0005788911229471471 \n",
      "\n",
      "Epoch:  7637  Learning Rate:  4.8275561762820026e-05  Varinance:  0.0005780824332672406 \n",
      "\n",
      "Epoch:  7638  Learning Rate:  4.82273103307942e-05  Varinance:  0.0005772748732971748 \n",
      "\n",
      "Epoch:  7639  Learning Rate:  4.817910712608268e-05  Varinance:  0.0005764684414587878 \n",
      "\n",
      "Epoch:  7640  Learning Rate:  4.81309521004823e-05  Varinance:  0.0005756631361761188 \n",
      "\n",
      "Epoch:  7641  Learning Rate:  4.8082845205838076e-05  Varinance:  0.0005748589558754117 \n",
      "\n",
      "Epoch:  7642  Learning Rate:  4.803478639404302e-05  Varinance:  0.0005740558989851091 \n",
      "\n",
      "Epoch:  7643  Learning Rate:  4.798677561703836e-05  Varinance:  0.0005732539639358458 \n",
      "\n",
      "Epoch:  7644  Learning Rate:  4.793881282681336e-05  Varinance:  0.0005724531491604518 \n",
      "\n",
      "Epoch:  7645  Learning Rate:  4.789089797540514e-05  Varinance:  0.0005716534530939458 \n",
      "\n",
      "Epoch:  7646  Learning Rate:  4.7843031014898924e-05  Varinance:  0.0005708548741735335 \n",
      "\n",
      "Epoch:  7647  Learning Rate:  4.779521189742767e-05  Varinance:  0.0005700574108386006 \n",
      "\n",
      "Epoch:  7648  Learning Rate:  4.77474405751723e-05  Varinance:  0.0005692610615307161 \n",
      "\n",
      "Epoch:  7649  Learning Rate:  4.769971700036153e-05  Varinance:  0.0005684658246936261 \n",
      "\n",
      "Epoch:  7650  Learning Rate:  4.765204112527168e-05  Varinance:  0.0005676716987732477 \n",
      "\n",
      "Epoch:  7651  Learning Rate:  4.7604412902226936e-05  Varinance:  0.0005668786822176718 \n",
      "\n",
      "Epoch:  7652  Learning Rate:  4.75568322835991e-05  Varinance:  0.0005660867734771567 \n",
      "\n",
      "Epoch:  7653  Learning Rate:  4.750929922180747e-05  Varinance:  0.0005652959710041262 \n",
      "\n",
      "Epoch:  7654  Learning Rate:  4.746181366931906e-05  Varinance:  0.0005645062732531633 \n",
      "\n",
      "Epoch:  7655  Learning Rate:  4.7414375578648237e-05  Varinance:  0.0005637176786810125 \n",
      "\n",
      "Epoch:  7656  Learning Rate:  4.7366984902356943e-05  Varinance:  0.0005629301857465747 \n",
      "\n",
      "Epoch:  7657  Learning Rate:  4.7319641593054535e-05  Varinance:  0.0005621437929109003 \n",
      "\n",
      "Epoch:  7658  Learning Rate:  4.727234560339763e-05  Varinance:  0.0005613584986371926 \n",
      "\n",
      "Epoch:  7659  Learning Rate:  4.7225096886090265e-05  Varinance:  0.000560574301390801 \n",
      "\n",
      "Epoch:  7660  Learning Rate:  4.717789539388376e-05  Varinance:  0.0005597911996392192 \n",
      "\n",
      "Epoch:  7661  Learning Rate:  4.713074107957654e-05  Varinance:  0.0005590091918520794 \n",
      "\n",
      "Epoch:  7662  Learning Rate:  4.7083633896014363e-05  Varinance:  0.0005582282765011545 \n",
      "\n",
      "Epoch:  7663  Learning Rate:  4.7036573796089976e-05  Varinance:  0.0005574484520603516 \n",
      "\n",
      "Epoch:  7664  Learning Rate:  4.69895607327433e-05  Varinance:  0.0005566697170057077 \n",
      "\n",
      "Epoch:  7665  Learning Rate:  4.6942594658961306e-05  Varinance:  0.0005558920698153912 \n",
      "\n",
      "Epoch:  7666  Learning Rate:  4.689567552777785e-05  Varinance:  0.0005551155089696954 \n",
      "\n",
      "Epoch:  7667  Learning Rate:  4.6848803292273834e-05  Varinance:  0.0005543400329510378 \n",
      "\n",
      "Epoch:  7668  Learning Rate:  4.6801977905577046e-05  Varinance:  0.0005535656402439527 \n",
      "\n",
      "Epoch:  7669  Learning Rate:  4.675519932086203e-05  Varinance:  0.0005527923293350945 \n",
      "\n",
      "Epoch:  7670  Learning Rate:  4.6708467491350265e-05  Varinance:  0.000552020098713232 \n",
      "\n",
      "Epoch:  7671  Learning Rate:  4.666178237030985e-05  Varinance:  0.0005512489468692415 \n",
      "\n",
      "Epoch:  7672  Learning Rate:  4.6615143911055687e-05  Varinance:  0.0005504788722961111 \n",
      "\n",
      "Epoch:  7673  Learning Rate:  4.6568552066949364e-05  Varinance:  0.0005497098734889337 \n",
      "\n",
      "Epoch:  7674  Learning Rate:  4.6522006791398945e-05  Varinance:  0.0005489419489449019 \n",
      "\n",
      "Epoch:  7675  Learning Rate:  4.6475508037859196e-05  Varinance:  0.0005481750971633104 \n",
      "\n",
      "Epoch:  7676  Learning Rate:  4.64290557598314e-05  Varinance:  0.0005474093166455493 \n",
      "\n",
      "Epoch:  7677  Learning Rate:  4.6382649910863195e-05  Varinance:  0.0005466446058951034 \n",
      "\n",
      "Epoch:  7678  Learning Rate:  4.63362904445488e-05  Varinance:  0.000545880963417545 \n",
      "\n",
      "Epoch:  7679  Learning Rate:  4.628997731452868e-05  Varinance:  0.0005451183877205368 \n",
      "\n",
      "Epoch:  7680  Learning Rate:  4.624371047448972e-05  Varinance:  0.0005443568773138267 \n",
      "\n",
      "Epoch:  7681  Learning Rate:  4.619748987816513e-05  Varinance:  0.0005435964307092412 \n",
      "\n",
      "Epoch:  7682  Learning Rate:  4.615131547933424e-05  Varinance:  0.0005428370464206886 \n",
      "\n",
      "Epoch:  7683  Learning Rate:  4.610518723182266e-05  Varinance:  0.0005420787229641525 \n",
      "\n",
      "Epoch:  7684  Learning Rate:  4.6059105089502205e-05  Varinance:  0.0005413214588576902 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7685  Learning Rate:  4.601306900629063e-05  Varinance:  0.0005405652526214267 \n",
      "\n",
      "Epoch:  7686  Learning Rate:  4.596707893615194e-05  Varinance:  0.0005398101027775571 \n",
      "\n",
      "Epoch:  7687  Learning Rate:  4.592113483309598e-05  Varinance:  0.0005390560078503409 \n",
      "\n",
      "Epoch:  7688  Learning Rate:  4.587523665117868e-05  Varinance:  0.0005383029663660962 \n",
      "\n",
      "Epoch:  7689  Learning Rate:  4.582938434450189e-05  Varinance:  0.0005375509768532027 \n",
      "\n",
      "Epoch:  7690  Learning Rate:  4.578357786721322e-05  Varinance:  0.0005368000378420952 \n",
      "\n",
      "Epoch:  7691  Learning Rate:  4.573781717350623e-05  Varinance:  0.000536050147865262 \n",
      "\n",
      "Epoch:  7692  Learning Rate:  4.5692102217620275e-05  Varinance:  0.0005353013054572387 \n",
      "\n",
      "Epoch:  7693  Learning Rate:  4.5646432953840303e-05  Varinance:  0.0005345535091546112 \n",
      "\n",
      "Epoch:  7694  Learning Rate:  4.560080933649712e-05  Varinance:  0.0005338067574960094 \n",
      "\n",
      "Epoch:  7695  Learning Rate:  4.555523131996705e-05  Varinance:  0.0005330610490221021 \n",
      "\n",
      "Epoch:  7696  Learning Rate:  4.5509698858672085e-05  Varinance:  0.0005323163822755994 \n",
      "\n",
      "Epoch:  7697  Learning Rate:  4.546421190707981e-05  Varinance:  0.0005315727558012464 \n",
      "\n",
      "Epoch:  7698  Learning Rate:  4.5418770419703197e-05  Varinance:  0.0005308301681458217 \n",
      "\n",
      "Epoch:  7699  Learning Rate:  4.5373374351100785e-05  Varinance:  0.0005300886178581319 \n",
      "\n",
      "Epoch:  7700  Learning Rate:  4.5328023655876544e-05  Varinance:  0.0005293481034890128 \n",
      "\n",
      "Epoch:  7701  Learning Rate:  4.52827182886797e-05  Varinance:  0.0005286086235913254 \n",
      "\n",
      "Epoch:  7702  Learning Rate:  4.523745820420496e-05  Varinance:  0.000527870176719949 \n",
      "\n",
      "Epoch:  7703  Learning Rate:  4.519224335719214e-05  Varinance:  0.0005271327614317845 \n",
      "\n",
      "Epoch:  7704  Learning Rate:  4.514707370242646e-05  Varinance:  0.0005263963762857483 \n",
      "\n",
      "Epoch:  7705  Learning Rate:  4.510194919473828e-05  Varinance:  0.0005256610198427702 \n",
      "\n",
      "Epoch:  7706  Learning Rate:  4.505686978900301e-05  Varinance:  0.0005249266906657881 \n",
      "\n",
      "Epoch:  7707  Learning Rate:  4.501183544014129e-05  Varinance:  0.0005241933873197499 \n",
      "\n",
      "Epoch:  7708  Learning Rate:  4.496684610311879e-05  Varinance:  0.0005234611083716085 \n",
      "\n",
      "Epoch:  7709  Learning Rate:  4.492190173294611e-05  Varinance:  0.0005227298523903153 \n",
      "\n",
      "Epoch:  7710  Learning Rate:  4.487700228467894e-05  Varinance:  0.0005219996179468244 \n",
      "\n",
      "Epoch:  7711  Learning Rate:  4.483214771341776e-05  Varinance:  0.0005212704036140849 \n",
      "\n",
      "Epoch:  7712  Learning Rate:  4.478733797430803e-05  Varinance:  0.0005205422079670402 \n",
      "\n",
      "Epoch:  7713  Learning Rate:  4.474257302254004e-05  Varinance:  0.000519815029582622 \n",
      "\n",
      "Epoch:  7714  Learning Rate:  4.4697852813348764e-05  Varinance:  0.0005190888670397526 \n",
      "\n",
      "Epoch:  7715  Learning Rate:  4.465317730201403e-05  Varinance:  0.0005183637189193396 \n",
      "\n",
      "Epoch:  7716  Learning Rate:  4.460854644386035e-05  Varinance:  0.0005176395838042701 \n",
      "\n",
      "Epoch:  7717  Learning Rate:  4.45639601942568e-05  Varinance:  0.0005169164602794137 \n",
      "\n",
      "Epoch:  7718  Learning Rate:  4.451941850861719e-05  Varinance:  0.0005161943469316159 \n",
      "\n",
      "Epoch:  7719  Learning Rate:  4.447492134239977e-05  Varinance:  0.0005154732423496973 \n",
      "\n",
      "Epoch:  7720  Learning Rate:  4.443046865110739e-05  Varinance:  0.0005147531451244471 \n",
      "\n",
      "Epoch:  7721  Learning Rate:  4.438606039028741e-05  Varinance:  0.0005140340538486261 \n",
      "\n",
      "Epoch:  7722  Learning Rate:  4.434169651553147e-05  Varinance:  0.0005133159671169604 \n",
      "\n",
      "Epoch:  7723  Learning Rate:  4.4297376982475746e-05  Varinance:  0.0005125988835261369 \n",
      "\n",
      "Epoch:  7724  Learning Rate:  4.4253101746800734e-05  Varinance:  0.0005118828016748056 \n",
      "\n",
      "Epoch:  7725  Learning Rate:  4.420887076423112e-05  Varinance:  0.0005111677201635733 \n",
      "\n",
      "Epoch:  7726  Learning Rate:  4.4164683990535996e-05  Varinance:  0.0005104536375950022 \n",
      "\n",
      "Epoch:  7727  Learning Rate:  4.41205413815285e-05  Varinance:  0.0005097405525736041 \n",
      "\n",
      "Epoch:  7728  Learning Rate:  4.407644289306606e-05  Varinance:  0.0005090284637058435 \n",
      "\n",
      "Epoch:  7729  Learning Rate:  4.403238848105023e-05  Varinance:  0.0005083173696001308 \n",
      "\n",
      "Epoch:  7730  Learning Rate:  4.3988378101426504e-05  Varinance:  0.0005076072688668183 \n",
      "\n",
      "Epoch:  7731  Learning Rate:  4.394441171018455e-05  Varinance:  0.000506898160118202 \n",
      "\n",
      "Epoch:  7732  Learning Rate:  4.390048926335801e-05  Varinance:  0.0005061900419685156 \n",
      "\n",
      "Epoch:  7733  Learning Rate:  4.3856610717024344e-05  Varinance:  0.0005054829130339296 \n",
      "\n",
      "Epoch:  7734  Learning Rate:  4.38127760273051e-05  Varinance:  0.000504776771932545 \n",
      "\n",
      "Epoch:  7735  Learning Rate:  4.376898515036549e-05  Varinance:  0.0005040716172843959 \n",
      "\n",
      "Epoch:  7736  Learning Rate:  4.372523804241468e-05  Varinance:  0.0005033674477114444 \n",
      "\n",
      "Epoch:  7737  Learning Rate:  4.368153465970559e-05  Varinance:  0.0005026642618375743 \n",
      "\n",
      "Epoch:  7738  Learning Rate:  4.363787495853476e-05  Varinance:  0.0005019620582885954 \n",
      "\n",
      "Epoch:  7739  Learning Rate:  4.359425889524253e-05  Varinance:  0.0005012608356922364 \n",
      "\n",
      "Epoch:  7740  Learning Rate:  4.355068642621286e-05  Varinance:  0.000500560592678141 \n",
      "\n",
      "Epoch:  7741  Learning Rate:  4.3507157507873215e-05  Varinance:  0.0004998613278778692 \n",
      "\n",
      "Epoch:  7742  Learning Rate:  4.3463672096694746e-05  Varinance:  0.0004991630399248924 \n",
      "\n",
      "Epoch:  7743  Learning Rate:  4.342023014919195e-05  Varinance:  0.0004984657274545915 \n",
      "\n",
      "Epoch:  7744  Learning Rate:  4.3376831621922916e-05  Varinance:  0.0004977693891042511 \n",
      "\n",
      "Epoch:  7745  Learning Rate:  4.333347647148916e-05  Varinance:  0.0004970740235130625 \n",
      "\n",
      "Epoch:  7746  Learning Rate:  4.3290164654535456e-05  Varinance:  0.0004963796293221176 \n",
      "\n",
      "Epoch:  7747  Learning Rate:  4.324689612775001e-05  Varinance:  0.0004956862051744038 \n",
      "\n",
      "Epoch:  7748  Learning Rate:  4.320367084786433e-05  Varinance:  0.0004949937497148075 \n",
      "\n",
      "Epoch:  7749  Learning Rate:  4.316048877165307e-05  Varinance:  0.0004943022615901068 \n",
      "\n",
      "Epoch:  7750  Learning Rate:  4.31173498559342e-05  Varinance:  0.0004936117394489712 \n",
      "\n",
      "Epoch:  7751  Learning Rate:  4.307425405756875e-05  Varinance:  0.0004929221819419551 \n",
      "\n",
      "Epoch:  7752  Learning Rate:  4.303120133346096e-05  Varinance:  0.0004922335877215013 \n",
      "\n",
      "Epoch:  7753  Learning Rate:  4.298819164055811e-05  Varinance:  0.0004915459554419341 \n",
      "\n",
      "Epoch:  7754  Learning Rate:  4.294522493585045e-05  Varinance:  0.0004908592837594562 \n",
      "\n",
      "Epoch:  7755  Learning Rate:  4.290230117637134e-05  Varinance:  0.0004901735713321491 \n",
      "\n",
      "Epoch:  7756  Learning Rate:  4.2859420319196946e-05  Varinance:  0.0004894888168199687 \n",
      "\n",
      "Epoch:  7757  Learning Rate:  4.2816582321446446e-05  Varinance:  0.0004888050188847434 \n",
      "\n",
      "Epoch:  7758  Learning Rate:  4.2773787140281875e-05  Varinance:  0.00048812217619016864 \n",
      "\n",
      "Epoch:  7759  Learning Rate:  4.2731034732907964e-05  Varinance:  0.0004874402874018091 \n",
      "\n",
      "Epoch:  7760  Learning Rate:  4.2688325056572346e-05  Varinance:  0.0004867593511870937 \n",
      "\n",
      "Epoch:  7761  Learning Rate:  4.264565806856539e-05  Varinance:  0.00048607936621530997 \n",
      "\n",
      "Epoch:  7762  Learning Rate:  4.260303372622e-05  Varinance:  0.0004854003311576075 \n",
      "\n",
      "Epoch:  7763  Learning Rate:  4.256045198691194e-05  Varinance:  0.00048472224468699114 \n",
      "\n",
      "Epoch:  7764  Learning Rate:  4.251791280805937e-05  Varinance:  0.0004840451054783203 \n",
      "\n",
      "Epoch:  7765  Learning Rate:  4.247541614712315e-05  Varinance:  0.00048336891220830295 \n",
      "\n",
      "Epoch:  7766  Learning Rate:  4.243296196160666e-05  Varinance:  0.00048269366355549855 \n",
      "\n",
      "Epoch:  7767  Learning Rate:  4.239055020905563e-05  Varinance:  0.0004820193582003121 \n",
      "\n",
      "Epoch:  7768  Learning Rate:  4.234818084705834e-05  Varinance:  0.0004813459948249899 \n",
      "\n",
      "Epoch:  7769  Learning Rate:  4.230585383324546e-05  Varinance:  0.0004806735721136213 \n",
      "\n",
      "Epoch:  7770  Learning Rate:  4.22635691252899e-05  Varinance:  0.00048000208875213334 \n",
      "\n",
      "Epoch:  7771  Learning Rate:  4.222132668090704e-05  Varinance:  0.0004793315434282894 \n",
      "\n",
      "Epoch:  7772  Learning Rate:  4.2179126457854324e-05  Varinance:  0.0004786619348316837 \n",
      "\n",
      "Epoch:  7773  Learning Rate:  4.213696841393158e-05  Varinance:  0.00047799326165374345 \n",
      "\n",
      "Epoch:  7774  Learning Rate:  4.2094852506980814e-05  Varinance:  0.00047732552258772376 \n",
      "\n",
      "Epoch:  7775  Learning Rate:  4.205277869488602e-05  Varinance:  0.00047665871632870304 \n",
      "\n",
      "Epoch:  7776  Learning Rate:  4.2010746935573416e-05  Varinance:  0.0004759928415735848 \n",
      "\n",
      "Epoch:  7777  Learning Rate:  4.1968757187011295e-05  Varinance:  0.00047532789702109233 \n",
      "\n",
      "Epoch:  7778  Learning Rate:  4.192680940720982e-05  Varinance:  0.00047466388137176746 \n",
      "\n",
      "Epoch:  7779  Learning Rate:  4.188490355422128e-05  Varinance:  0.0004740007933279648 \n",
      "\n",
      "Epoch:  7780  Learning Rate:  4.184303958613975e-05  Varinance:  0.00047333863159385427 \n",
      "\n",
      "Epoch:  7781  Learning Rate:  4.180121746110129e-05  Varinance:  0.0004726773948754161 \n",
      "\n",
      "Epoch:  7782  Learning Rate:  4.175943713728382e-05  Varinance:  0.0004720170818804356 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7783  Learning Rate:  4.1717698572906926e-05  Varinance:  0.00047135769131850574 \n",
      "\n",
      "Epoch:  7784  Learning Rate:  4.167600172623208e-05  Varinance:  0.0004706992219010214 \n",
      "\n",
      "Epoch:  7785  Learning Rate:  4.163434655556247e-05  Varinance:  0.00047004167234117837 \n",
      "\n",
      "Epoch:  7786  Learning Rate:  4.159273301924284e-05  Varinance:  0.0004693850413539677 \n",
      "\n",
      "Epoch:  7787  Learning Rate:  4.155116107565975e-05  Varinance:  0.00046872932765617767 \n",
      "\n",
      "Epoch:  7788  Learning Rate:  4.1509630683241145e-05  Varinance:  0.0004680745299663896 \n",
      "\n",
      "Epoch:  7789  Learning Rate:  4.1468141800456695e-05  Varinance:  0.0004674206470049722 \n",
      "\n",
      "Epoch:  7790  Learning Rate:  4.142669438581753e-05  Varinance:  0.0004667676774940842 \n",
      "\n",
      "Epoch:  7791  Learning Rate:  4.138528839787617e-05  Varinance:  0.00046611562015766884 \n",
      "\n",
      "Epoch:  7792  Learning Rate:  4.134392379522665e-05  Varinance:  0.0004654644737214525 \n",
      "\n",
      "Epoch:  7793  Learning Rate:  4.1302600536504414e-05  Varinance:  0.0004648142369129395 \n",
      "\n",
      "Epoch:  7794  Learning Rate:  4.126131858038612e-05  Varinance:  0.000464164908461414 \n",
      "\n",
      "Epoch:  7795  Learning Rate:  4.122007788558988e-05  Varinance:  0.00046351648709793536 \n",
      "\n",
      "Epoch:  7796  Learning Rate:  4.117887841087492e-05  Varinance:  0.0004628689715553333 \n",
      "\n",
      "Epoch:  7797  Learning Rate:  4.1137720115041805e-05  Varinance:  0.00046222236056821 \n",
      "\n",
      "Epoch:  7798  Learning Rate:  4.1096602956932275e-05  Varinance:  0.00046157665287293464 \n",
      "\n",
      "Epoch:  7799  Learning Rate:  4.105552689542908e-05  Varinance:  0.00046093184720764246 \n",
      "\n",
      "Epoch:  7800  Learning Rate:  4.1014491889456213e-05  Varinance:  0.00046028794231222893 \n",
      "\n",
      "Epoch:  7801  Learning Rate:  4.097349789797868e-05  Varinance:  0.00045964493692835244 \n",
      "\n",
      "Epoch:  7802  Learning Rate:  4.0932544880002426e-05  Varinance:  0.0004590028297994291 \n",
      "\n",
      "Epoch:  7803  Learning Rate:  4.0891632794574504e-05  Varinance:  0.0004583616196706281 \n",
      "\n",
      "Epoch:  7804  Learning Rate:  4.085076160078274e-05  Varinance:  0.00045772130528887384 \n",
      "\n",
      "Epoch:  7805  Learning Rate:  4.080993125775599e-05  Varinance:  0.0004570818854028415 \n",
      "\n",
      "Epoch:  7806  Learning Rate:  4.076914172466393e-05  Varinance:  0.00045644335876295165 \n",
      "\n",
      "Epoch:  7807  Learning Rate:  4.072839296071696e-05  Varinance:  0.00045580572412137293 \n",
      "\n",
      "Epoch:  7808  Learning Rate:  4.068768492516633e-05  Varinance:  0.0004551689802320165 \n",
      "\n",
      "Epoch:  7809  Learning Rate:  4.064701757730407e-05  Varinance:  0.0004545331258505348 \n",
      "\n",
      "Epoch:  7810  Learning Rate:  4.060639087646273e-05  Varinance:  0.00045389815973431654 \n",
      "\n",
      "Epoch:  7811  Learning Rate:  4.056580478201569e-05  Varinance:  0.0004532640806424884 \n",
      "\n",
      "Epoch:  7812  Learning Rate:  4.052525925337677e-05  Varinance:  0.0004526308873359105 \n",
      "\n",
      "Epoch:  7813  Learning Rate:  4.0484754250000483e-05  Varinance:  0.00045199857857717196 \n",
      "\n",
      "Epoch:  7814  Learning Rate:  4.0444289731381865e-05  Varinance:  0.00045136715313059245 \n",
      "\n",
      "Epoch:  7815  Learning Rate:  4.0403865657056304e-05  Varinance:  0.0004507366097622174 \n",
      "\n",
      "Epoch:  7816  Learning Rate:  4.036348198659977e-05  Varinance:  0.00045010694723981656 \n",
      "\n",
      "Epoch:  7817  Learning Rate:  4.032313867962861e-05  Varinance:  0.000449478164332879 \n",
      "\n",
      "Epoch:  7818  Learning Rate:  4.028283569579947e-05  Varinance:  0.00044885025981261477 \n",
      "\n",
      "Epoch:  7819  Learning Rate:  4.024257299480941e-05  Varinance:  0.00044822323245195074 \n",
      "\n",
      "Epoch:  7820  Learning Rate:  4.020235053639567e-05  Varinance:  0.00044759708102552547 \n",
      "\n",
      "Epoch:  7821  Learning Rate:  4.016216828033581e-05  Varinance:  0.0004469718043096918 \n",
      "\n",
      "Epoch:  7822  Learning Rate:  4.0122026186447616e-05  Varinance:  0.00044634740108251116 \n",
      "\n",
      "Epoch:  7823  Learning Rate:  4.008192421458892e-05  Varinance:  0.0004457238701237525 \n",
      "\n",
      "Epoch:  7824  Learning Rate:  4.004186232465777e-05  Varinance:  0.0004451012102148874 \n",
      "\n",
      "Epoch:  7825  Learning Rate:  4.000184047659232e-05  Varinance:  0.0004444794201390918 \n",
      "\n",
      "Epoch:  7826  Learning Rate:  3.996185863037065e-05  Varinance:  0.0004438584986812416 \n",
      "\n",
      "Epoch:  7827  Learning Rate:  3.992191674601097e-05  Varinance:  0.0004432384446279078 \n",
      "\n",
      "Epoch:  7828  Learning Rate:  3.988201478357133e-05  Varinance:  0.00044261925676735884 \n",
      "\n",
      "Epoch:  7829  Learning Rate:  3.9842152703149796e-05  Varinance:  0.00044200093388955516 \n",
      "\n",
      "Epoch:  7830  Learning Rate:  3.980233046488432e-05  Varinance:  0.0004413834747861484 \n",
      "\n",
      "Epoch:  7831  Learning Rate:  3.9762548028952584e-05  Varinance:  0.0004407668782504757 \n",
      "\n",
      "Epoch:  7832  Learning Rate:  3.97228053555722e-05  Varinance:  0.0004401511430775625 \n",
      "\n",
      "Epoch:  7833  Learning Rate:  3.9683102405000515e-05  Varinance:  0.0004395362680641172 \n",
      "\n",
      "Epoch:  7834  Learning Rate:  3.964343913753451e-05  Varinance:  0.00043892225200852723 \n",
      "\n",
      "Epoch:  7835  Learning Rate:  3.9603815513510976e-05  Varinance:  0.0004383090937108605 \n",
      "\n",
      "Epoch:  7836  Learning Rate:  3.956423149330622e-05  Varinance:  0.0004376967919728608 \n",
      "\n",
      "Epoch:  7837  Learning Rate:  3.952468703733626e-05  Varinance:  0.00043708534559794624 \n",
      "\n",
      "Epoch:  7838  Learning Rate:  3.948518210605666e-05  Varinance:  0.0004364747533912046 \n",
      "\n",
      "Epoch:  7839  Learning Rate:  3.9445716659962425e-05  Varinance:  0.00043586501415939486 \n",
      "\n",
      "Epoch:  7840  Learning Rate:  3.9406290659588136e-05  Varinance:  0.00043525612671094325 \n",
      "\n",
      "Epoch:  7841  Learning Rate:  3.936690406550783e-05  Varinance:  0.00043464808985593806 \n",
      "\n",
      "Epoch:  7842  Learning Rate:  3.9327556838334826e-05  Varinance:  0.000434040902406132 \n",
      "\n",
      "Epoch:  7843  Learning Rate:  3.9288248938721986e-05  Varinance:  0.00043343456317493736 \n",
      "\n",
      "Epoch:  7844  Learning Rate:  3.924898032736131e-05  Varinance:  0.00043282907097742454 \n",
      "\n",
      "Epoch:  7845  Learning Rate:  3.920975096498424e-05  Varinance:  0.00043222442463031695 \n",
      "\n",
      "Epoch:  7846  Learning Rate:  3.9170560812361434e-05  Varinance:  0.0004316206229519934 \n",
      "\n",
      "Epoch:  7847  Learning Rate:  3.913140983030267e-05  Varinance:  0.0004310176647624833 \n",
      "\n",
      "Epoch:  7848  Learning Rate:  3.9092297979657e-05  Varinance:  0.0004304155488834622 \n",
      "\n",
      "Epoch:  7849  Learning Rate:  3.9053225221312595e-05  Varinance:  0.0004298142741382539 \n",
      "\n",
      "Epoch:  7850  Learning Rate:  3.901419151619664e-05  Varinance:  0.0004292138393518254 \n",
      "\n",
      "Epoch:  7851  Learning Rate:  3.8975196825275485e-05  Varinance:  0.00042861424335078577 \n",
      "\n",
      "Epoch:  7852  Learning Rate:  3.893624110955436e-05  Varinance:  0.00042801548496338105 \n",
      "\n",
      "Epoch:  7853  Learning Rate:  3.88973243300776e-05  Varinance:  0.00042741756301949645 \n",
      "\n",
      "Epoch:  7854  Learning Rate:  3.8858446447928444e-05  Varinance:  0.00042682047635065175 \n",
      "\n",
      "Epoch:  7855  Learning Rate:  3.881960742422894e-05  Varinance:  0.00042622422378999695 \n",
      "\n",
      "Epoch:  7856  Learning Rate:  3.878080722014009e-05  Varinance:  0.0004256288041723142 \n",
      "\n",
      "Epoch:  7857  Learning Rate:  3.874204579686173e-05  Varinance:  0.0004250342163340126 \n",
      "\n",
      "Epoch:  7858  Learning Rate:  3.870332311563235e-05  Varinance:  0.0004244404591131276 \n",
      "\n",
      "Epoch:  7859  Learning Rate:  3.866463913772936e-05  Varinance:  0.00042384753134931577 \n",
      "\n",
      "Epoch:  7860  Learning Rate:  3.8625993824468694e-05  Varinance:  0.0004232554318838566 \n",
      "\n",
      "Epoch:  7861  Learning Rate:  3.858738713720507e-05  Varinance:  0.00042266415955964843 \n",
      "\n",
      "Epoch:  7862  Learning Rate:  3.854881903733183e-05  Varinance:  0.00042207371322120374 \n",
      "\n",
      "Epoch:  7863  Learning Rate:  3.851028948628081e-05  Varinance:  0.00042148409171465146 \n",
      "\n",
      "Epoch:  7864  Learning Rate:  3.847179844552248e-05  Varinance:  0.0004208952938877317 \n",
      "\n",
      "Epoch:  7865  Learning Rate:  3.8433345876565835e-05  Varinance:  0.0004203073185897949 \n",
      "\n",
      "Epoch:  7866  Learning Rate:  3.839493174095824e-05  Varinance:  0.00041972016467179676 \n",
      "\n",
      "Epoch:  7867  Learning Rate:  3.835655600028562e-05  Varinance:  0.0004191338309863003 \n",
      "\n",
      "Epoch:  7868  Learning Rate:  3.831821861617216e-05  Varinance:  0.0004185483163874716 \n",
      "\n",
      "Epoch:  7869  Learning Rate:  3.827991955028051e-05  Varinance:  0.0004179636197310751 \n",
      "\n",
      "Epoch:  7870  Learning Rate:  3.824165876431163e-05  Varinance:  0.0004173797398744759 \n",
      "\n",
      "Epoch:  7871  Learning Rate:  3.820343622000467e-05  Varinance:  0.00041679667567663545 \n",
      "\n",
      "Epoch:  7872  Learning Rate:  3.816525187913712e-05  Varinance:  0.00041621442599810684 \n",
      "\n",
      "Epoch:  7873  Learning Rate:  3.812710570352465e-05  Varinance:  0.0004156329897010371 \n",
      "\n",
      "Epoch:  7874  Learning Rate:  3.808899765502104e-05  Varinance:  0.00041505236564916227 \n",
      "\n",
      "Epoch:  7875  Learning Rate:  3.805092769551829e-05  Varinance:  0.00041447255270780627 \n",
      "\n",
      "Epoch:  7876  Learning Rate:  3.801289578694637e-05  Varinance:  0.00041389354974387594 \n",
      "\n",
      "Epoch:  7877  Learning Rate:  3.797490189127341e-05  Varinance:  0.0004133153556258633 \n",
      "\n",
      "Epoch:  7878  Learning Rate:  3.793694597050553e-05  Varinance:  0.00041273796922384104 \n",
      "\n",
      "Epoch:  7879  Learning Rate:  3.789902798668675e-05  Varinance:  0.00041216138940945783 \n",
      "\n",
      "Epoch:  7880  Learning Rate:  3.786114790189915e-05  Varinance:  0.0004115856150559413 \n",
      "\n",
      "Epoch:  7881  Learning Rate:  3.7823305678262576e-05  Varinance:  0.0004110106450380919 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7882  Learning Rate:  3.778550127793483e-05  Varinance:  0.00041043647823228317 \n",
      "\n",
      "Epoch:  7883  Learning Rate:  3.7747734663111544e-05  Varinance:  0.0004098631135164556 \n",
      "\n",
      "Epoch:  7884  Learning Rate:  3.771000579602604e-05  Varinance:  0.0004092905497701199 \n",
      "\n",
      "Epoch:  7885  Learning Rate:  3.767231463894947e-05  Varinance:  0.00040871878587435157 \n",
      "\n",
      "Epoch:  7886  Learning Rate:  3.763466115419071e-05  Varinance:  0.0004081478207117873 \n",
      "\n",
      "Epoch:  7887  Learning Rate:  3.759704530409621e-05  Varinance:  0.0004075776531666268 \n",
      "\n",
      "Epoch:  7888  Learning Rate:  3.7559467051050176e-05  Varinance:  0.0004070082821246278 \n",
      "\n",
      "Epoch:  7889  Learning Rate:  3.752192635747429e-05  Varinance:  0.0004064397064731053 \n",
      "\n",
      "Epoch:  7890  Learning Rate:  3.7484423185827895e-05  Varinance:  0.0004058719251009265 \n",
      "\n",
      "Epoch:  7891  Learning Rate:  3.744695749860783e-05  Varinance:  0.00040530493689851303 \n",
      "\n",
      "Epoch:  7892  Learning Rate:  3.740952925834837e-05  Varinance:  0.00040473874075783645 \n",
      "\n",
      "Epoch:  7893  Learning Rate:  3.7372138427621275e-05  Varinance:  0.00040417333557241417 \n",
      "\n",
      "Epoch:  7894  Learning Rate:  3.7334784969035755e-05  Varinance:  0.0004036087202373114 \n",
      "\n",
      "Epoch:  7895  Learning Rate:  3.729746884523828e-05  Varinance:  0.0004030448936491362 \n",
      "\n",
      "Epoch:  7896  Learning Rate:  3.7260190018912794e-05  Varinance:  0.0004024818547060387 \n",
      "\n",
      "Epoch:  7897  Learning Rate:  3.72229484527804e-05  Varinance:  0.00040191960230770614 \n",
      "\n",
      "Epoch:  7898  Learning Rate:  3.718574410959956e-05  Varinance:  0.00040135813535536514 \n",
      "\n",
      "Epoch:  7899  Learning Rate:  3.714857695216596e-05  Varinance:  0.0004007974527517771 \n",
      "\n",
      "Epoch:  7900  Learning Rate:  3.7111446943312375e-05  Varinance:  0.0004002375534012342 \n",
      "\n",
      "Epoch:  7901  Learning Rate:  3.7074354045908826e-05  Varinance:  0.00039967843620956114 \n",
      "\n",
      "Epoch:  7902  Learning Rate:  3.7037298222862446e-05  Varinance:  0.0003991201000841109 \n",
      "\n",
      "Epoch:  7903  Learning Rate:  3.7000279437117344e-05  Varinance:  0.00039856254393376314 \n",
      "\n",
      "Epoch:  7904  Learning Rate:  3.6963297651654794e-05  Varinance:  0.0003980057666689199 \n",
      "\n",
      "Epoch:  7905  Learning Rate:  3.692635282949294e-05  Varinance:  0.00039744976720150737 \n",
      "\n",
      "Epoch:  7906  Learning Rate:  3.6889444933687e-05  Varinance:  0.0003968945444449717 \n",
      "\n",
      "Epoch:  7907  Learning Rate:  3.6852573927329095e-05  Varinance:  0.000396340097314275 \n",
      "\n",
      "Epoch:  7908  Learning Rate:  3.681573977354816e-05  Varinance:  0.00039578642472589694 \n",
      "\n",
      "Epoch:  7909  Learning Rate:  3.677894243551006e-05  Varinance:  0.00039523352559783033 \n",
      "\n",
      "Epoch:  7910  Learning Rate:  3.67421818764175e-05  Varinance:  0.0003946813988495803 \n",
      "\n",
      "Epoch:  7911  Learning Rate:  3.670545805950984e-05  Varinance:  0.00039413004340215895 \n",
      "\n",
      "Epoch:  7912  Learning Rate:  3.666877094806333e-05  Varinance:  0.00039357945817808816 \n",
      "\n",
      "Epoch:  7913  Learning Rate:  3.663212050539079e-05  Varinance:  0.0003930296421013948 \n",
      "\n",
      "Epoch:  7914  Learning Rate:  3.6595506694841817e-05  Varinance:  0.00039248059409760673 \n",
      "\n",
      "Epoch:  7915  Learning Rate:  3.655892947980262e-05  Varinance:  0.0003919323130937549 \n",
      "\n",
      "Epoch:  7916  Learning Rate:  3.6522388823695904e-05  Varinance:  0.0003913847980183687 \n",
      "\n",
      "Epoch:  7917  Learning Rate:  3.648588468998107e-05  Varinance:  0.0003908380478014747 \n",
      "\n",
      "Epoch:  7918  Learning Rate:  3.6449417042153994e-05  Varinance:  0.00039029206137459224 \n",
      "\n",
      "Epoch:  7919  Learning Rate:  3.6412985843746965e-05  Varinance:  0.0003897468376707354 \n",
      "\n",
      "Epoch:  7920  Learning Rate:  3.637659105832884e-05  Varinance:  0.00038920237562440875 \n",
      "\n",
      "Epoch:  7921  Learning Rate:  3.6340232649504785e-05  Varinance:  0.0003886586741716032 \n",
      "\n",
      "Epoch:  7922  Learning Rate:  3.6303910580916395e-05  Varinance:  0.00038811573224979825 \n",
      "\n",
      "Epoch:  7923  Learning Rate:  3.626762481624166e-05  Varinance:  0.00038757354879795695 \n",
      "\n",
      "Epoch:  7924  Learning Rate:  3.6231375319194715e-05  Varinance:  0.00038703212275652515 \n",
      "\n",
      "Epoch:  7925  Learning Rate:  3.619516205352612e-05  Varinance:  0.000386491453067427 \n",
      "\n",
      "Epoch:  7926  Learning Rate:  3.615898498302262e-05  Varinance:  0.00038595153867406655 \n",
      "\n",
      "Epoch:  7927  Learning Rate:  3.6122844071507077e-05  Varinance:  0.00038541237852132416 \n",
      "\n",
      "Epoch:  7928  Learning Rate:  3.608673928283866e-05  Varinance:  0.00038487397155555203 \n",
      "\n",
      "Epoch:  7929  Learning Rate:  3.6050670580912497e-05  Varinance:  0.000384336316724576 \n",
      "\n",
      "Epoch:  7930  Learning Rate:  3.601463792965992e-05  Varinance:  0.00038379941297769164 \n",
      "\n",
      "Epoch:  7931  Learning Rate:  3.597864129304831e-05  Varinance:  0.00038326325926566244 \n",
      "\n",
      "Epoch:  7932  Learning Rate:  3.594268063508095e-05  Varinance:  0.0003827278545407159 \n",
      "\n",
      "Epoch:  7933  Learning Rate:  3.590675591979723e-05  Varinance:  0.0003821931977565452 \n",
      "\n",
      "Epoch:  7934  Learning Rate:  3.587086711127245e-05  Varinance:  0.0003816592878683052 \n",
      "\n",
      "Epoch:  7935  Learning Rate:  3.5835014173617736e-05  Varinance:  0.0003811261238326084 \n",
      "\n",
      "Epoch:  7936  Learning Rate:  3.579919707098021e-05  Varinance:  0.0003805937046075266 \n",
      "\n",
      "Epoch:  7937  Learning Rate:  3.576341576754272e-05  Varinance:  0.00038006202915258754 \n",
      "\n",
      "Epoch:  7938  Learning Rate:  3.5727670227523964e-05  Varinance:  0.00037953109642877014 \n",
      "\n",
      "Epoch:  7939  Learning Rate:  3.569196041517845e-05  Varinance:  0.0003790009053985069 \n",
      "\n",
      "Epoch:  7940  Learning Rate:  3.565628629479629e-05  Varinance:  0.0003784714550256792 \n",
      "\n",
      "Epoch:  7941  Learning Rate:  3.56206478307034e-05  Varinance:  0.0003779427442756164 \n",
      "\n",
      "Epoch:  7942  Learning Rate:  3.5585044987261345e-05  Varinance:  0.00037741477211509117 \n",
      "\n",
      "Epoch:  7943  Learning Rate:  3.554947772886721e-05  Varinance:  0.00037688753751232183 \n",
      "\n",
      "Epoch:  7944  Learning Rate:  3.551394601995379e-05  Varinance:  0.00037636103943696777 \n",
      "\n",
      "Epoch:  7945  Learning Rate:  3.5478449824989324e-05  Varinance:  0.00037583527686012584 \n",
      "\n",
      "Epoch:  7946  Learning Rate:  3.5442989108477635e-05  Varinance:  0.00037531024875433236 \n",
      "\n",
      "Epoch:  7947  Learning Rate:  3.540756383495805e-05  Varinance:  0.0003747859540935581 \n",
      "\n",
      "Epoch:  7948  Learning Rate:  3.537217396900521e-05  Varinance:  0.00037426239185320797 \n",
      "\n",
      "Epoch:  7949  Learning Rate:  3.533681947522929e-05  Varinance:  0.0003737395610101161 \n",
      "\n",
      "Epoch:  7950  Learning Rate:  3.530150031827582e-05  Varinance:  0.00037321746054254807 \n",
      "\n",
      "Epoch:  7951  Learning Rate:  3.5266216462825574e-05  Varinance:  0.00037269608943019664 \n",
      "\n",
      "Epoch:  7952  Learning Rate:  3.523096787359477e-05  Varinance:  0.0003721754466541781 \n",
      "\n",
      "Epoch:  7953  Learning Rate:  3.519575451533474e-05  Varinance:  0.00037165553119703377 \n",
      "\n",
      "Epoch:  7954  Learning Rate:  3.516057635283216e-05  Varinance:  0.0003711363420427259 \n",
      "\n",
      "Epoch:  7955  Learning Rate:  3.512543335090889e-05  Varinance:  0.00037061787817663683 \n",
      "\n",
      "Epoch:  7956  Learning Rate:  3.5090325474421866e-05  Varinance:  0.00037010013858556403 \n",
      "\n",
      "Epoch:  7957  Learning Rate:  3.505525268826325e-05  Varinance:  0.0003695831222577226 \n",
      "\n",
      "Epoch:  7958  Learning Rate:  3.502021495736026e-05  Varinance:  0.00036906682818274095 \n",
      "\n",
      "Epoch:  7959  Learning Rate:  3.498521224667513e-05  Varinance:  0.000368551255351657 \n",
      "\n",
      "Epoch:  7960  Learning Rate:  3.495024452120518e-05  Varinance:  0.00036803640275692007 \n",
      "\n",
      "Epoch:  7961  Learning Rate:  3.4915311745982646e-05  Varinance:  0.0003675222693923865 \n",
      "\n",
      "Epoch:  7962  Learning Rate:  3.488041388607476e-05  Varinance:  0.0003670088542533186 \n",
      "\n",
      "Epoch:  7963  Learning Rate:  3.48455509065837e-05  Varinance:  0.00036649615633638043 \n",
      "\n",
      "Epoch:  7964  Learning Rate:  3.4810722772646414e-05  Varinance:  0.00036598417463963963 \n",
      "\n",
      "Epoch:  7965  Learning Rate:  3.47759294494348e-05  Varinance:  0.0003654729081625635 \n",
      "\n",
      "Epoch:  7966  Learning Rate:  3.474117090215557e-05  Varinance:  0.0003649623559060151 \n",
      "\n",
      "Epoch:  7967  Learning Rate:  3.4706447096050106e-05  Varinance:  0.00036445251687225515 \n",
      "\n",
      "Epoch:  7968  Learning Rate:  3.467175799639466e-05  Varinance:  0.0003639433900649376 \n",
      "\n",
      "Epoch:  7969  Learning Rate:  3.4637103568500077e-05  Varinance:  0.00036343497448910875 \n",
      "\n",
      "Epoch:  7970  Learning Rate:  3.460248377771194e-05  Varinance:  0.00036292726915120315 \n",
      "\n",
      "Epoch:  7971  Learning Rate:  3.45678985894105e-05  Varinance:  0.00036242027305904496 \n",
      "\n",
      "Epoch:  7972  Learning Rate:  3.4533347969010495e-05  Varinance:  0.00036191398522184465 \n",
      "\n",
      "Epoch:  7973  Learning Rate:  3.4498831881961334e-05  Varinance:  0.00036140840465019456 \n",
      "\n",
      "Epoch:  7974  Learning Rate:  3.4464350293746965e-05  Varinance:  0.0003609035303560714 \n",
      "\n",
      "Epoch:  7975  Learning Rate:  3.4429903169885734e-05  Varinance:  0.0003603993613528313 \n",
      "\n",
      "Epoch:  7976  Learning Rate:  3.4395490475930566e-05  Varinance:  0.00035989589665520944 \n",
      "\n",
      "Epoch:  7977  Learning Rate:  3.4361112177468715e-05  Varinance:  0.0003593931352793153 \n",
      "\n",
      "Epoch:  7978  Learning Rate:  3.4326768240121905e-05  Varinance:  0.00035889107624263494 \n",
      "\n",
      "Epoch:  7979  Learning Rate:  3.429245862954622e-05  Varinance:  0.00035838971856402683 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7980  Learning Rate:  3.4258183311432e-05  Varinance:  0.00035788906126371823 \n",
      "\n",
      "Epoch:  7981  Learning Rate:  3.422394225150394e-05  Varinance:  0.0003573891033633069 \n",
      "\n",
      "Epoch:  7982  Learning Rate:  3.4189735415521016e-05  Varinance:  0.00035688984388575684 \n",
      "\n",
      "Epoch:  7983  Learning Rate:  3.415556276927633e-05  Varinance:  0.0003563912818553975 \n",
      "\n",
      "Epoch:  7984  Learning Rate:  3.412142427859728e-05  Varinance:  0.0003558934162979195 \n",
      "\n",
      "Epoch:  7985  Learning Rate:  3.408731990934533e-05  Varinance:  0.0003553962462403764 \n",
      "\n",
      "Epoch:  7986  Learning Rate:  3.405324962741613e-05  Varinance:  0.0003548997707111809 \n",
      "\n",
      "Epoch:  7987  Learning Rate:  3.401921339873942e-05  Varinance:  0.0003544039887401011 \n",
      "\n",
      "Epoch:  7988  Learning Rate:  3.3985211189278915e-05  Varinance:  0.00035390889935826234 \n",
      "\n",
      "Epoch:  7989  Learning Rate:  3.395124296503244e-05  Varinance:  0.0003534145015981429 \n",
      "\n",
      "Epoch:  7990  Learning Rate:  3.391730869203178e-05  Varinance:  0.0003529207944935731 \n",
      "\n",
      "Epoch:  7991  Learning Rate:  3.388340833634261e-05  Varinance:  0.0003524277770797313 \n",
      "\n",
      "Epoch:  7992  Learning Rate:  3.384954186406463e-05  Varinance:  0.0003519354483931455 \n",
      "\n",
      "Epoch:  7993  Learning Rate:  3.381570924133131e-05  Varinance:  0.00035144380747168934 \n",
      "\n",
      "Epoch:  7994  Learning Rate:  3.378191043431005e-05  Varinance:  0.0003509528533545792 \n",
      "\n",
      "Epoch:  7995  Learning Rate:  3.3748145409202063e-05  Varinance:  0.0003504625850823751 \n",
      "\n",
      "Epoch:  7996  Learning Rate:  3.371441413224226e-05  Varinance:  0.0003499730016969767 \n",
      "\n",
      "Epoch:  7997  Learning Rate:  3.368071656969941e-05  Varinance:  0.0003494841022416231 \n",
      "\n",
      "Epoch:  7998  Learning Rate:  3.3647052687875965e-05  Varinance:  0.00034899588576088776 \n",
      "\n",
      "Epoch:  7999  Learning Rate:  3.361342245310799e-05  Varinance:  0.0003485083513006807 \n",
      "\n",
      "Epoch:  8000  Learning Rate:  3.357982583176529e-05  Varinance:  0.00034802149790824497 \n",
      "\n",
      "Epoch:  8001  Learning Rate:  3.354626279025119e-05  Varinance:  0.00034753532463215254 \n",
      "\n",
      "Epoch:  8002  Learning Rate:  3.35127332950027e-05  Varinance:  0.0003470498305223065 \n",
      "\n",
      "Epoch:  8003  Learning Rate:  3.3479237312490245e-05  Varinance:  0.0003465650146299365 \n",
      "\n",
      "Epoch:  8004  Learning Rate:  3.344577480921795e-05  Varinance:  0.0003460808760075981 \n",
      "\n",
      "Epoch:  8005  Learning Rate:  3.341234575172325e-05  Varinance:  0.00034559741370916876 \n",
      "\n",
      "Epoch:  8006  Learning Rate:  3.3378950106577036e-05  Varinance:  0.0003451146267898494 \n",
      "\n",
      "Epoch:  8007  Learning Rate:  3.334558784038376e-05  Varinance:  0.00034463251430616064 \n",
      "\n",
      "Epoch:  8008  Learning Rate:  3.331225891978111e-05  Varinance:  0.0003441510753159395 \n",
      "\n",
      "Epoch:  8009  Learning Rate:  3.327896331144015e-05  Varinance:  0.0003436703088783409 \n",
      "\n",
      "Epoch:  8010  Learning Rate:  3.324570098206522e-05  Varinance:  0.0003431902140538341 \n",
      "\n",
      "Epoch:  8011  Learning Rate:  3.32124718983941e-05  Varinance:  0.000342710789904199 \n",
      "\n",
      "Epoch:  8012  Learning Rate:  3.3179276027197635e-05  Varinance:  0.0003422320354925279 \n",
      "\n",
      "Epoch:  8013  Learning Rate:  3.314611333527992e-05  Varinance:  0.00034175394988322154 \n",
      "\n",
      "Epoch:  8014  Learning Rate:  3.311298378947835e-05  Varinance:  0.0003412765321419881 \n",
      "\n",
      "Epoch:  8015  Learning Rate:  3.3079887356663334e-05  Varinance:  0.00034079978133583916 \n",
      "\n",
      "Epoch:  8016  Learning Rate:  3.304682400373837e-05  Varinance:  0.0003403236965330914 \n",
      "\n",
      "Epoch:  8017  Learning Rate:  3.301379369764023e-05  Varinance:  0.0003398482768033632 \n",
      "\n",
      "Epoch:  8018  Learning Rate:  3.2980796405338533e-05  Varinance:  0.00033937352121757053 \n",
      "\n",
      "Epoch:  8019  Learning Rate:  3.2947832093835934e-05  Varinance:  0.0003388994288479295 \n",
      "\n",
      "Epoch:  8020  Learning Rate:  3.2914900730168224e-05  Varinance:  0.00033842599876795126 \n",
      "\n",
      "Epoch:  8021  Learning Rate:  3.2882002281403996e-05  Varinance:  0.0003379532300524421 \n",
      "\n",
      "Epoch:  8022  Learning Rate:  3.284913671464473e-05  Varinance:  0.00033748112177749894 \n",
      "\n",
      "Epoch:  8023  Learning Rate:  3.2816303997024973e-05  Varinance:  0.000337009673020511 \n",
      "\n",
      "Epoch:  8024  Learning Rate:  3.278350409571195e-05  Varinance:  0.0003365388828601566 \n",
      "\n",
      "Epoch:  8025  Learning Rate:  3.275073697790575e-05  Varinance:  0.00033606875037639917 \n",
      "\n",
      "Epoch:  8026  Learning Rate:  3.2718002610839205e-05  Varinance:  0.0003355992746504891 \n",
      "\n",
      "Epoch:  8027  Learning Rate:  3.268530096177805e-05  Varinance:  0.00033513045476495985 \n",
      "\n",
      "Epoch:  8028  Learning Rate:  3.265263199802058e-05  Varinance:  0.000334662289803627 \n",
      "\n",
      "Epoch:  8029  Learning Rate:  3.261999568689777e-05  Varinance:  0.0003341947788515843 \n",
      "\n",
      "Epoch:  8030  Learning Rate:  3.258739199577343e-05  Varinance:  0.0003337279209952053 \n",
      "\n",
      "Epoch:  8031  Learning Rate:  3.25548208920438e-05  Varinance:  0.0003332617153221399 \n",
      "\n",
      "Epoch:  8032  Learning Rate:  3.252228234313771e-05  Varinance:  0.0003327961609213108 \n",
      "\n",
      "Epoch:  8033  Learning Rate:  3.248977631651674e-05  Varinance:  0.000332331256882915 \n",
      "\n",
      "Epoch:  8034  Learning Rate:  3.245730277967479e-05  Varinance:  0.00033186700229842015 \n",
      "\n",
      "Epoch:  8035  Learning Rate:  3.2424861700138266e-05  Varinance:  0.00033140339626056345 \n",
      "\n",
      "Epoch:  8036  Learning Rate:  3.2392453045466205e-05  Varinance:  0.00033094043786334796 \n",
      "\n",
      "Epoch:  8037  Learning Rate:  3.2360076783249885e-05  Varinance:  0.0003304781262020439 \n",
      "\n",
      "Epoch:  8038  Learning Rate:  3.2327732881112986e-05  Varinance:  0.00033001646037318557 \n",
      "\n",
      "Epoch:  8039  Learning Rate:  3.229542130671173e-05  Varinance:  0.00032955543947456756 \n",
      "\n",
      "Epoch:  8040  Learning Rate:  3.226314202773446e-05  Varinance:  0.0003290950626052465 \n",
      "\n",
      "Epoch:  8041  Learning Rate:  3.2230895011901914e-05  Varinance:  0.0003286353288655371 \n",
      "\n",
      "Epoch:  8042  Learning Rate:  3.2198680226967004e-05  Varinance:  0.00032817623735701144 \n",
      "\n",
      "Epoch:  8043  Learning Rate:  3.2166497640715064e-05  Varinance:  0.0003277177871824949 \n",
      "\n",
      "Epoch:  8044  Learning Rate:  3.213434722096344e-05  Varinance:  0.00032725997744606803 \n",
      "\n",
      "Epoch:  8045  Learning Rate:  3.210222893556166e-05  Varinance:  0.0003268028072530628 \n",
      "\n",
      "Epoch:  8046  Learning Rate:  3.2070142752391555e-05  Varinance:  0.00032634627571005937 \n",
      "\n",
      "Epoch:  8047  Learning Rate:  3.203808863936687e-05  Varinance:  0.00032589038192488767 \n",
      "\n",
      "Epoch:  8048  Learning Rate:  3.2006066564433434e-05  Varinance:  0.0003254351250066234 \n",
      "\n",
      "Epoch:  8049  Learning Rate:  3.1974076495569294e-05  Varinance:  0.0003249805040655874 \n",
      "\n",
      "Epoch:  8050  Learning Rate:  3.194211840078431e-05  Varinance:  0.0003245265182133416 \n",
      "\n",
      "Epoch:  8051  Learning Rate:  3.191019224812033e-05  Varinance:  0.00032407316656269086 \n",
      "\n",
      "Epoch:  8052  Learning Rate:  3.187829800565131e-05  Varinance:  0.00032362044822767936 \n",
      "\n",
      "Epoch:  8053  Learning Rate:  3.184643564148296e-05  Varinance:  0.0003231683623235873 \n",
      "\n",
      "Epoch:  8054  Learning Rate:  3.181460512375285e-05  Varinance:  0.0003227169079669323 \n",
      "\n",
      "Epoch:  8055  Learning Rate:  3.178280642063057e-05  Varinance:  0.00032226608427546576 \n",
      "\n",
      "Epoch:  8056  Learning Rate:  3.175103950031735e-05  Varinance:  0.00032181589036817224 \n",
      "\n",
      "Epoch:  8057  Learning Rate:  3.171930433104629e-05  Varinance:  0.000321366325365265 \n",
      "\n",
      "Epoch:  8058  Learning Rate:  3.168760088108214e-05  Varinance:  0.0003209173883881884 \n",
      "\n",
      "Epoch:  8059  Learning Rate:  3.165592911872157e-05  Varinance:  0.0003204690785596138 \n",
      "\n",
      "Epoch:  8060  Learning Rate:  3.162428901229275e-05  Varinance:  0.00032002139500343673 \n",
      "\n",
      "Epoch:  8061  Learning Rate:  3.159268053015553e-05  Varinance:  0.0003195743368447781 \n",
      "\n",
      "Epoch:  8062  Learning Rate:  3.156110364070153e-05  Varinance:  0.0003191279032099805 \n",
      "\n",
      "Epoch:  8063  Learning Rate:  3.152955831235379e-05  Varinance:  0.00031868209322660757 \n",
      "\n",
      "Epoch:  8064  Learning Rate:  3.149804451356695e-05  Varinance:  0.00031823690602344 \n",
      "\n",
      "Epoch:  8065  Learning Rate:  3.146656221282729e-05  Varinance:  0.0003177923407304772 \n",
      "\n",
      "Epoch:  8066  Learning Rate:  3.143511137865247e-05  Varinance:  0.000317348396478934 \n",
      "\n",
      "Epoch:  8067  Learning Rate:  3.140369197959159e-05  Varinance:  0.0003169050724012372 \n",
      "\n",
      "Epoch:  8068  Learning Rate:  3.137230398422537e-05  Varinance:  0.00031646236763102723 \n",
      "\n",
      "Epoch:  8069  Learning Rate:  3.134094736116574e-05  Varinance:  0.00031602028130315414 \n",
      "\n",
      "Epoch:  8070  Learning Rate:  3.1309622079056034e-05  Varinance:  0.00031557881255367734 \n",
      "\n",
      "Epoch:  8071  Learning Rate:  3.127832810657107e-05  Varinance:  0.0003151379605198613 \n",
      "\n",
      "Epoch:  8072  Learning Rate:  3.124706541241682e-05  Varinance:  0.0003146977243401773 \n",
      "\n",
      "Epoch:  8073  Learning Rate:  3.121583396533058e-05  Varinance:  0.00031425810315430035 \n",
      "\n",
      "Epoch:  8074  Learning Rate:  3.118463373408085e-05  Varinance:  0.0003138190961031055 \n",
      "\n",
      "Epoch:  8075  Learning Rate:  3.115346468746752e-05  Varinance:  0.00031338070232866965 \n",
      "\n",
      "Epoch:  8076  Learning Rate:  3.1122326794321467e-05  Varinance:  0.0003129429209742683 \n",
      "\n",
      "Epoch:  8077  Learning Rate:  3.1091220023504744e-05  Varinance:  0.00031250575118437184 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8078  Learning Rate:  3.1060144343910696e-05  Varinance:  0.0003120691921046478 \n",
      "\n",
      "Epoch:  8079  Learning Rate:  3.1029099724463575e-05  Varinance:  0.00031163324288195626 \n",
      "\n",
      "Epoch:  8080  Learning Rate:  3.0998086134118715e-05  Varinance:  0.00031119790266435004 \n",
      "\n",
      "Epoch:  8081  Learning Rate:  3.096710354186262e-05  Varinance:  0.00031076317060107014 \n",
      "\n",
      "Epoch:  8082  Learning Rate:  3.093615191671265e-05  Varinance:  0.00031032904584254783 \n",
      "\n",
      "Epoch:  8083  Learning Rate:  3.0905231227717125e-05  Varinance:  0.0003098955275404012 \n",
      "\n",
      "Epoch:  8084  Learning Rate:  3.0874341443955456e-05  Varinance:  0.0003094626148474318 \n",
      "\n",
      "Epoch:  8085  Learning Rate:  3.08434825345378e-05  Varinance:  0.0003090303069176264 \n",
      "\n",
      "Epoch:  8086  Learning Rate:  3.0812654468605194e-05  Varinance:  0.00030859860290615294 \n",
      "\n",
      "Epoch:  8087  Learning Rate:  3.078185721532968e-05  Varinance:  0.00030816750196936026 \n",
      "\n",
      "Epoch:  8088  Learning Rate:  3.0751090743913954e-05  Varinance:  0.00030773700326477407 \n",
      "\n",
      "Epoch:  8089  Learning Rate:  3.072035502359152e-05  Varinance:  0.0003073071059510985 \n",
      "\n",
      "Epoch:  8090  Learning Rate:  3.068965002362663e-05  Varinance:  0.0003068778091882131 \n",
      "\n",
      "Epoch:  8091  Learning Rate:  3.065897571331437e-05  Varinance:  0.00030644911213716943 \n",
      "\n",
      "Epoch:  8092  Learning Rate:  3.0628332061980376e-05  Varinance:  0.00030602101396019234 \n",
      "\n",
      "Epoch:  8093  Learning Rate:  3.059771903898094e-05  Varinance:  0.0003055935138206769 \n",
      "\n",
      "Epoch:  8094  Learning Rate:  3.056713661370315e-05  Varinance:  0.0003051666108831872 \n",
      "\n",
      "Epoch:  8095  Learning Rate:  3.053658475556453e-05  Varinance:  0.0003047403043134528 \n",
      "\n",
      "Epoch:  8096  Learning Rate:  3.0506063434013144e-05  Varinance:  0.00030431459327837036 \n",
      "\n",
      "Epoch:  8097  Learning Rate:  3.0475572618527792e-05  Varinance:  0.00030388947694600033 \n",
      "\n",
      "Epoch:  8098  Learning Rate:  3.0445112278617594e-05  Varinance:  0.0003034649544855637 \n",
      "\n",
      "Epoch:  8099  Learning Rate:  3.0414682383822165e-05  Varinance:  0.0003030410250674437 \n",
      "\n",
      "Epoch:  8100  Learning Rate:  3.0384282903711703e-05  Varinance:  0.0003026176878631821 \n",
      "\n",
      "Epoch:  8101  Learning Rate:  3.035391380788668e-05  Varinance:  0.0003021949420454781 \n",
      "\n",
      "Epoch:  8102  Learning Rate:  3.0323575065977938e-05  Varinance:  0.00030177278678818556 \n",
      "\n",
      "Epoch:  8103  Learning Rate:  3.0293266647646843e-05  Varinance:  0.0003013512212663136 \n",
      "\n",
      "Epoch:  8104  Learning Rate:  3.0262988522584917e-05  Varinance:  0.00030093024465602404 \n",
      "\n",
      "Epoch:  8105  Learning Rate:  3.0232740660514045e-05  Varinance:  0.00030050985613462795 \n",
      "\n",
      "Epoch:  8106  Learning Rate:  3.0202523031186293e-05  Varinance:  0.00030009005488058714 \n",
      "\n",
      "Epoch:  8107  Learning Rate:  3.0172335604384145e-05  Varinance:  0.0002996708400735108 \n",
      "\n",
      "Epoch:  8108  Learning Rate:  3.014217834992011e-05  Varinance:  0.00029925221089415437 \n",
      "\n",
      "Epoch:  8109  Learning Rate:  3.0112051237636888e-05  Varinance:  0.0002988341665244164 \n",
      "\n",
      "Epoch:  8110  Learning Rate:  3.0081954237407467e-05  Varinance:  0.0002984167061473399 \n",
      "\n",
      "Epoch:  8111  Learning Rate:  3.005188731913479e-05  Varinance:  0.00029799982894710894 \n",
      "\n",
      "Epoch:  8112  Learning Rate:  3.0021850452751885e-05  Varinance:  0.0002975835341090458 \n",
      "\n",
      "Epoch:  8113  Learning Rate:  2.999184360822198e-05  Varinance:  0.0002971678208196125 \n",
      "\n",
      "Epoch:  8114  Learning Rate:  2.996186675553819e-05  Varinance:  0.0002967526882664069 \n",
      "\n",
      "Epoch:  8115  Learning Rate:  2.99319198647236e-05  Varinance:  0.00029633813563816215 \n",
      "\n",
      "Epoch:  8116  Learning Rate:  2.9902002905831415e-05  Varinance:  0.0002959241621247434 \n",
      "\n",
      "Epoch:  8117  Learning Rate:  2.987211584894463e-05  Varinance:  0.00029551076691714906 \n",
      "\n",
      "Epoch:  8118  Learning Rate:  2.9842258664176134e-05  Varinance:  0.0002950979492075075 \n",
      "\n",
      "Epoch:  8119  Learning Rate:  2.9812431321668837e-05  Varinance:  0.0002946857081890742 \n",
      "\n",
      "Epoch:  8120  Learning Rate:  2.9782633791595348e-05  Varinance:  0.0002942740430562333 \n",
      "\n",
      "Epoch:  8121  Learning Rate:  2.9752866044158136e-05  Varinance:  0.00029386295300449367 \n",
      "\n",
      "Epoch:  8122  Learning Rate:  2.972312804958939e-05  Varinance:  0.0002934524372304886 \n",
      "\n",
      "Epoch:  8123  Learning Rate:  2.9693419778151226e-05  Varinance:  0.0002930424949319722 \n",
      "\n",
      "Epoch:  8124  Learning Rate:  2.9663741200135316e-05  Varinance:  0.0002926331253078206 \n",
      "\n",
      "Epoch:  8125  Learning Rate:  2.963409228586302e-05  Varinance:  0.0002922243275580293 \n",
      "\n",
      "Epoch:  8126  Learning Rate:  2.960447300568554e-05  Varinance:  0.0002918161008837098 \n",
      "\n",
      "Epoch:  8127  Learning Rate:  2.9574883329983525e-05  Varinance:  0.000291408444487091 \n",
      "\n",
      "Epoch:  8128  Learning Rate:  2.9545323229167257e-05  Varinance:  0.0002910013575715159 \n",
      "\n",
      "Epoch:  8129  Learning Rate:  2.951579267367673e-05  Varinance:  0.00029059483934144094 \n",
      "\n",
      "Epoch:  8130  Learning Rate:  2.9486291633981336e-05  Varinance:  0.00029018888900243225 \n",
      "\n",
      "Epoch:  8131  Learning Rate:  2.9456820080579983e-05  Varinance:  0.0002897835057611673 \n",
      "\n",
      "Epoch:  8132  Learning Rate:  2.942737798400122e-05  Varinance:  0.0002893786888254319 \n",
      "\n",
      "Epoch:  8133  Learning Rate:  2.9397965314802888e-05  Varinance:  0.0002889744374041169 \n",
      "\n",
      "Epoch:  8134  Learning Rate:  2.9368582043572323e-05  Varinance:  0.0002885707507072199 \n",
      "\n",
      "Epoch:  8135  Learning Rate:  2.9339228140926196e-05  Varinance:  0.0002881676279458416 \n",
      "\n",
      "Epoch:  8136  Learning Rate:  2.9309903577510706e-05  Varinance:  0.00028776506833218534 \n",
      "\n",
      "Epoch:  8137  Learning Rate:  2.928060832400124e-05  Varinance:  0.0002873630710795533 \n",
      "\n",
      "Epoch:  8138  Learning Rate:  2.9251342351102482e-05  Varinance:  0.00028696163540234816 \n",
      "\n",
      "Epoch:  8139  Learning Rate:  2.9222105629548564e-05  Varinance:  0.0002865607605160703 \n",
      "\n",
      "Epoch:  8140  Learning Rate:  2.9192898130102717e-05  Varinance:  0.0002861604456373143 \n",
      "\n",
      "Epoch:  8141  Learning Rate:  2.9163719823557374e-05  Varinance:  0.0002857606899837706 \n",
      "\n",
      "Epoch:  8142  Learning Rate:  2.913457068073434e-05  Varinance:  0.0002853614927742227 \n",
      "\n",
      "Epoch:  8143  Learning Rate:  2.910545067248441e-05  Varinance:  0.00028496285322854374 \n",
      "\n",
      "Epoch:  8144  Learning Rate:  2.9076359769687532e-05  Varinance:  0.0002845647705676982 \n",
      "\n",
      "Epoch:  8145  Learning Rate:  2.9047297943252896e-05  Varinance:  0.0002841672440137384 \n",
      "\n",
      "Epoch:  8146  Learning Rate:  2.9018265164118628e-05  Varinance:  0.000283770272789804 \n",
      "\n",
      "Epoch:  8147  Learning Rate:  2.898926140325189e-05  Varinance:  0.00028337385612011817 \n",
      "\n",
      "Epoch:  8148  Learning Rate:  2.8960286631649016e-05  Varinance:  0.00028297799322998953 \n",
      "\n",
      "Epoch:  8149  Learning Rate:  2.8931340820335192e-05  Varinance:  0.00028258268334580884 \n",
      "\n",
      "Epoch:  8150  Learning Rate:  2.89024239403646e-05  Varinance:  0.00028218792569504605 \n",
      "\n",
      "Epoch:  8151  Learning Rate:  2.88735359628203e-05  Varinance:  0.0002817937195062518 \n",
      "\n",
      "Epoch:  8152  Learning Rate:  2.8844676858814423e-05  Varinance:  0.0002814000640090539 \n",
      "\n",
      "Epoch:  8153  Learning Rate:  2.881584659948781e-05  Varinance:  0.00028100695843415695 \n",
      "\n",
      "Epoch:  8154  Learning Rate:  2.8787045156010147e-05  Varinance:  0.00028061440201333863 \n",
      "\n",
      "Epoch:  8155  Learning Rate:  2.8758272499580092e-05  Varinance:  0.0002802223939794513 \n",
      "\n",
      "Epoch:  8156  Learning Rate:  2.8729528601424926e-05  Varinance:  0.00027983093356641906 \n",
      "\n",
      "Epoch:  8157  Learning Rate:  2.870081343280071e-05  Varinance:  0.00027944002000923455 \n",
      "\n",
      "Epoch:  8158  Learning Rate:  2.867212696499237e-05  Varinance:  0.0002790496525439608 \n",
      "\n",
      "Epoch:  8159  Learning Rate:  2.8643469169313378e-05  Varinance:  0.0002786598304077274 \n",
      "\n",
      "Epoch:  8160  Learning Rate:  2.8614840017105904e-05  Varinance:  0.00027827055283873015 \n",
      "\n",
      "Epoch:  8161  Learning Rate:  2.858623947974087e-05  Varinance:  0.00027788181907622753 \n",
      "\n",
      "Epoch:  8162  Learning Rate:  2.8557667528617703e-05  Varinance:  0.00027749362836054244 \n",
      "\n",
      "Epoch:  8163  Learning Rate:  2.8529124135164395e-05  Varinance:  0.0002771059799330588 \n",
      "\n",
      "Epoch:  8164  Learning Rate:  2.8500609270837644e-05  Varinance:  0.0002767188730362188 \n",
      "\n",
      "Epoch:  8165  Learning Rate:  2.8472122907122547e-05  Varinance:  0.0002763323069135246 \n",
      "\n",
      "Epoch:  8166  Learning Rate:  2.8443665015532725e-05  Varinance:  0.00027594628080953454 \n",
      "\n",
      "Epoch:  8167  Learning Rate:  2.8415235567610237e-05  Varinance:  0.00027556079396986267 \n",
      "\n",
      "Epoch:  8168  Learning Rate:  2.838683453492574e-05  Varinance:  0.0002751758456411756 \n",
      "\n",
      "Epoch:  8169  Learning Rate:  2.8358461889078138e-05  Varinance:  0.0002747914350711937 \n",
      "\n",
      "Epoch:  8170  Learning Rate:  2.833011760169474e-05  Varinance:  0.00027440756150868824 \n",
      "\n",
      "Epoch:  8171  Learning Rate:  2.830180164443136e-05  Varinance:  0.0002740242242034785 \n",
      "\n",
      "Epoch:  8172  Learning Rate:  2.8273513988971978e-05  Varinance:  0.00027364142240643316 \n",
      "\n",
      "Epoch:  8173  Learning Rate:  2.8245254607028892e-05  Varinance:  0.00027325915536946695 \n",
      "\n",
      "Epoch:  8174  Learning Rate:  2.8217023470342814e-05  Varinance:  0.00027287742234554014 \n",
      "\n",
      "Epoch:  8175  Learning Rate:  2.8188820550682558e-05  Varinance:  0.00027249622258865514 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8176  Learning Rate:  2.8160645819845157e-05  Varinance:  0.0002721155553538579 \n",
      "\n",
      "Epoch:  8177  Learning Rate:  2.8132499249655967e-05  Varinance:  0.00027173541989723504 \n",
      "\n",
      "Epoch:  8178  Learning Rate:  2.810438081196838e-05  Varinance:  0.000271355815475911 \n",
      "\n",
      "Epoch:  8179  Learning Rate:  2.8076290478663885e-05  Varinance:  0.0002709767413480494 \n",
      "\n",
      "Epoch:  8180  Learning Rate:  2.8048228221652266e-05  Varinance:  0.0002705981967728498 \n",
      "\n",
      "Epoch:  8181  Learning Rate:  2.8020194012871204e-05  Varinance:  0.00027022018101054683 \n",
      "\n",
      "Epoch:  8182  Learning Rate:  2.7992187824286488e-05  Varinance:  0.0002698426933224075 \n",
      "\n",
      "Epoch:  8183  Learning Rate:  2.796420962789188e-05  Varinance:  0.0002694657329707319 \n",
      "\n",
      "Epoch:  8184  Learning Rate:  2.7936259395709283e-05  Varinance:  0.0002690892992188509 \n",
      "\n",
      "Epoch:  8185  Learning Rate:  2.790833709978841e-05  Varinance:  0.00026871339133112285 \n",
      "\n",
      "Epoch:  8186  Learning Rate:  2.7880442712206908e-05  Varinance:  0.00026833800857293535 \n",
      "\n",
      "Epoch:  8187  Learning Rate:  2.7852576205070494e-05  Varinance:  0.0002679631502107016 \n",
      "\n",
      "Epoch:  8188  Learning Rate:  2.7824737550512604e-05  Varinance:  0.0002675888155118602 \n",
      "\n",
      "Epoch:  8189  Learning Rate:  2.7796926720694537e-05  Varinance:  0.00026721500374487166 \n",
      "\n",
      "Epoch:  8190  Learning Rate:  2.7769143687805555e-05  Varinance:  0.00026684171417921975 \n",
      "\n",
      "Epoch:  8191  Learning Rate:  2.774138842406257e-05  Varinance:  0.0002664689460854089 \n",
      "\n",
      "Epoch:  8192  Learning Rate:  2.7713660901710282e-05  Varinance:  0.00026609669873496107 \n",
      "\n",
      "Epoch:  8193  Learning Rate:  2.768596109302125e-05  Varinance:  0.0002657249714004173 \n",
      "\n",
      "Epoch:  8194  Learning Rate:  2.7658288970295617e-05  Varinance:  0.0002653537633553346 \n",
      "\n",
      "Epoch:  8195  Learning Rate:  2.7630644505861208e-05  Varinance:  0.00026498307387428483 \n",
      "\n",
      "Epoch:  8196  Learning Rate:  2.7603027672073657e-05  Varinance:  0.00026461290223285235 \n",
      "\n",
      "Epoch:  8197  Learning Rate:  2.7575438441316082e-05  Varinance:  0.0002642432477076345 \n",
      "\n",
      "Epoch:  8198  Learning Rate:  2.7547876785999245e-05  Varinance:  0.00026387410957623945 \n",
      "\n",
      "Epoch:  8199  Learning Rate:  2.752034267856144e-05  Varinance:  0.000263505487117283 \n",
      "\n",
      "Epoch:  8200  Learning Rate:  2.7492836091468656e-05  Varinance:  0.00026313737961039014 \n",
      "\n",
      "Epoch:  8201  Learning Rate:  2.7465356997214256e-05  Varinance:  0.0002627697863361916 \n",
      "\n",
      "Epoch:  8202  Learning Rate:  2.7437905368319088e-05  Varinance:  0.00026240270657632383 \n",
      "\n",
      "Epoch:  8203  Learning Rate:  2.741048117733163e-05  Varinance:  0.000262036139613425 \n",
      "\n",
      "Epoch:  8204  Learning Rate:  2.7383084396827626e-05  Varinance:  0.00026167008473113707 \n",
      "\n",
      "Epoch:  8205  Learning Rate:  2.7355714999410262e-05  Varinance:  0.0002613045412141027 \n",
      "\n",
      "Epoch:  8206  Learning Rate:  2.7328372957710217e-05  Varinance:  0.0002609395083479624 \n",
      "\n",
      "Epoch:  8207  Learning Rate:  2.7301058244385414e-05  Varinance:  0.00026057498541935605 \n",
      "\n",
      "Epoch:  8208  Learning Rate:  2.727377083212108e-05  Varinance:  0.00026021097171592016 \n",
      "\n",
      "Epoch:  8209  Learning Rate:  2.724651069362989e-05  Varinance:  0.0002598474665262848 \n",
      "\n",
      "Epoch:  8210  Learning Rate:  2.7219277801651676e-05  Varinance:  0.00025948446914007535 \n",
      "\n",
      "Epoch:  8211  Learning Rate:  2.719207212895348e-05  Varinance:  0.000259121978847909 \n",
      "\n",
      "Epoch:  8212  Learning Rate:  2.7164893648329727e-05  Varinance:  0.0002587599949413944 \n",
      "\n",
      "Epoch:  8213  Learning Rate:  2.7137742332601884e-05  Varinance:  0.00025839851671312857 \n",
      "\n",
      "Epoch:  8214  Learning Rate:  2.7110618154618638e-05  Varinance:  0.00025803754345669785 \n",
      "\n",
      "Epoch:  8215  Learning Rate:  2.7083521087255757e-05  Varinance:  0.00025767707446667555 \n",
      "\n",
      "Epoch:  8216  Learning Rate:  2.705645110341627e-05  Varinance:  0.0002573171090386192 \n",
      "\n",
      "Epoch:  8217  Learning Rate:  2.7029408176030137e-05  Varinance:  0.00025695764646907146 \n",
      "\n",
      "Epoch:  8218  Learning Rate:  2.7002392278054386e-05  Varinance:  0.00025659868605555766 \n",
      "\n",
      "Epoch:  8219  Learning Rate:  2.6975403382473212e-05  Varinance:  0.00025624022709658446 \n",
      "\n",
      "Epoch:  8220  Learning Rate:  2.6948441462297667e-05  Varinance:  0.0002558822688916374 \n",
      "\n",
      "Epoch:  8221  Learning Rate:  2.6921506490565785e-05  Varinance:  0.000255524810741182 \n",
      "\n",
      "Epoch:  8222  Learning Rate:  2.689459844034268e-05  Varinance:  0.00025516785194666083 \n",
      "\n",
      "Epoch:  8223  Learning Rate:  2.686771728472026e-05  Varinance:  0.0002548113918104911 \n",
      "\n",
      "Epoch:  8224  Learning Rate:  2.6840862996817316e-05  Varinance:  0.0002544554296360658 \n",
      "\n",
      "Epoch:  8225  Learning Rate:  2.681403554977965e-05  Varinance:  0.00025409996472775063 \n",
      "\n",
      "Epoch:  8226  Learning Rate:  2.6787234916779778e-05  Varinance:  0.0002537449963908835 \n",
      "\n",
      "Epoch:  8227  Learning Rate:  2.6760461071016996e-05  Varinance:  0.0002533905239317714 \n",
      "\n",
      "Epoch:  8228  Learning Rate:  2.673371398571757e-05  Varinance:  0.0002530365466576917 \n",
      "\n",
      "Epoch:  8229  Learning Rate:  2.6706993634134355e-05  Varinance:  0.00025268306387688964 \n",
      "\n",
      "Epoch:  8230  Learning Rate:  2.6680299989547e-05  Varinance:  0.0002523300748985752 \n",
      "\n",
      "Epoch:  8231  Learning Rate:  2.6653633025261808e-05  Varinance:  0.0002519775790329249 \n",
      "\n",
      "Epoch:  8232  Learning Rate:  2.6626992714611915e-05  Varinance:  0.0002516255755910784 \n",
      "\n",
      "Epoch:  8233  Learning Rate:  2.660037903095695e-05  Varinance:  0.0002512740638851382 \n",
      "\n",
      "Epoch:  8234  Learning Rate:  2.6573791947683187e-05  Varinance:  0.00025092304322816634 \n",
      "\n",
      "Epoch:  8235  Learning Rate:  2.6547231438203636e-05  Varinance:  0.0002505725129341858 \n",
      "\n",
      "Epoch:  8236  Learning Rate:  2.652069747595773e-05  Varinance:  0.000250222472318178 \n",
      "\n",
      "Epoch:  8237  Learning Rate:  2.649419003441147e-05  Varinance:  0.0002498729206960798 \n",
      "\n",
      "Epoch:  8238  Learning Rate:  2.6467709087057493e-05  Varinance:  0.0002495238573847851 \n",
      "\n",
      "Epoch:  8239  Learning Rate:  2.644125460741481e-05  Varinance:  0.0002491752817021416 \n",
      "\n",
      "Epoch:  8240  Learning Rate:  2.641482656902889e-05  Varinance:  0.0002488271929669505 \n",
      "\n",
      "Epoch:  8241  Learning Rate:  2.6388424945471795e-05  Varinance:  0.0002484795904989629 \n",
      "\n",
      "Epoch:  8242  Learning Rate:  2.636204971034184e-05  Varinance:  0.0002481324736188821 \n",
      "\n",
      "Epoch:  8243  Learning Rate:  2.6335700837263743e-05  Varinance:  0.00024778584164835977 \n",
      "\n",
      "Epoch:  8244  Learning Rate:  2.6309378299888724e-05  Varinance:  0.00024743969390999437 \n",
      "\n",
      "Epoch:  8245  Learning Rate:  2.62830820718942e-05  Varinance:  0.0002470940297273316 \n",
      "\n",
      "Epoch:  8246  Learning Rate:  2.6256812126983936e-05  Varinance:  0.0002467488484248619 \n",
      "\n",
      "Epoch:  8247  Learning Rate:  2.6230568438887945e-05  Varinance:  0.0002464041493280199 \n",
      "\n",
      "Epoch:  8248  Learning Rate:  2.620435098136262e-05  Varinance:  0.00024605993176318096 \n",
      "\n",
      "Epoch:  8249  Learning Rate:  2.6178159728190465e-05  Varinance:  0.00024571619505766305 \n",
      "\n",
      "Epoch:  8250  Learning Rate:  2.6151994653180174e-05  Varinance:  0.0002453729385397238 \n",
      "\n",
      "Epoch:  8251  Learning Rate:  2.6125855730166755e-05  Varinance:  0.00024503016153855784 \n",
      "\n",
      "Epoch:  8252  Learning Rate:  2.6099742933011245e-05  Varinance:  0.00024468786338429824 \n",
      "\n",
      "Epoch:  8253  Learning Rate:  2.60736562356008e-05  Varinance:  0.00024434604340801345 \n",
      "\n",
      "Epoch:  8254  Learning Rate:  2.604759561184881e-05  Varinance:  0.00024400470094170678 \n",
      "\n",
      "Epoch:  8255  Learning Rate:  2.60215610356946e-05  Varinance:  0.00024366383531831346 \n",
      "\n",
      "Epoch:  8256  Learning Rate:  2.5995552481103548e-05  Varinance:  0.00024332344587170182 \n",
      "\n",
      "Epoch:  8257  Learning Rate:  2.596956992206719e-05  Varinance:  0.00024298353193667076 \n",
      "\n",
      "Epoch:  8258  Learning Rate:  2.594361333260292e-05  Varinance:  0.00024264409284894723 \n",
      "\n",
      "Epoch:  8259  Learning Rate:  2.5917682686754142e-05  Varinance:  0.00024230512794518733 \n",
      "\n",
      "Epoch:  8260  Learning Rate:  2.5891777958590167e-05  Varinance:  0.00024196663656297348 \n",
      "\n",
      "Epoch:  8261  Learning Rate:  2.5865899122206352e-05  Varinance:  0.0002416286180408138 \n",
      "\n",
      "Epoch:  8262  Learning Rate:  2.584004615172381e-05  Varinance:  0.00024129107171813938 \n",
      "\n",
      "Epoch:  8263  Learning Rate:  2.5814219021289536e-05  Varinance:  0.0002409539969353052 \n",
      "\n",
      "Epoch:  8264  Learning Rate:  2.5788417705076478e-05  Varinance:  0.0002406173930335878 \n",
      "\n",
      "Epoch:  8265  Learning Rate:  2.5762642177283272e-05  Varinance:  0.00024028125935518266 \n",
      "\n",
      "Epoch:  8266  Learning Rate:  2.5736892412134342e-05  Varinance:  0.0002399455952432056 \n",
      "\n",
      "Epoch:  8267  Learning Rate:  2.5711168383880025e-05  Varinance:  0.00023961040004168935 \n",
      "\n",
      "Epoch:  8268  Learning Rate:  2.5685470066796223e-05  Varinance:  0.0002392756730955837 \n",
      "\n",
      "Epoch:  8269  Learning Rate:  2.5659797435184586e-05  Varinance:  0.00023894141375075222 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8270  Learning Rate:  2.563415046337257e-05  Varinance:  0.00023860762135397335 \n",
      "\n",
      "Epoch:  8271  Learning Rate:  2.5608529125713157e-05  Varinance:  0.00023827429525293833 \n",
      "\n",
      "Epoch:  8272  Learning Rate:  2.558293339658495e-05  Varinance:  0.00023794143479624816 \n",
      "\n",
      "Epoch:  8273  Learning Rate:  2.555736325039232e-05  Varinance:  0.00023760903933341525 \n",
      "\n",
      "Epoch:  8274  Learning Rate:  2.5531818661565073e-05  Varinance:  0.0002372771082148606 \n",
      "\n",
      "Epoch:  8275  Learning Rate:  2.5506299604558616e-05  Varinance:  0.00023694564079191144 \n",
      "\n",
      "Epoch:  8276  Learning Rate:  2.548080605385384e-05  Varinance:  0.00023661463641680243 \n",
      "\n",
      "Epoch:  8277  Learning Rate:  2.5455337983957288e-05  Varinance:  0.00023628409444267275 \n",
      "\n",
      "Epoch:  8278  Learning Rate:  2.542989536940084e-05  Varinance:  0.00023595401422356552 \n",
      "\n",
      "Epoch:  8279  Learning Rate:  2.5404478184741833e-05  Varinance:  0.00023562439511442512 \n",
      "\n",
      "Epoch:  8280  Learning Rate:  2.537908640456318e-05  Varinance:  0.00023529523647109824 \n",
      "\n",
      "Epoch:  8281  Learning Rate:  2.535372000347304e-05  Varinance:  0.0002349665376503314 \n",
      "\n",
      "Epoch:  8282  Learning Rate:  2.532837895610498e-05  Varinance:  0.00023463829800976857 \n",
      "\n",
      "Epoch:  8283  Learning Rate:  2.5303063237118024e-05  Varinance:  0.00023431051690795218 \n",
      "\n",
      "Epoch:  8284  Learning Rate:  2.5277772821196413e-05  Varinance:  0.00023398319370432038 \n",
      "\n",
      "Epoch:  8285  Learning Rate:  2.5252507683049688e-05  Varinance:  0.00023365632775920674 \n",
      "\n",
      "Epoch:  8286  Learning Rate:  2.5227267797412793e-05  Varinance:  0.00023332991843383696 \n",
      "\n",
      "Epoch:  8287  Learning Rate:  2.52020531390458e-05  Varinance:  0.0002330039650903304 \n",
      "\n",
      "Epoch:  8288  Learning Rate:  2.5176863682734e-05  Varinance:  0.00023267846709169754 \n",
      "\n",
      "Epoch:  8289  Learning Rate:  2.5151699403288028e-05  Varinance:  0.00023235342380183748 \n",
      "\n",
      "Epoch:  8290  Learning Rate:  2.512656027554355e-05  Varinance:  0.00023202883458553911 \n",
      "\n",
      "Epoch:  8291  Learning Rate:  2.5101446274361448e-05  Varinance:  0.00023170469880847832 \n",
      "\n",
      "Epoch:  8292  Learning Rate:  2.507635737462766e-05  Varinance:  0.00023138101583721748 \n",
      "\n",
      "Epoch:  8293  Learning Rate:  2.505129355125339e-05  Varinance:  0.00023105778503920276 \n",
      "\n",
      "Epoch:  8294  Learning Rate:  2.5026254779174755e-05  Varinance:  0.00023073500578276503 \n",
      "\n",
      "Epoch:  8295  Learning Rate:  2.5001241033352935e-05  Varinance:  0.0002304126774371177 \n",
      "\n",
      "Epoch:  8296  Learning Rate:  2.4976252288774283e-05  Varinance:  0.00023009079937235413 \n",
      "\n",
      "Epoch:  8297  Learning Rate:  2.4951288520450002e-05  Varinance:  0.00022976937095944884 \n",
      "\n",
      "Epoch:  8298  Learning Rate:  2.492634970341627e-05  Varinance:  0.00022944839157025462 \n",
      "\n",
      "Epoch:  8299  Learning Rate:  2.4901435812734365e-05  Varinance:  0.00022912786057750222 \n",
      "\n",
      "Epoch:  8300  Learning Rate:  2.487654682349035e-05  Varinance:  0.00022880777735479748 \n",
      "\n",
      "Epoch:  8301  Learning Rate:  2.4851682710795188e-05  Varinance:  0.00022848814127662242 \n",
      "\n",
      "Epoch:  8302  Learning Rate:  2.4826843449784848e-05  Varinance:  0.00022816895171833294 \n",
      "\n",
      "Epoch:  8303  Learning Rate:  2.480202901562003e-05  Varinance:  0.00022785020805615627 \n",
      "\n",
      "Epoch:  8304  Learning Rate:  2.4777239383486246e-05  Varinance:  0.0002275319096671924 \n",
      "\n",
      "Epoch:  8305  Learning Rate:  2.475247452859396e-05  Varinance:  0.00022721405592941087 \n",
      "\n",
      "Epoch:  8306  Learning Rate:  2.472773442617826e-05  Varinance:  0.00022689664622165074 \n",
      "\n",
      "Epoch:  8307  Learning Rate:  2.4703019051499053e-05  Varinance:  0.00022657967992361746 \n",
      "\n",
      "Epoch:  8308  Learning Rate:  2.4678328379840907e-05  Varinance:  0.00022626315641588443 \n",
      "\n",
      "Epoch:  8309  Learning Rate:  2.4653662386513243e-05  Varinance:  0.00022594707507989014 \n",
      "\n",
      "Epoch:  8310  Learning Rate:  2.462902104685002e-05  Varinance:  0.00022563143529793605 \n",
      "\n",
      "Epoch:  8311  Learning Rate:  2.4604404336209856e-05  Varinance:  0.0002253162364531878 \n",
      "\n",
      "Epoch:  8312  Learning Rate:  2.4579812229976114e-05  Varinance:  0.00022500147792967227 \n",
      "\n",
      "Epoch:  8313  Learning Rate:  2.4555244703556657e-05  Varinance:  0.00022468715911227728 \n",
      "\n",
      "Epoch:  8314  Learning Rate:  2.4530701732383905e-05  Varinance:  0.00022437327938674864 \n",
      "\n",
      "Epoch:  8315  Learning Rate:  2.450618329191497e-05  Varinance:  0.00022405983813969152 \n",
      "\n",
      "Epoch:  8316  Learning Rate:  2.4481689357631374e-05  Varinance:  0.00022374683475856794 \n",
      "\n",
      "Epoch:  8317  Learning Rate:  2.4457219905039128e-05  Varinance:  0.0002234342686316945 \n",
      "\n",
      "Epoch:  8318  Learning Rate:  2.4432774909668875e-05  Varinance:  0.00022312213914824335 \n",
      "\n",
      "Epoch:  8319  Learning Rate:  2.440835434707556e-05  Varinance:  0.00022281044569823978 \n",
      "\n",
      "Epoch:  8320  Learning Rate:  2.4383958192838584e-05  Varinance:  0.0002224991876725613 \n",
      "\n",
      "Epoch:  8321  Learning Rate:  2.435958642256188e-05  Varinance:  0.00022218836446293534 \n",
      "\n",
      "Epoch:  8322  Learning Rate:  2.4335239011873627e-05  Varinance:  0.00022187797546194024 \n",
      "\n",
      "Epoch:  8323  Learning Rate:  2.431091593642641e-05  Varinance:  0.00022156802006300286 \n",
      "\n",
      "Epoch:  8324  Learning Rate:  2.4286617171897116e-05  Varinance:  0.00022125849766039616 \n",
      "\n",
      "Epoch:  8325  Learning Rate:  2.426234269398706e-05  Varinance:  0.0002209494076492406 \n",
      "\n",
      "Epoch:  8326  Learning Rate:  2.4238092478421724e-05  Varinance:  0.00022064074942550112 \n",
      "\n",
      "Epoch:  8327  Learning Rate:  2.421386650095084e-05  Varinance:  0.00022033252238598696 \n",
      "\n",
      "Epoch:  8328  Learning Rate:  2.4189664737348518e-05  Varinance:  0.00022002472592834877 \n",
      "\n",
      "Epoch:  8329  Learning Rate:  2.4165487163412946e-05  Varinance:  0.00021971735945107982 \n",
      "\n",
      "Epoch:  8330  Learning Rate:  2.414133375496651e-05  Varinance:  0.0002194104223535137 \n",
      "\n",
      "Epoch:  8331  Learning Rate:  2.4117204487855888e-05  Varinance:  0.00021910391403582194 \n",
      "\n",
      "Epoch:  8332  Learning Rate:  2.409309933795176e-05  Varinance:  0.00021879783389901515 \n",
      "\n",
      "Epoch:  8333  Learning Rate:  2.4069018281148933e-05  Varinance:  0.0002184921813449403 \n",
      "\n",
      "Epoch:  8334  Learning Rate:  2.4044961293366437e-05  Varinance:  0.00021818695577628043 \n",
      "\n",
      "Epoch:  8335  Learning Rate:  2.4020928350547237e-05  Varinance:  0.00021788215659655172 \n",
      "\n",
      "Epoch:  8336  Learning Rate:  2.399691942865835e-05  Varinance:  0.0002175777832101049 \n",
      "\n",
      "Epoch:  8337  Learning Rate:  2.3972934503690932e-05  Varinance:  0.00021727383502212277 \n",
      "\n",
      "Epoch:  8338  Learning Rate:  2.3948973551660016e-05  Varinance:  0.00021697031143861782 \n",
      "\n",
      "Epoch:  8339  Learning Rate:  2.392503654860465e-05  Varinance:  0.0002166672118664336 \n",
      "\n",
      "Epoch:  8340  Learning Rate:  2.390112347058778e-05  Varinance:  0.00021636453571324222 \n",
      "\n",
      "Epoch:  8341  Learning Rate:  2.3877234293696415e-05  Varinance:  0.00021606228238754205 \n",
      "\n",
      "Epoch:  8342  Learning Rate:  2.3853368994041333e-05  Varinance:  0.000215760451298659 \n",
      "\n",
      "Epoch:  8343  Learning Rate:  2.3829527547757193e-05  Varinance:  0.00021545904185674375 \n",
      "\n",
      "Epoch:  8344  Learning Rate:  2.3805709931002626e-05  Varinance:  0.0002151580534727712 \n",
      "\n",
      "Epoch:  8345  Learning Rate:  2.3781916119959974e-05  Varinance:  0.00021485748555853825 \n",
      "\n",
      "Epoch:  8346  Learning Rate:  2.3758146090835384e-05  Varinance:  0.00021455733752666435 \n",
      "\n",
      "Epoch:  8347  Learning Rate:  2.3734399819858905e-05  Varinance:  0.00021425760879058961 \n",
      "\n",
      "Epoch:  8348  Learning Rate:  2.3710677283284227e-05  Varinance:  0.00021395829876457245 \n",
      "\n",
      "Epoch:  8349  Learning Rate:  2.3686978457388764e-05  Varinance:  0.00021365940686369066 \n",
      "\n",
      "Epoch:  8350  Learning Rate:  2.3663303318473776e-05  Varinance:  0.00021336093250383872 \n",
      "\n",
      "Epoch:  8351  Learning Rate:  2.3639651842864074e-05  Varinance:  0.00021306287510172747 \n",
      "\n",
      "Epoch:  8352  Learning Rate:  2.3616024006908144e-05  Varinance:  0.0002127652340748815 \n",
      "\n",
      "Epoch:  8353  Learning Rate:  2.3592419786978233e-05  Varinance:  0.00021246800884164026 \n",
      "\n",
      "Epoch:  8354  Learning Rate:  2.3568839159470077e-05  Varinance:  0.00021217119882115565 \n",
      "\n",
      "Epoch:  8355  Learning Rate:  2.3545282100803037e-05  Varinance:  0.00021187480343338988 \n",
      "\n",
      "Epoch:  8356  Learning Rate:  2.3521748587420026e-05  Varinance:  0.00021157882209911665 \n",
      "\n",
      "Epoch:  8357  Learning Rate:  2.3498238595787602e-05  Varinance:  0.00021128325423991841 \n",
      "\n",
      "Epoch:  8358  Learning Rate:  2.347475210239573e-05  Varinance:  0.00021098809927818605 \n",
      "\n",
      "Epoch:  8359  Learning Rate:  2.3451289083757874e-05  Varinance:  0.00021069335663711616 \n",
      "\n",
      "Epoch:  8360  Learning Rate:  2.3427849516411102e-05  Varinance:  0.0002103990257407122 \n",
      "\n",
      "Epoch:  8361  Learning Rate:  2.3404433376915797e-05  Varinance:  0.00021010510601378242 \n",
      "\n",
      "Epoch:  8362  Learning Rate:  2.338104064185578e-05  Varinance:  0.00020981159688193736 \n",
      "\n",
      "Epoch:  8363  Learning Rate:  2.3357671287838386e-05  Varinance:  0.00020951849777159118 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8364  Learning Rate:  2.3334325291494235e-05  Varinance:  0.00020922580810995875 \n",
      "\n",
      "Epoch:  8365  Learning Rate:  2.3311002629477274e-05  Varinance:  0.00020893352732505565 \n",
      "\n",
      "Epoch:  8366  Learning Rate:  2.328770327846493e-05  Varinance:  0.00020864165484569535 \n",
      "\n",
      "Epoch:  8367  Learning Rate:  2.3264427215157804e-05  Varinance:  0.00020835019010149032 \n",
      "\n",
      "Epoch:  8368  Learning Rate:  2.324117441627979e-05  Varinance:  0.00020805913252284992 \n",
      "\n",
      "Epoch:  8369  Learning Rate:  2.321794485857817e-05  Varinance:  0.00020776848154097798 \n",
      "\n",
      "Epoch:  8370  Learning Rate:  2.3194738518823344e-05  Varinance:  0.00020747823658787406 \n",
      "\n",
      "Epoch:  8371  Learning Rate:  2.317155537380897e-05  Varinance:  0.00020718839709633094 \n",
      "\n",
      "Epoch:  8372  Learning Rate:  2.3148395400351857e-05  Varinance:  0.000206898962499934 \n",
      "\n",
      "Epoch:  8373  Learning Rate:  2.3125258575292115e-05  Varinance:  0.00020660993223305888 \n",
      "\n",
      "Epoch:  8374  Learning Rate:  2.310214487549288e-05  Varinance:  0.00020632130573087235 \n",
      "\n",
      "Epoch:  8375  Learning Rate:  2.30790542778404e-05  Varinance:  0.00020603308242933029 \n",
      "\n",
      "Epoch:  8376  Learning Rate:  2.3055986759244165e-05  Varinance:  0.00020574526176517555 \n",
      "\n",
      "Epoch:  8377  Learning Rate:  2.303294229663661e-05  Varinance:  0.00020545784317593875 \n",
      "\n",
      "Epoch:  8378  Learning Rate:  2.3009920866973227e-05  Varinance:  0.000205170826099936 \n",
      "\n",
      "Epoch:  8379  Learning Rate:  2.2986922447232673e-05  Varinance:  0.00020488420997626837 \n",
      "\n",
      "Epoch:  8380  Learning Rate:  2.296394701441648e-05  Varinance:  0.00020459799424481942 \n",
      "\n",
      "Epoch:  8381  Learning Rate:  2.2940994545549174e-05  Varinance:  0.00020431217834625628 \n",
      "\n",
      "Epoch:  8382  Learning Rate:  2.2918065017678368e-05  Varinance:  0.00020402676172202741 \n",
      "\n",
      "Epoch:  8383  Learning Rate:  2.2895158407874488e-05  Varinance:  0.0002037417438143605 \n",
      "\n",
      "Epoch:  8384  Learning Rate:  2.2872274693230924e-05  Varinance:  0.0002034571240662634 \n",
      "\n",
      "Epoch:  8385  Learning Rate:  2.284941385086392e-05  Varinance:  0.0002031729019215219 \n",
      "\n",
      "Epoch:  8386  Learning Rate:  2.2826575857912707e-05  Varinance:  0.000202889076824699 \n",
      "\n",
      "Epoch:  8387  Learning Rate:  2.280376069153926e-05  Varinance:  0.00020260564822113258 \n",
      "\n",
      "Epoch:  8388  Learning Rate:  2.2780968328928362e-05  Varinance:  0.00020232261555693646 \n",
      "\n",
      "Epoch:  8389  Learning Rate:  2.2758198747287733e-05  Varinance:  0.00020203997827899822 \n",
      "\n",
      "Epoch:  8390  Learning Rate:  2.2735451923847745e-05  Varinance:  0.00020175773583497702 \n",
      "\n",
      "Epoch:  8391  Learning Rate:  2.2712727835861536e-05  Varinance:  0.00020147588767330474 \n",
      "\n",
      "Epoch:  8392  Learning Rate:  2.2690026460605098e-05  Varinance:  0.00020119443324318335 \n",
      "\n",
      "Epoch:  8393  Learning Rate:  2.266734777537701e-05  Varinance:  0.0002009133719945847 \n",
      "\n",
      "Epoch:  8394  Learning Rate:  2.2644691757498545e-05  Varinance:  0.00020063270337824788 \n",
      "\n",
      "Epoch:  8395  Learning Rate:  2.2622058384313766e-05  Varinance:  0.00020035242684568035 \n",
      "\n",
      "Epoch:  8396  Learning Rate:  2.2599447633189255e-05  Varinance:  0.0002000725418491558 \n",
      "\n",
      "Epoch:  8397  Learning Rate:  2.257685948151422e-05  Varinance:  0.00019979304784171212 \n",
      "\n",
      "Epoch:  8398  Learning Rate:  2.2554293906700594e-05  Varinance:  0.00019951394427715214 \n",
      "\n",
      "Epoch:  8399  Learning Rate:  2.2531750886182746e-05  Varinance:  0.0001992352306100415 \n",
      "\n",
      "Epoch:  8400  Learning Rate:  2.2509230397417667e-05  Varinance:  0.00019895690629570813 \n",
      "\n",
      "Epoch:  8401  Learning Rate:  2.248673241788482e-05  Varinance:  0.00019867897079023974 \n",
      "\n",
      "Epoch:  8402  Learning Rate:  2.2464256925086306e-05  Varinance:  0.00019840142355048498 \n",
      "\n",
      "Epoch:  8403  Learning Rate:  2.2441803896546588e-05  Varinance:  0.00019812426403405118 \n",
      "\n",
      "Epoch:  8404  Learning Rate:  2.2419373309812594e-05  Varinance:  0.00019784749169930239 \n",
      "\n",
      "Epoch:  8405  Learning Rate:  2.239696514245382e-05  Varinance:  0.0001975711060053604 \n",
      "\n",
      "Epoch:  8406  Learning Rate:  2.2374579372062058e-05  Varinance:  0.0001972951064121024 \n",
      "\n",
      "Epoch:  8407  Learning Rate:  2.235221597625149e-05  Varinance:  0.00019701949238015931 \n",
      "\n",
      "Epoch:  8408  Learning Rate:  2.23298749326588e-05  Varinance:  0.00019674426337091638 \n",
      "\n",
      "Epoch:  8409  Learning Rate:  2.2307556218942905e-05  Varinance:  0.00019646941884651106 \n",
      "\n",
      "Epoch:  8410  Learning Rate:  2.2285259812785046e-05  Varinance:  0.00019619495826983242 \n",
      "\n",
      "Epoch:  8411  Learning Rate:  2.2262985691888898e-05  Varinance:  0.0001959208811045189 \n",
      "\n",
      "Epoch:  8412  Learning Rate:  2.2240733833980297e-05  Varinance:  0.00019564718681495922 \n",
      "\n",
      "Epoch:  8413  Learning Rate:  2.2218504216807344e-05  Varinance:  0.00019537387486629032 \n",
      "\n",
      "Epoch:  8414  Learning Rate:  2.2196296818140503e-05  Varinance:  0.0001951009447243953 \n",
      "\n",
      "Epoch:  8415  Learning Rate:  2.2174111615772324e-05  Varinance:  0.00019482839585590448 \n",
      "\n",
      "Epoch:  8416  Learning Rate:  2.215194858751761e-05  Varinance:  0.0001945562277281928 \n",
      "\n",
      "Epoch:  8417  Learning Rate:  2.212980771121329e-05  Varinance:  0.00019428443980937975 \n",
      "\n",
      "Epoch:  8418  Learning Rate:  2.2107688964718565e-05  Varinance:  0.00019401303156832677 \n",
      "\n",
      "Epoch:  8419  Learning Rate:  2.2085592325914648e-05  Varinance:  0.00019374200247463825 \n",
      "\n",
      "Epoch:  8420  Learning Rate:  2.2063517772704857e-05  Varinance:  0.0001934713519986596 \n",
      "\n",
      "Epoch:  8421  Learning Rate:  2.2041465283014714e-05  Varinance:  0.00019320107961147501 \n",
      "\n",
      "Epoch:  8422  Learning Rate:  2.2019434834791696e-05  Varinance:  0.00019293118478490867 \n",
      "\n",
      "Epoch:  8423  Learning Rate:  2.1997426406005306e-05  Varinance:  0.00019266166699152227 \n",
      "\n",
      "Epoch:  8424  Learning Rate:  2.1975439974647195e-05  Varinance:  0.00019239252570461455 \n",
      "\n",
      "Epoch:  8425  Learning Rate:  2.195347551873089e-05  Varinance:  0.00019212376039821915 \n",
      "\n",
      "Epoch:  8426  Learning Rate:  2.193153301629189e-05  Varinance:  0.00019185537054710534 \n",
      "\n",
      "Epoch:  8427  Learning Rate:  2.190961244538778e-05  Varinance:  0.00019158735562677624 \n",
      "\n",
      "Epoch:  8428  Learning Rate:  2.1887713784097938e-05  Varinance:  0.00019131971511346661 \n",
      "\n",
      "Epoch:  8429  Learning Rate:  2.1865837010523664e-05  Varinance:  0.0001910524484841439 \n",
      "\n",
      "Epoch:  8430  Learning Rate:  2.184398210278826e-05  Varinance:  0.0001907855552165059 \n",
      "\n",
      "Epoch:  8431  Learning Rate:  2.1822149039036784e-05  Varinance:  0.00019051903478898031 \n",
      "\n",
      "Epoch:  8432  Learning Rate:  2.1800337797436163e-05  Varinance:  0.0001902528866807225 \n",
      "\n",
      "Epoch:  8433  Learning Rate:  2.1778548356175118e-05  Varinance:  0.0001899871103716165 \n",
      "\n",
      "Epoch:  8434  Learning Rate:  2.175678069346428e-05  Varinance:  0.00018972170534227277 \n",
      "\n",
      "Epoch:  8435  Learning Rate:  2.173503478753595e-05  Varinance:  0.0001894566710740265 \n",
      "\n",
      "Epoch:  8436  Learning Rate:  2.1713310616644182e-05  Varinance:  0.00018919200704893828 \n",
      "\n",
      "Epoch:  8437  Learning Rate:  2.169160815906488e-05  Varinance:  0.00018892771274979205 \n",
      "\n",
      "Epoch:  8438  Learning Rate:  2.1669927393095544e-05  Varinance:  0.00018866378766009456 \n",
      "\n",
      "Epoch:  8439  Learning Rate:  2.1648268297055366e-05  Varinance:  0.00018840023126407302 \n",
      "\n",
      "Epoch:  8440  Learning Rate:  2.1626630849285326e-05  Varinance:  0.00018813704304667625 \n",
      "\n",
      "Epoch:  8441  Learning Rate:  2.160501502814794e-05  Varinance:  0.00018787422249357257 \n",
      "\n",
      "Epoch:  8442  Learning Rate:  2.1583420812027344e-05  Varinance:  0.00018761176909114772 \n",
      "\n",
      "Epoch:  8443  Learning Rate:  2.1561848179329396e-05  Varinance:  0.00018734968232650602 \n",
      "\n",
      "Epoch:  8444  Learning Rate:  2.1540297108481427e-05  Varinance:  0.00018708796168746787 \n",
      "\n",
      "Epoch:  8445  Learning Rate:  2.151876757793232e-05  Varinance:  0.00018682660666256964 \n",
      "\n",
      "Epoch:  8446  Learning Rate:  2.1497259566152624e-05  Varinance:  0.00018656561674106108 \n",
      "\n",
      "Epoch:  8447  Learning Rate:  2.1475773051634283e-05  Varinance:  0.00018630499141290645 \n",
      "\n",
      "Epoch:  8448  Learning Rate:  2.145430801289079e-05  Varinance:  0.00018604473016878254 \n",
      "\n",
      "Epoch:  8449  Learning Rate:  2.1432864428457055e-05  Varinance:  0.00018578483250007667 \n",
      "\n",
      "Epoch:  8450  Learning Rate:  2.141144227688957e-05  Varinance:  0.00018552529789888757 \n",
      "\n",
      "Epoch:  8451  Learning Rate:  2.139004153676615e-05  Varinance:  0.00018526612585802331 \n",
      "\n",
      "Epoch:  8452  Learning Rate:  2.1368662186686008e-05  Varinance:  0.0001850073158710007 \n",
      "\n",
      "Epoch:  8453  Learning Rate:  2.1347304205269875e-05  Varinance:  0.0001847488674320431 \n",
      "\n",
      "Epoch:  8454  Learning Rate:  2.1325967571159726e-05  Varinance:  0.00018449078003608148 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8455  Learning Rate:  2.1304652263018887e-05  Varinance:  0.00018423305317875237 \n",
      "\n",
      "Epoch:  8456  Learning Rate:  2.1283358259532124e-05  Varinance:  0.00018397568635639578 \n",
      "\n",
      "Epoch:  8457  Learning Rate:  2.1262085539405394e-05  Varinance:  0.00018371867906605646 \n",
      "\n",
      "Epoch:  8458  Learning Rate:  2.1240834081365935e-05  Varinance:  0.00018346203080548128 \n",
      "\n",
      "Epoch:  8459  Learning Rate:  2.121960386416237e-05  Varinance:  0.0001832057410731192 \n",
      "\n",
      "Epoch:  8460  Learning Rate:  2.1198394866564433e-05  Varinance:  0.0001829498093681188 \n",
      "\n",
      "Epoch:  8461  Learning Rate:  2.1177207067363094e-05  Varinance:  0.00018269423519032927 \n",
      "\n",
      "Epoch:  8462  Learning Rate:  2.1156040445370623e-05  Varinance:  0.00018243901804029854 \n",
      "\n",
      "Epoch:  8463  Learning Rate:  2.113489497942036e-05  Varinance:  0.00018218415741927128 \n",
      "\n",
      "Epoch:  8464  Learning Rate:  2.111377064836684e-05  Varinance:  0.00018192965282918985 \n",
      "\n",
      "Epoch:  8465  Learning Rate:  2.109266743108569e-05  Varinance:  0.00018167550377269208 \n",
      "\n",
      "Epoch:  8466  Learning Rate:  2.1071585306473765e-05  Varinance:  0.0001814217097531109 \n",
      "\n",
      "Epoch:  8467  Learning Rate:  2.1050524253448904e-05  Varinance:  0.00018116827027447213 \n",
      "\n",
      "Epoch:  8468  Learning Rate:  2.1029484250950013e-05  Varinance:  0.00018091518484149535 \n",
      "\n",
      "Epoch:  8469  Learning Rate:  2.1008465277937163e-05  Varinance:  0.0001806624529595921 \n",
      "\n",
      "Epoch:  8470  Learning Rate:  2.0987467313391337e-05  Varinance:  0.00018041007413486382 \n",
      "\n",
      "Epoch:  8471  Learning Rate:  2.096649033631454e-05  Varinance:  0.00018015804787410293 \n",
      "\n",
      "Epoch:  8472  Learning Rate:  2.0945534325729865e-05  Varinance:  0.0001799063736847908 \n",
      "\n",
      "Epoch:  8473  Learning Rate:  2.0924599260681257e-05  Varinance:  0.00017965505107509587 \n",
      "\n",
      "Epoch:  8474  Learning Rate:  2.090368512023362e-05  Varinance:  0.00017940407955387462 \n",
      "\n",
      "Epoch:  8475  Learning Rate:  2.0882791883472882e-05  Varinance:  0.00017915345863066934 \n",
      "\n",
      "Epoch:  8476  Learning Rate:  2.0861919529505767e-05  Varinance:  0.00017890318781570776 \n",
      "\n",
      "Epoch:  8477  Learning Rate:  2.0841068037459885e-05  Varinance:  0.00017865326661990085 \n",
      "\n",
      "Epoch:  8478  Learning Rate:  2.082023738648381e-05  Varinance:  0.00017840369455484383 \n",
      "\n",
      "Epoch:  8479  Learning Rate:  2.0799427555746858e-05  Varinance:  0.00017815447113281408 \n",
      "\n",
      "Epoch:  8480  Learning Rate:  2.0778638524439198e-05  Varinance:  0.0001779055958667695 \n",
      "\n",
      "Epoch:  8481  Learning Rate:  2.0757870271771752e-05  Varinance:  0.0001776570682703492 \n",
      "\n",
      "Epoch:  8482  Learning Rate:  2.0737122776976348e-05  Varinance:  0.00017740888785787155 \n",
      "\n",
      "Epoch:  8483  Learning Rate:  2.0716396019305452e-05  Varinance:  0.00017716105414433355 \n",
      "\n",
      "Epoch:  8484  Learning Rate:  2.069568997803226e-05  Varinance:  0.00017691356664540885 \n",
      "\n",
      "Epoch:  8485  Learning Rate:  2.067500463245081e-05  Varinance:  0.0001766664248774487 \n",
      "\n",
      "Epoch:  8486  Learning Rate:  2.0654339961875714e-05  Varinance:  0.00017641962835747985 \n",
      "\n",
      "Epoch:  8487  Learning Rate:  2.0633695945642265e-05  Varinance:  0.00017617317660320296 \n",
      "\n",
      "Epoch:  8488  Learning Rate:  2.0613072563106515e-05  Varinance:  0.0001759270691329933 \n",
      "\n",
      "Epoch:  8489  Learning Rate:  2.059246979364505e-05  Varinance:  0.00017568130546589858 \n",
      "\n",
      "Epoch:  8490  Learning Rate:  2.0571887616655052e-05  Varinance:  0.00017543588512163881 \n",
      "\n",
      "Epoch:  8491  Learning Rate:  2.055132601155443e-05  Varinance:  0.00017519080762060397 \n",
      "\n",
      "Epoch:  8492  Learning Rate:  2.0530784957781526e-05  Varinance:  0.00017494607248385493 \n",
      "\n",
      "Epoch:  8493  Learning Rate:  2.0510264434795257e-05  Varinance:  0.00017470167923312163 \n",
      "\n",
      "Epoch:  8494  Learning Rate:  2.0489764422075165e-05  Varinance:  0.00017445762739080124 \n",
      "\n",
      "Epoch:  8495  Learning Rate:  2.0469284899121205e-05  Varinance:  0.000174213916479959 \n",
      "\n",
      "Epoch:  8496  Learning Rate:  2.044882584545385e-05  Varinance:  0.0001739705460243262 \n",
      "\n",
      "Epoch:  8497  Learning Rate:  2.042838724061401e-05  Varinance:  0.0001737275155482996 \n",
      "\n",
      "Epoch:  8498  Learning Rate:  2.0407969064163146e-05  Varinance:  0.00017348482457693961 \n",
      "\n",
      "Epoch:  8499  Learning Rate:  2.038757129568305e-05  Varinance:  0.00017324247263597096 \n",
      "\n",
      "Epoch:  8500  Learning Rate:  2.036719391477591e-05  Varinance:  0.00017300045925178087 \n",
      "\n",
      "Epoch:  8501  Learning Rate:  2.0346836901064418e-05  Varinance:  0.0001727587839514174 \n",
      "\n",
      "Epoch:  8502  Learning Rate:  2.0326500234191524e-05  Varinance:  0.00017251744626259005 \n",
      "\n",
      "Epoch:  8503  Learning Rate:  2.0306183893820522e-05  Varinance:  0.00017227644571366795 \n",
      "\n",
      "Epoch:  8504  Learning Rate:  2.028588785963514e-05  Varinance:  0.0001720357818336793 \n",
      "\n",
      "Epoch:  8505  Learning Rate:  2.026561211133931e-05  Varinance:  0.0001717954541523093 \n",
      "\n",
      "Epoch:  8506  Learning Rate:  2.0245356628657246e-05  Varinance:  0.00017155546219990115 \n",
      "\n",
      "Epoch:  8507  Learning Rate:  2.022512139133353e-05  Varinance:  0.00017131580550745407 \n",
      "\n",
      "Epoch:  8508  Learning Rate:  2.0204906379132893e-05  Varinance:  0.0001710764836066216 \n",
      "\n",
      "Epoch:  8509  Learning Rate:  2.0184711571840318e-05  Varinance:  0.0001708374960297124 \n",
      "\n",
      "Epoch:  8510  Learning Rate:  2.0164536949260962e-05  Varinance:  0.00017059884230968825 \n",
      "\n",
      "Epoch:  8511  Learning Rate:  2.014438249122027e-05  Varinance:  0.00017036052198016354 \n",
      "\n",
      "Epoch:  8512  Learning Rate:  2.0124248177563748e-05  Varinance:  0.00017012253457540346 \n",
      "\n",
      "Epoch:  8513  Learning Rate:  2.0104133988157045e-05  Varinance:  0.00016988487963032453 \n",
      "\n",
      "Epoch:  8514  Learning Rate:  2.0084039902886042e-05  Varinance:  0.0001696475566804931 \n",
      "\n",
      "Epoch:  8515  Learning Rate:  2.0063965901656615e-05  Varinance:  0.0001694105652621234 \n",
      "\n",
      "Epoch:  8516  Learning Rate:  2.004391196439473e-05  Varinance:  0.00016917390491207838 \n",
      "\n",
      "Epoch:  8517  Learning Rate:  2.0023878071046508e-05  Varinance:  0.0001689375751678678 \n",
      "\n",
      "Epoch:  8518  Learning Rate:  2.000386420157803e-05  Varinance:  0.00016870157556764763 \n",
      "\n",
      "Epoch:  8519  Learning Rate:  1.9983870335975383e-05  Varinance:  0.00016846590565021828 \n",
      "\n",
      "Epoch:  8520  Learning Rate:  1.9963896454244775e-05  Varinance:  0.0001682305649550253 \n",
      "\n",
      "Epoch:  8521  Learning Rate:  1.9943942536412283e-05  Varinance:  0.0001679955530221575 \n",
      "\n",
      "Epoch:  8522  Learning Rate:  1.9924008562523958e-05  Varinance:  0.00016776086939234542 \n",
      "\n",
      "Epoch:  8523  Learning Rate:  1.9904094512645886e-05  Varinance:  0.00016752651360696204 \n",
      "\n",
      "Epoch:  8524  Learning Rate:  1.9884200366863988e-05  Varinance:  0.00016729248520802072 \n",
      "\n",
      "Epoch:  8525  Learning Rate:  1.9864326105284115e-05  Varinance:  0.00016705878373817485 \n",
      "\n",
      "Epoch:  8526  Learning Rate:  1.9844471708031964e-05  Varinance:  0.00016682540874071586 \n",
      "\n",
      "Epoch:  8527  Learning Rate:  1.9824637155253212e-05  Varinance:  0.00016659235975957418 \n",
      "\n",
      "Epoch:  8528  Learning Rate:  1.9804822427113268e-05  Varinance:  0.00016635963633931716 \n",
      "\n",
      "Epoch:  8529  Learning Rate:  1.9785027503797364e-05  Varinance:  0.00016612723802514762 \n",
      "\n",
      "Epoch:  8530  Learning Rate:  1.976525236551065e-05  Varinance:  0.00016589516436290463 \n",
      "\n",
      "Epoch:  8531  Learning Rate:  1.9745496992477945e-05  Varinance:  0.00016566341489906126 \n",
      "\n",
      "Epoch:  8532  Learning Rate:  1.9725761364943847e-05  Varinance:  0.00016543198918072456 \n",
      "\n",
      "Epoch:  8533  Learning Rate:  1.9706045463172795e-05  Varinance:  0.00016520088675563334 \n",
      "\n",
      "Epoch:  8534  Learning Rate:  1.968634926744884e-05  Varinance:  0.00016497010717215905 \n",
      "\n",
      "Epoch:  8535  Learning Rate:  1.9666672758075764e-05  Varinance:  0.00016473964997930414 \n",
      "\n",
      "Epoch:  8536  Learning Rate:  1.964701591537712e-05  Varinance:  0.00016450951472670009 \n",
      "\n",
      "Epoch:  8537  Learning Rate:  1.9627378719696025e-05  Varinance:  0.0001642797009646085 \n",
      "\n",
      "Epoch:  8538  Learning Rate:  1.9607761151395253e-05  Varinance:  0.00016405020824391913 \n",
      "\n",
      "Epoch:  8539  Learning Rate:  1.95881631908573e-05  Varinance:  0.0001638210361161484 \n",
      "\n",
      "Epoch:  8540  Learning Rate:  1.9568584818484173e-05  Varinance:  0.00016359218413344002 \n",
      "\n",
      "Epoch:  8541  Learning Rate:  1.9549026014697492e-05  Varinance:  0.00016336365184856308 \n",
      "\n",
      "Epoch:  8542  Learning Rate:  1.9529486759938423e-05  Varinance:  0.00016313543881491172 \n",
      "\n",
      "Epoch:  8543  Learning Rate:  1.9509967034667775e-05  Varinance:  0.0001629075445865031 \n",
      "\n",
      "Epoch:  8544  Learning Rate:  1.9490466819365788e-05  Varinance:  0.00016267996871797825 \n",
      "\n",
      "Epoch:  8545  Learning Rate:  1.9470986094532206e-05  Varinance:  0.00016245271076460038 \n",
      "\n",
      "Epoch:  8546  Learning Rate:  1.945152484068638e-05  Varinance:  0.0001622257702822531 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8547  Learning Rate:  1.9432083038367015e-05  Varinance:  0.0001619991468274413 \n",
      "\n",
      "Epoch:  8548  Learning Rate:  1.9412660668132272e-05  Varinance:  0.0001617728399572892 \n",
      "\n",
      "Epoch:  8549  Learning Rate:  1.939325771055985e-05  Varinance:  0.0001615468492295398 \n",
      "\n",
      "Epoch:  8550  Learning Rate:  1.9373874146246754e-05  Varinance:  0.0001613211742025533 \n",
      "\n",
      "Epoch:  8551  Learning Rate:  1.9354509955809383e-05  Varinance:  0.00016109581443530754 \n",
      "\n",
      "Epoch:  8552  Learning Rate:  1.9335165119883617e-05  Varinance:  0.0001608707694873965 \n",
      "\n",
      "Epoch:  8553  Learning Rate:  1.9315839619124583e-05  Varinance:  0.0001606460389190285 \n",
      "\n",
      "Epoch:  8554  Learning Rate:  1.9296533434206745e-05  Varinance:  0.0001604216222910272 \n",
      "\n",
      "Epoch:  8555  Learning Rate:  1.927724654582398e-05  Varinance:  0.00016019751916482937 \n",
      "\n",
      "Epoch:  8556  Learning Rate:  1.925797893468937e-05  Varinance:  0.0001599737291024848 \n",
      "\n",
      "Epoch:  8557  Learning Rate:  1.9238730581535296e-05  Varinance:  0.00015975025166665416 \n",
      "\n",
      "Epoch:  8558  Learning Rate:  1.9219501467113375e-05  Varinance:  0.00015952708642060997 \n",
      "\n",
      "Epoch:  8559  Learning Rate:  1.9200291572194553e-05  Varinance:  0.0001593042329282348 \n",
      "\n",
      "Epoch:  8560  Learning Rate:  1.9181100877568907e-05  Varinance:  0.00015908169075401967 \n",
      "\n",
      "Epoch:  8561  Learning Rate:  1.9161929364045704e-05  Varinance:  0.0001588594594630648 \n",
      "\n",
      "Epoch:  8562  Learning Rate:  1.9142777012453495e-05  Varinance:  0.00015863753862107766 \n",
      "\n",
      "Epoch:  8563  Learning Rate:  1.9123643803639892e-05  Varinance:  0.00015841592779437268 \n",
      "\n",
      "Epoch:  8564  Learning Rate:  1.9104529718471654e-05  Varinance:  0.00015819462654986934 \n",
      "\n",
      "Epoch:  8565  Learning Rate:  1.908543473783476e-05  Varinance:  0.00015797363445509296 \n",
      "\n",
      "Epoch:  8566  Learning Rate:  1.9066358842634193e-05  Varinance:  0.00015775295107817294 \n",
      "\n",
      "Epoch:  8567  Learning Rate:  1.9047302013794025e-05  Varinance:  0.00015753257598784116 \n",
      "\n",
      "Epoch:  8568  Learning Rate:  1.9028264232257488e-05  Varinance:  0.0001573125087534329 \n",
      "\n",
      "Epoch:  8569  Learning Rate:  1.9009245478986775e-05  Varinance:  0.00015709274894488474 \n",
      "\n",
      "Epoch:  8570  Learning Rate:  1.899024573496309e-05  Varinance:  0.00015687329613273422 \n",
      "\n",
      "Epoch:  8571  Learning Rate:  1.8971264981186755e-05  Varinance:  0.0001566541498881181 \n",
      "\n",
      "Epoch:  8572  Learning Rate:  1.895230319867698e-05  Varinance:  0.00015643530978277307 \n",
      "\n",
      "Epoch:  8573  Learning Rate:  1.8933360368471988e-05  Varinance:  0.00015621677538903405 \n",
      "\n",
      "Epoch:  8574  Learning Rate:  1.8914436471628902e-05  Varinance:  0.0001559985462798326 \n",
      "\n",
      "Epoch:  8575  Learning Rate:  1.8895531489223903e-05  Varinance:  0.00015578062202869762 \n",
      "\n",
      "Epoch:  8576  Learning Rate:  1.8876645402351964e-05  Varinance:  0.00015556300220975365 \n",
      "\n",
      "Epoch:  8577  Learning Rate:  1.8857778192126972e-05  Varinance:  0.00015534568639772033 \n",
      "\n",
      "Epoch:  8578  Learning Rate:  1.883892983968177e-05  Varinance:  0.00015512867416791055 \n",
      "\n",
      "Epoch:  8579  Learning Rate:  1.8820100326167986e-05  Varinance:  0.0001549119650962314 \n",
      "\n",
      "Epoch:  8580  Learning Rate:  1.8801289632756057e-05  Varinance:  0.00015469555875918234 \n",
      "\n",
      "Epoch:  8581  Learning Rate:  1.8782497740635364e-05  Varinance:  0.00015447945473385364 \n",
      "\n",
      "Epoch:  8582  Learning Rate:  1.8763724631013976e-05  Varinance:  0.00015426365259792716 \n",
      "\n",
      "Epoch:  8583  Learning Rate:  1.874497028511875e-05  Varinance:  0.00015404815192967452 \n",
      "\n",
      "Epoch:  8584  Learning Rate:  1.87262346841954e-05  Varinance:  0.00015383295230795668 \n",
      "\n",
      "Epoch:  8585  Learning Rate:  1.87075178095083e-05  Varinance:  0.0001536180533122221 \n",
      "\n",
      "Epoch:  8586  Learning Rate:  1.8688819642340532e-05  Varinance:  0.00015340345452250758 \n",
      "\n",
      "Epoch:  8587  Learning Rate:  1.8670140163994e-05  Varinance:  0.0001531891555194366 \n",
      "\n",
      "Epoch:  8588  Learning Rate:  1.8651479355789185e-05  Varinance:  0.00015297515588421766 \n",
      "\n",
      "Epoch:  8589  Learning Rate:  1.8632837199065282e-05  Varinance:  0.00015276145519864512 \n",
      "\n",
      "Epoch:  8590  Learning Rate:  1.8614213675180097e-05  Varinance:  0.00015254805304509728 \n",
      "\n",
      "Epoch:  8591  Learning Rate:  1.859560876551017e-05  Varinance:  0.0001523349490065361 \n",
      "\n",
      "Epoch:  8592  Learning Rate:  1.8577022451450562e-05  Varinance:  0.00015212214266650532 \n",
      "\n",
      "Epoch:  8593  Learning Rate:  1.8558454714414916e-05  Varinance:  0.00015190963360913132 \n",
      "\n",
      "Epoch:  8594  Learning Rate:  1.8539905535835565e-05  Varinance:  0.00015169742141912137 \n",
      "\n",
      "Epoch:  8595  Learning Rate:  1.8521374897163295e-05  Varinance:  0.0001514855056817621 \n",
      "\n",
      "Epoch:  8596  Learning Rate:  1.8502862779867434e-05  Varinance:  0.00015127388598292028 \n",
      "\n",
      "Epoch:  8597  Learning Rate:  1.8484369165435927e-05  Varinance:  0.000151062561909041 \n",
      "\n",
      "Epoch:  8598  Learning Rate:  1.8465894035375127e-05  Varinance:  0.00015085153304714724 \n",
      "\n",
      "Epoch:  8599  Learning Rate:  1.844743737120987e-05  Varinance:  0.00015064079898483818 \n",
      "\n",
      "Epoch:  8600  Learning Rate:  1.842899915448355e-05  Varinance:  0.00015043035931028984 \n",
      "\n",
      "Epoch:  8601  Learning Rate:  1.841057936675792e-05  Varinance:  0.0001502202136122536 \n",
      "\n",
      "Epoch:  8602  Learning Rate:  1.839217798961316e-05  Varinance:  0.00015001036148005448 \n",
      "\n",
      "Epoch:  8603  Learning Rate:  1.8373795004647954e-05  Varinance:  0.0001498008025035921 \n",
      "\n",
      "Epoch:  8604  Learning Rate:  1.835543039347929e-05  Varinance:  0.00014959153627333883 \n",
      "\n",
      "Epoch:  8605  Learning Rate:  1.833708413774254e-05  Varinance:  0.00014938256238033847 \n",
      "\n",
      "Epoch:  8606  Learning Rate:  1.8318756219091426e-05  Varinance:  0.00014917388041620682 \n",
      "\n",
      "Epoch:  8607  Learning Rate:  1.8300446619198094e-05  Varinance:  0.00014896548997313 \n",
      "\n",
      "Epoch:  8608  Learning Rate:  1.8282155319752904e-05  Varinance:  0.000148757390643864 \n",
      "\n",
      "Epoch:  8609  Learning Rate:  1.8263882302464522e-05  Varinance:  0.00014854958202173296 \n",
      "\n",
      "Epoch:  8610  Learning Rate:  1.8245627549059998e-05  Varinance:  0.00014834206370063 \n",
      "\n",
      "Epoch:  8611  Learning Rate:  1.8227391041284545e-05  Varinance:  0.0001481348352750154 \n",
      "\n",
      "Epoch:  8612  Learning Rate:  1.820917276090162e-05  Varinance:  0.00014792789633991536 \n",
      "\n",
      "Epoch:  8613  Learning Rate:  1.8190972689693005e-05  Varinance:  0.00014772124649092242 \n",
      "\n",
      "Epoch:  8614  Learning Rate:  1.8172790809458595e-05  Varinance:  0.00014751488532419393 \n",
      "\n",
      "Epoch:  8615  Learning Rate:  1.815462710201648e-05  Varinance:  0.00014730881243645158 \n",
      "\n",
      "Epoch:  8616  Learning Rate:  1.8136481549203006e-05  Varinance:  0.00014710302742497968 \n",
      "\n",
      "Epoch:  8617  Learning Rate:  1.8118354132872598e-05  Varinance:  0.0001468975298876259 \n",
      "\n",
      "Epoch:  8618  Learning Rate:  1.81002448348978e-05  Varinance:  0.00014669231942279973 \n",
      "\n",
      "Epoch:  8619  Learning Rate:  1.8082153637169377e-05  Varinance:  0.0001464873956294708 \n",
      "\n",
      "Epoch:  8620  Learning Rate:  1.8064080521596096e-05  Varinance:  0.0001462827581071698 \n",
      "\n",
      "Epoch:  8621  Learning Rate:  1.8046025470104846e-05  Varinance:  0.0001460784064559866 \n",
      "\n",
      "Epoch:  8622  Learning Rate:  1.8027988464640534e-05  Varinance:  0.00014587434027657004 \n",
      "\n",
      "Epoch:  8623  Learning Rate:  1.800996948716622e-05  Varinance:  0.00014567055917012593 \n",
      "\n",
      "Epoch:  8624  Learning Rate:  1.7991968519662898e-05  Varinance:  0.00014546706273841807 \n",
      "\n",
      "Epoch:  8625  Learning Rate:  1.797398554412956e-05  Varinance:  0.00014526385058376656 \n",
      "\n",
      "Epoch:  8626  Learning Rate:  1.79560205425833e-05  Varinance:  0.00014506092230904622 \n",
      "\n",
      "Epoch:  8627  Learning Rate:  1.7938073497059072e-05  Varinance:  0.00014485827751768745 \n",
      "\n",
      "Epoch:  8628  Learning Rate:  1.7920144389609808e-05  Varinance:  0.00014465591581367445 \n",
      "\n",
      "Epoch:  8629  Learning Rate:  1.790223320230646e-05  Varinance:  0.00014445383680154476 \n",
      "\n",
      "Epoch:  8630  Learning Rate:  1.7884339917237807e-05  Varinance:  0.0001442520400863877 \n",
      "\n",
      "Epoch:  8631  Learning Rate:  1.7866464516510524e-05  Varinance:  0.000144050525273845 \n",
      "\n",
      "Epoch:  8632  Learning Rate:  1.7848606982249282e-05  Varinance:  0.00014384929197010926 \n",
      "\n",
      "Epoch:  8633  Learning Rate:  1.783076729659651e-05  Varinance:  0.00014364833978192256 \n",
      "\n",
      "Epoch:  8634  Learning Rate:  1.781294544171252e-05  Varinance:  0.000143447668316577 \n",
      "\n",
      "Epoch:  8635  Learning Rate:  1.7795141399775425e-05  Varinance:  0.000143247277181913 \n",
      "\n",
      "Epoch:  8636  Learning Rate:  1.7777355152981242e-05  Varinance:  0.00014304716598631924 \n",
      "\n",
      "Epoch:  8637  Learning Rate:  1.7759586683543696e-05  Varinance:  0.00014284733433873048 \n",
      "\n",
      "Epoch:  8638  Learning Rate:  1.7741835973694282e-05  Varinance:  0.00014264778184862876 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8639  Learning Rate:  1.772410300568235e-05  Varinance:  0.00014244850812604148 \n",
      "\n",
      "Epoch:  8640  Learning Rate:  1.7706387761774902e-05  Varinance:  0.00014224951278154018 \n",
      "\n",
      "Epoch:  8641  Learning Rate:  1.768869022425666e-05  Varinance:  0.0001420507954262411 \n",
      "\n",
      "Epoch:  8642  Learning Rate:  1.7671010375430147e-05  Varinance:  0.00014185235567180352 \n",
      "\n",
      "Epoch:  8643  Learning Rate:  1.765334819761548e-05  Varinance:  0.0001416541931304295 \n",
      "\n",
      "Epoch:  8644  Learning Rate:  1.7635703673150456e-05  Varinance:  0.000141456307414862 \n",
      "\n",
      "Epoch:  8645  Learning Rate:  1.7618076784390603e-05  Varinance:  0.00014125869813838576 \n",
      "\n",
      "Epoch:  8646  Learning Rate:  1.7600467513708998e-05  Varinance:  0.0001410613649148258 \n",
      "\n",
      "Epoch:  8647  Learning Rate:  1.758287584349635e-05  Varinance:  0.0001408643073585458 \n",
      "\n",
      "Epoch:  8648  Learning Rate:  1.7565301756161036e-05  Varinance:  0.0001406675250844489 \n",
      "\n",
      "Epoch:  8649  Learning Rate:  1.7547745234128944e-05  Varinance:  0.000140471017707976 \n",
      "\n",
      "Epoch:  8650  Learning Rate:  1.753020625984355e-05  Varinance:  0.00014027478484510546 \n",
      "\n",
      "Epoch:  8651  Learning Rate:  1.7512684815765843e-05  Varinance:  0.00014007882611235132 \n",
      "\n",
      "Epoch:  8652  Learning Rate:  1.7495180884374446e-05  Varinance:  0.00013988314112676407 \n",
      "\n",
      "Epoch:  8653  Learning Rate:  1.7477694448165387e-05  Varinance:  0.0001396877295059293 \n",
      "\n",
      "Epoch:  8654  Learning Rate:  1.74602254896522e-05  Varinance:  0.00013949259086796584 \n",
      "\n",
      "Epoch:  8655  Learning Rate:  1.7442773991365994e-05  Varinance:  0.00013929772483152697 \n",
      "\n",
      "Epoch:  8656  Learning Rate:  1.7425339935855232e-05  Varinance:  0.00013910313101579835 \n",
      "\n",
      "Epoch:  8657  Learning Rate:  1.7407923305685828e-05  Varinance:  0.00013890880904049782 \n",
      "\n",
      "Epoch:  8658  Learning Rate:  1.7390524083441207e-05  Varinance:  0.00013871475852587383 \n",
      "\n",
      "Epoch:  8659  Learning Rate:  1.737314225172212e-05  Varinance:  0.00013852097909270597 \n",
      "\n",
      "Epoch:  8660  Learning Rate:  1.7355777793146705e-05  Varinance:  0.00013832747036230362 \n",
      "\n",
      "Epoch:  8661  Learning Rate:  1.7338430690350556e-05  Varinance:  0.0001381342319565045 \n",
      "\n",
      "Epoch:  8662  Learning Rate:  1.7321100925986544e-05  Varinance:  0.00013794126349767524 \n",
      "\n",
      "Epoch:  8663  Learning Rate:  1.7303788482724872e-05  Varinance:  0.00013774856460870984 \n",
      "\n",
      "Epoch:  8664  Learning Rate:  1.728649334325315e-05  Varinance:  0.00013755613491302934 \n",
      "\n",
      "Epoch:  8665  Learning Rate:  1.726921549027622e-05  Varinance:  0.0001373639740345801 \n",
      "\n",
      "Epoch:  8666  Learning Rate:  1.7251954906516216e-05  Varinance:  0.00013717208159783447 \n",
      "\n",
      "Epoch:  8667  Learning Rate:  1.7234711574712525e-05  Varinance:  0.0001369804572277896 \n",
      "\n",
      "Epoch:  8668  Learning Rate:  1.7217485477621873e-05  Varinance:  0.00013678910054996552 \n",
      "\n",
      "Epoch:  8669  Learning Rate:  1.720027659801814e-05  Varinance:  0.00013659801119040639 \n",
      "\n",
      "Epoch:  8670  Learning Rate:  1.71830849186924e-05  Varinance:  0.0001364071887756785 \n",
      "\n",
      "Epoch:  8671  Learning Rate:  1.7165910422453046e-05  Varinance:  0.0001362166329328693 \n",
      "\n",
      "Epoch:  8672  Learning Rate:  1.7148753092125542e-05  Varinance:  0.0001360263432895878 \n",
      "\n",
      "Epoch:  8673  Learning Rate:  1.713161291055253e-05  Varinance:  0.00013583631947396306 \n",
      "\n",
      "Epoch:  8674  Learning Rate:  1.7114489860593887e-05  Varinance:  0.00013564656111464377 \n",
      "\n",
      "Epoch:  8675  Learning Rate:  1.7097383925126532e-05  Varinance:  0.00013545706784079674 \n",
      "\n",
      "Epoch:  8676  Learning Rate:  1.7080295087044497e-05  Varinance:  0.00013526783928210758 \n",
      "\n",
      "Epoch:  8677  Learning Rate:  1.7063223329259e-05  Varinance:  0.00013507887506877906 \n",
      "\n",
      "Epoch:  8678  Learning Rate:  1.7046168634698256e-05  Varinance:  0.00013489017483152998 \n",
      "\n",
      "Epoch:  8679  Learning Rate:  1.7029130986307534e-05  Varinance:  0.00013470173820159567 \n",
      "\n",
      "Epoch:  8680  Learning Rate:  1.7012110367049253e-05  Varinance:  0.00013451356481072635 \n",
      "\n",
      "Epoch:  8681  Learning Rate:  1.699510675990275e-05  Varinance:  0.00013432565429118698 \n",
      "\n",
      "Epoch:  8682  Learning Rate:  1.697812014786443e-05  Varinance:  0.00013413800627575542 \n",
      "\n",
      "Epoch:  8683  Learning Rate:  1.6961150513947638e-05  Varinance:  0.0001339506203977233 \n",
      "\n",
      "Epoch:  8684  Learning Rate:  1.6944197841182807e-05  Varinance:  0.00013376349629089454 \n",
      "\n",
      "Epoch:  8685  Learning Rate:  1.6927262112617224e-05  Varinance:  0.00013357663358958387 \n",
      "\n",
      "Epoch:  8686  Learning Rate:  1.691034331131514e-05  Varinance:  0.00013339003192861762 \n",
      "\n",
      "Epoch:  8687  Learning Rate:  1.6893441420357802e-05  Varinance:  0.000133203690943332 \n",
      "\n",
      "Epoch:  8688  Learning Rate:  1.6876556422843296e-05  Varinance:  0.0001330176102695728 \n",
      "\n",
      "Epoch:  8689  Learning Rate:  1.6859688301886586e-05  Varinance:  0.00013283178954369402 \n",
      "\n",
      "Epoch:  8690  Learning Rate:  1.6842837040619613e-05  Varinance:  0.00013264622840255814 \n",
      "\n",
      "Epoch:  8691  Learning Rate:  1.6826002622191084e-05  Varinance:  0.00013246092648353503 \n",
      "\n",
      "Epoch:  8692  Learning Rate:  1.680918502976655e-05  Varinance:  0.00013227588342450044 \n",
      "\n",
      "Epoch:  8693  Learning Rate:  1.679238424652848e-05  Varinance:  0.00013209109886383675 \n",
      "\n",
      "Epoch:  8694  Learning Rate:  1.6775600255676054e-05  Varinance:  0.0001319065724404311 \n",
      "\n",
      "Epoch:  8695  Learning Rate:  1.675883304042525e-05  Varinance:  0.00013172230379367552 \n",
      "\n",
      "Epoch:  8696  Learning Rate:  1.6742082584008913e-05  Varinance:  0.000131538292563465 \n",
      "\n",
      "Epoch:  8697  Learning Rate:  1.6725348869676556e-05  Varinance:  0.00013135453839019826 \n",
      "\n",
      "Epoch:  8698  Learning Rate:  1.670863188069446e-05  Varinance:  0.00013117104091477642 \n",
      "\n",
      "Epoch:  8699  Learning Rate:  1.6691931600345613e-05  Varinance:  0.00013098779977860155 \n",
      "\n",
      "Epoch:  8700  Learning Rate:  1.6675248011929782e-05  Varinance:  0.00013080481462357735 \n",
      "\n",
      "Epoch:  8701  Learning Rate:  1.6658581098763355e-05  Varinance:  0.0001306220850921075 \n",
      "\n",
      "Epoch:  8702  Learning Rate:  1.664193084417938e-05  Varinance:  0.00013043961082709554 \n",
      "\n",
      "Epoch:  8703  Learning Rate:  1.6625297231527674e-05  Varinance:  0.0001302573914719431 \n",
      "\n",
      "Epoch:  8704  Learning Rate:  1.6608680244174582e-05  Varinance:  0.0001300754266705507 \n",
      "\n",
      "Epoch:  8705  Learning Rate:  1.6592079865503086e-05  Varinance:  0.00012989371606731627 \n",
      "\n",
      "Epoch:  8706  Learning Rate:  1.657549607891287e-05  Varinance:  0.00012971225930713382 \n",
      "\n",
      "Epoch:  8707  Learning Rate:  1.6558928867820113e-05  Varinance:  0.0001295310560353942 \n",
      "\n",
      "Epoch:  8708  Learning Rate:  1.6542378215657576e-05  Varinance:  0.00012935010589798333 \n",
      "\n",
      "Epoch:  8709  Learning Rate:  1.652584410587466e-05  Varinance:  0.00012916940854128204 \n",
      "\n",
      "Epoch:  8710  Learning Rate:  1.650932652193723e-05  Varinance:  0.00012898896361216453 \n",
      "\n",
      "Epoch:  8711  Learning Rate:  1.6492825447327665e-05  Varinance:  0.00012880877075799885 \n",
      "\n",
      "Epoch:  8712  Learning Rate:  1.6476340865544953e-05  Varinance:  0.00012862882962664588 \n",
      "\n",
      "Epoch:  8713  Learning Rate:  1.645987276010448e-05  Varinance:  0.00012844913986645754 \n",
      "\n",
      "Epoch:  8714  Learning Rate:  1.644342111453814e-05  Varinance:  0.00012826970112627778 \n",
      "\n",
      "Epoch:  8715  Learning Rate:  1.642698591239425e-05  Varinance:  0.00012809051305544085 \n",
      "\n",
      "Epoch:  8716  Learning Rate:  1.6410567137237675e-05  Varinance:  0.0001279115753037711 \n",
      "\n",
      "Epoch:  8717  Learning Rate:  1.6394164772649605e-05  Varinance:  0.00012773288752158138 \n",
      "\n",
      "Epoch:  8718  Learning Rate:  1.6377778802227643e-05  Varinance:  0.00012755444935967368 \n",
      "\n",
      "Epoch:  8719  Learning Rate:  1.636140920958588e-05  Varinance:  0.00012737626046933787 \n",
      "\n",
      "Epoch:  8720  Learning Rate:  1.6345055978354686e-05  Varinance:  0.00012719832050235024 \n",
      "\n",
      "Epoch:  8721  Learning Rate:  1.6328719092180807e-05  Varinance:  0.00012702062911097424 \n",
      "\n",
      "Epoch:  8722  Learning Rate:  1.6312398534727408e-05  Varinance:  0.00012684318594795882 \n",
      "\n",
      "Epoch:  8723  Learning Rate:  1.6296094289673906e-05  Varinance:  0.0001266659906665383 \n",
      "\n",
      "Epoch:  8724  Learning Rate:  1.627980634071602e-05  Varinance:  0.00012648904292043075 \n",
      "\n",
      "Epoch:  8725  Learning Rate:  1.6263534671565858e-05  Varinance:  0.0001263123423638386 \n",
      "\n",
      "Epoch:  8726  Learning Rate:  1.6247279265951725e-05  Varinance:  0.00012613588865144742 \n",
      "\n",
      "Epoch:  8727  Learning Rate:  1.6231040107618182e-05  Varinance:  0.0001259596814384244 \n",
      "\n",
      "Epoch:  8728  Learning Rate:  1.6214817180326134e-05  Varinance:  0.00012578372038041927 \n",
      "\n",
      "Epoch:  8729  Learning Rate:  1.6198610467852613e-05  Varinance:  0.0001256080051335624 \n",
      "\n",
      "Epoch:  8730  Learning Rate:  1.6182419953990906e-05  Varinance:  0.00012543253535446492 \n",
      "\n",
      "Epoch:  8731  Learning Rate:  1.6166245622550477e-05  Varinance:  0.0001252573107002169 \n",
      "\n",
      "Epoch:  8732  Learning Rate:  1.6150087457357047e-05  Varinance:  0.0001250823308283881 \n",
      "\n",
      "Epoch:  8733  Learning Rate:  1.613394544225242e-05  Varinance:  0.00012490759539702671 \n",
      "\n",
      "Epoch:  8734  Learning Rate:  1.611781956109455e-05  Varinance:  0.0001247331040646579 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8735  Learning Rate:  1.6101709797757612e-05  Varinance:  0.00012455885649028455 \n",
      "\n",
      "Epoch:  8736  Learning Rate:  1.6085616136131816e-05  Varinance:  0.0001243848523333859 \n",
      "\n",
      "Epoch:  8737  Learning Rate:  1.6069538560123467e-05  Varinance:  0.00012421109125391623 \n",
      "\n",
      "Epoch:  8738  Learning Rate:  1.6053477053655043e-05  Varinance:  0.00012403757291230548 \n",
      "\n",
      "Epoch:  8739  Learning Rate:  1.6037431600665013e-05  Varinance:  0.00012386429696945776 \n",
      "\n",
      "Epoch:  8740  Learning Rate:  1.602140218510789e-05  Varinance:  0.0001236912630867511 \n",
      "\n",
      "Epoch:  8741  Learning Rate:  1.600538879095432e-05  Varinance:  0.00012351847092603594 \n",
      "\n",
      "Epoch:  8742  Learning Rate:  1.5989391402190873e-05  Varinance:  0.00012334592014963573 \n",
      "\n",
      "Epoch:  8743  Learning Rate:  1.597341000282013e-05  Varinance:  0.00012317361042034564 \n",
      "\n",
      "Epoch:  8744  Learning Rate:  1.595744457686075e-05  Varinance:  0.00012300154140143125 \n",
      "\n",
      "Epoch:  8745  Learning Rate:  1.5941495108347277e-05  Varinance:  0.0001228297127566292 \n",
      "\n",
      "Epoch:  8746  Learning Rate:  1.5925561581330244e-05  Varinance:  0.0001226581241501457 \n",
      "\n",
      "Epoch:  8747  Learning Rate:  1.5909643979876084e-05  Varinance:  0.0001224867752466562 \n",
      "\n",
      "Epoch:  8748  Learning Rate:  1.5893742288067264e-05  Varinance:  0.000122315665711304 \n",
      "\n",
      "Epoch:  8749  Learning Rate:  1.5877856490002057e-05  Varinance:  0.00012214479520970077 \n",
      "\n",
      "Epoch:  8750  Learning Rate:  1.5861986569794632e-05  Varinance:  0.00012197416340792534 \n",
      "\n",
      "Epoch:  8751  Learning Rate:  1.5846132511575125e-05  Varinance:  0.00012180376997252243 \n",
      "\n",
      "Epoch:  8752  Learning Rate:  1.5830294299489454e-05  Varinance:  0.00012163361457050308 \n",
      "\n",
      "Epoch:  8753  Learning Rate:  1.581447191769937e-05  Varinance:  0.00012146369686934345 \n",
      "\n",
      "Epoch:  8754  Learning Rate:  1.5798665350382554e-05  Varinance:  0.00012129401653698434 \n",
      "\n",
      "Epoch:  8755  Learning Rate:  1.5782874581732402e-05  Varinance:  0.00012112457324182979 \n",
      "\n",
      "Epoch:  8756  Learning Rate:  1.576709959595812e-05  Varinance:  0.00012095536665274775 \n",
      "\n",
      "Epoch:  8757  Learning Rate:  1.5751340377284777e-05  Varinance:  0.00012078639643906874 \n",
      "\n",
      "Epoch:  8758  Learning Rate:  1.573559690995312e-05  Varinance:  0.00012061766227058454 \n",
      "\n",
      "Epoch:  8759  Learning Rate:  1.5719869178219686e-05  Varinance:  0.00012044916381754885 \n",
      "\n",
      "Epoch:  8760  Learning Rate:  1.5704157166356715e-05  Varinance:  0.00012028090075067588 \n",
      "\n",
      "Epoch:  8761  Learning Rate:  1.568846085865224e-05  Varinance:  0.00012011287274113995 \n",
      "\n",
      "Epoch:  8762  Learning Rate:  1.5672780239409938e-05  Varinance:  0.00011994507946057422 \n",
      "\n",
      "Epoch:  8763  Learning Rate:  1.565711529294915e-05  Varinance:  0.00011977752058107109 \n",
      "\n",
      "Epoch:  8764  Learning Rate:  1.5641466003604992e-05  Varinance:  0.00011961019577518106 \n",
      "\n",
      "Epoch:  8765  Learning Rate:  1.562583235572814e-05  Varinance:  0.0001194431047159115 \n",
      "\n",
      "Epoch:  8766  Learning Rate:  1.5610214333684913e-05  Varinance:  0.00011927624707672711 \n",
      "\n",
      "Epoch:  8767  Learning Rate:  1.5594611921857352e-05  Varinance:  0.00011910962253154862 \n",
      "\n",
      "Epoch:  8768  Learning Rate:  1.5579025104643012e-05  Varinance:  0.00011894323075475244 \n",
      "\n",
      "Epoch:  8769  Learning Rate:  1.556345386645505e-05  Varinance:  0.00011877707142116925 \n",
      "\n",
      "Epoch:  8770  Learning Rate:  1.5547898191722274e-05  Varinance:  0.00011861114420608463 \n",
      "\n",
      "Epoch:  8771  Learning Rate:  1.5532358064888987e-05  Varinance:  0.00011844544878523777 \n",
      "\n",
      "Epoch:  8772  Learning Rate:  1.551683347041503e-05  Varinance:  0.00011827998483482015 \n",
      "\n",
      "Epoch:  8773  Learning Rate:  1.550132439277587e-05  Varinance:  0.00011811475203147629 \n",
      "\n",
      "Epoch:  8774  Learning Rate:  1.548583081646239e-05  Varinance:  0.00011794975005230224 \n",
      "\n",
      "Epoch:  8775  Learning Rate:  1.5470352725981015e-05  Varinance:  0.00011778497857484526 \n",
      "\n",
      "Epoch:  8776  Learning Rate:  1.5454890105853632e-05  Varinance:  0.00011762043727710247 \n",
      "\n",
      "Epoch:  8777  Learning Rate:  1.543944294061767e-05  Varinance:  0.00011745612583752151 \n",
      "\n",
      "Epoch:  8778  Learning Rate:  1.5424011214825933e-05  Varinance:  0.00011729204393499913 \n",
      "\n",
      "Epoch:  8779  Learning Rate:  1.540859491304667e-05  Varinance:  0.00011712819124888005 \n",
      "\n",
      "Epoch:  8780  Learning Rate:  1.539319401986363e-05  Varinance:  0.00011696456745895755 \n",
      "\n",
      "Epoch:  8781  Learning Rate:  1.5377808519875896e-05  Varinance:  0.00011680117224547207 \n",
      "\n",
      "Epoch:  8782  Learning Rate:  1.5362438397697935e-05  Varinance:  0.00011663800528911092 \n",
      "\n",
      "Epoch:  8783  Learning Rate:  1.5347083637959674e-05  Varinance:  0.00011647506627100682 \n",
      "\n",
      "Epoch:  8784  Learning Rate:  1.5331744225306334e-05  Varinance:  0.00011631235487273857 \n",
      "\n",
      "Epoch:  8785  Learning Rate:  1.5316420144398473e-05  Varinance:  0.00011614987077632986 \n",
      "\n",
      "Epoch:  8786  Learning Rate:  1.5301111379912053e-05  Varinance:  0.00011598761366424789 \n",
      "\n",
      "Epoch:  8787  Learning Rate:  1.5285817916538294e-05  Varinance:  0.00011582558321940406 \n",
      "\n",
      "Epoch:  8788  Learning Rate:  1.5270539738983695e-05  Varinance:  0.00011566377912515256 \n",
      "\n",
      "Epoch:  8789  Learning Rate:  1.5255276831970135e-05  Varinance:  0.00011550220106529012 \n",
      "\n",
      "Epoch:  8790  Learning Rate:  1.524002918023468e-05  Varinance:  0.0001153408487240546 \n",
      "\n",
      "Epoch:  8791  Learning Rate:  1.5224796768529672e-05  Varinance:  0.0001151797217861255 \n",
      "\n",
      "Epoch:  8792  Learning Rate:  1.5209579581622676e-05  Varinance:  0.0001150188199366229 \n",
      "\n",
      "Epoch:  8793  Learning Rate:  1.5194377604296557e-05  Varinance:  0.00011485814286110611 \n",
      "\n",
      "Epoch:  8794  Learning Rate:  1.5179190821349307e-05  Varinance:  0.0001146976902455743 \n",
      "\n",
      "Epoch:  8795  Learning Rate:  1.5164019217594119e-05  Varinance:  0.00011453746177646512 \n",
      "\n",
      "Epoch:  8796  Learning Rate:  1.5148862777859435e-05  Varinance:  0.00011437745714065442 \n",
      "\n",
      "Epoch:  8797  Learning Rate:  1.5133721486988796e-05  Varinance:  0.00011421767602545489 \n",
      "\n",
      "Epoch:  8798  Learning Rate:  1.5118595329840875e-05  Varinance:  0.00011405811811861664 \n",
      "\n",
      "Epoch:  8799  Learning Rate:  1.5103484291289572e-05  Varinance:  0.00011389878310832593 \n",
      "\n",
      "Epoch:  8800  Learning Rate:  1.5088388356223817e-05  Varinance:  0.00011373967068320411 \n",
      "\n",
      "Epoch:  8801  Learning Rate:  1.5073307509547651e-05  Varinance:  0.00011358078053230802 \n",
      "\n",
      "Epoch:  8802  Learning Rate:  1.5058241736180275e-05  Varinance:  0.00011342211234512894 \n",
      "\n",
      "Epoch:  8803  Learning Rate:  1.5043191021055891e-05  Varinance:  0.00011326366581159126 \n",
      "\n",
      "Epoch:  8804  Learning Rate:  1.5028155349123757e-05  Varinance:  0.00011310544062205319 \n",
      "\n",
      "Epoch:  8805  Learning Rate:  1.5013134705348249e-05  Varinance:  0.00011294743646730525 \n",
      "\n",
      "Epoch:  8806  Learning Rate:  1.49981290747087e-05  Varinance:  0.0001127896530385702 \n",
      "\n",
      "Epoch:  8807  Learning Rate:  1.4983138442199472e-05  Varinance:  0.00011263209002750142 \n",
      "\n",
      "Epoch:  8808  Learning Rate:  1.496816279282991e-05  Varinance:  0.00011247474712618374 \n",
      "\n",
      "Epoch:  8809  Learning Rate:  1.4953202111624414e-05  Varinance:  0.00011231762402713212 \n",
      "\n",
      "Epoch:  8810  Learning Rate:  1.4938256383622277e-05  Varinance:  0.00011216072042329042 \n",
      "\n",
      "Epoch:  8811  Learning Rate:  1.4923325593877739e-05  Varinance:  0.00011200403600803211 \n",
      "\n",
      "Epoch:  8812  Learning Rate:  1.490840972746007e-05  Varinance:  0.00011184757047515877 \n",
      "\n",
      "Epoch:  8813  Learning Rate:  1.4893508769453366e-05  Varinance:  0.0001116913235188999 \n",
      "\n",
      "Epoch:  8814  Learning Rate:  1.487862270495665e-05  Varinance:  0.00011153529483391162 \n",
      "\n",
      "Epoch:  8815  Learning Rate:  1.4863751519083902e-05  Varinance:  0.00011137948411527711 \n",
      "\n",
      "Epoch:  8816  Learning Rate:  1.4848895196963914e-05  Varinance:  0.00011122389105850563 \n",
      "\n",
      "Epoch:  8817  Learning Rate:  1.4834053723740331e-05  Varinance:  0.00011106851535953109 \n",
      "\n",
      "Epoch:  8818  Learning Rate:  1.4819227084571736e-05  Varinance:  0.00011091335671471284 \n",
      "\n",
      "Epoch:  8819  Learning Rate:  1.4804415264631463e-05  Varinance:  0.00011075841482083416 \n",
      "\n",
      "Epoch:  8820  Learning Rate:  1.478961824910766e-05  Varinance:  0.00011060368937510213 \n",
      "\n",
      "Epoch:  8821  Learning Rate:  1.4774836023203364e-05  Varinance:  0.00011044918007514625 \n",
      "\n",
      "Epoch:  8822  Learning Rate:  1.4760068572136323e-05  Varinance:  0.00011029488661901899 \n",
      "\n",
      "Epoch:  8823  Learning Rate:  1.4745315881139085e-05  Varinance:  0.00011014080870519459 \n",
      "\n",
      "Epoch:  8824  Learning Rate:  1.4730577935458931e-05  Varinance:  0.00010998694603256801 \n",
      "\n",
      "Epoch:  8825  Learning Rate:  1.4715854720357964e-05  Varinance:  0.00010983329830045534 \n",
      "\n",
      "Epoch:  8826  Learning Rate:  1.4701146221112946e-05  Varinance:  0.0001096798652085926 \n",
      "\n",
      "Epoch:  8827  Learning Rate:  1.4686452423015346e-05  Varinance:  0.0001095266464571354 \n",
      "\n",
      "Epoch:  8828  Learning Rate:  1.467177331137142e-05  Varinance:  0.00010937364174665767 \n",
      "\n",
      "Epoch:  8829  Learning Rate:  1.4657108871502029e-05  Varinance:  0.0001092208507781522 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8830  Learning Rate:  1.4642459088742702e-05  Varinance:  0.00010906827325302945 \n",
      "\n",
      "Epoch:  8831  Learning Rate:  1.4627823948443713e-05  Varinance:  0.0001089159088731165 \n",
      "\n",
      "Epoch:  8832  Learning Rate:  1.4613203435969892e-05  Varinance:  0.00010876375734065747 \n",
      "\n",
      "Epoch:  8833  Learning Rate:  1.4598597536700695e-05  Varinance:  0.00010861181835831229 \n",
      "\n",
      "Epoch:  8834  Learning Rate:  1.458400623603028e-05  Varinance:  0.00010846009162915641 \n",
      "\n",
      "Epoch:  8835  Learning Rate:  1.4569429519367314e-05  Varinance:  0.00010830857685667954 \n",
      "\n",
      "Epoch:  8836  Learning Rate:  1.4554867372135059e-05  Varinance:  0.00010815727374478614 \n",
      "\n",
      "Epoch:  8837  Learning Rate:  1.4540319779771415e-05  Varinance:  0.00010800618199779435 \n",
      "\n",
      "Epoch:  8838  Learning Rate:  1.452578672772876e-05  Varinance:  0.00010785530132043474 \n",
      "\n",
      "Epoch:  8839  Learning Rate:  1.4511268201474042e-05  Varinance:  0.00010770463141785097 \n",
      "\n",
      "Epoch:  8840  Learning Rate:  1.4496764186488711e-05  Varinance:  0.00010755417199559844 \n",
      "\n",
      "Epoch:  8841  Learning Rate:  1.4482274668268801e-05  Varinance:  0.00010740392275964398 \n",
      "\n",
      "Epoch:  8842  Learning Rate:  1.4467799632324767e-05  Varinance:  0.00010725388341636468 \n",
      "\n",
      "Epoch:  8843  Learning Rate:  1.4453339064181542e-05  Varinance:  0.00010710405367254836 \n",
      "\n",
      "Epoch:  8844  Learning Rate:  1.4438892949378613e-05  Varinance:  0.00010695443323539242 \n",
      "\n",
      "Epoch:  8845  Learning Rate:  1.4424461273469836e-05  Varinance:  0.00010680502181250278 \n",
      "\n",
      "Epoch:  8846  Learning Rate:  1.4410044022023508e-05  Varinance:  0.00010665581911189432 \n",
      "\n",
      "Epoch:  8847  Learning Rate:  1.4395641180622431e-05  Varinance:  0.00010650682484198973 \n",
      "\n",
      "Epoch:  8848  Learning Rate:  1.4381252734863734e-05  Varinance:  0.00010635803871161906 \n",
      "\n",
      "Epoch:  8849  Learning Rate:  1.4366878670358942e-05  Varinance:  0.00010620946043001871 \n",
      "\n",
      "Epoch:  8850  Learning Rate:  1.4352518972734046e-05  Varinance:  0.00010606108970683171 \n",
      "\n",
      "Epoch:  8851  Learning Rate:  1.4338173627629319e-05  Varinance:  0.00010591292625210678 \n",
      "\n",
      "Epoch:  8852  Learning Rate:  1.4323842620699387e-05  Varinance:  0.00010576496977629707 \n",
      "\n",
      "Epoch:  8853  Learning Rate:  1.4309525937613295e-05  Varinance:  0.00010561721999026083 \n",
      "\n",
      "Epoch:  8854  Learning Rate:  1.4295223564054333e-05  Varinance:  0.00010546967660526001 \n",
      "\n",
      "Epoch:  8855  Learning Rate:  1.4280935485720125e-05  Varinance:  0.00010532233933296012 \n",
      "\n",
      "Epoch:  8856  Learning Rate:  1.4266661688322571e-05  Varinance:  0.0001051752078854289 \n",
      "\n",
      "Epoch:  8857  Learning Rate:  1.4252402157587918e-05  Varinance:  0.0001050282819751369 \n",
      "\n",
      "Epoch:  8858  Learning Rate:  1.423815687925661e-05  Varinance:  0.0001048815613149563 \n",
      "\n",
      "Epoch:  8859  Learning Rate:  1.4223925839083343e-05  Varinance:  0.00010473504561815981 \n",
      "\n",
      "Epoch:  8860  Learning Rate:  1.4209709022837125e-05  Varinance:  0.00010458873459842133 \n",
      "\n",
      "Epoch:  8861  Learning Rate:  1.4195506416301115e-05  Varinance:  0.0001044426279698145 \n",
      "\n",
      "Epoch:  8862  Learning Rate:  1.4181318005272677e-05  Varinance:  0.00010429672544681259 \n",
      "\n",
      "Epoch:  8863  Learning Rate:  1.4167143775563454e-05  Varinance:  0.00010415102674428716 \n",
      "\n",
      "Epoch:  8864  Learning Rate:  1.4152983712999184e-05  Varinance:  0.00010400553157750869 \n",
      "\n",
      "Epoch:  8865  Learning Rate:  1.4138837803419784e-05  Varinance:  0.00010386023966214534 \n",
      "\n",
      "Epoch:  8866  Learning Rate:  1.4124706032679389e-05  Varinance:  0.00010371515071426201 \n",
      "\n",
      "Epoch:  8867  Learning Rate:  1.4110588386646206e-05  Varinance:  0.00010357026445032074 \n",
      "\n",
      "Epoch:  8868  Learning Rate:  1.4096484851202558e-05  Varinance:  0.00010342558058717968 \n",
      "\n",
      "Epoch:  8869  Learning Rate:  1.4082395412244961e-05  Varinance:  0.00010328109884209196 \n",
      "\n",
      "Epoch:  8870  Learning Rate:  1.4068320055683951e-05  Varinance:  0.00010313681893270624 \n",
      "\n",
      "Epoch:  8871  Learning Rate:  1.405425876744417e-05  Varinance:  0.00010299274057706546 \n",
      "\n",
      "Epoch:  8872  Learning Rate:  1.4040211533464303e-05  Varinance:  0.00010284886349360659 \n",
      "\n",
      "Epoch:  8873  Learning Rate:  1.402617833969716e-05  Varinance:  0.00010270518740115942 \n",
      "\n",
      "Epoch:  8874  Learning Rate:  1.401215917210953e-05  Varinance:  0.00010256171201894705 \n",
      "\n",
      "Epoch:  8875  Learning Rate:  1.3998154016682212e-05  Varinance:  0.00010241843706658482 \n",
      "\n",
      "Epoch:  8876  Learning Rate:  1.3984162859410104e-05  Varinance:  0.00010227536226407924 \n",
      "\n",
      "Epoch:  8877  Learning Rate:  1.3970185686302021e-05  Varinance:  0.00010213248733182843 \n",
      "\n",
      "Epoch:  8878  Learning Rate:  1.3956222483380761e-05  Varinance:  0.00010198981199062098 \n",
      "\n",
      "Epoch:  8879  Learning Rate:  1.3942273236683175e-05  Varinance:  0.00010184733596163574 \n",
      "\n",
      "Epoch:  8880  Learning Rate:  1.3928337932259988e-05  Varinance:  0.00010170505896644043 \n",
      "\n",
      "Epoch:  8881  Learning Rate:  1.3914416556175866e-05  Varinance:  0.00010156298072699236 \n",
      "\n",
      "Epoch:  8882  Learning Rate:  1.3900509094509488e-05  Varinance:  0.0001014211009656372 \n",
      "\n",
      "Epoch:  8883  Learning Rate:  1.3886615533353358e-05  Varinance:  0.00010127941940510796 \n",
      "\n",
      "Epoch:  8884  Learning Rate:  1.3872735858813923e-05  Varinance:  0.00010113793576852555 \n",
      "\n",
      "Epoch:  8885  Learning Rate:  1.3858870057011477e-05  Varinance:  0.00010099664977939746 \n",
      "\n",
      "Epoch:  8886  Learning Rate:  1.3845018114080267e-05  Varinance:  0.00010085556116161762 \n",
      "\n",
      "Epoch:  8887  Learning Rate:  1.3831180016168323e-05  Varinance:  0.00010071466963946513 \n",
      "\n",
      "Epoch:  8888  Learning Rate:  1.3817355749437528e-05  Varinance:  0.00010057397493760481 \n",
      "\n",
      "Epoch:  8889  Learning Rate:  1.3803545300063655e-05  Varinance:  0.0001004334767810861 \n",
      "\n",
      "Epoch:  8890  Learning Rate:  1.3789748654236233e-05  Varinance:  0.000100293174895342 \n",
      "\n",
      "Epoch:  8891  Learning Rate:  1.3775965798158591e-05  Varinance:  0.00010015306900618963 \n",
      "\n",
      "Epoch:  8892  Learning Rate:  1.3762196718047918e-05  Varinance:  0.00010001315883982891 \n",
      "\n",
      "Epoch:  8893  Learning Rate:  1.3748441400135111e-05  Varinance:  9.987344412284246e-05 \n",
      "\n",
      "Epoch:  8894  Learning Rate:  1.3734699830664826e-05  Varinance:  9.973392458219434e-05 \n",
      "\n",
      "Epoch:  8895  Learning Rate:  1.3720971995895539e-05  Varinance:  9.95945999452305e-05 \n",
      "\n",
      "Epoch:  8896  Learning Rate:  1.370725788209939e-05  Varinance:  9.945546993967783e-05 \n",
      "\n",
      "Epoch:  8897  Learning Rate:  1.3693557475562245e-05  Varinance:  9.931653429364302e-05 \n",
      "\n",
      "Epoch:  8898  Learning Rate:  1.367987076258374e-05  Varinance:  9.917779273561314e-05 \n",
      "\n",
      "Epoch:  8899  Learning Rate:  1.3666197729477134e-05  Varinance:  9.903924499445436e-05 \n",
      "\n",
      "Epoch:  8900  Learning Rate:  1.36525383625694e-05  Varinance:  9.890089079941177e-05 \n",
      "\n",
      "Epoch:  8901  Learning Rate:  1.363889264820114e-05  Varinance:  9.876272988010819e-05 \n",
      "\n",
      "Epoch:  8902  Learning Rate:  1.3625260572726691e-05  Varinance:  9.86247619665447e-05 \n",
      "\n",
      "Epoch:  8903  Learning Rate:  1.3611642122513948e-05  Varinance:  9.848698678909952e-05 \n",
      "\n",
      "Epoch:  8904  Learning Rate:  1.359803728394444e-05  Varinance:  9.8349404078527e-05 \n",
      "\n",
      "Epoch:  8905  Learning Rate:  1.358444604341337e-05  Varinance:  9.821201356595816e-05 \n",
      "\n",
      "Epoch:  8906  Learning Rate:  1.357086838732948e-05  Varinance:  9.80748149828994e-05 \n",
      "\n",
      "Epoch:  8907  Learning Rate:  1.355730430211508e-05  Varinance:  9.793780806123244e-05 \n",
      "\n",
      "Epoch:  8908  Learning Rate:  1.3543753774206139e-05  Varinance:  9.7800992533213e-05 \n",
      "\n",
      "Epoch:  8909  Learning Rate:  1.3530216790052097e-05  Varinance:  9.76643681314713e-05 \n",
      "\n",
      "Epoch:  8910  Learning Rate:  1.3516693336115951e-05  Varinance:  9.752793458901115e-05 \n",
      "\n",
      "Epoch:  8911  Learning Rate:  1.3503183398874293e-05  Varinance:  9.739169163920876e-05 \n",
      "\n",
      "Epoch:  8912  Learning Rate:  1.3489686964817157e-05  Varinance:  9.725563901581334e-05 \n",
      "\n",
      "Epoch:  8913  Learning Rate:  1.3476204020448086e-05  Varinance:  9.711977645294591e-05 \n",
      "\n",
      "Epoch:  8914  Learning Rate:  1.3462734552284183e-05  Varinance:  9.698410368509903e-05 \n",
      "\n",
      "Epoch:  8915  Learning Rate:  1.3449278546855956e-05  Varinance:  9.684862044713565e-05 \n",
      "\n",
      "Epoch:  8916  Learning Rate:  1.3435835990707394e-05  Varinance:  9.671332647428968e-05 \n",
      "\n",
      "Epoch:  8917  Learning Rate:  1.3422406870395917e-05  Varinance:  9.657822150216483e-05 \n",
      "\n",
      "Epoch:  8918  Learning Rate:  1.3408991172492456e-05  Varinance:  9.644330526673371e-05 \n",
      "\n",
      "Epoch:  8919  Learning Rate:  1.3395588883581285e-05  Varinance:  9.630857750433821e-05 \n",
      "\n",
      "Epoch:  8920  Learning Rate:  1.3382199990260089e-05  Varinance:  9.617403795168844e-05 \n",
      "\n",
      "Epoch:  8921  Learning Rate:  1.3368824479140023e-05  Varinance:  9.603968634586242e-05 \n",
      "\n",
      "Epoch:  8922  Learning Rate:  1.3355462336845549e-05  Varinance:  9.5905522424305e-05 \n",
      "\n",
      "Epoch:  8923  Learning Rate:  1.3342113550014501e-05  Varinance:  9.577154592482832e-05 \n",
      "\n",
      "Epoch:  8924  Learning Rate:  1.3328778105298139e-05  Varinance:  9.563775658561075e-05 \n",
      "\n",
      "Epoch:  8925  Learning Rate:  1.3315455989360993e-05  Varinance:  9.550415414519596e-05 \n",
      "\n",
      "Epoch:  8926  Learning Rate:  1.3302147188880922e-05  Varinance:  9.537073834249332e-05 \n",
      "\n",
      "Epoch:  8927  Learning Rate:  1.3288851690549174e-05  Varinance:  9.52375089167768e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8928  Learning Rate:  1.3275569481070222e-05  Varinance:  9.510446560768473e-05 \n",
      "\n",
      "Epoch:  8929  Learning Rate:  1.3262300547161834e-05  Varinance:  9.497160815521869e-05 \n",
      "\n",
      "Epoch:  8930  Learning Rate:  1.3249044875555123e-05  Varinance:  9.483893629974394e-05 \n",
      "\n",
      "Epoch:  8931  Learning Rate:  1.323580245299439e-05  Varinance:  9.470644978198848e-05 \n",
      "\n",
      "Epoch:  8932  Learning Rate:  1.3222573266237213e-05  Varinance:  9.457414834304194e-05 \n",
      "\n",
      "Epoch:  8933  Learning Rate:  1.3209357302054382e-05  Varinance:  9.444203172435619e-05 \n",
      "\n",
      "Epoch:  8934  Learning Rate:  1.3196154547229976e-05  Varinance:  9.431009966774428e-05 \n",
      "\n",
      "Epoch:  8935  Learning Rate:  1.3182964988561218e-05  Varinance:  9.41783519153794e-05 \n",
      "\n",
      "Epoch:  8936  Learning Rate:  1.3169788612858523e-05  Varinance:  9.404678820979546e-05 \n",
      "\n",
      "Epoch:  8937  Learning Rate:  1.3156625406945562e-05  Varinance:  9.391540829388584e-05 \n",
      "\n",
      "Epoch:  8938  Learning Rate:  1.3143475357659105e-05  Varinance:  9.378421191090328e-05 \n",
      "\n",
      "Epoch:  8939  Learning Rate:  1.3130338451849078e-05  Varinance:  9.365319880445866e-05 \n",
      "\n",
      "Epoch:  8940  Learning Rate:  1.3117214676378618e-05  Varinance:  9.352236871852155e-05 \n",
      "\n",
      "Epoch:  8941  Learning Rate:  1.310410401812393e-05  Varinance:  9.339172139741918e-05 \n",
      "\n",
      "Epoch:  8942  Learning Rate:  1.3091006463974328e-05  Varinance:  9.326125658583542e-05 \n",
      "\n",
      "Epoch:  8943  Learning Rate:  1.3077922000832304e-05  Varinance:  9.313097402881138e-05 \n",
      "\n",
      "Epoch:  8944  Learning Rate:  1.3064850615613369e-05  Varinance:  9.300087347174407e-05 \n",
      "\n",
      "Epoch:  8945  Learning Rate:  1.3051792295246118e-05  Varinance:  9.287095466038638e-05 \n",
      "\n",
      "Epoch:  8946  Learning Rate:  1.3038747026672271e-05  Varinance:  9.274121734084593e-05 \n",
      "\n",
      "Epoch:  8947  Learning Rate:  1.3025714796846536e-05  Varinance:  9.261166125958544e-05 \n",
      "\n",
      "Epoch:  8948  Learning Rate:  1.3012695592736687e-05  Varinance:  9.248228616342186e-05 \n",
      "\n",
      "Epoch:  8949  Learning Rate:  1.299968940132349e-05  Varinance:  9.23530917995253e-05 \n",
      "\n",
      "Epoch:  8950  Learning Rate:  1.2986696209600801e-05  Varinance:  9.222407791541956e-05 \n",
      "\n",
      "Epoch:  8951  Learning Rate:  1.2973716004575406e-05  Varinance:  9.209524425898101e-05 \n",
      "\n",
      "Epoch:  8952  Learning Rate:  1.296074877326707e-05  Varinance:  9.196659057843836e-05 \n",
      "\n",
      "Epoch:  8953  Learning Rate:  1.2947794502708613e-05  Varinance:  9.183811662237155e-05 \n",
      "\n",
      "Epoch:  8954  Learning Rate:  1.2934853179945734e-05  Varinance:  9.170982213971225e-05 \n",
      "\n",
      "Epoch:  8955  Learning Rate:  1.2921924792037092e-05  Varinance:  9.158170687974284e-05 \n",
      "\n",
      "Epoch:  8956  Learning Rate:  1.2909009326054345e-05  Varinance:  9.145377059209547e-05 \n",
      "\n",
      "Epoch:  8957  Learning Rate:  1.2896106769081997e-05  Varinance:  9.132601302675251e-05 \n",
      "\n",
      "Epoch:  8958  Learning Rate:  1.288321710821747e-05  Varinance:  9.11984339340455e-05 \n",
      "\n",
      "Epoch:  8959  Learning Rate:  1.2870340330571146e-05  Varinance:  9.107103306465482e-05 \n",
      "\n",
      "Epoch:  8960  Learning Rate:  1.2857476423266227e-05  Varinance:  9.094381016960874e-05 \n",
      "\n",
      "Epoch:  8961  Learning Rate:  1.284462537343878e-05  Varinance:  9.081676500028376e-05 \n",
      "\n",
      "Epoch:  8962  Learning Rate:  1.2831787168237798e-05  Varinance:  9.068989730840374e-05 \n",
      "\n",
      "Epoch:  8963  Learning Rate:  1.2818961794825056e-05  Varinance:  9.056320684603887e-05 \n",
      "\n",
      "Epoch:  8964  Learning Rate:  1.2806149240375177e-05  Varinance:  9.043669336560617e-05 \n",
      "\n",
      "Epoch:  8965  Learning Rate:  1.279334949207558e-05  Varinance:  9.031035661986837e-05 \n",
      "\n",
      "Epoch:  8966  Learning Rate:  1.2780562537126564e-05  Varinance:  9.018419636193375e-05 \n",
      "\n",
      "Epoch:  8967  Learning Rate:  1.2767788362741154e-05  Varinance:  9.005821234525498e-05 \n",
      "\n",
      "Epoch:  8968  Learning Rate:  1.2755026956145147e-05  Varinance:  8.993240432362966e-05 \n",
      "\n",
      "Epoch:  8969  Learning Rate:  1.274227830457718e-05  Varinance:  8.98067720511993e-05 \n",
      "\n",
      "Epoch:  8970  Learning Rate:  1.2729542395288578e-05  Varinance:  8.968131528244841e-05 \n",
      "\n",
      "Epoch:  8971  Learning Rate:  1.271681921554341e-05  Varinance:  8.955603377220493e-05 \n",
      "\n",
      "Epoch:  8972  Learning Rate:  1.2704108752618543e-05  Varinance:  8.943092727563916e-05 \n",
      "\n",
      "Epoch:  8973  Learning Rate:  1.2691410993803487e-05  Varinance:  8.930599554826354e-05 \n",
      "\n",
      "Epoch:  8974  Learning Rate:  1.2678725926400456e-05  Varinance:  8.918123834593163e-05 \n",
      "\n",
      "Epoch:  8975  Learning Rate:  1.2666053537724435e-05  Varinance:  8.90566554248385e-05 \n",
      "\n",
      "Epoch:  8976  Learning Rate:  1.2653393815103006e-05  Varinance:  8.893224654151978e-05 \n",
      "\n",
      "Epoch:  8977  Learning Rate:  1.264074674587642e-05  Varinance:  8.880801145285079e-05 \n",
      "\n",
      "Epoch:  8978  Learning Rate:  1.2628112317397662e-05  Varinance:  8.868394991604691e-05 \n",
      "\n",
      "Epoch:  8979  Learning Rate:  1.261549051703227e-05  Varinance:  8.856006168866255e-05 \n",
      "\n",
      "Epoch:  8980  Learning Rate:  1.2602881332158447e-05  Varinance:  8.843634652859098e-05 \n",
      "\n",
      "Epoch:  8981  Learning Rate:  1.2590284750166984e-05  Varinance:  8.831280419406318e-05 \n",
      "\n",
      "Epoch:  8982  Learning Rate:  1.2577700758461343e-05  Varinance:  8.818943444364835e-05 \n",
      "\n",
      "Epoch:  8983  Learning Rate:  1.2565129344457507e-05  Varinance:  8.806623703625298e-05 \n",
      "\n",
      "Epoch:  8984  Learning Rate:  1.2552570495584043e-05  Varinance:  8.794321173111987e-05 \n",
      "\n",
      "Epoch:  8985  Learning Rate:  1.2540024199282142e-05  Varinance:  8.782035828782861e-05 \n",
      "\n",
      "Epoch:  8986  Learning Rate:  1.2527490443005485e-05  Varinance:  8.769767646629454e-05 \n",
      "\n",
      "Epoch:  8987  Learning Rate:  1.2514969214220292e-05  Varinance:  8.757516602676848e-05 \n",
      "\n",
      "Epoch:  8988  Learning Rate:  1.2502460500405378e-05  Varinance:  8.745282672983576e-05 \n",
      "\n",
      "Epoch:  8989  Learning Rate:  1.2489964289052008e-05  Varinance:  8.733065833641657e-05 \n",
      "\n",
      "Epoch:  8990  Learning Rate:  1.2477480567663945e-05  Varinance:  8.720866060776517e-05 \n",
      "\n",
      "Epoch:  8991  Learning Rate:  1.246500932375751e-05  Varinance:  8.708683330546878e-05 \n",
      "\n",
      "Epoch:  8992  Learning Rate:  1.245255054486144e-05  Varinance:  8.696517619144821e-05 \n",
      "\n",
      "Epoch:  8993  Learning Rate:  1.244010421851693e-05  Varinance:  8.684368902795667e-05 \n",
      "\n",
      "Epoch:  8994  Learning Rate:  1.2427670332277697e-05  Varinance:  8.672237157757965e-05 \n",
      "\n",
      "Epoch:  8995  Learning Rate:  1.241524887370983e-05  Varinance:  8.660122360323385e-05 \n",
      "\n",
      "Epoch:  8996  Learning Rate:  1.2402839830391873e-05  Varinance:  8.648024486816762e-05 \n",
      "\n",
      "Epoch:  8997  Learning Rate:  1.239044318991476e-05  Varinance:  8.635943513596005e-05 \n",
      "\n",
      "Epoch:  8998  Learning Rate:  1.2378058939881888e-05  Varinance:  8.623879417052e-05 \n",
      "\n",
      "Epoch:  8999  Learning Rate:  1.2365687067908988e-05  Varinance:  8.611832173608667e-05 \n",
      "\n",
      "Epoch:  9000  Learning Rate:  1.2353327561624167e-05  Varinance:  8.599801759722856e-05 \n",
      "\n",
      "Epoch:  9001  Learning Rate:  1.2340980408667957e-05  Varinance:  8.587788151884263e-05 \n",
      "\n",
      "Epoch:  9002  Learning Rate:  1.2328645596693182e-05  Varinance:  8.575791326615469e-05 \n",
      "\n",
      "Epoch:  9003  Learning Rate:  1.2316323113365012e-05  Varinance:  8.56381126047184e-05 \n",
      "\n",
      "Epoch:  9004  Learning Rate:  1.2304012946361005e-05  Varinance:  8.551847930041509e-05 \n",
      "\n",
      "Epoch:  9005  Learning Rate:  1.2291715083370965e-05  Varinance:  8.539901311945265e-05 \n",
      "\n",
      "Epoch:  9006  Learning Rate:  1.2279429512097013e-05  Varinance:  8.527971382836608e-05 \n",
      "\n",
      "Epoch:  9007  Learning Rate:  1.226715622025362e-05  Varinance:  8.516058119401647e-05 \n",
      "\n",
      "Epoch:  9008  Learning Rate:  1.225489519556747e-05  Varinance:  8.504161498359018e-05 \n",
      "\n",
      "Epoch:  9009  Learning Rate:  1.2242646425777534e-05  Varinance:  8.492281496459921e-05 \n",
      "\n",
      "Epoch:  9010  Learning Rate:  1.2230409898635023e-05  Varinance:  8.480418090488025e-05 \n",
      "\n",
      "Epoch:  9011  Learning Rate:  1.2218185601903452e-05  Varinance:  8.468571257259443e-05 \n",
      "\n",
      "Epoch:  9012  Learning Rate:  1.2205973523358501e-05  Varinance:  8.456740973622628e-05 \n",
      "\n",
      "Epoch:  9013  Learning Rate:  1.219377365078807e-05  Varinance:  8.444927216458422e-05 \n",
      "\n",
      "Epoch:  9014  Learning Rate:  1.2181585971992326e-05  Varinance:  8.433129962679965e-05 \n",
      "\n",
      "Epoch:  9015  Learning Rate:  1.216941047478357e-05  Varinance:  8.421349189232601e-05 \n",
      "\n",
      "Epoch:  9016  Learning Rate:  1.215724714698628e-05  Varinance:  8.409584873093928e-05 \n",
      "\n",
      "Epoch:  9017  Learning Rate:  1.2145095976437172e-05  Varinance:  8.397836991273688e-05 \n",
      "\n",
      "Epoch:  9018  Learning Rate:  1.2132956950985052e-05  Varinance:  8.386105520813754e-05 \n",
      "\n",
      "Epoch:  9019  Learning Rate:  1.2120830058490874e-05  Varinance:  8.37439043878803e-05 \n",
      "\n",
      "Epoch:  9020  Learning Rate:  1.2108715286827788e-05  Varinance:  8.362691722302487e-05 \n",
      "\n",
      "Epoch:  9021  Learning Rate:  1.2096612623880994e-05  Varinance:  8.351009348495081e-05 \n",
      "\n",
      "Epoch:  9022  Learning Rate:  1.2084522057547813e-05  Varinance:  8.33934329453566e-05 \n",
      "\n",
      "Epoch:  9023  Learning Rate:  1.2072443575737716e-05  Varinance:  8.32769353762601e-05 \n",
      "\n",
      "Epoch:  9024  Learning Rate:  1.2060377166372202e-05  Varinance:  8.316060054999748e-05 \n",
      "\n",
      "Epoch:  9025  Learning Rate:  1.2048322817384859e-05  Varinance:  8.304442823922312e-05 \n",
      "\n",
      "Epoch:  9026  Learning Rate:  1.2036280516721317e-05  Varinance:  8.292841821690856e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9027  Learning Rate:  1.2024250252339315e-05  Varinance:  8.28125702563429e-05 \n",
      "\n",
      "Epoch:  9028  Learning Rate:  1.2012232012208568e-05  Varinance:  8.269688413113198e-05 \n",
      "\n",
      "Epoch:  9029  Learning Rate:  1.2000225784310812e-05  Varinance:  8.258135961519743e-05 \n",
      "\n",
      "Epoch:  9030  Learning Rate:  1.1988231556639863e-05  Varinance:  8.246599648277717e-05 \n",
      "\n",
      "Epoch:  9031  Learning Rate:  1.1976249317201468e-05  Varinance:  8.235079450842435e-05 \n",
      "\n",
      "Epoch:  9032  Learning Rate:  1.1964279054013367e-05  Varinance:  8.22357534670072e-05 \n",
      "\n",
      "Epoch:  9033  Learning Rate:  1.1952320755105341e-05  Varinance:  8.2120873133708e-05 \n",
      "\n",
      "Epoch:  9034  Learning Rate:  1.1940374408519065e-05  Varinance:  8.200615328402358e-05 \n",
      "\n",
      "Epoch:  9035  Learning Rate:  1.1928440002308168e-05  Varinance:  8.189159369376432e-05 \n",
      "\n",
      "Epoch:  9036  Learning Rate:  1.1916517524538292e-05  Varinance:  8.177719413905343e-05 \n",
      "\n",
      "Epoch:  9037  Learning Rate:  1.1904606963286933e-05  Varinance:  8.166295439632722e-05 \n",
      "\n",
      "Epoch:  9038  Learning Rate:  1.1892708306643509e-05  Varinance:  8.154887424233422e-05 \n",
      "\n",
      "Epoch:  9039  Learning Rate:  1.1880821542709402e-05  Varinance:  8.143495345413496e-05 \n",
      "\n",
      "Epoch:  9040  Learning Rate:  1.1868946659597827e-05  Varinance:  8.1321191809101e-05 \n",
      "\n",
      "Epoch:  9041  Learning Rate:  1.1857083645433904e-05  Varinance:  8.120758908491527e-05 \n",
      "\n",
      "Epoch:  9042  Learning Rate:  1.1845232488354592e-05  Varinance:  8.109414505957136e-05 \n",
      "\n",
      "Epoch:  9043  Learning Rate:  1.1833393176508776e-05  Varinance:  8.09808595113725e-05 \n",
      "\n",
      "Epoch:  9044  Learning Rate:  1.1821565698057123e-05  Varinance:  8.086773221893208e-05 \n",
      "\n",
      "Epoch:  9045  Learning Rate:  1.180975004117213e-05  Varinance:  8.075476296117259e-05 \n",
      "\n",
      "Epoch:  9046  Learning Rate:  1.1797946194038187e-05  Varinance:  8.06419515173255e-05 \n",
      "\n",
      "Epoch:  9047  Learning Rate:  1.178615414485142e-05  Varinance:  8.052929766693033e-05 \n",
      "\n",
      "Epoch:  9048  Learning Rate:  1.1774373881819757e-05  Varinance:  8.04168011898349e-05 \n",
      "\n",
      "Epoch:  9049  Learning Rate:  1.176260539316298e-05  Varinance:  8.030446186619463e-05 \n",
      "\n",
      "Epoch:  9050  Learning Rate:  1.1750848667112576e-05  Varinance:  8.019227947647164e-05 \n",
      "\n",
      "Epoch:  9051  Learning Rate:  1.1739103691911798e-05  Varinance:  8.008025380143511e-05 \n",
      "\n",
      "Epoch:  9052  Learning Rate:  1.172737045581571e-05  Varinance:  7.996838462216038e-05 \n",
      "\n",
      "Epoch:  9053  Learning Rate:  1.1715648947091055e-05  Varinance:  7.985667172002876e-05 \n",
      "\n",
      "Epoch:  9054  Learning Rate:  1.1703939154016304e-05  Varinance:  7.974511487672653e-05 \n",
      "\n",
      "Epoch:  9055  Learning Rate:  1.1692241064881702e-05  Varinance:  7.963371387424534e-05 \n",
      "\n",
      "Epoch:  9056  Learning Rate:  1.168055466798914e-05  Varinance:  7.952246849488143e-05 \n",
      "\n",
      "Epoch:  9057  Learning Rate:  1.1668879951652218e-05  Varinance:  7.941137852123473e-05 \n",
      "\n",
      "Epoch:  9058  Learning Rate:  1.16572169041962e-05  Varinance:  7.930044373620929e-05 \n",
      "\n",
      "Epoch:  9059  Learning Rate:  1.1645565513958078e-05  Varinance:  7.918966392301229e-05 \n",
      "\n",
      "Epoch:  9060  Learning Rate:  1.1633925769286441e-05  Varinance:  7.907903886515393e-05 \n",
      "\n",
      "Epoch:  9061  Learning Rate:  1.1622297658541524e-05  Varinance:  7.896856834644636e-05 \n",
      "\n",
      "Epoch:  9062  Learning Rate:  1.1610681170095251e-05  Varinance:  7.885825215100419e-05 \n",
      "\n",
      "Epoch:  9063  Learning Rate:  1.1599076292331117e-05  Varinance:  7.874809006324365e-05 \n",
      "\n",
      "Epoch:  9064  Learning Rate:  1.1587483013644222e-05  Varinance:  7.863808186788164e-05 \n",
      "\n",
      "Epoch:  9065  Learning Rate:  1.1575901322441328e-05  Varinance:  7.852822734993629e-05 \n",
      "\n",
      "Epoch:  9066  Learning Rate:  1.156433120714072e-05  Varinance:  7.8418526294726e-05 \n",
      "\n",
      "Epoch:  9067  Learning Rate:  1.155277265617226e-05  Varinance:  7.830897848786869e-05 \n",
      "\n",
      "Epoch:  9068  Learning Rate:  1.1541225657977442e-05  Varinance:  7.819958371528215e-05 \n",
      "\n",
      "Epoch:  9069  Learning Rate:  1.1529690201009243e-05  Varinance:  7.809034176318314e-05 \n",
      "\n",
      "Epoch:  9070  Learning Rate:  1.1518166273732185e-05  Varinance:  7.798125241808715e-05 \n",
      "\n",
      "Epoch:  9071  Learning Rate:  1.1506653864622382e-05  Varinance:  7.787231546680753e-05 \n",
      "\n",
      "Epoch:  9072  Learning Rate:  1.1495152962167402e-05  Varinance:  7.776353069645585e-05 \n",
      "\n",
      "Epoch:  9073  Learning Rate:  1.1483663554866342e-05  Varinance:  7.765489789444105e-05 \n",
      "\n",
      "Epoch:  9074  Learning Rate:  1.1472185631229773e-05  Varinance:  7.754641684846867e-05 \n",
      "\n",
      "Epoch:  9075  Learning Rate:  1.1460719179779813e-05  Varinance:  7.743808734654121e-05 \n",
      "\n",
      "Epoch:  9076  Learning Rate:  1.1449264189049987e-05  Varinance:  7.732990917695721e-05 \n",
      "\n",
      "Epoch:  9077  Learning Rate:  1.1437820647585285e-05  Varinance:  7.722188212831106e-05 \n",
      "\n",
      "Epoch:  9078  Learning Rate:  1.14263885439422e-05  Varinance:  7.711400598949206e-05 \n",
      "\n",
      "Epoch:  9079  Learning Rate:  1.1414967866688616e-05  Varinance:  7.700628054968488e-05 \n",
      "\n",
      "Epoch:  9080  Learning Rate:  1.1403558604403828e-05  Varinance:  7.689870559836864e-05 \n",
      "\n",
      "Epoch:  9081  Learning Rate:  1.1392160745678614e-05  Varinance:  7.679128092531618e-05 \n",
      "\n",
      "Epoch:  9082  Learning Rate:  1.1380774279115096e-05  Varinance:  7.668400632059438e-05 \n",
      "\n",
      "Epoch:  9083  Learning Rate:  1.1369399193326783e-05  Varinance:  7.65768815745633e-05 \n",
      "\n",
      "Epoch:  9084  Learning Rate:  1.1358035476938634e-05  Varinance:  7.646990647787599e-05 \n",
      "\n",
      "Epoch:  9085  Learning Rate:  1.1346683118586908e-05  Varinance:  7.636308082147752e-05 \n",
      "\n",
      "Epoch:  9086  Learning Rate:  1.1335342106919225e-05  Varinance:  7.625640439660542e-05 \n",
      "\n",
      "Epoch:  9087  Learning Rate:  1.1324012430594614e-05  Varinance:  7.614987699478885e-05 \n",
      "\n",
      "Epoch:  9088  Learning Rate:  1.1312694078283377e-05  Varinance:  7.604349840784778e-05 \n",
      "\n",
      "Epoch:  9089  Learning Rate:  1.1301387038667162e-05  Varinance:  7.593726842789345e-05 \n",
      "\n",
      "Epoch:  9090  Learning Rate:  1.1290091300438906e-05  Varinance:  7.583118684732732e-05 \n",
      "\n",
      "Epoch:  9091  Learning Rate:  1.1278806852302913e-05  Varinance:  7.572525345884103e-05 \n",
      "\n",
      "Epoch:  9092  Learning Rate:  1.126753368297471e-05  Varinance:  7.561946805541541e-05 \n",
      "\n",
      "Epoch:  9093  Learning Rate:  1.1256271781181111e-05  Varinance:  7.55138304303209e-05 \n",
      "\n",
      "Epoch:  9094  Learning Rate:  1.124502113566025e-05  Varinance:  7.540834037711668e-05 \n",
      "\n",
      "Epoch:  9095  Learning Rate:  1.1233781735161463e-05  Varinance:  7.530299768965e-05 \n",
      "\n",
      "Epoch:  9096  Learning Rate:  1.1222553568445325e-05  Varinance:  7.519780216205646e-05 \n",
      "\n",
      "Epoch:  9097  Learning Rate:  1.1211336624283712e-05  Varinance:  7.509275358875908e-05 \n",
      "\n",
      "Epoch:  9098  Learning Rate:  1.120013089145966e-05  Varinance:  7.498785176446826e-05 \n",
      "\n",
      "Epoch:  9099  Learning Rate:  1.1188936358767407e-05  Varinance:  7.488309648418073e-05 \n",
      "\n",
      "Epoch:  9100  Learning Rate:  1.1177753015012471e-05  Varinance:  7.477848754318004e-05 \n",
      "\n",
      "Epoch:  9101  Learning Rate:  1.1166580849011478e-05  Varinance:  7.467402473703567e-05 \n",
      "\n",
      "Epoch:  9102  Learning Rate:  1.1155419849592246e-05  Varinance:  7.456970786160237e-05 \n",
      "\n",
      "Epoch:  9103  Learning Rate:  1.1144270005593813e-05  Varinance:  7.446553671302037e-05 \n",
      "\n",
      "Epoch:  9104  Learning Rate:  1.1133131305866315e-05  Varinance:  7.436151108771463e-05 \n",
      "\n",
      "Epoch:  9105  Learning Rate:  1.1122003739271049e-05  Varinance:  7.425763078239457e-05 \n",
      "\n",
      "Epoch:  9106  Learning Rate:  1.111088729468043e-05  Varinance:  7.415389559405327e-05 \n",
      "\n",
      "Epoch:  9107  Learning Rate:  1.109978196097805e-05  Varinance:  7.405030531996772e-05 \n",
      "\n",
      "Epoch:  9108  Learning Rate:  1.1088687727058557e-05  Varinance:  7.39468597576982e-05 \n",
      "\n",
      "Epoch:  9109  Learning Rate:  1.1077604581827697e-05  Varinance:  7.384355870508729e-05 \n",
      "\n",
      "Epoch:  9110  Learning Rate:  1.1066532514202361e-05  Varinance:  7.374040196026044e-05 \n",
      "\n",
      "Epoch:  9111  Learning Rate:  1.105547151311046e-05  Varinance:  7.3637389321625e-05 \n",
      "\n",
      "Epoch:  9112  Learning Rate:  1.1044421567490975e-05  Varinance:  7.353452058786999e-05 \n",
      "\n",
      "Epoch:  9113  Learning Rate:  1.1033382666293998e-05  Varinance:  7.343179555796533e-05 \n",
      "\n",
      "Epoch:  9114  Learning Rate:  1.1022354798480604e-05  Varinance:  7.332921403116213e-05 \n",
      "\n",
      "Epoch:  9115  Learning Rate:  1.101133795302291e-05  Varinance:  7.322677580699197e-05 \n",
      "\n",
      "Epoch:  9116  Learning Rate:  1.1000332118904105e-05  Varinance:  7.3124480685266e-05 \n",
      "\n",
      "Epoch:  9117  Learning Rate:  1.0989337285118337e-05  Varinance:  7.302232846607557e-05 \n",
      "\n",
      "Epoch:  9118  Learning Rate:  1.0978353440670749e-05  Varinance:  7.2920318949791e-05 \n",
      "\n",
      "Epoch:  9119  Learning Rate:  1.0967380574577535e-05  Varinance:  7.281845193706174e-05 \n",
      "\n",
      "Epoch:  9120  Learning Rate:  1.0956418675865812e-05  Varinance:  7.271672722881527e-05 \n",
      "\n",
      "Epoch:  9121  Learning Rate:  1.0945467733573677e-05  Varinance:  7.261514462625755e-05 \n",
      "\n",
      "Epoch:  9122  Learning Rate:  1.0934527736750168e-05  Varinance:  7.251370393087231e-05 \n",
      "\n",
      "Epoch:  9123  Learning Rate:  1.0923598674455327e-05  Varinance:  7.241240494442016e-05 \n",
      "\n",
      "Epoch:  9124  Learning Rate:  1.0912680535760069e-05  Varinance:  7.231124746893904e-05 \n",
      "\n",
      "Epoch:  9125  Learning Rate:  1.0901773309746238e-05  Varinance:  7.221023130674329e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9126  Learning Rate:  1.0890876985506646e-05  Varinance:  7.21093562604236e-05 \n",
      "\n",
      "Epoch:  9127  Learning Rate:  1.0879991552144945e-05  Varinance:  7.200862213284596e-05 \n",
      "\n",
      "Epoch:  9128  Learning Rate:  1.0869116998775684e-05  Varinance:  7.190802872715224e-05 \n",
      "\n",
      "Epoch:  9129  Learning Rate:  1.0858253314524347e-05  Varinance:  7.18075758467592e-05 \n",
      "\n",
      "Epoch:  9130  Learning Rate:  1.0847400488527229e-05  Varinance:  7.170726329535791e-05 \n",
      "\n",
      "Epoch:  9131  Learning Rate:  1.0836558509931485e-05  Varinance:  7.160709087691405e-05 \n",
      "\n",
      "Epoch:  9132  Learning Rate:  1.0825727367895173e-05  Varinance:  7.150705839566701e-05 \n",
      "\n",
      "Epoch:  9133  Learning Rate:  1.081490705158713e-05  Varinance:  7.140716565612977e-05 \n",
      "\n",
      "Epoch:  9134  Learning Rate:  1.0804097550187041e-05  Varinance:  7.130741246308803e-05 \n",
      "\n",
      "Epoch:  9135  Learning Rate:  1.0793298852885383e-05  Varinance:  7.12077986216006e-05 \n",
      "\n",
      "Epoch:  9136  Learning Rate:  1.0782510948883497e-05  Varinance:  7.110832393699857e-05 \n",
      "\n",
      "Epoch:  9137  Learning Rate:  1.077173382739346e-05  Varinance:  7.100898821488463e-05 \n",
      "\n",
      "Epoch:  9138  Learning Rate:  1.0760967477638125e-05  Varinance:  7.09097912611334e-05 \n",
      "\n",
      "Epoch:  9139  Learning Rate:  1.0750211888851185e-05  Varinance:  7.08107328818907e-05 \n",
      "\n",
      "Epoch:  9140  Learning Rate:  1.073946705027703e-05  Varinance:  7.071181288357276e-05 \n",
      "\n",
      "Epoch:  9141  Learning Rate:  1.0728732951170802e-05  Varinance:  7.061303107286662e-05 \n",
      "\n",
      "Epoch:  9142  Learning Rate:  1.0718009580798436e-05  Varinance:  7.051438725672926e-05 \n",
      "\n",
      "Epoch:  9143  Learning Rate:  1.0707296928436546e-05  Varinance:  7.041588124238744e-05 \n",
      "\n",
      "Epoch:  9144  Learning Rate:  1.0696594983372459e-05  Varinance:  7.031751283733687e-05 \n",
      "\n",
      "Epoch:  9145  Learning Rate:  1.0685903734904263e-05  Varinance:  7.021928184934253e-05 \n",
      "\n",
      "Epoch:  9146  Learning Rate:  1.0675223172340693e-05  Varinance:  7.012118808643795e-05 \n",
      "\n",
      "Epoch:  9147  Learning Rate:  1.0664553285001167e-05  Varinance:  7.002323135692447e-05 \n",
      "\n",
      "Epoch:  9148  Learning Rate:  1.0653894062215832e-05  Varinance:  6.992541146937157e-05 \n",
      "\n",
      "Epoch:  9149  Learning Rate:  1.064324549332545e-05  Varinance:  6.982772823261609e-05 \n",
      "\n",
      "Epoch:  9150  Learning Rate:  1.0632607567681444e-05  Varinance:  6.973018145576194e-05 \n",
      "\n",
      "Epoch:  9151  Learning Rate:  1.0621980274645876e-05  Varinance:  6.963277094817944e-05 \n",
      "\n",
      "Epoch:  9152  Learning Rate:  1.0611363603591486e-05  Varinance:  6.953549651950551e-05 \n",
      "\n",
      "Epoch:  9153  Learning Rate:  1.0600757543901583e-05  Varinance:  6.943835797964305e-05 \n",
      "\n",
      "Epoch:  9154  Learning Rate:  1.059016208497009e-05  Varinance:  6.934135513876009e-05 \n",
      "\n",
      "Epoch:  9155  Learning Rate:  1.0579577216201582e-05  Varinance:  6.924448780729028e-05 \n",
      "\n",
      "Epoch:  9156  Learning Rate:  1.0569002927011172e-05  Varinance:  6.914775579593194e-05 \n",
      "\n",
      "Epoch:  9157  Learning Rate:  1.0558439206824552e-05  Varinance:  6.905115891564793e-05 \n",
      "\n",
      "Epoch:  9158  Learning Rate:  1.0547886045078036e-05  Varinance:  6.895469697766486e-05 \n",
      "\n",
      "Epoch:  9159  Learning Rate:  1.0537343431218445e-05  Varinance:  6.885836979347343e-05 \n",
      "\n",
      "Epoch:  9160  Learning Rate:  1.0526811354703143e-05  Varinance:  6.876217717482761e-05 \n",
      "\n",
      "Epoch:  9161  Learning Rate:  1.0516289805000094e-05  Varinance:  6.866611893374403e-05 \n",
      "\n",
      "Epoch:  9162  Learning Rate:  1.0505778771587726e-05  Varinance:  6.857019488250232e-05 \n",
      "\n",
      "Epoch:  9163  Learning Rate:  1.0495278243954985e-05  Varinance:  6.847440483364417e-05 \n",
      "\n",
      "Epoch:  9164  Learning Rate:  1.0484788211601382e-05  Varinance:  6.837874859997327e-05 \n",
      "\n",
      "Epoch:  9165  Learning Rate:  1.0474308664036864e-05  Varinance:  6.828322599455447e-05 \n",
      "\n",
      "Epoch:  9166  Learning Rate:  1.0463839590781884e-05  Varinance:  6.818783683071413e-05 \n",
      "\n",
      "Epoch:  9167  Learning Rate:  1.0453380981367347e-05  Varinance:  6.809258092203938e-05 \n",
      "\n",
      "Epoch:  9168  Learning Rate:  1.0442932825334681e-05  Varinance:  6.799745808237738e-05 \n",
      "\n",
      "Epoch:  9169  Learning Rate:  1.0432495112235711e-05  Varinance:  6.790246812583572e-05 \n",
      "\n",
      "Epoch:  9170  Learning Rate:  1.0422067831632704e-05  Varinance:  6.780761086678157e-05 \n",
      "\n",
      "Epoch:  9171  Learning Rate:  1.0411650973098416e-05  Varinance:  6.77128861198415e-05 \n",
      "\n",
      "Epoch:  9172  Learning Rate:  1.040124452621597e-05  Varinance:  6.761829369990073e-05 \n",
      "\n",
      "Epoch:  9173  Learning Rate:  1.0390848480578895e-05  Varinance:  6.752383342210338e-05 \n",
      "\n",
      "Epoch:  9174  Learning Rate:  1.0380462825791186e-05  Varinance:  6.742950510185187e-05 \n",
      "\n",
      "Epoch:  9175  Learning Rate:  1.037008755146717e-05  Varinance:  6.73353085548061e-05 \n",
      "\n",
      "Epoch:  9176  Learning Rate:  1.035972264723155e-05  Varinance:  6.724124359688384e-05 \n",
      "\n",
      "Epoch:  9177  Learning Rate:  1.0349368102719459e-05  Varinance:  6.714731004425989e-05 \n",
      "\n",
      "Epoch:  9178  Learning Rate:  1.0339023907576333e-05  Varinance:  6.705350771336603e-05 \n",
      "\n",
      "Epoch:  9179  Learning Rate:  1.0328690051457957e-05  Varinance:  6.695983642089004e-05 \n",
      "\n",
      "Epoch:  9180  Learning Rate:  1.0318366524030512e-05  Varinance:  6.686629598377619e-05 \n",
      "\n",
      "Epoch:  9181  Learning Rate:  1.0308053314970452e-05  Varinance:  6.677288621922443e-05 \n",
      "\n",
      "Epoch:  9182  Learning Rate:  1.0297750413964566e-05  Varinance:  6.667960694468978e-05 \n",
      "\n",
      "Epoch:  9183  Learning Rate:  1.0287457810709933e-05  Varinance:  6.658645797788256e-05 \n",
      "\n",
      "Epoch:  9184  Learning Rate:  1.0277175494913987e-05  Varinance:  6.649343913676767e-05 \n",
      "\n",
      "Epoch:  9185  Learning Rate:  1.026690345629439e-05  Varinance:  6.640055023956439e-05 \n",
      "\n",
      "Epoch:  9186  Learning Rate:  1.0256641684579089e-05  Varinance:  6.63077911047456e-05 \n",
      "\n",
      "Epoch:  9187  Learning Rate:  1.0246390169506346e-05  Varinance:  6.621516155103813e-05 \n",
      "\n",
      "Epoch:  9188  Learning Rate:  1.0236148900824625e-05  Varinance:  6.612266139742203e-05 \n",
      "\n",
      "Epoch:  9189  Learning Rate:  1.022591786829264e-05  Varinance:  6.60302904631299e-05 \n",
      "\n",
      "Epoch:  9190  Learning Rate:  1.0215697061679392e-05  Varinance:  6.593804856764718e-05 \n",
      "\n",
      "Epoch:  9191  Learning Rate:  1.0205486470764059e-05  Varinance:  6.584593553071137e-05 \n",
      "\n",
      "Epoch:  9192  Learning Rate:  1.0195286085336029e-05  Varinance:  6.575395117231197e-05 \n",
      "\n",
      "Epoch:  9193  Learning Rate:  1.0185095895194952e-05  Varinance:  6.56620953126895e-05 \n",
      "\n",
      "Epoch:  9194  Learning Rate:  1.0174915890150618e-05  Varinance:  6.557036777233599e-05 \n",
      "\n",
      "Epoch:  9195  Learning Rate:  1.0164746060023005e-05  Varinance:  6.547876837199428e-05 \n",
      "\n",
      "Epoch:  9196  Learning Rate:  1.0154586394642316e-05  Varinance:  6.538729693265722e-05 \n",
      "\n",
      "Epoch:  9197  Learning Rate:  1.014443688384887e-05  Varinance:  6.529595327556808e-05 \n",
      "\n",
      "Epoch:  9198  Learning Rate:  1.0134297517493152e-05  Varinance:  6.520473722221978e-05 \n",
      "\n",
      "Epoch:  9199  Learning Rate:  1.0124168285435778e-05  Varinance:  6.511364859435464e-05 \n",
      "\n",
      "Epoch:  9200  Learning Rate:  1.0114049177547551e-05  Varinance:  6.502268721396373e-05 \n",
      "\n",
      "Epoch:  9201  Learning Rate:  1.0103940183709343e-05  Varinance:  6.493185290328709e-05 \n",
      "\n",
      "Epoch:  9202  Learning Rate:  1.0093841293812143e-05  Varinance:  6.484114548481313e-05 \n",
      "\n",
      "Epoch:  9203  Learning Rate:  1.0083752497757097e-05  Varinance:  6.475056478127783e-05 \n",
      "\n",
      "Epoch:  9204  Learning Rate:  1.0073673785455391e-05  Varinance:  6.466011061566521e-05 \n",
      "\n",
      "Epoch:  9205  Learning Rate:  1.006360514682829e-05  Varinance:  6.456978281120652e-05 \n",
      "\n",
      "Epoch:  9206  Learning Rate:  1.0053546571807191e-05  Varinance:  6.447958119137964e-05 \n",
      "\n",
      "Epoch:  9207  Learning Rate:  1.0043498050333504e-05  Varinance:  6.43895055799094e-05 \n",
      "\n",
      "Epoch:  9208  Learning Rate:  1.0033459572358685e-05  Varinance:  6.42995558007667e-05 \n",
      "\n",
      "Epoch:  9209  Learning Rate:  1.0023431127844293e-05  Varinance:  6.420973167816854e-05 \n",
      "\n",
      "Epoch:  9210  Learning Rate:  1.0013412706761864e-05  Varinance:  6.41200330365771e-05 \n",
      "\n",
      "Epoch:  9211  Learning Rate:  1.0003404299092957e-05  Varinance:  6.403045970070011e-05 \n",
      "\n",
      "Epoch:  9212  Learning Rate:  9.993405894829202e-06  Varinance:  6.394101149549022e-05 \n",
      "\n",
      "Epoch:  9213  Learning Rate:  9.983417483972174e-06  Varinance:  6.385168824614422e-05 \n",
      "\n",
      "Epoch:  9214  Learning Rate:  9.973439056533463e-06  Varinance:  6.376248977810348e-05 \n",
      "\n",
      "Epoch:  9215  Learning Rate:  9.963470602534622e-06  Varinance:  6.367341591705309e-05 \n",
      "\n",
      "Epoch:  9216  Learning Rate:  9.95351211200723e-06  Varinance:  6.358446648892176e-05 \n",
      "\n",
      "Epoch:  9217  Learning Rate:  9.943563574992781e-06  Varinance:  6.349564131988103e-05 \n",
      "\n",
      "Epoch:  9218  Learning Rate:  9.933624981542716e-06  Varinance:  6.340694023634563e-05 \n",
      "\n",
      "Epoch:  9219  Learning Rate:  9.923696321718479e-06  Varinance:  6.331836306497276e-05 \n",
      "\n",
      "Epoch:  9220  Learning Rate:  9.913777585591392e-06  Varinance:  6.322990963266146e-05 \n",
      "\n",
      "Epoch:  9221  Learning Rate:  9.903868763242697e-06  Varinance:  6.314157976655289e-05 \n",
      "\n",
      "Epoch:  9222  Learning Rate:  9.89396984476361e-06  Varinance:  6.305337329402962e-05 \n",
      "\n",
      "Epoch:  9223  Learning Rate:  9.884080820255193e-06  Varinance:  6.296529004271541e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9224  Learning Rate:  9.8742016798284e-06  Varinance:  6.287732984047456e-05 \n",
      "\n",
      "Epoch:  9225  Learning Rate:  9.864332413604129e-06  Varinance:  6.27894925154121e-05 \n",
      "\n",
      "Epoch:  9226  Learning Rate:  9.854473011713092e-06  Varinance:  6.270177789587327e-05 \n",
      "\n",
      "Epoch:  9227  Learning Rate:  9.84462346429587e-06  Varinance:  6.26141858104427e-05 \n",
      "\n",
      "Epoch:  9228  Learning Rate:  9.834783761502951e-06  Varinance:  6.252671608794485e-05 \n",
      "\n",
      "Epoch:  9229  Learning Rate:  9.824953893494615e-06  Varinance:  6.243936855744318e-05 \n",
      "\n",
      "Epoch:  9230  Learning Rate:  9.815133850440988e-06  Varinance:  6.235214304824007e-05 \n",
      "\n",
      "Epoch:  9231  Learning Rate:  9.805323622522015e-06  Varinance:  6.226503938987602e-05 \n",
      "\n",
      "Epoch:  9232  Learning Rate:  9.795523199927496e-06  Varinance:  6.217805741212997e-05 \n",
      "\n",
      "Epoch:  9233  Learning Rate:  9.785732572856994e-06  Varinance:  6.20911969450187e-05 \n",
      "\n",
      "Epoch:  9234  Learning Rate:  9.775951731519864e-06  Varinance:  6.200445781879604e-05 \n",
      "\n",
      "Epoch:  9235  Learning Rate:  9.766180666135299e-06  Varinance:  6.191783986395338e-05 \n",
      "\n",
      "Epoch:  9236  Learning Rate:  9.756419366932212e-06  Varinance:  6.183134291121868e-05 \n",
      "\n",
      "Epoch:  9237  Learning Rate:  9.746667824149287e-06  Varinance:  6.17449667915566e-05 \n",
      "\n",
      "Epoch:  9238  Learning Rate:  9.736926028035018e-06  Varinance:  6.165871133616751e-05 \n",
      "\n",
      "Epoch:  9239  Learning Rate:  9.727193968847586e-06  Varinance:  6.157257637648797e-05 \n",
      "\n",
      "Epoch:  9240  Learning Rate:  9.717471636854917e-06  Varinance:  6.148656174419e-05 \n",
      "\n",
      "Epoch:  9241  Learning Rate:  9.707759022334712e-06  Varinance:  6.140066727118042e-05 \n",
      "\n",
      "Epoch:  9242  Learning Rate:  9.69805611557434e-06  Varinance:  6.131489278960124e-05 \n",
      "\n",
      "Epoch:  9243  Learning Rate:  9.688362906870871e-06  Varinance:  6.122923813182882e-05 \n",
      "\n",
      "Epoch:  9244  Learning Rate:  9.678679386531136e-06  Varinance:  6.114370313047381e-05 \n",
      "\n",
      "Epoch:  9245  Learning Rate:  9.669005544871594e-06  Varinance:  6.105828761838036e-05 \n",
      "\n",
      "Epoch:  9246  Learning Rate:  9.659341372218401e-06  Varinance:  6.0972991428626457e-05 \n",
      "\n",
      "Epoch:  9247  Learning Rate:  9.64968685890737e-06  Varinance:  6.088781439452327e-05 \n",
      "\n",
      "Epoch:  9248  Learning Rate:  9.640041995284018e-06  Varinance:  6.0802756349614516e-05 \n",
      "\n",
      "Epoch:  9249  Learning Rate:  9.630406771703465e-06  Varinance:  6.0717817127676756e-05 \n",
      "\n",
      "Epoch:  9250  Learning Rate:  9.620781178530468e-06  Varinance:  6.063299656271866e-05 \n",
      "\n",
      "Epoch:  9251  Learning Rate:  9.61116520613947e-06  Varinance:  6.0548294488980866e-05 \n",
      "\n",
      "Epoch:  9252  Learning Rate:  9.601558844914479e-06  Varinance:  6.04637107409353e-05 \n",
      "\n",
      "Epoch:  9253  Learning Rate:  9.591962085249114e-06  Varinance:  6.037924515328538e-05 \n",
      "\n",
      "Epoch:  9254  Learning Rate:  9.582374917546652e-06  Varinance:  6.029489756096549e-05 \n",
      "\n",
      "Epoch:  9255  Learning Rate:  9.572797332219908e-06  Varinance:  6.021066779914026e-05 \n",
      "\n",
      "Epoch:  9256  Learning Rate:  9.563229319691274e-06  Varinance:  6.0126555703204915e-05 \n",
      "\n",
      "Epoch:  9257  Learning Rate:  9.553670870392775e-06  Varinance:  6.004256110878453e-05 \n",
      "\n",
      "Epoch:  9258  Learning Rate:  9.544121974765942e-06  Varinance:  5.9958683851733884e-05 \n",
      "\n",
      "Epoch:  9259  Learning Rate:  9.534582623261878e-06  Varinance:  5.9874923768136776e-05 \n",
      "\n",
      "Epoch:  9260  Learning Rate:  9.525052806341217e-06  Varinance:  5.979128069430629e-05 \n",
      "\n",
      "Epoch:  9261  Learning Rate:  9.515532514474173e-06  Varinance:  5.9707754466784155e-05 \n",
      "\n",
      "Epoch:  9262  Learning Rate:  9.506021738140436e-06  Varinance:  5.9624344922340185e-05 \n",
      "\n",
      "Epoch:  9263  Learning Rate:  9.496520467829212e-06  Varinance:  5.9541051897972476e-05 \n",
      "\n",
      "Epoch:  9264  Learning Rate:  9.487028694039263e-06  Varinance:  5.945787523090676e-05 \n",
      "\n",
      "Epoch:  9265  Learning Rate:  9.4775464072788e-06  Varinance:  5.937481475859627e-05 \n",
      "\n",
      "Epoch:  9266  Learning Rate:  9.468073598065518e-06  Varinance:  5.9291870318720946e-05 \n",
      "\n",
      "Epoch:  9267  Learning Rate:  9.458610256926638e-06  Varinance:  5.9209041749187876e-05 \n",
      "\n",
      "Epoch:  9268  Learning Rate:  9.449156374398806e-06  Varinance:  5.912632888813052e-05 \n",
      "\n",
      "Epoch:  9269  Learning Rate:  9.439711941028116e-06  Varinance:  5.904373157390817e-05 \n",
      "\n",
      "Epoch:  9270  Learning Rate:  9.43027694737017e-06  Varinance:  5.8961249645106243e-05 \n",
      "\n",
      "Epoch:  9271  Learning Rate:  9.42085138398996e-06  Varinance:  5.887888294053564e-05 \n",
      "\n",
      "Epoch:  9272  Learning Rate:  9.4114352414619e-06  Varinance:  5.8796631299232126e-05 \n",
      "\n",
      "Epoch:  9273  Learning Rate:  9.402028510369884e-06  Varinance:  5.8714494560456634e-05 \n",
      "\n",
      "Epoch:  9274  Learning Rate:  9.392631181307162e-06  Varinance:  5.8632472563694546e-05 \n",
      "\n",
      "Epoch:  9275  Learning Rate:  9.383243244876405e-06  Varinance:  5.855056514865558e-05 \n",
      "\n",
      "Epoch:  9276  Learning Rate:  9.373864691689656e-06  Varinance:  5.846877215527306e-05 \n",
      "\n",
      "Epoch:  9277  Learning Rate:  9.364495512368396e-06  Varinance:  5.838709342370423e-05 \n",
      "\n",
      "Epoch:  9278  Learning Rate:  9.35513569754343e-06  Varinance:  5.830552879432963e-05 \n",
      "\n",
      "Epoch:  9279  Learning Rate:  9.345785237854924e-06  Varinance:  5.8224078107752474e-05 \n",
      "\n",
      "Epoch:  9280  Learning Rate:  9.336444123952451e-06  Varinance:  5.814274120479893e-05 \n",
      "\n",
      "Epoch:  9281  Learning Rate:  9.32711234649488e-06  Varinance:  5.806151792651749e-05 \n",
      "\n",
      "Epoch:  9282  Learning Rate:  9.317789896150418e-06  Varinance:  5.7980408114178706e-05 \n",
      "\n",
      "Epoch:  9283  Learning Rate:  9.308476763596644e-06  Varinance:  5.789941160927464e-05 \n",
      "\n",
      "Epoch:  9284  Learning Rate:  9.29917293952041e-06  Varinance:  5.781852825351907e-05 \n",
      "\n",
      "Epoch:  9285  Learning Rate:  9.289878414617873e-06  Varinance:  5.773775788884685e-05 \n",
      "\n",
      "Epoch:  9286  Learning Rate:  9.28059317959454e-06  Varinance:  5.7657100357413406e-05 \n",
      "\n",
      "Epoch:  9287  Learning Rate:  9.271317225165163e-06  Varinance:  5.757655550159492e-05 \n",
      "\n",
      "Epoch:  9288  Learning Rate:  9.262050542053765e-06  Varinance:  5.7496123163987696e-05 \n",
      "\n",
      "Epoch:  9289  Learning Rate:  9.252793120993698e-06  Varinance:  5.741580318740803e-05 \n",
      "\n",
      "Epoch:  9290  Learning Rate:  9.243544952727523e-06  Varinance:  5.733559541489146e-05 \n",
      "\n",
      "Epoch:  9291  Learning Rate:  9.234306028007072e-06  Varinance:  5.725549968969314e-05 \n",
      "\n",
      "Epoch:  9292  Learning Rate:  9.2250763375934e-06  Varinance:  5.717551585528717e-05 \n",
      "\n",
      "Epoch:  9293  Learning Rate:  9.215855872256854e-06  Varinance:  5.709564375536602e-05 \n",
      "\n",
      "Epoch:  9294  Learning Rate:  9.206644622776946e-06  Varinance:  5.701588323384079e-05 \n",
      "\n",
      "Epoch:  9295  Learning Rate:  9.197442579942413e-06  Varinance:  5.693623413484057e-05 \n",
      "\n",
      "Epoch:  9296  Learning Rate:  9.188249734551241e-06  Varinance:  5.685669630271228e-05 \n",
      "\n",
      "Epoch:  9297  Learning Rate:  9.17906607741057e-06  Varinance:  5.6777269582019965e-05 \n",
      "\n",
      "Epoch:  9298  Learning Rate:  9.169891599336726e-06  Varinance:  5.669795381754512e-05 \n",
      "\n",
      "Epoch:  9299  Learning Rate:  9.16072629115526e-06  Varinance:  5.6618748854286075e-05 \n",
      "\n",
      "Epoch:  9300  Learning Rate:  9.151570143700847e-06  Varinance:  5.653965453745741e-05 \n",
      "\n",
      "Epoch:  9301  Learning Rate:  9.142423147817328e-06  Varinance:  5.646067071249021e-05 \n",
      "\n",
      "Epoch:  9302  Learning Rate:  9.133285294357732e-06  Varinance:  5.63817972250314e-05 \n",
      "\n",
      "Epoch:  9303  Learning Rate:  9.124156574184195e-06  Varinance:  5.6303033920943595e-05 \n",
      "\n",
      "Epoch:  9304  Learning Rate:  9.115036978167973e-06  Varinance:  5.6224380646304504e-05 \n",
      "\n",
      "Epoch:  9305  Learning Rate:  9.105926497189505e-06  Varinance:  5.6145837247407104e-05 \n",
      "\n",
      "Epoch:  9306  Learning Rate:  9.096825122138294e-06  Varinance:  5.6067403570759104e-05 \n",
      "\n",
      "Epoch:  9307  Learning Rate:  9.087732843912964e-06  Varinance:  5.598907946308234e-05 \n",
      "\n",
      "Epoch:  9308  Learning Rate:  9.078649653421218e-06  Varinance:  5.59108647713131e-05 \n",
      "\n",
      "Epoch:  9309  Learning Rate:  9.069575541579899e-06  Varinance:  5.583275934260135e-05 \n",
      "\n",
      "Epoch:  9310  Learning Rate:  9.060510499314877e-06  Varinance:  5.575476302431071e-05 \n",
      "\n",
      "Epoch:  9311  Learning Rate:  9.051454517561092e-06  Varinance:  5.567687566401772e-05 \n",
      "\n",
      "Epoch:  9312  Learning Rate:  9.042407587262597e-06  Varinance:  5.559909710951216e-05 \n",
      "\n",
      "Epoch:  9313  Learning Rate:  9.033369699372442e-06  Varinance:  5.5521427208796414e-05 \n",
      "\n",
      "Epoch:  9314  Learning Rate:  9.024340844852722e-06  Varinance:  5.544386581008494e-05 \n",
      "\n",
      "Epoch:  9315  Learning Rate:  9.015321014674616e-06  Varinance:  5.536641276180451e-05 \n",
      "\n",
      "Epoch:  9316  Learning Rate:  9.006310199818277e-06  Varinance:  5.528906791259353e-05 \n",
      "\n",
      "Epoch:  9317  Learning Rate:  8.99730839127287e-06  Varinance:  5.521183111130198e-05 \n",
      "\n",
      "Epoch:  9318  Learning Rate:  8.988315580036622e-06  Varinance:  5.513470220699068e-05 \n",
      "\n",
      "Epoch:  9319  Learning Rate:  8.9793317571167e-06  Varinance:  5.505768104893159e-05 \n",
      "\n",
      "Epoch:  9320  Learning Rate:  8.970356913529271e-06  Varinance:  5.4980767486607245e-05 \n",
      "\n",
      "Epoch:  9321  Learning Rate:  8.961391040299518e-06  Varinance:  5.4903961369710164e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9322  Learning Rate:  8.952434128461551e-06  Varinance:  5.4827262548143107e-05 \n",
      "\n",
      "Epoch:  9323  Learning Rate:  8.943486169058459e-06  Varinance:  5.475067087201845e-05 \n",
      "\n",
      "Epoch:  9324  Learning Rate:  8.934547153142267e-06  Varinance:  5.467418619165801e-05 \n",
      "\n",
      "Epoch:  9325  Learning Rate:  8.925617071773987e-06  Varinance:  5.459780835759246e-05 \n",
      "\n",
      "Epoch:  9326  Learning Rate:  8.916695916023523e-06  Varinance:  5.4521537220561495e-05 \n",
      "\n",
      "Epoch:  9327  Learning Rate:  8.907783676969702e-06  Varinance:  5.444537263151342e-05 \n",
      "\n",
      "Epoch:  9328  Learning Rate:  8.898880345700316e-06  Varinance:  5.4369314441604374e-05 \n",
      "\n",
      "Epoch:  9329  Learning Rate:  8.889985913312017e-06  Varinance:  5.4293362502198774e-05 \n",
      "\n",
      "Epoch:  9330  Learning Rate:  8.881100370910358e-06  Varinance:  5.421751666486857e-05 \n",
      "\n",
      "Epoch:  9331  Learning Rate:  8.872223709609823e-06  Varinance:  5.414177678139312e-05 \n",
      "\n",
      "Epoch:  9332  Learning Rate:  8.863355920533739e-06  Varinance:  5.406614270375861e-05 \n",
      "\n",
      "Epoch:  9333  Learning Rate:  8.854496994814298e-06  Varinance:  5.3990614284158225e-05 \n",
      "\n",
      "Epoch:  9334  Learning Rate:  8.845646923592606e-06  Varinance:  5.391519137499168e-05 \n",
      "\n",
      "Epoch:  9335  Learning Rate:  8.836805698018575e-06  Varinance:  5.383987382886457e-05 \n",
      "\n",
      "Epoch:  9336  Learning Rate:  8.82797330925096e-06  Varinance:  5.37646614985887e-05 \n",
      "\n",
      "Epoch:  9337  Learning Rate:  8.819149748457408e-06  Varinance:  5.368955423718146e-05 \n",
      "\n",
      "Epoch:  9338  Learning Rate:  8.81033500681434e-06  Varinance:  5.36145518978653e-05 \n",
      "\n",
      "Epoch:  9339  Learning Rate:  8.80152907550701e-06  Varinance:  5.353965433406801e-05 \n",
      "\n",
      "Epoch:  9340  Learning Rate:  8.792731945729475e-06  Varinance:  5.346486139942203e-05 \n",
      "\n",
      "Epoch:  9341  Learning Rate:  8.783943608684635e-06  Varinance:  5.339017294776436e-05 \n",
      "\n",
      "Epoch:  9342  Learning Rate:  8.775164055584135e-06  Varinance:  5.33155888331359e-05 \n",
      "\n",
      "Epoch:  9343  Learning Rate:  8.766393277648405e-06  Varinance:  5.3241108909781754e-05 \n",
      "\n",
      "Epoch:  9344  Learning Rate:  8.7576312661067e-06  Varinance:  5.316673303215062e-05 \n",
      "\n",
      "Epoch:  9345  Learning Rate:  8.748878012196992e-06  Varinance:  5.309246105489425e-05 \n",
      "\n",
      "Epoch:  9346  Learning Rate:  8.740133507166008e-06  Varinance:  5.301829283286771e-05 \n",
      "\n",
      "Epoch:  9347  Learning Rate:  8.731397742269276e-06  Varinance:  5.294422822112876e-05 \n",
      "\n",
      "Epoch:  9348  Learning Rate:  8.722670708771014e-06  Varinance:  5.287026707493772e-05 \n",
      "\n",
      "Epoch:  9349  Learning Rate:  8.713952397944171e-06  Varinance:  5.279640924975682e-05 \n",
      "\n",
      "Epoch:  9350  Learning Rate:  8.705242801070469e-06  Varinance:  5.272265460125048e-05 \n",
      "\n",
      "Epoch:  9351  Learning Rate:  8.696541909440293e-06  Varinance:  5.264900298528478e-05 \n",
      "\n",
      "Epoch:  9352  Learning Rate:  8.687849714352735e-06  Varinance:  5.2575454257926844e-05 \n",
      "\n",
      "Epoch:  9353  Learning Rate:  8.67916620711563e-06  Varinance:  5.250200827544515e-05 \n",
      "\n",
      "Epoch:  9354  Learning Rate:  8.670491379045458e-06  Varinance:  5.242866489430887e-05 \n",
      "\n",
      "Epoch:  9355  Learning Rate:  8.661825221467385e-06  Varinance:  5.235542397118778e-05 \n",
      "\n",
      "Epoch:  9356  Learning Rate:  8.65316772571524e-06  Varinance:  5.2282285362951603e-05 \n",
      "\n",
      "Epoch:  9357  Learning Rate:  8.64451888313156e-06  Varinance:  5.2209248926670286e-05 \n",
      "\n",
      "Epoch:  9358  Learning Rate:  8.635878685067483e-06  Varinance:  5.213631451961344e-05 \n",
      "\n",
      "Epoch:  9359  Learning Rate:  8.627247122882793e-06  Varinance:  5.2063481999249805e-05 \n",
      "\n",
      "Epoch:  9360  Learning Rate:  8.618624187945962e-06  Varinance:  5.19907512232475e-05 \n",
      "\n",
      "Epoch:  9361  Learning Rate:  8.610009871634035e-06  Varinance:  5.191812204947337e-05 \n",
      "\n",
      "Epoch:  9362  Learning Rate:  8.601404165332684e-06  Varinance:  5.1845594335992926e-05 \n",
      "\n",
      "Epoch:  9363  Learning Rate:  8.592807060436229e-06  Varinance:  5.177316794106966e-05 \n",
      "\n",
      "Epoch:  9364  Learning Rate:  8.584218548347552e-06  Varinance:  5.170084272316537e-05 \n",
      "\n",
      "Epoch:  9365  Learning Rate:  8.575638620478123e-06  Varinance:  5.162861854093955e-05 \n",
      "\n",
      "Epoch:  9366  Learning Rate:  8.567067268248044e-06  Varinance:  5.1556495253248885e-05 \n",
      "\n",
      "Epoch:  9367  Learning Rate:  8.558504483085946e-06  Varinance:  5.1484472719147494e-05 \n",
      "\n",
      "Epoch:  9368  Learning Rate:  8.549950256429031e-06  Varinance:  5.14125507978863e-05 \n",
      "\n",
      "Epoch:  9369  Learning Rate:  8.5414045797231e-06  Varinance:  5.1340729348912956e-05 \n",
      "\n",
      "Epoch:  9370  Learning Rate:  8.532867444422459e-06  Varinance:  5.126900823187117e-05 \n",
      "\n",
      "Epoch:  9371  Learning Rate:  8.524338841989975e-06  Varinance:  5.1197387306601014e-05 \n",
      "\n",
      "Epoch:  9372  Learning Rate:  8.515818763897028e-06  Varinance:  5.112586643313832e-05 \n",
      "\n",
      "Epoch:  9373  Learning Rate:  8.507307201623569e-06  Varinance:  5.105444547171423e-05 \n",
      "\n",
      "Epoch:  9374  Learning Rate:  8.49880414665802e-06  Varinance:  5.0983124282755355e-05 \n",
      "\n",
      "Epoch:  9375  Learning Rate:  8.490309590497312e-06  Varinance:  5.091190272688323e-05 \n",
      "\n",
      "Epoch:  9376  Learning Rate:  8.481823524646916e-06  Varinance:  5.084078066491416e-05 \n",
      "\n",
      "Epoch:  9377  Learning Rate:  8.473345940620752e-06  Varinance:  5.0769757957858655e-05 \n",
      "\n",
      "Epoch:  9378  Learning Rate:  8.46487682994122e-06  Varinance:  5.069883446692162e-05 \n",
      "\n",
      "Epoch:  9379  Learning Rate:  8.456416184139238e-06  Varinance:  5.062801005350186e-05 \n",
      "\n",
      "Epoch:  9380  Learning Rate:  8.447963994754146e-06  Varinance:  5.055728457919153e-05 \n",
      "\n",
      "Epoch:  9381  Learning Rate:  8.439520253333737e-06  Varinance:  5.0486657905776425e-05 \n",
      "\n",
      "Epoch:  9382  Learning Rate:  8.431084951434299e-06  Varinance:  5.041612989523531e-05 \n",
      "\n",
      "Epoch:  9383  Learning Rate:  8.422658080620515e-06  Varinance:  5.034570040973986e-05 \n",
      "\n",
      "Epoch:  9384  Learning Rate:  8.414239632465514e-06  Varinance:  5.0275369311654e-05 \n",
      "\n",
      "Epoch:  9385  Learning Rate:  8.405829598550832e-06  Varinance:  5.020513646353424e-05 \n",
      "\n",
      "Epoch:  9386  Learning Rate:  8.397427970466465e-06  Varinance:  5.013500172812905e-05 \n",
      "\n",
      "Epoch:  9387  Learning Rate:  8.389034739810767e-06  Varinance:  5.00649649683784e-05 \n",
      "\n",
      "Epoch:  9388  Learning Rate:  8.380649898190491e-06  Varinance:  4.9995026047413966e-05 \n",
      "\n",
      "Epoch:  9389  Learning Rate:  8.372273437220829e-06  Varinance:  4.992518482855856e-05 \n",
      "\n",
      "Epoch:  9390  Learning Rate:  8.363905348525301e-06  Varinance:  4.9855441175326004e-05 \n",
      "\n",
      "Epoch:  9391  Learning Rate:  8.355545623735804e-06  Varinance:  4.978579495142052e-05 \n",
      "\n",
      "Epoch:  9392  Learning Rate:  8.347194254492642e-06  Varinance:  4.9716246020736996e-05 \n",
      "\n",
      "Epoch:  9393  Learning Rate:  8.33885123244443e-06  Varinance:  4.964679424736045e-05 \n",
      "\n",
      "Epoch:  9394  Learning Rate:  8.330516549248131e-06  Varinance:  4.957743949556553e-05 \n",
      "\n",
      "Epoch:  9395  Learning Rate:  8.32219019656909e-06  Varinance:  4.9508181629816704e-05 \n",
      "\n",
      "Epoch:  9396  Learning Rate:  8.313872166080938e-06  Varinance:  4.943902051476773e-05 \n",
      "\n",
      "Epoch:  9397  Learning Rate:  8.305562449465631e-06  Varinance:  4.936995601526154e-05 \n",
      "\n",
      "Epoch:  9398  Learning Rate:  8.29726103841348e-06  Varinance:  4.930098799632954e-05 \n",
      "\n",
      "Epoch:  9399  Learning Rate:  8.288967924623059e-06  Varinance:  4.923211632319201e-05 \n",
      "\n",
      "Epoch:  9400  Learning Rate:  8.280683099801254e-06  Varinance:  4.916334086125751e-05 \n",
      "\n",
      "Epoch:  9401  Learning Rate:  8.272406555663223e-06  Varinance:  4.9094661476122325e-05 \n",
      "\n",
      "Epoch:  9402  Learning Rate:  8.264138283932454e-06  Varinance:  4.9026078033570777e-05 \n",
      "\n",
      "Epoch:  9403  Learning Rate:  8.255878276340655e-06  Varinance:  4.895759039957468e-05 \n",
      "\n",
      "Epoch:  9404  Learning Rate:  8.247626524627808e-06  Varinance:  4.888919844029282e-05 \n",
      "\n",
      "Epoch:  9405  Learning Rate:  8.239383020542185e-06  Varinance:  4.882090202207123e-05 \n",
      "\n",
      "Epoch:  9406  Learning Rate:  8.23114775584027e-06  Varinance:  4.875270101144253e-05 \n",
      "\n",
      "Epoch:  9407  Learning Rate:  8.222920722286783e-06  Varinance:  4.86845952751259e-05 \n",
      "\n",
      "Epoch:  9408  Learning Rate:  8.214701911654718e-06  Varinance:  4.861658468002645e-05 \n",
      "\n",
      "Epoch:  9409  Learning Rate:  8.206491315725248e-06  Varinance:  4.854866909323547e-05 \n",
      "\n",
      "Epoch:  9410  Learning Rate:  8.198288926287765e-06  Varinance:  4.848084838202994e-05 \n",
      "\n",
      "Epoch:  9411  Learning Rate:  8.190094735139905e-06  Varinance:  4.8413122413871946e-05 \n",
      "\n",
      "Epoch:  9412  Learning Rate:  8.181908734087462e-06  Varinance:  4.834549105640902e-05 \n",
      "\n",
      "Epoch:  9413  Learning Rate:  8.173730914944422e-06  Varinance:  4.8277954177473496e-05 \n",
      "\n",
      "Epoch:  9414  Learning Rate:  8.16556126953299e-06  Varinance:  4.821051164508241e-05 \n",
      "\n",
      "Epoch:  9415  Learning Rate:  8.15739978968351e-06  Varinance:  4.8143163327436926e-05 \n",
      "\n",
      "Epoch:  9416  Learning Rate:  8.1492464672345e-06  Varinance:  4.8075909092922594e-05 \n",
      "\n",
      "Epoch:  9417  Learning Rate:  8.14110129403262e-06  Varinance:  4.80087488101088e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9418  Learning Rate:  8.132964261932728e-06  Varinance:  4.79416823477483e-05 \n",
      "\n",
      "Epoch:  9419  Learning Rate:  8.124835362797775e-06  Varinance:  4.7874709574777444e-05 \n",
      "\n",
      "Epoch:  9420  Learning Rate:  8.116714588498847e-06  Varinance:  4.7807830360315587e-05 \n",
      "\n",
      "Epoch:  9421  Learning Rate:  8.108601930915201e-06  Varinance:  4.774104457366501e-05 \n",
      "\n",
      "Epoch:  9422  Learning Rate:  8.100497381934159e-06  Varinance:  4.767435208431031e-05 \n",
      "\n",
      "Epoch:  9423  Learning Rate:  8.09240093345116e-06  Varinance:  4.7607752761918676e-05 \n",
      "\n",
      "Epoch:  9424  Learning Rate:  8.084312577369784e-06  Varinance:  4.754124647633934e-05 \n",
      "\n",
      "Epoch:  9425  Learning Rate:  8.076232305601659e-06  Varinance:  4.747483309760315e-05 \n",
      "\n",
      "Epoch:  9426  Learning Rate:  8.068160110066498e-06  Varinance:  4.740851249592271e-05 \n",
      "\n",
      "Epoch:  9427  Learning Rate:  8.060095982692134e-06  Varinance:  4.7342284541691887e-05 \n",
      "\n",
      "Epoch:  9428  Learning Rate:  8.052039915414424e-06  Varinance:  4.7276149105485675e-05 \n",
      "\n",
      "Epoch:  9429  Learning Rate:  8.043991900177285e-06  Varinance:  4.72101060580596e-05 \n",
      "\n",
      "Epoch:  9430  Learning Rate:  8.035951928932732e-06  Varinance:  4.714415527035002e-05 \n",
      "\n",
      "Epoch:  9431  Learning Rate:  8.02791999364078e-06  Varinance:  4.7078296613473575e-05 \n",
      "\n",
      "Epoch:  9432  Learning Rate:  8.019896086269487e-06  Varinance:  4.70125299587267e-05 \n",
      "\n",
      "Epoch:  9433  Learning Rate:  8.011880198794935e-06  Varinance:  4.694685517758587e-05 \n",
      "\n",
      "Epoch:  9434  Learning Rate:  8.003872323201265e-06  Varinance:  4.688127214170703e-05 \n",
      "\n",
      "Epoch:  9435  Learning Rate:  7.995872451480585e-06  Varinance:  4.68157807229255e-05 \n",
      "\n",
      "Epoch:  9436  Learning Rate:  7.987880575633009e-06  Varinance:  4.675038079325539e-05 \n",
      "\n",
      "Epoch:  9437  Learning Rate:  7.979896687666687e-06  Varinance:  4.668507222488983e-05 \n",
      "\n",
      "Epoch:  9438  Learning Rate:  7.971920779597717e-06  Varinance:  4.661985489020052e-05 \n",
      "\n",
      "Epoch:  9439  Learning Rate:  7.963952843450178e-06  Varinance:  4.65547286617372e-05 \n",
      "\n",
      "Epoch:  9440  Learning Rate:  7.955992871256161e-06  Varinance:  4.648969341222788e-05 \n",
      "\n",
      "Epoch:  9441  Learning Rate:  7.948040855055678e-06  Varinance:  4.64247490145783e-05 \n",
      "\n",
      "Epoch:  9442  Learning Rate:  7.940096786896697e-06  Varinance:  4.635989534187184e-05 \n",
      "\n",
      "Epoch:  9443  Learning Rate:  7.93216065883518e-06  Varinance:  4.629513226736889e-05 \n",
      "\n",
      "Epoch:  9444  Learning Rate:  7.924232462934982e-06  Varinance:  4.623045966450718e-05 \n",
      "\n",
      "Epoch:  9445  Learning Rate:  7.916312191267893e-06  Varinance:  4.616587740690121e-05 \n",
      "\n",
      "Epoch:  9446  Learning Rate:  7.90839983591367e-06  Varinance:  4.610138536834183e-05 \n",
      "\n",
      "Epoch:  9447  Learning Rate:  7.900495388959941e-06  Varinance:  4.60369834227964e-05 \n",
      "\n",
      "Epoch:  9448  Learning Rate:  7.89259884250226e-06  Varinance:  4.597267144440829e-05 \n",
      "\n",
      "Epoch:  9449  Learning Rate:  7.884710188644065e-06  Varinance:  4.590844930749676e-05 \n",
      "\n",
      "Epoch:  9450  Learning Rate:  7.87682941949673e-06  Varinance:  4.58443168865564e-05 \n",
      "\n",
      "Epoch:  9451  Learning Rate:  7.86895652717947e-06  Varinance:  4.5780274056257355e-05 \n",
      "\n",
      "Epoch:  9452  Learning Rate:  7.86109150381938e-06  Varinance:  4.571632069144488e-05 \n",
      "\n",
      "Epoch:  9453  Learning Rate:  7.853234341551463e-06  Varinance:  4.56524566671388e-05 \n",
      "\n",
      "Epoch:  9454  Learning Rate:  7.84538503251854e-06  Varinance:  4.5588681858533785e-05 \n",
      "\n",
      "Epoch:  9455  Learning Rate:  7.837543568871292e-06  Varinance:  4.552499614099877e-05 \n",
      "\n",
      "Epoch:  9456  Learning Rate:  7.829709942768277e-06  Varinance:  4.5461399390076875e-05 \n",
      "\n",
      "Epoch:  9457  Learning Rate:  7.82188414637586e-06  Varinance:  4.5397891481484844e-05 \n",
      "\n",
      "Epoch:  9458  Learning Rate:  7.814066171868225e-06  Varinance:  4.533447229111328e-05 \n",
      "\n",
      "Epoch:  9459  Learning Rate:  7.80625601142743e-06  Varinance:  4.527114169502615e-05 \n",
      "\n",
      "Epoch:  9460  Learning Rate:  7.798453657243296e-06  Varinance:  4.520789956946035e-05 \n",
      "\n",
      "Epoch:  9461  Learning Rate:  7.790659101513454e-06  Varinance:  4.514474579082585e-05 \n",
      "\n",
      "Epoch:  9462  Learning Rate:  7.782872336443375e-06  Varinance:  4.508168023570526e-05 \n",
      "\n",
      "Epoch:  9463  Learning Rate:  7.775093354246284e-06  Varinance:  4.501870278085363e-05 \n",
      "\n",
      "Epoch:  9464  Learning Rate:  7.767322147143195e-06  Varinance:  4.495581330319794e-05 \n",
      "\n",
      "Epoch:  9465  Learning Rate:  7.759558707362885e-06  Varinance:  4.489301167983737e-05 \n",
      "\n",
      "Epoch:  9466  Learning Rate:  7.751803027141943e-06  Varinance:  4.483029778804277e-05 \n",
      "\n",
      "Epoch:  9467  Learning Rate:  7.744055098724676e-06  Varinance:  4.476767150525619e-05 \n",
      "\n",
      "Epoch:  9468  Learning Rate:  7.736314914363138e-06  Varinance:  4.4705132709091146e-05 \n",
      "\n",
      "Epoch:  9469  Learning Rate:  7.728582466317172e-06  Varinance:  4.46426812773321e-05 \n",
      "\n",
      "Epoch:  9470  Learning Rate:  7.720857746854316e-06  Varinance:  4.458031708793403e-05 \n",
      "\n",
      "Epoch:  9471  Learning Rate:  7.713140748249839e-06  Varinance:  4.451804001902263e-05 \n",
      "\n",
      "Epoch:  9472  Learning Rate:  7.705431462786765e-06  Varinance:  4.4455849948893785e-05 \n",
      "\n",
      "Epoch:  9473  Learning Rate:  7.697729882755796e-06  Varinance:  4.439374675601345e-05 \n",
      "\n",
      "Epoch:  9474  Learning Rate:  7.690036000455339e-06  Varinance:  4.433173031901714e-05 \n",
      "\n",
      "Epoch:  9475  Learning Rate:  7.682349808191535e-06  Varinance:  4.426980051671015e-05 \n",
      "\n",
      "Epoch:  9476  Learning Rate:  7.674671298278182e-06  Varinance:  4.420795722806706e-05 \n",
      "\n",
      "Epoch:  9477  Learning Rate:  7.66700046303675e-06  Varinance:  4.414620033223131e-05 \n",
      "\n",
      "Epoch:  9478  Learning Rate:  7.659337294796435e-06  Varinance:  4.408452970851538e-05 \n",
      "\n",
      "Epoch:  9479  Learning Rate:  7.651681785894055e-06  Varinance:  4.4022945236400294e-05 \n",
      "\n",
      "Epoch:  9480  Learning Rate:  7.644033928674094e-06  Varinance:  4.3961446795535494e-05 \n",
      "\n",
      "Epoch:  9481  Learning Rate:  7.63639371548869e-06  Varinance:  4.390003426573832e-05 \n",
      "\n",
      "Epoch:  9482  Learning Rate:  7.628761138697648e-06  Varinance:  4.383870752699424e-05 \n",
      "\n",
      "Epoch:  9483  Learning Rate:  7.621136190668382e-06  Varinance:  4.377746645945637e-05 \n",
      "\n",
      "Epoch:  9484  Learning Rate:  7.613518863775927e-06  Varinance:  4.371631094344503e-05 \n",
      "\n",
      "Epoch:  9485  Learning Rate:  7.605909150402984e-06  Varinance:  4.365524085944794e-05 \n",
      "\n",
      "Epoch:  9486  Learning Rate:  7.598307042939826e-06  Varinance:  4.3594256088119714e-05 \n",
      "\n",
      "Epoch:  9487  Learning Rate:  7.59071253378433e-06  Varinance:  4.3533356510281764e-05 \n",
      "\n",
      "Epoch:  9488  Learning Rate:  7.583125615342015e-06  Varinance:  4.347254200692173e-05 \n",
      "\n",
      "Epoch:  9489  Learning Rate:  7.575546280025946e-06  Varinance:  4.341181245919379e-05 \n",
      "\n",
      "Epoch:  9490  Learning Rate:  7.567974520256776e-06  Varinance:  4.3351167748418094e-05 \n",
      "\n",
      "Epoch:  9491  Learning Rate:  7.5604103284627695e-06  Varinance:  4.3290607756080394e-05 \n",
      "\n",
      "Epoch:  9492  Learning Rate:  7.552853697079721e-06  Varinance:  4.32301323638322e-05 \n",
      "\n",
      "Epoch:  9493  Learning Rate:  7.545304618550987e-06  Varinance:  4.316974145349029e-05 \n",
      "\n",
      "Epoch:  9494  Learning Rate:  7.537763085327514e-06  Varinance:  4.310943490703662e-05 \n",
      "\n",
      "Epoch:  9495  Learning Rate:  7.5302290898677525e-06  Varinance:  4.304921260661778e-05 \n",
      "\n",
      "Epoch:  9496  Learning Rate:  7.522702624637709e-06  Varinance:  4.2989074434545216e-05 \n",
      "\n",
      "Epoch:  9497  Learning Rate:  7.515183682110905e-06  Varinance:  4.292902027329479e-05 \n",
      "\n",
      "Epoch:  9498  Learning Rate:  7.507672254768421e-06  Varinance:  4.286905000550631e-05 \n",
      "\n",
      "Epoch:  9499  Learning Rate:  7.500168335098818e-06  Varinance:  4.280916351398376e-05 \n",
      "\n",
      "Epoch:  9500  Learning Rate:  7.492671915598161e-06  Varinance:  4.274936068169478e-05 \n",
      "\n",
      "Epoch:  9501  Learning Rate:  7.48518298877006e-06  Varinance:  4.2689641391770544e-05 \n",
      "\n",
      "Epoch:  9502  Learning Rate:  7.477701547125569e-06  Varinance:  4.263000552750529e-05 \n",
      "\n",
      "Epoch:  9503  Learning Rate:  7.470227583183236e-06  Varinance:  4.2570452972356504e-05 \n",
      "\n",
      "Epoch:  9504  Learning Rate:  7.462761089469121e-06  Varinance:  4.2510983609944475e-05 \n",
      "\n",
      "Epoch:  9505  Learning Rate:  7.455302058516719e-06  Varinance:  4.245159732405185e-05 \n",
      "\n",
      "Epoch:  9506  Learning Rate:  7.447850482866982e-06  Varinance:  4.2392293998623864e-05 \n",
      "\n",
      "Epoch:  9507  Learning Rate:  7.4404063550683626e-06  Varinance:  4.233307351776778e-05 \n",
      "\n",
      "Epoch:  9508  Learning Rate:  7.432969667676718e-06  Varinance:  4.227393576575283e-05 \n",
      "\n",
      "Epoch:  9509  Learning Rate:  7.425540413255361e-06  Varinance:  4.2214880627009715e-05 \n",
      "\n",
      "Epoch:  9510  Learning Rate:  7.418118584375022e-06  Varinance:  4.2155907986130814e-05 \n",
      "\n",
      "Epoch:  9511  Learning Rate:  7.4107041736139005e-06  Varinance:  4.209701772786968e-05 \n",
      "\n",
      "Epoch:  9512  Learning Rate:  7.4032971735575685e-06  Varinance:  4.2038209737140686e-05 \n",
      "\n",
      "Epoch:  9513  Learning Rate:  7.395897576799013e-06  Varinance:  4.197948389901918e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9514  Learning Rate:  7.388505375938666e-06  Varinance:  4.1920840098740965e-05 \n",
      "\n",
      "Epoch:  9515  Learning Rate:  7.381120563584309e-06  Varinance:  4.1862278221702275e-05 \n",
      "\n",
      "Epoch:  9516  Learning Rate:  7.3737431323511184e-06  Varinance:  4.1803798153459184e-05 \n",
      "\n",
      "Epoch:  9517  Learning Rate:  7.366373074861688e-06  Varinance:  4.1745399779727885e-05 \n",
      "\n",
      "Epoch:  9518  Learning Rate:  7.359010383745946e-06  Varinance:  4.168708298638421e-05 \n",
      "\n",
      "Epoch:  9519  Learning Rate:  7.351655051641187e-06  Varinance:  4.16288476594632e-05 \n",
      "\n",
      "Epoch:  9520  Learning Rate:  7.344307071192107e-06  Varinance:  4.157069368515932e-05 \n",
      "\n",
      "Epoch:  9521  Learning Rate:  7.336966435050709e-06  Varinance:  4.151262094982595e-05 \n",
      "\n",
      "Epoch:  9522  Learning Rate:  7.329633135876344e-06  Varinance:  4.14546293399753e-05 \n",
      "\n",
      "Epoch:  9523  Learning Rate:  7.322307166335741e-06  Varinance:  4.13967187422779e-05 \n",
      "\n",
      "Epoch:  9524  Learning Rate:  7.314988519102913e-06  Varinance:  4.133888904356281e-05 \n",
      "\n",
      "Epoch:  9525  Learning Rate:  7.307677186859214e-06  Varinance:  4.128114013081719e-05 \n",
      "\n",
      "Epoch:  9526  Learning Rate:  7.3003731622932965e-06  Varinance:  4.122347189118586e-05 \n",
      "\n",
      "Epoch:  9527  Learning Rate:  7.293076438101164e-06  Varinance:  4.1165884211971494e-05 \n",
      "\n",
      "Epoch:  9528  Learning Rate:  7.285787006986076e-06  Varinance:  4.110837698063416e-05 \n",
      "\n",
      "Epoch:  9529  Learning Rate:  7.27850486165859e-06  Varinance:  4.105095008479119e-05 \n",
      "\n",
      "Epoch:  9530  Learning Rate:  7.271229994836587e-06  Varinance:  4.0993603412216697e-05 \n",
      "\n",
      "Epoch:  9531  Learning Rate:  7.263962399245182e-06  Varinance:  4.093633685084179e-05 \n",
      "\n",
      "Epoch:  9532  Learning Rate:  7.25670206761677e-06  Varinance:  4.087915028875413e-05 \n",
      "\n",
      "Epoch:  9533  Learning Rate:  7.249448992691044e-06  Varinance:  4.0822043614197496e-05 \n",
      "\n",
      "Epoch:  9534  Learning Rate:  7.242203167214913e-06  Varinance:  4.0765016715572015e-05 \n",
      "\n",
      "Epoch:  9535  Learning Rate:  7.234964583942541e-06  Varinance:  4.070806948143371e-05 \n",
      "\n",
      "Epoch:  9536  Learning Rate:  7.227733235635368e-06  Varinance:  4.0651201800494054e-05 \n",
      "\n",
      "Epoch:  9537  Learning Rate:  7.220509115062033e-06  Varinance:  4.059441356162023e-05 \n",
      "\n",
      "Epoch:  9538  Learning Rate:  7.213292214998403e-06  Varinance:  4.053770465383458e-05 \n",
      "\n",
      "Epoch:  9539  Learning Rate:  7.206082528227601e-06  Varinance:  4.0481074966314556e-05 \n",
      "\n",
      "Epoch:  9540  Learning Rate:  7.198880047539928e-06  Varinance:  4.04245243883922e-05 \n",
      "\n",
      "Epoch:  9541  Learning Rate:  7.191684765732903e-06  Varinance:  4.0368052809554375e-05 \n",
      "\n",
      "Epoch:  9542  Learning Rate:  7.184496675611229e-06  Varinance:  4.031166011944234e-05 \n",
      "\n",
      "Epoch:  9543  Learning Rate:  7.1773157699868415e-06  Varinance:  4.025534620785128e-05 \n",
      "\n",
      "Epoch:  9544  Learning Rate:  7.170142041678825e-06  Varinance:  4.019911096473057e-05 \n",
      "\n",
      "Epoch:  9545  Learning Rate:  7.162975483513433e-06  Varinance:  4.014295428018323e-05 \n",
      "\n",
      "Epoch:  9546  Learning Rate:  7.155816088324134e-06  Varinance:  4.0086876044465894e-05 \n",
      "\n",
      "Epoch:  9547  Learning Rate:  7.1486638489515195e-06  Varinance:  4.0030876147988275e-05 \n",
      "\n",
      "Epoch:  9548  Learning Rate:  7.141518758243337e-06  Varinance:  3.99749544813134e-05 \n",
      "\n",
      "Epoch:  9549  Learning Rate:  7.134380809054522e-06  Varinance:  3.991911093515716e-05 \n",
      "\n",
      "Epoch:  9550  Learning Rate:  7.12724999424711e-06  Varinance:  3.986334540038793e-05 \n",
      "\n",
      "Epoch:  9551  Learning Rate:  7.1201263066902736e-06  Varinance:  3.980765776802673e-05 \n",
      "\n",
      "Epoch:  9552  Learning Rate:  7.1130097392603485e-06  Varinance:  3.975204792924674e-05 \n",
      "\n",
      "Epoch:  9553  Learning Rate:  7.105900284840757e-06  Varinance:  3.9696515775373265e-05 \n",
      "\n",
      "Epoch:  9554  Learning Rate:  7.098797936322029e-06  Varinance:  3.964106119788318e-05 \n",
      "\n",
      "Epoch:  9555  Learning Rate:  7.0917026866018415e-06  Varinance:  3.958568408840521e-05 \n",
      "\n",
      "Epoch:  9556  Learning Rate:  7.084614528584932e-06  Varinance:  3.953038433871943e-05 \n",
      "\n",
      "Epoch:  9557  Learning Rate:  7.0775334551831416e-06  Varinance:  3.9475161840756925e-05 \n",
      "\n",
      "Epoch:  9558  Learning Rate:  7.070459459315383e-06  Varinance:  3.9420016486599926e-05 \n",
      "\n",
      "Epoch:  9559  Learning Rate:  7.063392533907686e-06  Varinance:  3.936494816848137e-05 \n",
      "\n",
      "Epoch:  9560  Learning Rate:  7.056332671893111e-06  Varinance:  3.930995677878481e-05 \n",
      "\n",
      "Epoch:  9561  Learning Rate:  7.049279866211784e-06  Varinance:  3.925504221004391e-05 \n",
      "\n",
      "Epoch:  9562  Learning Rate:  7.042234109810923e-06  Varinance:  3.920020435494269e-05 \n",
      "\n",
      "Epoch:  9563  Learning Rate:  7.035195395644759e-06  Varinance:  3.914544310631507e-05 \n",
      "\n",
      "Epoch:  9564  Learning Rate:  7.0281637166745635e-06  Varinance:  3.9090758357144477e-05 \n",
      "\n",
      "Epoch:  9565  Learning Rate:  7.021139065868683e-06  Varinance:  3.903615000056404e-05 \n",
      "\n",
      "Epoch:  9566  Learning Rate:  7.014121436202454e-06  Varinance:  3.8981617929856126e-05 \n",
      "\n",
      "Epoch:  9567  Learning Rate:  7.007110820658233e-06  Varinance:  3.892716203845222e-05 \n",
      "\n",
      "Epoch:  9568  Learning Rate:  7.00010721222543e-06  Varinance:  3.887278221993249e-05 \n",
      "\n",
      "Epoch:  9569  Learning Rate:  6.99311060390042e-06  Varinance:  3.8818478368025984e-05 \n",
      "\n",
      "Epoch:  9570  Learning Rate:  6.9861209886865865e-06  Varinance:  3.876425037661018e-05 \n",
      "\n",
      "Epoch:  9571  Learning Rate:  6.9791383595943365e-06  Varinance:  3.871009813971064e-05 \n",
      "\n",
      "Epoch:  9572  Learning Rate:  6.972162709641026e-06  Varinance:  3.865602155150113e-05 \n",
      "\n",
      "Epoch:  9573  Learning Rate:  6.965194031851007e-06  Varinance:  3.86020205063032e-05 \n",
      "\n",
      "Epoch:  9574  Learning Rate:  6.958232319255589e-06  Varinance:  3.854809489858612e-05 \n",
      "\n",
      "Epoch:  9575  Learning Rate:  6.951277564893081e-06  Varinance:  3.849424462296634e-05 \n",
      "\n",
      "Epoch:  9576  Learning Rate:  6.944329761808717e-06  Varinance:  3.844046957420776e-05 \n",
      "\n",
      "Epoch:  9577  Learning Rate:  6.937388903054682e-06  Varinance:  3.8386769647221275e-05 \n",
      "\n",
      "Epoch:  9578  Learning Rate:  6.9304549816901405e-06  Varinance:  3.8333144737064395e-05 \n",
      "\n",
      "Epoch:  9579  Learning Rate:  6.923527990781158e-06  Varinance:  3.8279594738941415e-05 \n",
      "\n",
      "Epoch:  9580  Learning Rate:  6.9166079234007305e-06  Varinance:  3.822611954820298e-05 \n",
      "\n",
      "Epoch:  9581  Learning Rate:  6.909694772628816e-06  Varinance:  3.8172719060345975e-05 \n",
      "\n",
      "Epoch:  9582  Learning Rate:  6.902788531552249e-06  Varinance:  3.811939317101308e-05 \n",
      "\n",
      "Epoch:  9583  Learning Rate:  6.895889193264776e-06  Varinance:  3.806614177599296e-05 \n",
      "\n",
      "Epoch:  9584  Learning Rate:  6.888996750867085e-06  Varinance:  3.801296477121986e-05 \n",
      "\n",
      "Epoch:  9585  Learning Rate:  6.882111197466718e-06  Varinance:  3.795986205277319e-05 \n",
      "\n",
      "Epoch:  9586  Learning Rate:  6.875232526178109e-06  Varinance:  3.7906833516877744e-05 \n",
      "\n",
      "Epoch:  9587  Learning Rate:  6.868360730122612e-06  Varinance:  3.785387905990323e-05 \n",
      "\n",
      "Epoch:  9588  Learning Rate:  6.8614958024284175e-06  Varinance:  3.780099857836416e-05 \n",
      "\n",
      "Epoch:  9589  Learning Rate:  6.854637736230597e-06  Varinance:  3.7748191968919444e-05 \n",
      "\n",
      "Epoch:  9590  Learning Rate:  6.8477865246710734e-06  Varinance:  3.7695459128372525e-05 \n",
      "\n",
      "Epoch:  9591  Learning Rate:  6.840942160898655e-06  Varinance:  3.7642799953671036e-05 \n",
      "\n",
      "Epoch:  9592  Learning Rate:  6.83410463806897e-06  Varinance:  3.759021434190635e-05 \n",
      "\n",
      "Epoch:  9593  Learning Rate:  6.827273949344478e-06  Varinance:  3.7537702190313806e-05 \n",
      "\n",
      "Epoch:  9594  Learning Rate:  6.820450087894518e-06  Varinance:  3.7485263396272246e-05 \n",
      "\n",
      "Epoch:  9595  Learning Rate:  6.813633046895213e-06  Varinance:  3.7432897857303924e-05 \n",
      "\n",
      "Epoch:  9596  Learning Rate:  6.806822819529511e-06  Varinance:  3.7380605471074046e-05 \n",
      "\n",
      "Epoch:  9597  Learning Rate:  6.800019398987209e-06  Varinance:  3.7328386135390995e-05 \n",
      "\n",
      "Epoch:  9598  Learning Rate:  6.793222778464872e-06  Varinance:  3.7276239748205894e-05 \n",
      "\n",
      "Epoch:  9599  Learning Rate:  6.786432951165867e-06  Varinance:  3.7224166207612236e-05 \n",
      "\n",
      "Epoch:  9600  Learning Rate:  6.779649910300391e-06  Varinance:  3.717216541184606e-05 \n",
      "\n",
      "Epoch:  9601  Learning Rate:  6.77287364908539e-06  Varinance:  3.712023725928559e-05 \n",
      "\n",
      "Epoch:  9602  Learning Rate:  6.766104160744591e-06  Varinance:  3.7068381648450796e-05 \n",
      "\n",
      "Epoch:  9603  Learning Rate:  6.759341438508528e-06  Varinance:  3.70165984780036e-05 \n",
      "\n",
      "Epoch:  9604  Learning Rate:  6.752585475614468e-06  Varinance:  3.696488764674745e-05 \n",
      "\n",
      "Epoch:  9605  Learning Rate:  6.745836265306444e-06  Varinance:  3.691324905362719e-05 \n",
      "\n",
      "Epoch:  9606  Learning Rate:  6.739093800835238e-06  Varinance:  3.686168259772869e-05 \n",
      "\n",
      "Epoch:  9607  Learning Rate:  6.7323580754584055e-06  Varinance:  3.6810188178278944e-05 \n",
      "\n",
      "Epoch:  9608  Learning Rate:  6.725629082440209e-06  Varinance:  3.675876569464574e-05 \n",
      "\n",
      "Epoch:  9609  Learning Rate:  6.718906815051644e-06  Varinance:  3.670741504633726e-05 \n",
      "\n",
      "Epoch:  9610  Learning Rate:  6.712191266570465e-06  Varinance:  3.665613613300223e-05 \n",
      "\n",
      "Epoch:  9611  Learning Rate:  6.705482430281112e-06  Varinance:  3.6604928854429546e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9612  Learning Rate:  6.698780299474737e-06  Varinance:  3.655379311054813e-05 \n",
      "\n",
      "Epoch:  9613  Learning Rate:  6.6920848674492325e-06  Varinance:  3.65027288014265e-05 \n",
      "\n",
      "Epoch:  9614  Learning Rate:  6.685396127509152e-06  Varinance:  3.645173582727299e-05 \n",
      "\n",
      "Epoch:  9615  Learning Rate:  6.678714072965745e-06  Varinance:  3.640081408843532e-05 \n",
      "\n",
      "Epoch:  9616  Learning Rate:  6.672038697136977e-06  Varinance:  3.634996348540024e-05 \n",
      "\n",
      "Epoch:  9617  Learning Rate:  6.665369993347464e-06  Varinance:  3.629918391879371e-05 \n",
      "\n",
      "Epoch:  9618  Learning Rate:  6.658707954928488e-06  Varinance:  3.624847528938044e-05 \n",
      "\n",
      "Epoch:  9619  Learning Rate:  6.652052575218034e-06  Varinance:  3.619783749806384e-05 \n",
      "\n",
      "Epoch:  9620  Learning Rate:  6.645403847560709e-06  Varinance:  3.614727044588554e-05 \n",
      "\n",
      "Epoch:  9621  Learning Rate:  6.638761765307785e-06  Varinance:  3.609677403402564e-05 \n",
      "\n",
      "Epoch:  9622  Learning Rate:  6.632126321817167e-06  Varinance:  3.604634816380226e-05 \n",
      "\n",
      "Epoch:  9623  Learning Rate:  6.625497510453437e-06  Varinance:  3.59959927366712e-05 \n",
      "\n",
      "Epoch:  9624  Learning Rate:  6.618875324587769e-06  Varinance:  3.5945707654226086e-05 \n",
      "\n",
      "Epoch:  9625  Learning Rate:  6.612259757597966e-06  Varinance:  3.589549281819799e-05 \n",
      "\n",
      "Epoch:  9626  Learning Rate:  6.605650802868482e-06  Varinance:  3.58453481304553e-05 \n",
      "\n",
      "Epoch:  9627  Learning Rate:  6.599048453790352e-06  Varinance:  3.57952734930033e-05 \n",
      "\n",
      "Epoch:  9628  Learning Rate:  6.592452703761214e-06  Varinance:  3.574526880798438e-05 \n",
      "\n",
      "Epoch:  9629  Learning Rate:  6.5858635461853406e-06  Varinance:  3.5695333977677595e-05 \n",
      "\n",
      "Epoch:  9630  Learning Rate:  6.579280974473562e-06  Varinance:  3.564546890449837e-05 \n",
      "\n",
      "Epoch:  9631  Learning Rate:  6.5727049820432955e-06  Varinance:  3.55956734909986e-05 \n",
      "\n",
      "Epoch:  9632  Learning Rate:  6.56613556231857e-06  Varinance:  3.5545947639866276e-05 \n",
      "\n",
      "Epoch:  9633  Learning Rate:  6.559572708729954e-06  Varinance:  3.5496291253925376e-05 \n",
      "\n",
      "Epoch:  9634  Learning Rate:  6.553016414714593e-06  Varinance:  3.544670423613546e-05 \n",
      "\n",
      "Epoch:  9635  Learning Rate:  6.546466673716182e-06  Varinance:  3.539718648959181e-05 \n",
      "\n",
      "Epoch:  9636  Learning Rate:  6.5399234791850005e-06  Varinance:  3.534773791752511e-05 \n",
      "\n",
      "Epoch:  9637  Learning Rate:  6.533386824577844e-06  Varinance:  3.529835842330102e-05 \n",
      "\n",
      "Epoch:  9638  Learning Rate:  6.526856703358045e-06  Varinance:  3.5249047910420396e-05 \n",
      "\n",
      "Epoch:  9639  Learning Rate:  6.520333108995505e-06  Varinance:  3.519980628251882e-05 \n",
      "\n",
      "Epoch:  9640  Learning Rate:  6.513816034966617e-06  Varinance:  3.515063344336657e-05 \n",
      "\n",
      "Epoch:  9641  Learning Rate:  6.507305474754294e-06  Varinance:  3.510152929686815e-05 \n",
      "\n",
      "Epoch:  9642  Learning Rate:  6.500801421848002e-06  Varinance:  3.505249374706251e-05 \n",
      "\n",
      "Epoch:  9643  Learning Rate:  6.494303869743671e-06  Varinance:  3.500352669812265e-05 \n",
      "\n",
      "Epoch:  9644  Learning Rate:  6.487812811943742e-06  Varinance:  3.495462805435525e-05 \n",
      "\n",
      "Epoch:  9645  Learning Rate:  6.4813282419571756e-06  Varinance:  3.490579772020084e-05 \n",
      "\n",
      "Epoch:  9646  Learning Rate:  6.474850153299392e-06  Varinance:  3.48570356002334e-05 \n",
      "\n",
      "Epoch:  9647  Learning Rate:  6.4683785394922896e-06  Varinance:  3.4808341599160284e-05 \n",
      "\n",
      "Epoch:  9648  Learning Rate:  6.461913394064276e-06  Varinance:  3.475971562182175e-05 \n",
      "\n",
      "Epoch:  9649  Learning Rate:  6.455454710550197e-06  Varinance:  3.471115757319122e-05 \n",
      "\n",
      "Epoch:  9650  Learning Rate:  6.449002482491365e-06  Varinance:  3.4662667358374814e-05 \n",
      "\n",
      "Epoch:  9651  Learning Rate:  6.442556703435542e-06  Varinance:  3.4614244882611074e-05 \n",
      "\n",
      "Epoch:  9652  Learning Rate:  6.436117366936971e-06  Varinance:  3.456589005127109e-05 \n",
      "\n",
      "Epoch:  9653  Learning Rate:  6.429684466556302e-06  Varinance:  3.451760276985806e-05 \n",
      "\n",
      "Epoch:  9654  Learning Rate:  6.4232579958606255e-06  Varinance:  3.44693829440073e-05 \n",
      "\n",
      "Epoch:  9655  Learning Rate:  6.41683794842349e-06  Varinance:  3.442123047948573e-05 \n",
      "\n",
      "Epoch:  9656  Learning Rate:  6.4104243178248395e-06  Varinance:  3.43731452821921e-05 \n",
      "\n",
      "Epoch:  9657  Learning Rate:  6.404017097651028e-06  Varinance:  3.4325127258156616e-05 \n",
      "\n",
      "Epoch:  9658  Learning Rate:  6.397616281494861e-06  Varinance:  3.427717631354058e-05 \n",
      "\n",
      "Epoch:  9659  Learning Rate:  6.391221862955508e-06  Varinance:  3.4229292354636573e-05 \n",
      "\n",
      "Epoch:  9660  Learning Rate:  6.384833835638539e-06  Varinance:  3.418147528786799e-05 \n",
      "\n",
      "Epoch:  9661  Learning Rate:  6.378452193155949e-06  Varinance:  3.413372501978906e-05 \n",
      "\n",
      "Epoch:  9662  Learning Rate:  6.372076929126082e-06  Varinance:  3.408604145708431e-05 \n",
      "\n",
      "Epoch:  9663  Learning Rate:  6.365708037173666e-06  Varinance:  3.403842450656885e-05 \n",
      "\n",
      "Epoch:  9664  Learning Rate:  6.359345510929827e-06  Varinance:  3.399087407518796e-05 \n",
      "\n",
      "Epoch:  9665  Learning Rate:  6.352989344032032e-06  Varinance:  3.3943390070016716e-05 \n",
      "\n",
      "Epoch:  9666  Learning Rate:  6.346639530124108e-06  Varinance:  3.3895972398260206e-05 \n",
      "\n",
      "Epoch:  9667  Learning Rate:  6.340296062856232e-06  Varinance:  3.3848620967253134e-05 \n",
      "\n",
      "Epoch:  9668  Learning Rate:  6.333958935884959e-06  Varinance:  3.380133568445949e-05 \n",
      "\n",
      "Epoch:  9669  Learning Rate:  6.32762814287315e-06  Varinance:  3.37541164574727e-05 \n",
      "\n",
      "Epoch:  9670  Learning Rate:  6.321303677489999e-06  Varinance:  3.370696319401522e-05 \n",
      "\n",
      "Epoch:  9671  Learning Rate:  6.314985533411063e-06  Varinance:  3.365987580193848e-05 \n",
      "\n",
      "Epoch:  9672  Learning Rate:  6.3086737043181885e-06  Varinance:  3.3612854189222457e-05 \n",
      "\n",
      "Epoch:  9673  Learning Rate:  6.302368183899532e-06  Varinance:  3.356589826397586e-05 \n",
      "\n",
      "Epoch:  9674  Learning Rate:  6.296068965849596e-06  Varinance:  3.3519007934435744e-05 \n",
      "\n",
      "Epoch:  9675  Learning Rate:  6.289776043869151e-06  Varinance:  3.347218310896721e-05 \n",
      "\n",
      "Epoch:  9676  Learning Rate:  6.283489411665261e-06  Varinance:  3.3425423696063524e-05 \n",
      "\n",
      "Epoch:  9677  Learning Rate:  6.277209062951319e-06  Varinance:  3.337872960434573e-05 \n",
      "\n",
      "Epoch:  9678  Learning Rate:  6.270934991446962e-06  Varinance:  3.333210074256258e-05 \n",
      "\n",
      "Epoch:  9679  Learning Rate:  6.264667190878108e-06  Varinance:  3.3285537019590144e-05 \n",
      "\n",
      "Epoch:  9680  Learning Rate:  6.258405654976979e-06  Varinance:  3.323903834443193e-05 \n",
      "\n",
      "Epoch:  9681  Learning Rate:  6.252150377482026e-06  Varinance:  3.31926046262186e-05 \n",
      "\n",
      "Epoch:  9682  Learning Rate:  6.245901352137972e-06  Varinance:  3.3146235774207555e-05 \n",
      "\n",
      "Epoch:  9683  Learning Rate:  6.239658572695779e-06  Varinance:  3.3099931697783156e-05 \n",
      "\n",
      "Epoch:  9684  Learning Rate:  6.23342203291269e-06  Varinance:  3.305369230645628e-05 \n",
      "\n",
      "Epoch:  9685  Learning Rate:  6.227191726552153e-06  Varinance:  3.3007517509864264e-05 \n",
      "\n",
      "Epoch:  9686  Learning Rate:  6.220967647383851e-06  Varinance:  3.2961407217770536e-05 \n",
      "\n",
      "Epoch:  9687  Learning Rate:  6.214749789183726e-06  Varinance:  3.2915361340064714e-05 \n",
      "\n",
      "Epoch:  9688  Learning Rate:  6.208538145733906e-06  Varinance:  3.286937978676233e-05 \n",
      "\n",
      "Epoch:  9689  Learning Rate:  6.20233271082274e-06  Varinance:  3.282346246800443e-05 \n",
      "\n",
      "Epoch:  9690  Learning Rate:  6.196133478244813e-06  Varinance:  3.277760929405776e-05 \n",
      "\n",
      "Epoch:  9691  Learning Rate:  6.189940441800879e-06  Varinance:  3.2731820175314385e-05 \n",
      "\n",
      "Epoch:  9692  Learning Rate:  6.183753595297894e-06  Varinance:  3.268609502229158e-05 \n",
      "\n",
      "Epoch:  9693  Learning Rate:  6.1775729325490286e-06  Varinance:  3.2640433745631466e-05 \n",
      "\n",
      "Epoch:  9694  Learning Rate:  6.171398447373611e-06  Varinance:  3.259483625610117e-05 \n",
      "\n",
      "Epoch:  9695  Learning Rate:  6.165230133597145e-06  Varinance:  3.254930246459246e-05 \n",
      "\n",
      "Epoch:  9696  Learning Rate:  6.1590679850513355e-06  Varinance:  3.250383228212141e-05 \n",
      "\n",
      "Epoch:  9697  Learning Rate:  6.152911995574026e-06  Varinance:  3.2458425619828606e-05 \n",
      "\n",
      "Epoch:  9698  Learning Rate:  6.146762159009224e-06  Varinance:  3.2413082388978665e-05 \n",
      "\n",
      "Epoch:  9699  Learning Rate:  6.140618469207082e-06  Varinance:  3.2367802500960267e-05 \n",
      "\n",
      "Epoch:  9700  Learning Rate:  6.1344809200239335e-06  Varinance:  3.232258586728566e-05 \n",
      "\n",
      "Epoch:  9701  Learning Rate:  6.128349505322214e-06  Varinance:  3.227743239959092e-05 \n",
      "\n",
      "Epoch:  9702  Learning Rate:  6.1222242189705015e-06  Varinance:  3.2232342009635536e-05 \n",
      "\n",
      "Epoch:  9703  Learning Rate:  6.116105054843528e-06  Varinance:  3.21873146093021e-05 \n",
      "\n",
      "Epoch:  9704  Learning Rate:  6.109992006822119e-06  Varinance:  3.2142350110596484e-05 \n",
      "\n",
      "Epoch:  9705  Learning Rate:  6.103885068793215e-06  Varinance:  3.2097448425647426e-05 \n",
      "\n",
      "Epoch:  9706  Learning Rate:  6.097784234649899e-06  Varinance:  3.205260946670645e-05 \n",
      "\n",
      "Epoch:  9707  Learning Rate:  6.091689498291327e-06  Varinance:  3.200783314614752e-05 \n",
      "\n",
      "Epoch:  9708  Learning Rate:  6.08560085362275e-06  Varinance:  3.196311937646716e-05 \n",
      "\n",
      "Epoch:  9709  Learning Rate:  6.079518294555544e-06  Varinance:  3.191846807028415e-05 \n",
      "\n",
      "Epoch:  9710  Learning Rate:  6.0734418150071395e-06  Varinance:  3.1873879140339166e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9711  Learning Rate:  6.067371408901045e-06  Varinance:  3.182935249949493e-05 \n",
      "\n",
      "Epoch:  9712  Learning Rate:  6.061307070166876e-06  Varinance:  3.178488806073587e-05 \n",
      "\n",
      "Epoch:  9713  Learning Rate:  6.055248792740282e-06  Varinance:  3.1740485737167994e-05 \n",
      "\n",
      "Epoch:  9714  Learning Rate:  6.049196570562986e-06  Varinance:  3.169614544201855e-05 \n",
      "\n",
      "Epoch:  9715  Learning Rate:  6.043150397582753e-06  Varinance:  3.165186708863617e-05 \n",
      "\n",
      "Epoch:  9716  Learning Rate:  6.037110267753433e-06  Varinance:  3.160765059049052e-05 \n",
      "\n",
      "Epoch:  9717  Learning Rate:  6.031076175034883e-06  Varinance:  3.1563495861172004e-05 \n",
      "\n",
      "Epoch:  9718  Learning Rate:  6.025048113393001e-06  Varinance:  3.151940281439189e-05 \n",
      "\n",
      "Epoch:  9719  Learning Rate:  6.0190260767997436e-06  Varinance:  3.1475371363981924e-05 \n",
      "\n",
      "Epoch:  9720  Learning Rate:  6.013010059233065e-06  Varinance:  3.143140142389429e-05 \n",
      "\n",
      "Epoch:  9721  Learning Rate:  6.007000054676937e-06  Varinance:  3.1387492908201224e-05 \n",
      "\n",
      "Epoch:  9722  Learning Rate:  6.000996057121373e-06  Varinance:  3.134364573109514e-05 \n",
      "\n",
      "Epoch:  9723  Learning Rate:  5.994998060562368e-06  Varinance:  3.1299859806888345e-05 \n",
      "\n",
      "Epoch:  9724  Learning Rate:  5.989006059001912e-06  Varinance:  3.125613505001266e-05 \n",
      "\n",
      "Epoch:  9725  Learning Rate:  5.983020046448026e-06  Varinance:  3.1212471375019637e-05 \n",
      "\n",
      "Epoch:  9726  Learning Rate:  5.977040016914682e-06  Varinance:  3.1168868696580114e-05 \n",
      "\n",
      "Epoch:  9727  Learning Rate:  5.9710659644218445e-06  Varinance:  3.11253269294842e-05 \n",
      "\n",
      "Epoch:  9728  Learning Rate:  5.965097882995479e-06  Varinance:  3.108184598864087e-05 \n",
      "\n",
      "Epoch:  9729  Learning Rate:  5.959135766667495e-06  Varinance:  3.1038425789078146e-05 \n",
      "\n",
      "Epoch:  9730  Learning Rate:  5.9531796094757725e-06  Varinance:  3.0995066245942716e-05 \n",
      "\n",
      "Epoch:  9731  Learning Rate:  5.9472294054641455e-06  Varinance:  3.095176727449969e-05 \n",
      "\n",
      "Epoch:  9732  Learning Rate:  5.94128514868243e-06  Varinance:  3.090852879013266e-05 \n",
      "\n",
      "Epoch:  9733  Learning Rate:  5.935346833186359e-06  Varinance:  3.086535070834348e-05 \n",
      "\n",
      "Epoch:  9734  Learning Rate:  5.929414453037604e-06  Varinance:  3.082223294475184e-05 \n",
      "\n",
      "Epoch:  9735  Learning Rate:  5.923488002303808e-06  Varinance:  3.0779175415095485e-05 \n",
      "\n",
      "Epoch:  9736  Learning Rate:  5.917567475058508e-06  Varinance:  3.073617803522982e-05 \n",
      "\n",
      "Epoch:  9737  Learning Rate:  5.911652865381164e-06  Varinance:  3.069324072112784e-05 \n",
      "\n",
      "Epoch:  9738  Learning Rate:  5.90574416735719e-06  Varinance:  3.0650363388879764e-05 \n",
      "\n",
      "Epoch:  9739  Learning Rate:  5.899841375077875e-06  Varinance:  3.060754595469321e-05 \n",
      "\n",
      "Epoch:  9740  Learning Rate:  5.893944482640417e-06  Varinance:  3.0564788334892815e-05 \n",
      "\n",
      "Epoch:  9741  Learning Rate:  5.888053484147943e-06  Varinance:  3.052209044591998e-05 \n",
      "\n",
      "Epoch:  9742  Learning Rate:  5.882168373709442e-06  Varinance:  3.0479452204332984e-05 \n",
      "\n",
      "Epoch:  9743  Learning Rate:  5.876289145439796e-06  Varinance:  3.04368735268066e-05 \n",
      "\n",
      "Epoch:  9744  Learning Rate:  5.8704157934597955e-06  Varinance:  3.039435433013209e-05 \n",
      "\n",
      "Epoch:  9745  Learning Rate:  5.864548311896078e-06  Varinance:  3.0351894531216767e-05 \n",
      "\n",
      "Epoch:  9746  Learning Rate:  5.858686694881161e-06  Varinance:  3.03094940470842e-05 \n",
      "\n",
      "Epoch:  9747  Learning Rate:  5.852830936553416e-06  Varinance:  3.026715279487385e-05 \n",
      "\n",
      "Epoch:  9748  Learning Rate:  5.8469810310571065e-06  Varinance:  3.0224870691840798e-05 \n",
      "\n",
      "Epoch:  9749  Learning Rate:  5.841136972542314e-06  Varinance:  3.0182647655355866e-05 \n",
      "\n",
      "Epoch:  9750  Learning Rate:  5.835298755164972e-06  Varinance:  3.0140483602905245e-05 \n",
      "\n",
      "Epoch:  9751  Learning Rate:  5.829466373086882e-06  Varinance:  3.0098378452090455e-05 \n",
      "\n",
      "Epoch:  9752  Learning Rate:  5.823639820475649e-06  Varinance:  3.0056332120627964e-05 \n",
      "\n",
      "Epoch:  9753  Learning Rate:  5.8178190915047125e-06  Varinance:  3.0014344526349352e-05 \n",
      "\n",
      "Epoch:  9754  Learning Rate:  5.812004180353363e-06  Varinance:  2.9972415587200968e-05 \n",
      "\n",
      "Epoch:  9755  Learning Rate:  5.8061950812066776e-06  Varinance:  2.993054522124365e-05 \n",
      "\n",
      "Epoch:  9756  Learning Rate:  5.800391788255547e-06  Varinance:  2.988873334665285e-05 \n",
      "\n",
      "Epoch:  9757  Learning Rate:  5.794594295696699e-06  Varinance:  2.9846979881718262e-05 \n",
      "\n",
      "Epoch:  9758  Learning Rate:  5.788802597732628e-06  Varinance:  2.9805284744843793e-05 \n",
      "\n",
      "Epoch:  9759  Learning Rate:  5.783016688571639e-06  Varinance:  2.9763647854547186e-05 \n",
      "\n",
      "Epoch:  9760  Learning Rate:  5.777236562427809e-06  Varinance:  2.9722069129460146e-05 \n",
      "\n",
      "Epoch:  9761  Learning Rate:  5.7714622135210334e-06  Varinance:  2.9680548488328073e-05 \n",
      "\n",
      "Epoch:  9762  Learning Rate:  5.765693636076953e-06  Varinance:  2.9639085850009693e-05 \n",
      "\n",
      "Epoch:  9763  Learning Rate:  5.759930824326978e-06  Varinance:  2.9597681133477262e-05 \n",
      "\n",
      "Epoch:  9764  Learning Rate:  5.754173772508318e-06  Varinance:  2.9556334257816165e-05 \n",
      "\n",
      "Epoch:  9765  Learning Rate:  5.74842247486391e-06  Varinance:  2.9515045142224876e-05 \n",
      "\n",
      "Epoch:  9766  Learning Rate:  5.742676925642445e-06  Varinance:  2.9473813706014594e-05 \n",
      "\n",
      "Epoch:  9767  Learning Rate:  5.736937119098395e-06  Varinance:  2.943263986860939e-05 \n",
      "\n",
      "Epoch:  9768  Learning Rate:  5.731203049491942e-06  Varinance:  2.939152354954589e-05 \n",
      "\n",
      "Epoch:  9769  Learning Rate:  5.725474711089006e-06  Varinance:  2.9350464668472982e-05 \n",
      "\n",
      "Epoch:  9770  Learning Rate:  5.719752098161269e-06  Varinance:  2.930946314515195e-05 \n",
      "\n",
      "Epoch:  9771  Learning Rate:  5.714035204986106e-06  Varinance:  2.9268518899456117e-05 \n",
      "\n",
      "Epoch:  9772  Learning Rate:  5.708324025846614e-06  Varinance:  2.9227631851370785e-05 \n",
      "\n",
      "Epoch:  9773  Learning Rate:  5.702618555031634e-06  Varinance:  2.9186801920992895e-05 \n",
      "\n",
      "Epoch:  9774  Learning Rate:  5.696918786835684e-06  Varinance:  2.9146029028531144e-05 \n",
      "\n",
      "Epoch:  9775  Learning Rate:  5.691224715558997e-06  Varinance:  2.9105313094305717e-05 \n",
      "\n",
      "Epoch:  9776  Learning Rate:  5.685536335507487e-06  Varinance:  2.9064654038747936e-05 \n",
      "\n",
      "Epoch:  9777  Learning Rate:  5.679853640992798e-06  Varinance:  2.9024051782400442e-05 \n",
      "\n",
      "Epoch:  9778  Learning Rate:  5.674176626332224e-06  Varinance:  2.8983506245916813e-05 \n",
      "\n",
      "Epoch:  9779  Learning Rate:  5.668505285848738e-06  Varinance:  2.894301735006154e-05 \n",
      "\n",
      "Epoch:  9780  Learning Rate:  5.662839613871021e-06  Varinance:  2.8902585015709624e-05 \n",
      "\n",
      "Epoch:  9781  Learning Rate:  5.657179604733389e-06  Varinance:  2.8862209163846788e-05 \n",
      "\n",
      "Epoch:  9782  Learning Rate:  5.6515252527758234e-06  Varinance:  2.88218897155691e-05 \n",
      "\n",
      "Epoch:  9783  Learning Rate:  5.645876552343992e-06  Varinance:  2.878162659208273e-05 \n",
      "\n",
      "Epoch:  9784  Learning Rate:  5.640233497789182e-06  Varinance:  2.874141971470405e-05 \n",
      "\n",
      "Epoch:  9785  Learning Rate:  5.634596083468332e-06  Varinance:  2.8701269004859317e-05 \n",
      "\n",
      "Epoch:  9786  Learning Rate:  5.6289643037440435e-06  Varinance:  2.8661174384084588e-05 \n",
      "\n",
      "Epoch:  9787  Learning Rate:  5.623338152984528e-06  Varinance:  2.8621135774025387e-05 \n",
      "\n",
      "Epoch:  9788  Learning Rate:  5.6177176255636245e-06  Varinance:  2.8581153096436854e-05 \n",
      "\n",
      "Epoch:  9789  Learning Rate:  5.612102715860824e-06  Varinance:  2.854122627318343e-05 \n",
      "\n",
      "Epoch:  9790  Learning Rate:  5.606493418261208e-06  Varinance:  2.8501355226238544e-05 \n",
      "\n",
      "Epoch:  9791  Learning Rate:  5.600889727155477e-06  Varinance:  2.84615398776848e-05 \n",
      "\n",
      "Epoch:  9792  Learning Rate:  5.59529163693993e-06  Varinance:  2.8421780149713576e-05 \n",
      "\n",
      "Epoch:  9793  Learning Rate:  5.5896991420164965e-06  Varinance:  2.838207596462502e-05 \n",
      "\n",
      "Epoch:  9794  Learning Rate:  5.58411223679267e-06  Varinance:  2.834242724482765e-05 \n",
      "\n",
      "Epoch:  9795  Learning Rate:  5.578530915681536e-06  Varinance:  2.830283391283854e-05 \n",
      "\n",
      "Epoch:  9796  Learning Rate:  5.572955173101793e-06  Varinance:  2.8263295891283003e-05 \n",
      "\n",
      "Epoch:  9797  Learning Rate:  5.567385003477687e-06  Varinance:  2.822381310289429e-05 \n",
      "\n",
      "Epoch:  9798  Learning Rate:  5.561820401239039e-06  Varinance:  2.818438547051374e-05 \n",
      "\n",
      "Epoch:  9799  Learning Rate:  5.5562613608212654e-06  Varinance:  2.814501291709049e-05 \n",
      "\n",
      "Epoch:  9800  Learning Rate:  5.550707876665315e-06  Varinance:  2.8105695365681143e-05 \n",
      "\n",
      "Epoch:  9801  Learning Rate:  5.545159943217695e-06  Varinance:  2.8066432739449955e-05 \n",
      "\n",
      "Epoch:  9802  Learning Rate:  5.539617554930489e-06  Varinance:  2.8027224961668473e-05 \n",
      "\n",
      "Epoch:  9803  Learning Rate:  5.5340807062613005e-06  Varinance:  2.7988071955715465e-05 \n",
      "\n",
      "Epoch:  9804  Learning Rate:  5.52854939167327e-06  Varinance:  2.7948973645076602e-05 \n",
      "\n",
      "Epoch:  9805  Learning Rate:  5.5230236056351005e-06  Varinance:  2.790992995334458e-05 \n",
      "\n",
      "Epoch:  9806  Learning Rate:  5.517503342620997e-06  Varinance:  2.7870940804218834e-05 \n",
      "\n",
      "Epoch:  9807  Learning Rate:  5.511988597110697e-06  Varinance:  2.7832006121505243e-05 \n",
      "\n",
      "Epoch:  9808  Learning Rate:  5.506479363589443e-06  Varinance:  2.7793125829116288e-05 \n",
      "\n",
      "Epoch:  9809  Learning Rate:  5.500975636548021e-06  Varinance:  2.7754299851070665e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9810  Learning Rate:  5.495477410482694e-06  Varinance:  2.7715528111493286e-05 \n",
      "\n",
      "Epoch:  9811  Learning Rate:  5.489984679895226e-06  Varinance:  2.7676810534614894e-05 \n",
      "\n",
      "Epoch:  9812  Learning Rate:  5.484497439292905e-06  Varinance:  2.763814704477223e-05 \n",
      "\n",
      "Epoch:  9813  Learning Rate:  5.479015683188481e-06  Varinance:  2.7599537566407747e-05 \n",
      "\n",
      "Epoch:  9814  Learning Rate:  5.4735394061001856e-06  Varinance:  2.7560982024069288e-05 \n",
      "\n",
      "Epoch:  9815  Learning Rate:  5.468068602551763e-06  Varinance:  2.7522480342410248e-05 \n",
      "\n",
      "Epoch:  9816  Learning Rate:  5.4626032670723986e-06  Varinance:  2.7484032446189232e-05 \n",
      "\n",
      "Epoch:  9817  Learning Rate:  5.457143394196747e-06  Varinance:  2.744563826027001e-05 \n",
      "\n",
      "Epoch:  9818  Learning Rate:  5.451688978464954e-06  Varinance:  2.7407297709621146e-05 \n",
      "\n",
      "Epoch:  9819  Learning Rate:  5.446240014422593e-06  Varinance:  2.7369010719316204e-05 \n",
      "\n",
      "Epoch:  9820  Learning Rate:  5.440796496620692e-06  Varinance:  2.7330777214533377e-05 \n",
      "\n",
      "Epoch:  9821  Learning Rate:  5.435358419615749e-06  Varinance:  2.729259712055526e-05 \n",
      "\n",
      "Epoch:  9822  Learning Rate:  5.42992577796968e-06  Varinance:  2.7254470362768967e-05 \n",
      "\n",
      "Epoch:  9823  Learning Rate:  5.42449856624984e-06  Varinance:  2.7216396866665794e-05 \n",
      "\n",
      "Epoch:  9824  Learning Rate:  5.41907677902901e-06  Varinance:  2.7178376557841166e-05 \n",
      "\n",
      "Epoch:  9825  Learning Rate:  5.413660410885419e-06  Varinance:  2.714040936199431e-05 \n",
      "\n",
      "Epoch:  9826  Learning Rate:  5.408249456402691e-06  Varinance:  2.710249520492839e-05 \n",
      "\n",
      "Epoch:  9827  Learning Rate:  5.402843910169861e-06  Varinance:  2.7064634012550224e-05 \n",
      "\n",
      "Epoch:  9828  Learning Rate:  5.3974437667814005e-06  Varinance:  2.702682571086998e-05 \n",
      "\n",
      "Epoch:  9829  Learning Rate:  5.3920490208371554e-06  Varinance:  2.6989070226001353e-05 \n",
      "\n",
      "Epoch:  9830  Learning Rate:  5.386659666942373e-06  Varinance:  2.695136748416119e-05 \n",
      "\n",
      "Epoch:  9831  Learning Rate:  5.381275699707714e-06  Varinance:  2.691371741166945e-05 \n",
      "\n",
      "Epoch:  9832  Learning Rate:  5.3758971137492045e-06  Varinance:  2.68761199349489e-05 \n",
      "\n",
      "Epoch:  9833  Learning Rate:  5.370523903688246e-06  Varinance:  2.6838574980525217e-05 \n",
      "\n",
      "Epoch:  9834  Learning Rate:  5.36515606415165e-06  Varinance:  2.6801082475026733e-05 \n",
      "\n",
      "Epoch:  9835  Learning Rate:  5.359793589771564e-06  Varinance:  2.676364234518411e-05 \n",
      "\n",
      "Epoch:  9836  Learning Rate:  5.354436475185505e-06  Varinance:  2.6726254517830546e-05 \n",
      "\n",
      "Epoch:  9837  Learning Rate:  5.3490847150363766e-06  Varinance:  2.6688918919901366e-05 \n",
      "\n",
      "Epoch:  9838  Learning Rate:  5.34373830397241e-06  Varinance:  2.665163547843403e-05 \n",
      "\n",
      "Epoch:  9839  Learning Rate:  5.338397236647192e-06  Varinance:  2.6614404120567782e-05 \n",
      "\n",
      "Epoch:  9840  Learning Rate:  5.333061507719646e-06  Varinance:  2.6577224773543775e-05 \n",
      "\n",
      "Epoch:  9841  Learning Rate:  5.327731111854062e-06  Varinance:  2.6540097364704833e-05 \n",
      "\n",
      "Epoch:  9842  Learning Rate:  5.322406043720034e-06  Varinance:  2.650302182149511e-05 \n",
      "\n",
      "Epoch:  9843  Learning Rate:  5.317086297992483e-06  Varinance:  2.646599807146027e-05 \n",
      "\n",
      "Epoch:  9844  Learning Rate:  5.311771869351682e-06  Varinance:  2.6429026042247152e-05 \n",
      "\n",
      "Epoch:  9845  Learning Rate:  5.3064627524831945e-06  Varinance:  2.6392105661603708e-05 \n",
      "\n",
      "Epoch:  9846  Learning Rate:  5.301158942077892e-06  Varinance:  2.6355236857378685e-05 \n",
      "\n",
      "Epoch:  9847  Learning Rate:  5.295860432831982e-06  Varinance:  2.6318419557521762e-05 \n",
      "\n",
      "Epoch:  9848  Learning Rate:  5.290567219446947e-06  Varinance:  2.6281653690083272e-05 \n",
      "\n",
      "Epoch:  9849  Learning Rate:  5.285279296629563e-06  Varinance:  2.6244939183213925e-05 \n",
      "\n",
      "Epoch:  9850  Learning Rate:  5.279996659091924e-06  Varinance:  2.6208275965164925e-05 \n",
      "\n",
      "Epoch:  9851  Learning Rate:  5.2747193015513855e-06  Varinance:  2.6171663964287675e-05 \n",
      "\n",
      "Epoch:  9852  Learning Rate:  5.269447218730578e-06  Varinance:  2.6135103109033714e-05 \n",
      "\n",
      "Epoch:  9853  Learning Rate:  5.264180405357438e-06  Varinance:  2.609859332795438e-05 \n",
      "\n",
      "Epoch:  9854  Learning Rate:  5.258918856165142e-06  Varinance:  2.6062134549700973e-05 \n",
      "\n",
      "Epoch:  9855  Learning Rate:  5.253662565892141e-06  Varinance:  2.6025726703024466e-05 \n",
      "\n",
      "Epoch:  9856  Learning Rate:  5.248411529282133e-06  Varinance:  2.5989369716775205e-05 \n",
      "\n",
      "Epoch:  9857  Learning Rate:  5.243165741084102e-06  Varinance:  2.5953063519903107e-05 \n",
      "\n",
      "Epoch:  9858  Learning Rate:  5.237925196052249e-06  Varinance:  2.5916808041457253e-05 \n",
      "\n",
      "Epoch:  9859  Learning Rate:  5.232689888946019e-06  Varinance:  2.588060321058592e-05 \n",
      "\n",
      "Epoch:  9860  Learning Rate:  5.227459814530123e-06  Varinance:  2.58444489565362e-05 \n",
      "\n",
      "Epoch:  9861  Learning Rate:  5.222234967574478e-06  Varinance:  2.580834520865418e-05 \n",
      "\n",
      "Epoch:  9862  Learning Rate:  5.217015342854226e-06  Varinance:  2.5772291896384637e-05 \n",
      "\n",
      "Epoch:  9863  Learning Rate:  5.211800935149761e-06  Varinance:  2.5736288949270786e-05 \n",
      "\n",
      "Epoch:  9864  Learning Rate:  5.206591739246665e-06  Varinance:  2.5700336296954383e-05 \n",
      "\n",
      "Epoch:  9865  Learning Rate:  5.201387749935733e-06  Varinance:  2.5664433869175498e-05 \n",
      "\n",
      "Epoch:  9866  Learning Rate:  5.196188962012994e-06  Varinance:  2.5628581595772192e-05 \n",
      "\n",
      "Epoch:  9867  Learning Rate:  5.19099537027965e-06  Varinance:  2.5592779406680697e-05 \n",
      "\n",
      "Epoch:  9868  Learning Rate:  5.1858069695421e-06  Varinance:  2.5557027231935053e-05 \n",
      "\n",
      "Epoch:  9869  Learning Rate:  5.18062375461196e-06  Varinance:  2.55213250016671e-05 \n",
      "\n",
      "Epoch:  9870  Learning Rate:  5.175445720306007e-06  Varinance:  2.548567264610615e-05 \n",
      "\n",
      "Epoch:  9871  Learning Rate:  5.170272861446205e-06  Varinance:  2.5450070095579103e-05 \n",
      "\n",
      "Epoch:  9872  Learning Rate:  5.165105172859686e-06  Varinance:  2.5414517280510206e-05 \n",
      "\n",
      "Epoch:  9873  Learning Rate:  5.159942649378781e-06  Varinance:  2.537901413142075e-05 \n",
      "\n",
      "Epoch:  9874  Learning Rate:  5.154785285840954e-06  Varinance:  2.5343560578929228e-05 \n",
      "\n",
      "Epoch:  9875  Learning Rate:  5.149633077088834e-06  Varinance:  2.530815655375101e-05 \n",
      "\n",
      "Epoch:  9876  Learning Rate:  5.144486017970228e-06  Varinance:  2.5272801986698305e-05 \n",
      "\n",
      "Epoch:  9877  Learning Rate:  5.13934410333807e-06  Varinance:  2.5237496808679827e-05 \n",
      "\n",
      "Epoch:  9878  Learning Rate:  5.134207328050434e-06  Varinance:  2.5202240950700954e-05 \n",
      "\n",
      "Epoch:  9879  Learning Rate:  5.129075686970563e-06  Varinance:  2.516703434386344e-05 \n",
      "\n",
      "Epoch:  9880  Learning Rate:  5.123949174966806e-06  Varinance:  2.5131876919365167e-05 \n",
      "\n",
      "Epoch:  9881  Learning Rate:  5.118827786912642e-06  Varinance:  2.5096768608500245e-05 \n",
      "\n",
      "Epoch:  9882  Learning Rate:  5.1137115176867015e-06  Varinance:  2.5061709342658728e-05 \n",
      "\n",
      "Epoch:  9883  Learning Rate:  5.108600362172705e-06  Varinance:  2.5026699053326565e-05 \n",
      "\n",
      "Epoch:  9884  Learning Rate:  5.103494315259495e-06  Varinance:  2.499173767208528e-05 \n",
      "\n",
      "Epoch:  9885  Learning Rate:  5.098393371841017e-06  Varinance:  2.4956825130612097e-05 \n",
      "\n",
      "Epoch:  9886  Learning Rate:  5.0932975268163455e-06  Varinance:  2.4921961360679707e-05 \n",
      "\n",
      "Epoch:  9887  Learning Rate:  5.088206775089624e-06  Varinance:  2.488714629415596e-05 \n",
      "\n",
      "Epoch:  9888  Learning Rate:  5.0831211115700935e-06  Varinance:  2.4852379863004023e-05 \n",
      "\n",
      "Epoch:  9889  Learning Rate:  5.078040531172107e-06  Varinance:  2.4817661999282066e-05 \n",
      "\n",
      "Epoch:  9890  Learning Rate:  5.0729650288150746e-06  Varinance:  2.478299263514321e-05 \n",
      "\n",
      "Epoch:  9891  Learning Rate:  5.067894599423485e-06  Varinance:  2.474837170283524e-05 \n",
      "\n",
      "Epoch:  9892  Learning Rate:  5.062829237926926e-06  Varinance:  2.471379913470069e-05 \n",
      "\n",
      "Epoch:  9893  Learning Rate:  5.057768939260027e-06  Varinance:  2.4679274863176638e-05 \n",
      "\n",
      "Epoch:  9894  Learning Rate:  5.052713698362479e-06  Varinance:  2.46447988207944e-05 \n",
      "\n",
      "Epoch:  9895  Learning Rate:  5.04766351017906e-06  Varinance:  2.4610370940179672e-05 \n",
      "\n",
      "Epoch:  9896  Learning Rate:  5.042618369659572e-06  Varinance:  2.4575991154052232e-05 \n",
      "\n",
      "Epoch:  9897  Learning Rate:  5.037578271758865e-06  Varinance:  2.4541659395225892e-05 \n",
      "\n",
      "Epoch:  9898  Learning Rate:  5.032543211436859e-06  Varinance:  2.4507375596608175e-05 \n",
      "\n",
      "Epoch:  9899  Learning Rate:  5.027513183658482e-06  Varinance:  2.4473139691200486e-05 \n",
      "\n",
      "Epoch:  9900  Learning Rate:  5.022488183393709e-06  Varinance:  2.44389516120978e-05 \n",
      "\n",
      "Epoch:  9901  Learning Rate:  5.017468205617529e-06  Varinance:  2.4404811292488433e-05 \n",
      "\n",
      "Epoch:  9902  Learning Rate:  5.0124532453099814e-06  Varinance:  2.437071866565417e-05 \n",
      "\n",
      "Epoch:  9903  Learning Rate:  5.007443297456097e-06  Varinance:  2.4336673664969957e-05 \n",
      "\n",
      "Epoch:  9904  Learning Rate:  5.0024383570459175e-06  Varinance:  2.430267622390385e-05 \n",
      "\n",
      "Epoch:  9905  Learning Rate:  4.997438419074522e-06  Varinance:  2.426872627601671e-05 \n",
      "\n",
      "Epoch:  9906  Learning Rate:  4.9924434785419615e-06  Varinance:  2.423482375496236e-05 \n",
      "\n",
      "Epoch:  9907  Learning Rate:  4.987453530453286e-06  Varinance:  2.4200968594487292e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9908  Learning Rate:  4.982468569818567e-06  Varinance:  2.416716072843043e-05 \n",
      "\n",
      "Epoch:  9909  Learning Rate:  4.977488591652832e-06  Varinance:  2.4133400090723244e-05 \n",
      "\n",
      "Epoch:  9910  Learning Rate:  4.972513590976095e-06  Varinance:  2.4099686615389464e-05 \n",
      "\n",
      "Epoch:  9911  Learning Rate:  4.967543562813372e-06  Varinance:  2.4066020236545017e-05 \n",
      "\n",
      "Epoch:  9912  Learning Rate:  4.962578502194625e-06  Varinance:  2.403240088839776e-05 \n",
      "\n",
      "Epoch:  9913  Learning Rate:  4.957618404154787e-06  Varinance:  2.3998828505247568e-05 \n",
      "\n",
      "Epoch:  9914  Learning Rate:  4.952663263733774e-06  Varinance:  2.3965303021486113e-05 \n",
      "\n",
      "Epoch:  9915  Learning Rate:  4.947713075976437e-06  Varinance:  2.3931824371596575e-05 \n",
      "\n",
      "Epoch:  9916  Learning Rate:  4.942767835932588e-06  Varinance:  2.3898392490153795e-05 \n",
      "\n",
      "Epoch:  9917  Learning Rate:  4.937827538656978e-06  Varinance:  2.386500731182397e-05 \n",
      "\n",
      "Epoch:  9918  Learning Rate:  4.9328921792093284e-06  Varinance:  2.38316687713646e-05 \n",
      "\n",
      "Epoch:  9919  Learning Rate:  4.927961752654268e-06  Varinance:  2.379837680362421e-05 \n",
      "\n",
      "Epoch:  9920  Learning Rate:  4.923036254061362e-06  Varinance:  2.376513134354246e-05 \n",
      "\n",
      "Epoch:  9921  Learning Rate:  4.91811567850513e-06  Varinance:  2.37319323261499e-05 \n",
      "\n",
      "Epoch:  9922  Learning Rate:  4.913200021064986e-06  Varinance:  2.3698779686567705e-05 \n",
      "\n",
      "Epoch:  9923  Learning Rate:  4.908289276825263e-06  Varinance:  2.366567336000782e-05 \n",
      "\n",
      "Epoch:  9924  Learning Rate:  4.903383440875235e-06  Varinance:  2.363261328177265e-05 \n",
      "\n",
      "Epoch:  9925  Learning Rate:  4.898482508309057e-06  Varinance:  2.359959938725502e-05 \n",
      "\n",
      "Epoch:  9926  Learning Rate:  4.893586474225787e-06  Varinance:  2.3566631611937893e-05 \n",
      "\n",
      "Epoch:  9927  Learning Rate:  4.8886953337294065e-06  Varinance:  2.3533709891394468e-05 \n",
      "\n",
      "Epoch:  9928  Learning Rate:  4.883809081928767e-06  Varinance:  2.3500834161287965e-05 \n",
      "\n",
      "Epoch:  9929  Learning Rate:  4.878927713937609e-06  Varinance:  2.346800435737134e-05 \n",
      "\n",
      "Epoch:  9930  Learning Rate:  4.87405122487458e-06  Varinance:  2.3435220415487435e-05 \n",
      "\n",
      "Epoch:  9931  Learning Rate:  4.8691796098631816e-06  Varinance:  2.3402482271568706e-05 \n",
      "\n",
      "Epoch:  9932  Learning Rate:  4.8643128640317996e-06  Varinance:  2.3369789861637e-05 \n",
      "\n",
      "Epoch:  9933  Learning Rate:  4.859450982513677e-06  Varinance:  2.333714312180365e-05 \n",
      "\n",
      "Epoch:  9934  Learning Rate:  4.8545939604469515e-06  Varinance:  2.3304541988269205e-05 \n",
      "\n",
      "Epoch:  9935  Learning Rate:  4.8497417929745905e-06  Varinance:  2.3271986397323377e-05 \n",
      "\n",
      "Epoch:  9936  Learning Rate:  4.8448944752444185e-06  Varinance:  2.3239476285344754e-05 \n",
      "\n",
      "Epoch:  9937  Learning Rate:  4.840052002409134e-06  Varinance:  2.3207011588800933e-05 \n",
      "\n",
      "Epoch:  9938  Learning Rate:  4.8352143696262545e-06  Varinance:  2.317459224424825e-05 \n",
      "\n",
      "Epoch:  9939  Learning Rate:  4.8303815720581395e-06  Varinance:  2.314221818833156e-05 \n",
      "\n",
      "Epoch:  9940  Learning Rate:  4.825553604872008e-06  Varinance:  2.310988935778434e-05 \n",
      "\n",
      "Epoch:  9941  Learning Rate:  4.8207304632398835e-06  Varinance:  2.30776056894284e-05 \n",
      "\n",
      "Epoch:  9942  Learning Rate:  4.815912142338614e-06  Varinance:  2.304536712017386e-05 \n",
      "\n",
      "Epoch:  9943  Learning Rate:  4.811098637349899e-06  Varinance:  2.3013173587018845e-05 \n",
      "\n",
      "Epoch:  9944  Learning Rate:  4.806289943460221e-06  Varinance:  2.2981025027049615e-05 \n",
      "\n",
      "Epoch:  9945  Learning Rate:  4.801486055860878e-06  Varinance:  2.294892137744032e-05 \n",
      "\n",
      "Epoch:  9946  Learning Rate:  4.796686969748e-06  Varinance:  2.2916862575452755e-05 \n",
      "\n",
      "Epoch:  9947  Learning Rate:  4.791892680322492e-06  Varinance:  2.288484855843647e-05 \n",
      "\n",
      "Epoch:  9948  Learning Rate:  4.787103182790063e-06  Varinance:  2.285287926382851e-05 \n",
      "\n",
      "Epoch:  9949  Learning Rate:  4.782318472361207e-06  Varinance:  2.2820954629153337e-05 \n",
      "\n",
      "Epoch:  9950  Learning Rate:  4.777538544251231e-06  Varinance:  2.2789074592022597e-05 \n",
      "\n",
      "Epoch:  9951  Learning Rate:  4.772763393680197e-06  Varinance:  2.275723909013518e-05 \n",
      "\n",
      "Epoch:  9952  Learning Rate:  4.767993015872947e-06  Varinance:  2.2725448061277042e-05 \n",
      "\n",
      "Epoch:  9953  Learning Rate:  4.763227406059117e-06  Varinance:  2.2693701443320895e-05 \n",
      "\n",
      "Epoch:  9954  Learning Rate:  4.7584665594730905e-06  Varinance:  2.2661999174226385e-05 \n",
      "\n",
      "Epoch:  9955  Learning Rate:  4.753710471354012e-06  Varinance:  2.2630341192039772e-05 \n",
      "\n",
      "Epoch:  9956  Learning Rate:  4.7489591369458095e-06  Varinance:  2.2598727434893917e-05 \n",
      "\n",
      "Epoch:  9957  Learning Rate:  4.744212551497139e-06  Varinance:  2.256715784100797e-05 \n",
      "\n",
      "Epoch:  9958  Learning Rate:  4.739470710261408e-06  Varinance:  2.253563234868752e-05 \n",
      "\n",
      "Epoch:  9959  Learning Rate:  4.73473360849679e-06  Varinance:  2.250415089632434e-05 \n",
      "\n",
      "Epoch:  9960  Learning Rate:  4.730001241466175e-06  Varinance:  2.247271342239614e-05 \n",
      "\n",
      "Epoch:  9961  Learning Rate:  4.7252736044371874e-06  Varinance:  2.24413198654667e-05 \n",
      "\n",
      "Epoch:  9962  Learning Rate:  4.720550692682206e-06  Varinance:  2.240997016418558e-05 \n",
      "\n",
      "Epoch:  9963  Learning Rate:  4.715832501478311e-06  Varinance:  2.2378664257288085e-05 \n",
      "\n",
      "Epoch:  9964  Learning Rate:  4.7111190261073105e-06  Varinance:  2.2347402083594994e-05 \n",
      "\n",
      "Epoch:  9965  Learning Rate:  4.7064102618557195e-06  Varinance:  2.2316183582012657e-05 \n",
      "\n",
      "Epoch:  9966  Learning Rate:  4.701706204014792e-06  Varinance:  2.2285008691532778e-05 \n",
      "\n",
      "Epoch:  9967  Learning Rate:  4.69700684788046e-06  Varinance:  2.2253877351232167e-05 \n",
      "\n",
      "Epoch:  9968  Learning Rate:  4.692312188753359e-06  Varinance:  2.2222789500272858e-05 \n",
      "\n",
      "Epoch:  9969  Learning Rate:  4.687622221938846e-06  Varinance:  2.2191745077901836e-05 \n",
      "\n",
      "Epoch:  9970  Learning Rate:  4.682936942746946e-06  Varinance:  2.216074402345099e-05 \n",
      "\n",
      "Epoch:  9971  Learning Rate:  4.6782563464923705e-06  Varinance:  2.2129786276336857e-05 \n",
      "\n",
      "Epoch:  9972  Learning Rate:  4.673580428494539e-06  Varinance:  2.2098871776060704e-05 \n",
      "\n",
      "Epoch:  9973  Learning Rate:  4.668909184077525e-06  Varinance:  2.2068000462208328e-05 \n",
      "\n",
      "Epoch:  9974  Learning Rate:  4.6642426085700775e-06  Varinance:  2.2037172274449798e-05 \n",
      "\n",
      "Epoch:  9975  Learning Rate:  4.659580697305635e-06  Varinance:  2.200638715253958e-05 \n",
      "\n",
      "Epoch:  9976  Learning Rate:  4.654923445622278e-06  Varinance:  2.1975645036316263e-05 \n",
      "\n",
      "Epoch:  9977  Learning Rate:  4.650270848862746e-06  Varinance:  2.194494586570252e-05 \n",
      "\n",
      "Epoch:  9978  Learning Rate:  4.645622902374459e-06  Varinance:  2.1914289580704827e-05 \n",
      "\n",
      "Epoch:  9979  Learning Rate:  4.640979601509461e-06  Varinance:  2.188367612141359e-05 \n",
      "\n",
      "Epoch:  9980  Learning Rate:  4.6363409416244515e-06  Varinance:  2.185310542800291e-05 \n",
      "\n",
      "Epoch:  9981  Learning Rate:  4.631706918080762e-06  Varinance:  2.1822577440730335e-05 \n",
      "\n",
      "Epoch:  9982  Learning Rate:  4.627077526244384e-06  Varinance:  2.1792092099937002e-05 \n",
      "\n",
      "Epoch:  9983  Learning Rate:  4.622452761485919e-06  Varinance:  2.1761649346047334e-05 \n",
      "\n",
      "Epoch:  9984  Learning Rate:  4.617832619180592e-06  Varinance:  2.173124911956903e-05 \n",
      "\n",
      "Epoch:  9985  Learning Rate:  4.613217094708278e-06  Varinance:  2.170089136109278e-05 \n",
      "\n",
      "Epoch:  9986  Learning Rate:  4.608606183453442e-06  Varinance:  2.1670576011292382e-05 \n",
      "\n",
      "Epoch:  9987  Learning Rate:  4.603999880805166e-06  Varinance:  2.1640303010924504e-05 \n",
      "\n",
      "Epoch:  9988  Learning Rate:  4.599398182157162e-06  Varinance:  2.1610072300828462e-05 \n",
      "\n",
      "Epoch:  9989  Learning Rate:  4.594801082907724e-06  Varinance:  2.1579883821926342e-05 \n",
      "\n",
      "Epoch:  9990  Learning Rate:  4.5902085784597425e-06  Varinance:  2.154973751522271e-05 \n",
      "\n",
      "Epoch:  9991  Learning Rate:  4.585620664220731e-06  Varinance:  2.151963332180459e-05 \n",
      "\n",
      "Epoch:  9992  Learning Rate:  4.581037335602766e-06  Varinance:  2.148957118284119e-05 \n",
      "\n",
      "Epoch:  9993  Learning Rate:  4.57645858802251e-06  Varinance:  2.145955103958401e-05 \n",
      "\n",
      "Epoch:  9994  Learning Rate:  4.571884416901232e-06  Varinance:  2.1429572833366635e-05 \n",
      "\n",
      "Epoch:  9995  Learning Rate:  4.567314817664751e-06  Varinance:  2.1399636505604476e-05 \n",
      "\n",
      "Epoch:  9996  Learning Rate:  4.562749785743469e-06  Varinance:  2.1369741997794907e-05 \n",
      "\n",
      "Epoch:  9997  Learning Rate:  4.558189316572345e-06  Varinance:  2.1339889251517024e-05 \n",
      "\n",
      "Epoch:  9998  Learning Rate:  4.553633405590925e-06  Varinance:  2.1310078208431418e-05 \n",
      "\n",
      "Epoch:  9999  Learning Rate:  4.54908204824329e-06  Varinance:  2.12803088102803e-05 \n",
      "\n",
      "Epoch:  10000  Learning Rate:  4.544535239978074e-06  Varinance:  2.1250580998887227e-05 \n",
      "\n",
      "Epoch:  10001  Learning Rate:  4.539992976248485e-06  Varinance:  2.1220894716157055e-05 \n",
      "\n",
      "Epoch:  10002  Learning Rate:  4.5354552525122516e-06  Varinance:  2.1191249904075685e-05 \n",
      "\n",
      "Epoch:  10003  Learning Rate:  4.5309220642316395e-06  Varinance:  2.1161646504710186e-05 \n",
      "\n",
      "Epoch:  10004  Learning Rate:  4.526393406873477e-06  Varinance:  2.1132084460208545e-05 \n",
      "\n",
      "Epoch:  10005  Learning Rate:  4.5218692759091e-06  Varinance:  2.110256371279946e-05 \n",
      "\n",
      "Epoch:  10006  Learning Rate:  4.517349666814367e-06  Varinance:  2.107308420479244e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10007  Learning Rate:  4.5128345750696845e-06  Varinance:  2.1043645878577558e-05 \n",
      "\n",
      "Epoch:  10008  Learning Rate:  4.508323996159954e-06  Varinance:  2.1014248676625396e-05 \n",
      "\n",
      "Epoch:  10009  Learning Rate:  4.503817925574595e-06  Varinance:  2.0984892541486785e-05 \n",
      "\n",
      "Epoch:  10010  Learning Rate:  4.499316358807529e-06  Varinance:  2.0955577415792936e-05 \n",
      "\n",
      "Epoch:  10011  Learning Rate:  4.494819291357204e-06  Varinance:  2.0926303242255195e-05 \n",
      "\n",
      "Epoch:  10012  Learning Rate:  4.4903267187265465e-06  Varinance:  2.0897069963664822e-05 \n",
      "\n",
      "Epoch:  10013  Learning Rate:  4.485838636422972e-06  Varinance:  2.0867877522893114e-05 \n",
      "\n",
      "Epoch:  10014  Learning Rate:  4.481355039958417e-06  Varinance:  2.0838725862891138e-05 \n",
      "\n",
      "Epoch:  10015  Learning Rate:  4.4768759248492765e-06  Varinance:  2.0809614926689687e-05 \n",
      "\n",
      "Epoch:  10016  Learning Rate:  4.4724012866164235e-06  Varinance:  2.078054465739903e-05 \n",
      "\n",
      "Epoch:  10017  Learning Rate:  4.46793112078524e-06  Varinance:  2.0751514998209027e-05 \n",
      "\n",
      "Epoch:  10018  Learning Rate:  4.463465422885548e-06  Varinance:  2.072252589238888e-05 \n",
      "\n",
      "Epoch:  10019  Learning Rate:  4.459004188451643e-06  Varinance:  2.0693577283286956e-05 \n",
      "\n",
      "Epoch:  10020  Learning Rate:  4.454547413022307e-06  Varinance:  2.0664669114330855e-05 \n",
      "\n",
      "Epoch:  10021  Learning Rate:  4.450095092140754e-06  Varinance:  2.0635801329027176e-05 \n",
      "\n",
      "Epoch:  10022  Learning Rate:  4.445647221354657e-06  Varinance:  2.0606973870961476e-05 \n",
      "\n",
      "Epoch:  10023  Learning Rate:  4.44120379621616e-06  Varinance:  2.0578186683798003e-05 \n",
      "\n",
      "Epoch:  10024  Learning Rate:  4.4367648122818285e-06  Varinance:  2.0549439711279827e-05 \n",
      "\n",
      "Epoch:  10025  Learning Rate:  4.432330265112679e-06  Varinance:  2.0520732897228597e-05 \n",
      "\n",
      "Epoch:  10026  Learning Rate:  4.427900150274157e-06  Varinance:  2.0492066185544326e-05 \n",
      "\n",
      "Epoch:  10027  Learning Rate:  4.4234744633361614e-06  Varinance:  2.0463439520205523e-05 \n",
      "\n",
      "Epoch:  10028  Learning Rate:  4.419053199872998e-06  Varinance:  2.0434852845268908e-05 \n",
      "\n",
      "Epoch:  10029  Learning Rate:  4.414636355463395e-06  Varinance:  2.0406306104869392e-05 \n",
      "\n",
      "Epoch:  10030  Learning Rate:  4.4102239256905224e-06  Varinance:  2.0377799243219816e-05 \n",
      "\n",
      "Epoch:  10031  Learning Rate:  4.405815906141943e-06  Varinance:  2.0349332204611064e-05 \n",
      "\n",
      "Epoch:  10032  Learning Rate:  4.40141229240963e-06  Varinance:  2.0320904933411844e-05 \n",
      "\n",
      "Epoch:  10033  Learning Rate:  4.397013080089984e-06  Varinance:  2.0292517374068475e-05 \n",
      "\n",
      "Epoch:  10034  Learning Rate:  4.3926182647837844e-06  Varinance:  2.026416947110498e-05 \n",
      "\n",
      "Epoch:  10035  Learning Rate:  4.3882278420962074e-06  Varinance:  2.0235861169122854e-05 \n",
      "\n",
      "Epoch:  10036  Learning Rate:  4.383841807636847e-06  Varinance:  2.020759241280101e-05 \n",
      "\n",
      "Epoch:  10037  Learning Rate:  4.379460157019658e-06  Varinance:  2.017936314689554e-05 \n",
      "\n",
      "Epoch:  10038  Learning Rate:  4.375082885862984e-06  Varinance:  2.015117331623982e-05 \n",
      "\n",
      "Epoch:  10039  Learning Rate:  4.370709989789569e-06  Varinance:  2.0123022865744278e-05 \n",
      "\n",
      "Epoch:  10040  Learning Rate:  4.366341464426506e-06  Varinance:  2.009491174039621e-05 \n",
      "\n",
      "Epoch:  10041  Learning Rate:  4.361977305405272e-06  Varinance:  2.006683988525986e-05 \n",
      "\n",
      "Epoch:  10042  Learning Rate:  4.3576175083617e-06  Varinance:  2.0038807245476177e-05 \n",
      "\n",
      "Epoch:  10043  Learning Rate:  4.3532620689360065e-06  Varinance:  2.0010813766262796e-05 \n",
      "\n",
      "Epoch:  10044  Learning Rate:  4.348910982772746e-06  Varinance:  1.9982859392913755e-05 \n",
      "\n",
      "Epoch:  10045  Learning Rate:  4.344564245520822e-06  Varinance:  1.9954944070799637e-05 \n",
      "\n",
      "Epoch:  10046  Learning Rate:  4.340221852833513e-06  Varinance:  1.992706774536733e-05 \n",
      "\n",
      "Epoch:  10047  Learning Rate:  4.335883800368419e-06  Varinance:  1.989923036213983e-05 \n",
      "\n",
      "Epoch:  10048  Learning Rate:  4.3315500837874785e-06  Varinance:  1.987143186671634e-05 \n",
      "\n",
      "Epoch:  10049  Learning Rate:  4.327220698756991e-06  Varinance:  1.9843672204772022e-05 \n",
      "\n",
      "Epoch:  10050  Learning Rate:  4.322895640947562e-06  Varinance:  1.981595132205796e-05 \n",
      "\n",
      "Epoch:  10051  Learning Rate:  4.318574906034128e-06  Varinance:  1.978826916440093e-05 \n",
      "\n",
      "Epoch:  10052  Learning Rate:  4.3142584896959655e-06  Varinance:  1.9760625677703475e-05 \n",
      "\n",
      "Epoch:  10053  Learning Rate:  4.309946387616654e-06  Varinance:  1.973302080794372e-05 \n",
      "\n",
      "Epoch:  10054  Learning Rate:  4.305638595484081e-06  Varinance:  1.970545450117514e-05 \n",
      "\n",
      "Epoch:  10055  Learning Rate:  4.30133510899047e-06  Varinance:  1.9677926703526702e-05 \n",
      "\n",
      "Epoch:  10056  Learning Rate:  4.297035923832326e-06  Varinance:  1.9650437361202562e-05 \n",
      "\n",
      "Epoch:  10057  Learning Rate:  4.2927410357104644e-06  Varinance:  1.962298642048208e-05 \n",
      "\n",
      "Epoch:  10058  Learning Rate:  4.288450440329989e-06  Varinance:  1.9595573827719555e-05 \n",
      "\n",
      "Epoch:  10059  Learning Rate:  4.284164133400319e-06  Varinance:  1.956819952934433e-05 \n",
      "\n",
      "Epoch:  10060  Learning Rate:  4.279882110635138e-06  Varinance:  1.9540863471860584e-05 \n",
      "\n",
      "Epoch:  10061  Learning Rate:  4.275604367752418e-06  Varinance:  1.9513565601847124e-05 \n",
      "\n",
      "Epoch:  10062  Learning Rate:  4.271330900474429e-06  Varinance:  1.948630586595749e-05 \n",
      "\n",
      "Epoch:  10063  Learning Rate:  4.267061704527697e-06  Varinance:  1.945908421091974e-05 \n",
      "\n",
      "Epoch:  10064  Learning Rate:  4.2627967756430165e-06  Varinance:  1.9431900583536254e-05 \n",
      "\n",
      "Epoch:  10065  Learning Rate:  4.258536109555475e-06  Varinance:  1.9404754930683822e-05 \n",
      "\n",
      "Epoch:  10066  Learning Rate:  4.254279702004399e-06  Varinance:  1.9377647199313423e-05 \n",
      "\n",
      "Epoch:  10067  Learning Rate:  4.250027548733371e-06  Varinance:  1.9350577336450168e-05 \n",
      "\n",
      "Epoch:  10068  Learning Rate:  4.245779645490253e-06  Varinance:  1.9323545289193073e-05 \n",
      "\n",
      "Epoch:  10069  Learning Rate:  4.241535988027135e-06  Varinance:  1.929655100471515e-05 \n",
      "\n",
      "Epoch:  10070  Learning Rate:  4.2372965721003505e-06  Varinance:  1.9269594430263226e-05 \n",
      "\n",
      "Epoch:  10071  Learning Rate:  4.233061393470499e-06  Varinance:  1.92426755131577e-05 \n",
      "\n",
      "Epoch:  10072  Learning Rate:  4.228830447902393e-06  Varinance:  1.9215794200792677e-05 \n",
      "\n",
      "Epoch:  10073  Learning Rate:  4.224603731165089e-06  Varinance:  1.918895044063571e-05 \n",
      "\n",
      "Epoch:  10074  Learning Rate:  4.220381239031859e-06  Varinance:  1.916214418022778e-05 \n",
      "\n",
      "Epoch:  10075  Learning Rate:  4.2161629672802286e-06  Varinance:  1.9135375367183038e-05 \n",
      "\n",
      "Epoch:  10076  Learning Rate:  4.211948911691915e-06  Varinance:  1.910864394918893e-05 \n",
      "\n",
      "Epoch:  10077  Learning Rate:  4.2077390680528585e-06  Varinance:  1.908194987400597e-05 \n",
      "\n",
      "Epoch:  10078  Learning Rate:  4.203533432153227e-06  Varinance:  1.9055293089467558e-05 \n",
      "\n",
      "Epoch:  10079  Learning Rate:  4.199331999787379e-06  Varinance:  1.9028673543480055e-05 \n",
      "\n",
      "Epoch:  10080  Learning Rate:  4.195134766753873e-06  Varinance:  1.9002091184022578e-05 \n",
      "\n",
      "Epoch:  10081  Learning Rate:  4.19094172885549e-06  Varinance:  1.8975545959146938e-05 \n",
      "\n",
      "Epoch:  10082  Learning Rate:  4.1867528818991856e-06  Varinance:  1.8949037816977422e-05 \n",
      "\n",
      "Epoch:  10083  Learning Rate:  4.182568221696105e-06  Varinance:  1.8922566705710878e-05 \n",
      "\n",
      "Epoch:  10084  Learning Rate:  4.178387744061601e-06  Varinance:  1.8896132573616524e-05 \n",
      "\n",
      "Epoch:  10085  Learning Rate:  4.17421144481519e-06  Varinance:  1.886973536903575e-05 \n",
      "\n",
      "Epoch:  10086  Learning Rate:  4.170039319780564e-06  Varinance:  1.8843375040382202e-05 \n",
      "\n",
      "Epoch:  10087  Learning Rate:  4.165871364785613e-06  Varinance:  1.8817051536141568e-05 \n",
      "\n",
      "Epoch:  10088  Learning Rate:  4.161707575662374e-06  Varinance:  1.8790764804871518e-05 \n",
      "\n",
      "Epoch:  10089  Learning Rate:  4.157547948247057e-06  Varinance:  1.8764514795201498e-05 \n",
      "\n",
      "Epoch:  10090  Learning Rate:  4.153392478380027e-06  Varinance:  1.8738301455832813e-05 \n",
      "\n",
      "Epoch:  10091  Learning Rate:  4.14924116190583e-06  Varinance:  1.8712124735538426e-05 \n",
      "\n",
      "Epoch:  10092  Learning Rate:  4.14509399467314e-06  Varinance:  1.8685984583162767e-05 \n",
      "\n",
      "Epoch:  10093  Learning Rate:  4.140950972534782e-06  Varinance:  1.865988094762183e-05 \n",
      "\n",
      "Epoch:  10094  Learning Rate:  4.13681209134775e-06  Varinance:  1.863381377790293e-05 \n",
      "\n",
      "Epoch:  10095  Learning Rate:  4.132677346973154e-06  Varinance:  1.860778302306469e-05 \n",
      "\n",
      "Epoch:  10096  Learning Rate:  4.128546735276242e-06  Varinance:  1.8581788632236782e-05 \n",
      "\n",
      "Epoch:  10097  Learning Rate:  4.124420252126416e-06  Varinance:  1.8555830554620055e-05 \n",
      "\n",
      "Epoch:  10098  Learning Rate:  4.120297893397187e-06  Varinance:  1.852990873948632e-05 \n",
      "\n",
      "Epoch:  10099  Learning Rate:  4.116179654966187e-06  Varinance:  1.8504023136178148e-05 \n",
      "\n",
      "Epoch:  10100  Learning Rate:  4.112065532715192e-06  Varinance:  1.8478173694108977e-05 \n",
      "\n",
      "Epoch:  10101  Learning Rate:  4.107955522530073e-06  Varinance:  1.8452360362762883e-05 \n",
      "\n",
      "Epoch:  10102  Learning Rate:  4.103849620300811e-06  Varinance:  1.8426583091694543e-05 \n",
      "\n",
      "Epoch:  10103  Learning Rate:  4.099747821921518e-06  Varinance:  1.8400841830529002e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10104  Learning Rate:  4.095650123290389e-06  Varinance:  1.8375136528961784e-05 \n",
      "\n",
      "Epoch:  10105  Learning Rate:  4.091556520309726e-06  Varinance:  1.834946713675868e-05 \n",
      "\n",
      "Epoch:  10106  Learning Rate:  4.087467008885914e-06  Varinance:  1.8323833603755557e-05 \n",
      "\n",
      "Epoch:  10107  Learning Rate:  4.083381584929462e-06  Varinance:  1.8298235879858455e-05 \n",
      "\n",
      "Epoch:  10108  Learning Rate:  4.079300244354934e-06  Varinance:  1.8272673915043376e-05 \n",
      "\n",
      "Epoch:  10109  Learning Rate:  4.075222983080982e-06  Varinance:  1.824714765935622e-05 \n",
      "\n",
      "Epoch:  10110  Learning Rate:  4.071149797030361e-06  Varinance:  1.8221657062912574e-05 \n",
      "\n",
      "Epoch:  10111  Learning Rate:  4.067080682129876e-06  Varinance:  1.8196202075897824e-05 \n",
      "\n",
      "Epoch:  10112  Learning Rate:  4.063015634310405e-06  Varinance:  1.817078264856693e-05 \n",
      "\n",
      "Epoch:  10113  Learning Rate:  4.058954649506913e-06  Varinance:  1.8145398731244253e-05 \n",
      "\n",
      "Epoch:  10114  Learning Rate:  4.054897723658411e-06  Varinance:  1.8120050274323647e-05 \n",
      "\n",
      "Epoch:  10115  Learning Rate:  4.050844852707961e-06  Varinance:  1.809473722826823e-05 \n",
      "\n",
      "Epoch:  10116  Learning Rate:  4.04679603260271e-06  Varinance:  1.806945954361035e-05 \n",
      "\n",
      "Epoch:  10117  Learning Rate:  4.042751259293828e-06  Varinance:  1.804421717095137e-05 \n",
      "\n",
      "Epoch:  10118  Learning Rate:  4.038710528736536e-06  Varinance:  1.8019010060961756e-05 \n",
      "\n",
      "Epoch:  10119  Learning Rate:  4.0346738368901165e-06  Varinance:  1.799383816438088e-05 \n",
      "\n",
      "Epoch:  10120  Learning Rate:  4.030641179717869e-06  Varinance:  1.796870143201683e-05 \n",
      "\n",
      "Epoch:  10121  Learning Rate:  4.026612553187137e-06  Varinance:  1.794359981474653e-05 \n",
      "\n",
      "Epoch:  10122  Learning Rate:  4.022587953269288e-06  Varinance:  1.7918533263515475e-05 \n",
      "\n",
      "Epoch:  10123  Learning Rate:  4.018567375939734e-06  Varinance:  1.789350172933772e-05 \n",
      "\n",
      "Epoch:  10124  Learning Rate:  4.01455081717789e-06  Varinance:  1.7868505163295667e-05 \n",
      "\n",
      "Epoch:  10125  Learning Rate:  4.010538272967191e-06  Varinance:  1.7843543516540146e-05 \n",
      "\n",
      "Epoch:  10126  Learning Rate:  4.006529739295107e-06  Varinance:  1.781861674029022e-05 \n",
      "\n",
      "Epoch:  10127  Learning Rate:  4.002525212153095e-06  Varinance:  1.7793724785833012e-05 \n",
      "\n",
      "Epoch:  10128  Learning Rate:  3.9985246875366235e-06  Varinance:  1.776886760452378e-05 \n",
      "\n",
      "Epoch:  10129  Learning Rate:  3.994528161445178e-06  Varinance:  1.774404514778575e-05 \n",
      "\n",
      "Epoch:  10130  Learning Rate:  3.990535629882228e-06  Varinance:  1.77192573671099e-05 \n",
      "\n",
      "Epoch:  10131  Learning Rate:  3.986547088855233e-06  Varinance:  1.7694504214055074e-05 \n",
      "\n",
      "Epoch:  10132  Learning Rate:  3.982562534375665e-06  Varinance:  1.7669785640247757e-05 \n",
      "\n",
      "Epoch:  10133  Learning Rate:  3.978581962458964e-06  Varinance:  1.764510159738204e-05 \n",
      "\n",
      "Epoch:  10134  Learning Rate:  3.974605369124558e-06  Varinance:  1.7620452037219397e-05 \n",
      "\n",
      "Epoch:  10135  Learning Rate:  3.970632750395845e-06  Varinance:  1.7595836911588788e-05 \n",
      "\n",
      "Epoch:  10136  Learning Rate:  3.966664102300219e-06  Varinance:  1.7571256172386467e-05 \n",
      "\n",
      "Epoch:  10137  Learning Rate:  3.962699420869026e-06  Varinance:  1.754670977157579e-05 \n",
      "\n",
      "Epoch:  10138  Learning Rate:  3.958738702137578e-06  Varinance:  1.7522197661187313e-05 \n",
      "\n",
      "Epoch:  10139  Learning Rate:  3.954781942145169e-06  Varinance:  1.7497719793318575e-05 \n",
      "\n",
      "Epoch:  10140  Learning Rate:  3.950829136935031e-06  Varinance:  1.747327612013406e-05 \n",
      "\n",
      "Epoch:  10141  Learning Rate:  3.946880282554354e-06  Varinance:  1.7448866593864993e-05 \n",
      "\n",
      "Epoch:  10142  Learning Rate:  3.942935375054294e-06  Varinance:  1.7424491166809404e-05 \n",
      "\n",
      "Epoch:  10143  Learning Rate:  3.938994410489937e-06  Varinance:  1.7400149791331983e-05 \n",
      "\n",
      "Epoch:  10144  Learning Rate:  3.935057384920313e-06  Varinance:  1.737584241986385e-05 \n",
      "\n",
      "Epoch:  10145  Learning Rate:  3.9311242944084086e-06  Varinance:  1.7351569004902695e-05 \n",
      "\n",
      "Epoch:  10146  Learning Rate:  3.927195135021125e-06  Varinance:  1.7327329499012512e-05 \n",
      "\n",
      "Epoch:  10147  Learning Rate:  3.923269902829298e-06  Varinance:  1.7303123854823607e-05 \n",
      "\n",
      "Epoch:  10148  Learning Rate:  3.919348593907708e-06  Varinance:  1.7278952025032362e-05 \n",
      "\n",
      "Epoch:  10149  Learning Rate:  3.915431204335037e-06  Varinance:  1.7254813962401334e-05 \n",
      "\n",
      "Epoch:  10150  Learning Rate:  3.911517730193898e-06  Varinance:  1.7230709619759068e-05 \n",
      "\n",
      "Epoch:  10151  Learning Rate:  3.907608167570808e-06  Varinance:  1.7206638949999915e-05 \n",
      "\n",
      "Epoch:  10152  Learning Rate:  3.903702512556218e-06  Varinance:  1.7182601906084123e-05 \n",
      "\n",
      "Epoch:  10153  Learning Rate:  3.899800761244466e-06  Varinance:  1.7158598441037618e-05 \n",
      "\n",
      "Epoch:  10154  Learning Rate:  3.895902909733793e-06  Varinance:  1.7134628507951984e-05 \n",
      "\n",
      "Epoch:  10155  Learning Rate:  3.8920089541263615e-06  Varinance:  1.711069205998424e-05 \n",
      "\n",
      "Epoch:  10156  Learning Rate:  3.888118890528208e-06  Varinance:  1.7086789050356932e-05 \n",
      "\n",
      "Epoch:  10157  Learning Rate:  3.884232715049262e-06  Varinance:  1.7062919432357957e-05 \n",
      "\n",
      "Epoch:  10158  Learning Rate:  3.880350423803363e-06  Varinance:  1.703908315934037e-05 \n",
      "\n",
      "Epoch:  10159  Learning Rate:  3.87647201290821e-06  Varinance:  1.7015280184722488e-05 \n",
      "\n",
      "Epoch:  10160  Learning Rate:  3.872597478485387e-06  Varinance:  1.6991510461987665e-05 \n",
      "\n",
      "Epoch:  10161  Learning Rate:  3.86872681666037e-06  Varinance:  1.696777394468427e-05 \n",
      "\n",
      "Epoch:  10162  Learning Rate:  3.864860023562494e-06  Varinance:  1.694407058642547e-05 \n",
      "\n",
      "Epoch:  10163  Learning Rate:  3.860997095324956e-06  Varinance:  1.6920400340889333e-05 \n",
      "\n",
      "Epoch:  10164  Learning Rate:  3.857138028084842e-06  Varinance:  1.689676316181862e-05 \n",
      "\n",
      "Epoch:  10165  Learning Rate:  3.853282817983078e-06  Varinance:  1.6873159003020636e-05 \n",
      "\n",
      "Epoch:  10166  Learning Rate:  3.849431461164453e-06  Varinance:  1.6849587818367296e-05 \n",
      "\n",
      "Epoch:  10167  Learning Rate:  3.845583953777603e-06  Varinance:  1.6826049561794933e-05 \n",
      "\n",
      "Epoch:  10168  Learning Rate:  3.841740291975034e-06  Varinance:  1.6802544187304254e-05 \n",
      "\n",
      "Epoch:  10169  Learning Rate:  3.837900471913077e-06  Varinance:  1.6779071648960134e-05 \n",
      "\n",
      "Epoch:  10170  Learning Rate:  3.834064489751904e-06  Varinance:  1.6755631900891714e-05 \n",
      "\n",
      "Epoch:  10171  Learning Rate:  3.830232341655549e-06  Varinance:  1.6732224897292204e-05 \n",
      "\n",
      "Epoch:  10172  Learning Rate:  3.826404023791853e-06  Varinance:  1.670885059241873e-05 \n",
      "\n",
      "Epoch:  10173  Learning Rate:  3.822579532332494e-06  Varinance:  1.668550894059239e-05 \n",
      "\n",
      "Epoch:  10174  Learning Rate:  3.8187588634529925e-06  Varinance:  1.6662199896198085e-05 \n",
      "\n",
      "Epoch:  10175  Learning Rate:  3.8149420133326725e-06  Varinance:  1.6638923413684448e-05 \n",
      "\n",
      "Epoch:  10176  Learning Rate:  3.8111289781546776e-06  Varinance:  1.661567944756367e-05 \n",
      "\n",
      "Epoch:  10177  Learning Rate:  3.8073197541059844e-06  Varinance:  1.659246795241157e-05 \n",
      "\n",
      "Epoch:  10178  Learning Rate:  3.8035143373773632e-06  Varinance:  1.656928888286743e-05 \n",
      "\n",
      "Epoch:  10179  Learning Rate:  3.7997127241633895e-06  Varinance:  1.6546142193633797e-05 \n",
      "\n",
      "Epoch:  10180  Learning Rate:  3.7959149106624632e-06  Varinance:  1.6523027839476598e-05 \n",
      "\n",
      "Epoch:  10181  Learning Rate:  3.792120893076764e-06  Varinance:  1.649994577522492e-05 \n",
      "\n",
      "Epoch:  10182  Learning Rate:  3.7883306676122733e-06  Varinance:  1.647689595577097e-05 \n",
      "\n",
      "Epoch:  10183  Learning Rate:  3.784544230478759e-06  Varinance:  1.6453878336069895e-05 \n",
      "\n",
      "Epoch:  10184  Learning Rate:  3.7807615778897985e-06  Varinance:  1.6430892871139847e-05 \n",
      "\n",
      "Epoch:  10185  Learning Rate:  3.7769827060627297e-06  Varinance:  1.640793951606182e-05 \n",
      "\n",
      "Epoch:  10186  Learning Rate:  3.7732076112186763e-06  Varinance:  1.6385018225979465e-05 \n",
      "\n",
      "Epoch:  10187  Learning Rate:  3.769436289582554e-06  Varinance:  1.6362128956099194e-05 \n",
      "\n",
      "Epoch:  10188  Learning Rate:  3.765668737383036e-06  Varinance:  1.6339271661689953e-05 \n",
      "\n",
      "Epoch:  10189  Learning Rate:  3.761904950852563e-06  Varinance:  1.6316446298083214e-05 \n",
      "\n",
      "Epoch:  10190  Learning Rate:  3.7581449262273604e-06  Varinance:  1.629365282067276e-05 \n",
      "\n",
      "Epoch:  10191  Learning Rate:  3.754388659747397e-06  Varinance:  1.6270891184914766e-05 \n",
      "\n",
      "Epoch:  10192  Learning Rate:  3.7506361476564e-06  Varinance:  1.6248161346327643e-05 \n",
      "\n",
      "Epoch:  10193  Learning Rate:  3.74688738620187e-06  Varinance:  1.6225463260491852e-05 \n",
      "\n",
      "Epoch:  10194  Learning Rate:  3.743142371635038e-06  Varinance:  1.620279688304999e-05 \n",
      "\n",
      "Epoch:  10195  Learning Rate:  3.7394011002108827e-06  Varinance:  1.6180162169706623e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10196  Learning Rate:  3.7356635681881464e-06  Varinance:  1.615755907622811e-05 \n",
      "\n",
      "Epoch:  10197  Learning Rate:  3.7319297718292896e-06  Varinance:  1.6134987558442684e-05 \n",
      "\n",
      "Epoch:  10198  Learning Rate:  3.7281997074005153e-06  Varinance:  1.611244757224026e-05 \n",
      "\n",
      "Epoch:  10199  Learning Rate:  3.7244733711717526e-06  Varinance:  1.6089939073572404e-05 \n",
      "\n",
      "Epoch:  10200  Learning Rate:  3.7207507594166786e-06  Varinance:  1.6067462018452126e-05 \n",
      "\n",
      "Epoch:  10201  Learning Rate:  3.7170318684126737e-06  Varinance:  1.6045016362953967e-05 \n",
      "\n",
      "Epoch:  10202  Learning Rate:  3.71331669444084e-06  Varinance:  1.602260206321384e-05 \n",
      "\n",
      "Epoch:  10203  Learning Rate:  3.7096052337860168e-06  Varinance:  1.600021907542884e-05 \n",
      "\n",
      "Epoch:  10204  Learning Rate:  3.7058974827367365e-06  Varinance:  1.5977867355857343e-05 \n",
      "\n",
      "Epoch:  10205  Learning Rate:  3.702193437585242e-06  Varinance:  1.59555468608188e-05 \n",
      "\n",
      "Epoch:  10206  Learning Rate:  3.698493094627499e-06  Varinance:  1.5933257546693714e-05 \n",
      "\n",
      "Epoch:  10207  Learning Rate:  3.6947964501631595e-06  Varinance:  1.5910999369923433e-05 \n",
      "\n",
      "Epoch:  10208  Learning Rate:  3.6911035004955716e-06  Varinance:  1.588877228701024e-05 \n",
      "\n",
      "Epoch:  10209  Learning Rate:  3.687414241931798e-06  Varinance:  1.5866576254517188e-05 \n",
      "\n",
      "Epoch:  10210  Learning Rate:  3.683728670782574e-06  Varinance:  1.584441122906792e-05 \n",
      "\n",
      "Epoch:  10211  Learning Rate:  3.6800467833623208e-06  Varinance:  1.582227716734676e-05 \n",
      "\n",
      "Epoch:  10212  Learning Rate:  3.6763685759891643e-06  Varinance:  1.580017402609852e-05 \n",
      "\n",
      "Epoch:  10213  Learning Rate:  3.6726940449848905e-06  Varinance:  1.5778101762128452e-05 \n",
      "\n",
      "Epoch:  10214  Learning Rate:  3.6690231866749675e-06  Varinance:  1.575606033230208e-05 \n",
      "\n",
      "Epoch:  10215  Learning Rate:  3.66535599738853e-06  Varinance:  1.5734049693545264e-05 \n",
      "\n",
      "Epoch:  10216  Learning Rate:  3.6616924734584025e-06  Varinance:  1.5712069802844035e-05 \n",
      "\n",
      "Epoch:  10217  Learning Rate:  3.6580326112210534e-06  Varinance:  1.5690120617244424e-05 \n",
      "\n",
      "Epoch:  10218  Learning Rate:  3.654376407016614e-06  Varinance:  1.566820209385257e-05 \n",
      "\n",
      "Epoch:  10219  Learning Rate:  3.650723857188892e-06  Varinance:  1.5646314189834484e-05 \n",
      "\n",
      "Epoch:  10220  Learning Rate:  3.6470749580853316e-06  Varinance:  1.5624456862416054e-05 \n",
      "\n",
      "Epoch:  10221  Learning Rate:  3.6434297060570274e-06  Varinance:  1.5602630068882834e-05 \n",
      "\n",
      "Epoch:  10222  Learning Rate:  3.639788097458739e-06  Varinance:  1.5580833766580134e-05 \n",
      "\n",
      "Epoch:  10223  Learning Rate:  3.636150128648851e-06  Varinance:  1.5559067912912847e-05 \n",
      "\n",
      "Epoch:  10224  Learning Rate:  3.632515795989389e-06  Varinance:  1.55373324653453e-05 \n",
      "\n",
      "Epoch:  10225  Learning Rate:  3.628885095846031e-06  Varinance:  1.5515627381401302e-05 \n",
      "\n",
      "Epoch:  10226  Learning Rate:  3.625258024588072e-06  Varinance:  1.549395261866399e-05 \n",
      "\n",
      "Epoch:  10227  Learning Rate:  3.621634578588433e-06  Varinance:  1.547230813477578e-05 \n",
      "\n",
      "Epoch:  10228  Learning Rate:  3.6180147542236814e-06  Varinance:  1.5450693887438166e-05 \n",
      "\n",
      "Epoch:  10229  Learning Rate:  3.614398547873985e-06  Varinance:  1.542910983441183e-05 \n",
      "\n",
      "Epoch:  10230  Learning Rate:  3.6107859559231374e-06  Varinance:  1.540755593351645e-05 \n",
      "\n",
      "Epoch:  10231  Learning Rate:  3.607176974758541e-06  Varinance:  1.538603214263055e-05 \n",
      "\n",
      "Epoch:  10232  Learning Rate:  3.603571600771226e-06  Varinance:  1.536453841969158e-05 \n",
      "\n",
      "Epoch:  10233  Learning Rate:  3.5999698303558117e-06  Varinance:  1.5343074722695715e-05 \n",
      "\n",
      "Epoch:  10234  Learning Rate:  3.596371659910522e-06  Varinance:  1.5321641009697848e-05 \n",
      "\n",
      "Epoch:  10235  Learning Rate:  3.592777085837198e-06  Varinance:  1.530023723881137e-05 \n",
      "\n",
      "Epoch:  10236  Learning Rate:  3.589186104541259e-06  Varinance:  1.527886336820828e-05 \n",
      "\n",
      "Epoch:  10237  Learning Rate:  3.585598712431717e-06  Varinance:  1.5257519356119007e-05 \n",
      "\n",
      "Epoch:  10238  Learning Rate:  3.5820149059211936e-06  Varinance:  1.5236205160832246e-05 \n",
      "\n",
      "Epoch:  10239  Learning Rate:  3.578434681425874e-06  Varinance:  1.5214920740695042e-05 \n",
      "\n",
      "Epoch:  10240  Learning Rate:  3.5748580353655275e-06  Varinance:  1.5193666054112605e-05 \n",
      "\n",
      "Epoch:  10241  Learning Rate:  3.5712849641635215e-06  Varinance:  1.5172441059548276e-05 \n",
      "\n",
      "Epoch:  10242  Learning Rate:  3.567715464246776e-06  Varinance:  1.5151245715523338e-05 \n",
      "\n",
      "Epoch:  10243  Learning Rate:  3.564149532045787e-06  Varinance:  1.5130079980617102e-05 \n",
      "\n",
      "Epoch:  10244  Learning Rate:  3.5605871639946322e-06  Varinance:  1.5108943813466742e-05 \n",
      "\n",
      "Epoch:  10245  Learning Rate:  3.5570283565309393e-06  Varinance:  1.5087837172767133e-05 \n",
      "\n",
      "Epoch:  10246  Learning Rate:  3.553473106095898e-06  Varinance:  1.5066760017270932e-05 \n",
      "\n",
      "Epoch:  10247  Learning Rate:  3.5499214091342536e-06  Varinance:  1.5045712305788391e-05 \n",
      "\n",
      "Epoch:  10248  Learning Rate:  3.5463732620943203e-06  Varinance:  1.5024693997187326e-05 \n",
      "\n",
      "Epoch:  10249  Learning Rate:  3.5428286614279446e-06  Varinance:  1.5003705050392941e-05 \n",
      "\n",
      "Epoch:  10250  Learning Rate:  3.5392876035905194e-06  Varinance:  1.4982745424387894e-05 \n",
      "\n",
      "Epoch:  10251  Learning Rate:  3.5357500850409983e-06  Varinance:  1.496181507821214e-05 \n",
      "\n",
      "Epoch:  10252  Learning Rate:  3.5322161022418576e-06  Varinance:  1.494091397096278e-05 \n",
      "\n",
      "Epoch:  10253  Learning Rate:  3.5286856516591068e-06  Varinance:  1.4920042061794133e-05 \n",
      "\n",
      "Epoch:  10254  Learning Rate:  3.5251587297623083e-06  Varinance:  1.4899199309917548e-05 \n",
      "\n",
      "Epoch:  10255  Learning Rate:  3.521635333024533e-06  Varinance:  1.4878385674601383e-05 \n",
      "\n",
      "Epoch:  10256  Learning Rate:  3.5181154579223785e-06  Varinance:  1.4857601115170822e-05 \n",
      "\n",
      "Epoch:  10257  Learning Rate:  3.514599100935981e-06  Varinance:  1.4836845591007942e-05 \n",
      "\n",
      "Epoch:  10258  Learning Rate:  3.5110862585489774e-06  Varinance:  1.4816119061551562e-05 \n",
      "\n",
      "Epoch:  10259  Learning Rate:  3.5075769272485245e-06  Varinance:  1.4795421486297092e-05 \n",
      "\n",
      "Epoch:  10260  Learning Rate:  3.5040711035252857e-06  Varinance:  1.4774752824796594e-05 \n",
      "\n",
      "Epoch:  10261  Learning Rate:  3.500568783873448e-06  Varinance:  1.475411303665862e-05 \n",
      "\n",
      "Epoch:  10262  Learning Rate:  3.4970699647906865e-06  Varinance:  1.4733502081548164e-05 \n",
      "\n",
      "Epoch:  10263  Learning Rate:  3.4935746427781743e-06  Varinance:  1.471291991918649e-05 \n",
      "\n",
      "Epoch:  10264  Learning Rate:  3.4900828143406025e-06  Varinance:  1.469236650935121e-05 \n",
      "\n",
      "Epoch:  10265  Learning Rate:  3.4865944759861363e-06  Varinance:  1.4671841811876126e-05 \n",
      "\n",
      "Epoch:  10266  Learning Rate:  3.4831096242264296e-06  Varinance:  1.4651345786651068e-05 \n",
      "\n",
      "Epoch:  10267  Learning Rate:  3.479628255576644e-06  Varinance:  1.4630878393621985e-05 \n",
      "\n",
      "Epoch:  10268  Learning Rate:  3.476150366555404e-06  Varinance:  1.461043959279077e-05 \n",
      "\n",
      "Epoch:  10269  Learning Rate:  3.4726759536848144e-06  Varinance:  1.459002934421512e-05 \n",
      "\n",
      "Epoch:  10270  Learning Rate:  3.469205013490473e-06  Varinance:  1.4569647608008609e-05 \n",
      "\n",
      "Epoch:  10271  Learning Rate:  3.465737542501435e-06  Varinance:  1.4549294344340496e-05 \n",
      "\n",
      "Epoch:  10272  Learning Rate:  3.462273537250222e-06  Varinance:  1.4528969513435722e-05 \n",
      "\n",
      "Epoch:  10273  Learning Rate:  3.4588129942728413e-06  Varinance:  1.4508673075574702e-05 \n",
      "\n",
      "Epoch:  10274  Learning Rate:  3.455355910108743e-06  Varinance:  1.448840499109342e-05 \n",
      "\n",
      "Epoch:  10275  Learning Rate:  3.451902281300842e-06  Varinance:  1.4468165220383274e-05 \n",
      "\n",
      "Epoch:  10276  Learning Rate:  3.4484521043955042e-06  Varinance:  1.444795372389091e-05 \n",
      "\n",
      "Epoch:  10277  Learning Rate:  3.445005375942565e-06  Varinance:  1.4427770462118307e-05 \n",
      "\n",
      "Epoch:  10278  Learning Rate:  3.4415620924952885e-06  Varinance:  1.4407615395622598e-05 \n",
      "\n",
      "Epoch:  10279  Learning Rate:  3.4381222506103844e-06  Varinance:  1.4387488485016039e-05 \n",
      "\n",
      "Epoch:  10280  Learning Rate:  3.4346858468480244e-06  Varinance:  1.4367389690965831e-05 \n",
      "\n",
      "Epoch:  10281  Learning Rate:  3.431252877771797e-06  Varinance:  1.4347318974194207e-05 \n",
      "\n",
      "Epoch:  10282  Learning Rate:  3.4278233399487274e-06  Varinance:  1.432727629547826e-05 \n",
      "\n",
      "Epoch:  10283  Learning Rate:  3.42439722994929e-06  Varinance:  1.4307261615649803e-05 \n",
      "\n",
      "Epoch:  10284  Learning Rate:  3.4209745443473673e-06  Varinance:  1.4287274895595442e-05 \n",
      "\n",
      "Epoch:  10285  Learning Rate:  3.417555279720268e-06  Varinance:  1.4267316096256397e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10286  Learning Rate:  3.414139432648739e-06  Varinance:  1.4247385178628477e-05 \n",
      "\n",
      "Epoch:  10287  Learning Rate:  3.4107269997169274e-06  Varinance:  1.4227482103761907e-05 \n",
      "\n",
      "Epoch:  10288  Learning Rate:  3.407317977512394e-06  Varinance:  1.4207606832761391e-05 \n",
      "\n",
      "Epoch:  10289  Learning Rate:  3.403912362626127e-06  Varinance:  1.4187759326785975e-05 \n",
      "\n",
      "Epoch:  10290  Learning Rate:  3.4005101516525076e-06  Varinance:  1.4167939547048882e-05 \n",
      "\n",
      "Epoch:  10291  Learning Rate:  3.3971113411893233e-06  Varinance:  1.4148147454817603e-05 \n",
      "\n",
      "Epoch:  10292  Learning Rate:  3.3937159278377563e-06  Varinance:  1.4128383011413705e-05 \n",
      "\n",
      "Epoch:  10293  Learning Rate:  3.390323908202406e-06  Varinance:  1.4108646178212818e-05 \n",
      "\n",
      "Epoch:  10294  Learning Rate:  3.3869352788912468e-06  Varinance:  1.4088936916644444e-05 \n",
      "\n",
      "Epoch:  10295  Learning Rate:  3.3835500365156433e-06  Varinance:  1.4069255188192053e-05 \n",
      "\n",
      "Epoch:  10296  Learning Rate:  3.3801681776903635e-06  Varinance:  1.4049600954392917e-05 \n",
      "\n",
      "Epoch:  10297  Learning Rate:  3.3767896990335434e-06  Varinance:  1.402997417683796e-05 \n",
      "\n",
      "Epoch:  10298  Learning Rate:  3.373414597166698e-06  Varinance:  1.4010374817171842e-05 \n",
      "\n",
      "Epoch:  10299  Learning Rate:  3.3700428687147357e-06  Varinance:  1.3990802837092775e-05 \n",
      "\n",
      "Epoch:  10300  Learning Rate:  3.366674510305924e-06  Varinance:  1.3971258198352507e-05 \n",
      "\n",
      "Epoch:  10301  Learning Rate:  3.363309518571897e-06  Varinance:  1.3951740862756139e-05 \n",
      "\n",
      "Epoch:  10302  Learning Rate:  3.3599478901476748e-06  Varinance:  1.3932250792162206e-05 \n",
      "\n",
      "Epoch:  10303  Learning Rate:  3.3565896216716227e-06  Varinance:  1.391278794848252e-05 \n",
      "\n",
      "Epoch:  10304  Learning Rate:  3.3532347097854663e-06  Varinance:  1.3893352293682035e-05 \n",
      "\n",
      "Epoch:  10305  Learning Rate:  3.349883151134305e-06  Varinance:  1.3873943789778904e-05 \n",
      "\n",
      "Epoch:  10306  Learning Rate:  3.346534942366574e-06  Varinance:  1.385456239884432e-05 \n",
      "\n",
      "Epoch:  10307  Learning Rate:  3.3431900801340638e-06  Varinance:  1.383520808300249e-05 \n",
      "\n",
      "Epoch:  10308  Learning Rate:  3.3398485610919065e-06  Varinance:  1.381588080443044e-05 \n",
      "\n",
      "Epoch:  10309  Learning Rate:  3.3365103818985944e-06  Varinance:  1.3796580525358133e-05 \n",
      "\n",
      "Epoch:  10310  Learning Rate:  3.3331755392159426e-06  Varinance:  1.3777307208068273e-05 \n",
      "\n",
      "Epoch:  10311  Learning Rate:  3.3298440297091015e-06  Varinance:  1.3758060814896194e-05 \n",
      "\n",
      "Epoch:  10312  Learning Rate:  3.3265158500465745e-06  Varinance:  1.3738841308229916e-05 \n",
      "\n",
      "Epoch:  10313  Learning Rate:  3.323190996900174e-06  Varinance:  1.371964865050997e-05 \n",
      "\n",
      "Epoch:  10314  Learning Rate:  3.319869466945041e-06  Varinance:  1.3700482804229387e-05 \n",
      "\n",
      "Epoch:  10315  Learning Rate:  3.3165512568596583e-06  Varinance:  1.3681343731933528e-05 \n",
      "\n",
      "Epoch:  10316  Learning Rate:  3.3132363633258084e-06  Varinance:  1.3662231396220133e-05 \n",
      "\n",
      "Epoch:  10317  Learning Rate:  3.309924783028592e-06  Varinance:  1.3643145759739207e-05 \n",
      "\n",
      "Epoch:  10318  Learning Rate:  3.306616512656441e-06  Varinance:  1.3624086785192848e-05 \n",
      "\n",
      "Epoch:  10319  Learning Rate:  3.303311548901077e-06  Varinance:  1.360505443533534e-05 \n",
      "\n",
      "Epoch:  10320  Learning Rate:  3.3000098884575325e-06  Varinance:  1.3586048672972962e-05 \n",
      "\n",
      "Epoch:  10321  Learning Rate:  3.296711528024157e-06  Varinance:  1.3567069460963986e-05 \n",
      "\n",
      "Epoch:  10322  Learning Rate:  3.293416464302584e-06  Varinance:  1.3548116762218495e-05 \n",
      "\n",
      "Epoch:  10323  Learning Rate:  3.2901246939977495e-06  Varinance:  1.352919053969845e-05 \n",
      "\n",
      "Epoch:  10324  Learning Rate:  3.2868362138178776e-06  Varinance:  1.3510290756417567e-05 \n",
      "\n",
      "Epoch:  10325  Learning Rate:  3.2835510204745e-06  Varinance:  1.349141737544114e-05 \n",
      "\n",
      "Epoch:  10326  Learning Rate:  3.2802691106824156e-06  Varinance:  1.3472570359886149e-05 \n",
      "\n",
      "Epoch:  10327  Learning Rate:  3.27699048115971e-06  Varinance:  1.3453749672921065e-05 \n",
      "\n",
      "Epoch:  10328  Learning Rate:  3.2737151286277636e-06  Varinance:  1.3434955277765833e-05 \n",
      "\n",
      "Epoch:  10329  Learning Rate:  3.2704430498112194e-06  Varinance:  1.3416187137691716e-05 \n",
      "\n",
      "Epoch:  10330  Learning Rate:  3.2671742414379913e-06  Varinance:  1.3397445216021348e-05 \n",
      "\n",
      "Epoch:  10331  Learning Rate:  3.263908700239283e-06  Varinance:  1.3378729476128604e-05 \n",
      "\n",
      "Epoch:  10332  Learning Rate:  3.260646422949547e-06  Varinance:  1.3360039881438456e-05 \n",
      "\n",
      "Epoch:  10333  Learning Rate:  3.2573874063065e-06  Varinance:  1.3341376395427035e-05 \n",
      "\n",
      "Epoch:  10334  Learning Rate:  3.254131647051136e-06  Varinance:  1.33227389816215e-05 \n",
      "\n",
      "Epoch:  10335  Learning Rate:  3.2508791419276905e-06  Varinance:  1.3304127603599885e-05 \n",
      "\n",
      "Epoch:  10336  Learning Rate:  3.2476298876836514e-06  Varinance:  1.328554222499118e-05 \n",
      "\n",
      "Epoch:  10337  Learning Rate:  3.244383881069778e-06  Varinance:  1.3266982809475157e-05 \n",
      "\n",
      "Epoch:  10338  Learning Rate:  3.2411411188400543e-06  Varinance:  1.324844932078235e-05 \n",
      "\n",
      "Epoch:  10339  Learning Rate:  3.2379015977517207e-06  Varinance:  1.3229941722693893e-05 \n",
      "\n",
      "Epoch:  10340  Learning Rate:  3.2346653145652487e-06  Varinance:  1.3211459979041582e-05 \n",
      "\n",
      "Epoch:  10341  Learning Rate:  3.2314322660443664e-06  Varinance:  1.3193004053707738e-05 \n",
      "\n",
      "Epoch:  10342  Learning Rate:  3.228202448956019e-06  Varinance:  1.3174573910625078e-05 \n",
      "\n",
      "Epoch:  10343  Learning Rate:  3.2249758600703843e-06  Varinance:  1.3156169513776758e-05 \n",
      "\n",
      "Epoch:  10344  Learning Rate:  3.221752496160884e-06  Varinance:  1.3137790827196236e-05 \n",
      "\n",
      "Epoch:  10345  Learning Rate:  3.2185323540041484e-06  Varinance:  1.311943781496723e-05 \n",
      "\n",
      "Epoch:  10346  Learning Rate:  3.2153154303800296e-06  Varinance:  1.3101110441223568e-05 \n",
      "\n",
      "Epoch:  10347  Learning Rate:  3.2121017220716148e-06  Varinance:  1.308280867014925e-05 \n",
      "\n",
      "Epoch:  10348  Learning Rate:  3.208891225865189e-06  Varinance:  1.3064532465978307e-05 \n",
      "\n",
      "Epoch:  10349  Learning Rate:  3.2056839385502513e-06  Varinance:  1.3046281792994666e-05 \n",
      "\n",
      "Epoch:  10350  Learning Rate:  3.2024798569195254e-06  Varinance:  1.302805661553222e-05 \n",
      "\n",
      "Epoch:  10351  Learning Rate:  3.199278977768923e-06  Varinance:  1.3009856897974654e-05 \n",
      "\n",
      "Epoch:  10352  Learning Rate:  3.196081297897559e-06  Varinance:  1.2991682604755443e-05 \n",
      "\n",
      "Epoch:  10353  Learning Rate:  3.192886814107765e-06  Varinance:  1.297353370035767e-05 \n",
      "\n",
      "Epoch:  10354  Learning Rate:  3.189695523205051e-06  Varinance:  1.2955410149314104e-05 \n",
      "\n",
      "Epoch:  10355  Learning Rate:  3.1865074219981263e-06  Varinance:  1.2937311916207064e-05 \n",
      "\n",
      "Epoch:  10356  Learning Rate:  3.1833225072988837e-06  Varinance:  1.2919238965668272e-05 \n",
      "\n",
      "Epoch:  10357  Learning Rate:  3.180140775922419e-06  Varinance:  1.2901191262378933e-05 \n",
      "\n",
      "Epoch:  10358  Learning Rate:  3.176962224686995e-06  Varinance:  1.2883168771069562e-05 \n",
      "\n",
      "Epoch:  10359  Learning Rate:  3.1737868504140555e-06  Varinance:  1.2865171456519973e-05 \n",
      "\n",
      "Epoch:  10360  Learning Rate:  3.170614649928236e-06  Varinance:  1.2847199283559106e-05 \n",
      "\n",
      "Epoch:  10361  Learning Rate:  3.167445620057331e-06  Varinance:  1.2829252217065108e-05 \n",
      "\n",
      "Epoch:  10362  Learning Rate:  3.1642797576323042e-06  Varinance:  1.2811330221965188e-05 \n",
      "\n",
      "Epoch:  10363  Learning Rate:  3.1611170594873042e-06  Varinance:  1.2793433263235482e-05 \n",
      "\n",
      "Epoch:  10364  Learning Rate:  3.157957522459627e-06  Varinance:  1.277556130590112e-05 \n",
      "\n",
      "Epoch:  10365  Learning Rate:  3.1548011433897303e-06  Varinance:  1.2757714315036071e-05 \n",
      "\n",
      "Epoch:  10366  Learning Rate:  3.151647919121245e-06  Varinance:  1.2739892255763114e-05 \n",
      "\n",
      "Epoch:  10367  Learning Rate:  3.148497846500942e-06  Varinance:  1.2722095093253682e-05 \n",
      "\n",
      "Epoch:  10368  Learning Rate:  3.145350922378742e-06  Varinance:  1.2704322792727935e-05 \n",
      "\n",
      "Epoch:  10369  Learning Rate:  3.142207143607732e-06  Varinance:  1.2686575319454613e-05 \n",
      "\n",
      "Epoch:  10370  Learning Rate:  3.1390665070441278e-06  Varinance:  1.266885263875091e-05 \n",
      "\n",
      "Epoch:  10371  Learning Rate:  3.1359290095472917e-06  Varinance:  1.2651154715982538e-05 \n",
      "\n",
      "Epoch:  10372  Learning Rate:  3.1327946479797207e-06  Varinance:  1.2633481516563566e-05 \n",
      "\n",
      "Epoch:  10373  Learning Rate:  3.129663419207065e-06  Varinance:  1.261583300595641e-05 \n",
      "\n",
      "Epoch:  10374  Learning Rate:  3.1265353200980897e-06  Varinance:  1.2598209149671654e-05 \n",
      "\n",
      "Epoch:  10375  Learning Rate:  3.1234103475246886e-06  Varinance:  1.2580609913268136e-05 \n",
      "\n",
      "Epoch:  10376  Learning Rate:  3.1202884983619008e-06  Varinance:  1.2563035262352811e-05 \n",
      "\n",
      "Epoch:  10377  Learning Rate:  3.117169769487872e-06  Varinance:  1.2545485162580604e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10378  Learning Rate:  3.1140541577838663e-06  Varinance:  1.2527959579654492e-05 \n",
      "\n",
      "Epoch:  10379  Learning Rate:  3.1109416601342836e-06  Varinance:  1.251045847932534e-05 \n",
      "\n",
      "Epoch:  10380  Learning Rate:  3.107832273426621e-06  Varinance:  1.249298182739188e-05 \n",
      "\n",
      "Epoch:  10381  Learning Rate:  3.1047259945514847e-06  Varinance:  1.2475529589700555e-05 \n",
      "\n",
      "Epoch:  10382  Learning Rate:  3.101622820402607e-06  Varinance:  1.2458101732145584e-05 \n",
      "\n",
      "Epoch:  10383  Learning Rate:  3.0985227478768084e-06  Varinance:  1.2440698220668834e-05 \n",
      "\n",
      "Epoch:  10384  Learning Rate:  3.095425773874016e-06  Varinance:  1.242331902125968e-05 \n",
      "\n",
      "Epoch:  10385  Learning Rate:  3.0923318952972495e-06  Varinance:  1.2405964099955081e-05 \n",
      "\n",
      "Epoch:  10386  Learning Rate:  3.0892411090526418e-06  Varinance:  1.2388633422839409e-05 \n",
      "\n",
      "Epoch:  10387  Learning Rate:  3.0861534120494004e-06  Varinance:  1.2371326956044447e-05 \n",
      "\n",
      "Epoch:  10388  Learning Rate:  3.0830688011998235e-06  Varinance:  1.2354044665749219e-05 \n",
      "\n",
      "Epoch:  10389  Learning Rate:  3.079987273419309e-06  Varinance:  1.2336786518180063e-05 \n",
      "\n",
      "Epoch:  10390  Learning Rate:  3.0769088256263257e-06  Varinance:  1.2319552479610499e-05 \n",
      "\n",
      "Epoch:  10391  Learning Rate:  3.0738334547424186e-06  Varinance:  1.230234251636109e-05 \n",
      "\n",
      "Epoch:  10392  Learning Rate:  3.0707611576922274e-06  Varinance:  1.2285156594799526e-05 \n",
      "\n",
      "Epoch:  10393  Learning Rate:  3.06769193140345e-06  Varinance:  1.2267994681340442e-05 \n",
      "\n",
      "Epoch:  10394  Learning Rate:  3.064625772806854e-06  Varinance:  1.2250856742445423e-05 \n",
      "\n",
      "Epoch:  10395  Learning Rate:  3.0615626788362925e-06  Varinance:  1.2233742744622843e-05 \n",
      "\n",
      "Epoch:  10396  Learning Rate:  3.058502646428664e-06  Varinance:  1.2216652654427916e-05 \n",
      "\n",
      "Epoch:  10397  Learning Rate:  3.0554456725239323e-06  Varinance:  1.219958643846259e-05 \n",
      "\n",
      "Epoch:  10398  Learning Rate:  3.0523917540651327e-06  Varinance:  1.2182544063375394e-05 \n",
      "\n",
      "Epoch:  10399  Learning Rate:  3.0493408879983414e-06  Varinance:  1.216552549586152e-05 \n",
      "\n",
      "Epoch:  10400  Learning Rate:  3.046293071272692e-06  Varinance:  1.2148530702662681e-05 \n",
      "\n",
      "Epoch:  10401  Learning Rate:  3.043248300840363e-06  Varinance:  1.2131559650566984e-05 \n",
      "\n",
      "Epoch:  10402  Learning Rate:  3.0402065736565934e-06  Varinance:  1.2114612306409006e-05 \n",
      "\n",
      "Epoch:  10403  Learning Rate:  3.037167886679651e-06  Varinance:  1.209768863706962e-05 \n",
      "\n",
      "Epoch:  10404  Learning Rate:  3.0341322368708426e-06  Varinance:  1.2080788609476e-05 \n",
      "\n",
      "Epoch:  10405  Learning Rate:  3.0310996211945295e-06  Varinance:  1.2063912190601449e-05 \n",
      "\n",
      "Epoch:  10406  Learning Rate:  3.02807003661809e-06  Varinance:  1.2047059347465471e-05 \n",
      "\n",
      "Epoch:  10407  Learning Rate:  3.0250434801119346e-06  Varinance:  1.2030230047133646e-05 \n",
      "\n",
      "Epoch:  10408  Learning Rate:  3.0220199486495163e-06  Varinance:  1.2013424256717497e-05 \n",
      "\n",
      "Epoch:  10409  Learning Rate:  3.0189994392072986e-06  Varinance:  1.1996641943374554e-05 \n",
      "\n",
      "Epoch:  10410  Learning Rate:  3.0159819487647667e-06  Varinance:  1.1979883074308209e-05 \n",
      "\n",
      "Epoch:  10411  Learning Rate:  3.01296747430444e-06  Varinance:  1.1963147616767684e-05 \n",
      "\n",
      "Epoch:  10412  Learning Rate:  3.009956012811839e-06  Varinance:  1.1946435538047897e-05 \n",
      "\n",
      "Epoch:  10413  Learning Rate:  3.0069475612754954e-06  Varinance:  1.1929746805489511e-05 \n",
      "\n",
      "Epoch:  10414  Learning Rate:  3.0039421166869696e-06  Varinance:  1.1913081386478817e-05 \n",
      "\n",
      "Epoch:  10415  Learning Rate:  3.0009396760408108e-06  Varinance:  1.1896439248447601e-05 \n",
      "\n",
      "Epoch:  10416  Learning Rate:  2.997940236334578e-06  Varinance:  1.1879820358873207e-05 \n",
      "\n",
      "Epoch:  10417  Learning Rate:  2.994943794568826e-06  Varinance:  1.1863224685278397e-05 \n",
      "\n",
      "Epoch:  10418  Learning Rate:  2.991950347747124e-06  Varinance:  1.1846652195231314e-05 \n",
      "\n",
      "Epoch:  10419  Learning Rate:  2.9889598928760183e-06  Varinance:  1.1830102856345355e-05 \n",
      "\n",
      "Epoch:  10420  Learning Rate:  2.9859724269650496e-06  Varinance:  1.1813576636279214e-05 \n",
      "\n",
      "Epoch:  10421  Learning Rate:  2.982987947026762e-06  Varinance:  1.179707350273677e-05 \n",
      "\n",
      "Epoch:  10422  Learning Rate:  2.98000645007667e-06  Varinance:  1.1780593423466952e-05 \n",
      "\n",
      "Epoch:  10423  Learning Rate:  2.9770279331332714e-06  Varinance:  1.1764136366263809e-05 \n",
      "\n",
      "Epoch:  10424  Learning Rate:  2.974052393218059e-06  Varinance:  1.1747702298966357e-05 \n",
      "\n",
      "Epoch:  10425  Learning Rate:  2.971079827355488e-06  Varinance:  1.173129118945856e-05 \n",
      "\n",
      "Epoch:  10426  Learning Rate:  2.968110232572986e-06  Varinance:  1.1714903005669188e-05 \n",
      "\n",
      "Epoch:  10427  Learning Rate:  2.9651436059009693e-06  Varinance:  1.1698537715571873e-05 \n",
      "\n",
      "Epoch:  10428  Learning Rate:  2.962179944372806e-06  Varinance:  1.1682195287184984e-05 \n",
      "\n",
      "Epoch:  10429  Learning Rate:  2.9592192450248285e-06  Varinance:  1.1665875688571514e-05 \n",
      "\n",
      "Epoch:  10430  Learning Rate:  2.956261504896348e-06  Varinance:  1.164957888783912e-05 \n",
      "\n",
      "Epoch:  10431  Learning Rate:  2.9533067210296185e-06  Varinance:  1.1633304853140002e-05 \n",
      "\n",
      "Epoch:  10432  Learning Rate:  2.950354890469856e-06  Varinance:  1.1617053552670867e-05 \n",
      "\n",
      "Epoch:  10433  Learning Rate:  2.9474060102652254e-06  Varinance:  1.1600824954672786e-05 \n",
      "\n",
      "Epoch:  10434  Learning Rate:  2.944460077466855e-06  Varinance:  1.158461902743126e-05 \n",
      "\n",
      "Epoch:  10435  Learning Rate:  2.941517089128808e-06  Varinance:  1.1568435739276096e-05 \n",
      "\n",
      "Epoch:  10436  Learning Rate:  2.93857704230809e-06  Varinance:  1.1552275058581274e-05 \n",
      "\n",
      "Epoch:  10437  Learning Rate:  2.935639934064664e-06  Varinance:  1.1536136953765026e-05 \n",
      "\n",
      "Epoch:  10438  Learning Rate:  2.932705761461417e-06  Varinance:  1.1520021393289675e-05 \n",
      "\n",
      "Epoch:  10439  Learning Rate:  2.9297745215641706e-06  Varinance:  1.1503928345661623e-05 \n",
      "\n",
      "Epoch:  10440  Learning Rate:  2.926846211441695e-06  Varinance:  1.1487857779431208e-05 \n",
      "\n",
      "Epoch:  10441  Learning Rate:  2.9239208281656752e-06  Varinance:  1.1471809663192763e-05 \n",
      "\n",
      "Epoch:  10442  Learning Rate:  2.9209983688107216e-06  Varinance:  1.145578396558449e-05 \n",
      "\n",
      "Epoch:  10443  Learning Rate:  2.9180788304543855e-06  Varinance:  1.1439780655288344e-05 \n",
      "\n",
      "Epoch:  10444  Learning Rate:  2.915162210177123e-06  Varinance:  1.1423799701030093e-05 \n",
      "\n",
      "Epoch:  10445  Learning Rate:  2.9122485050623085e-06  Varinance:  1.1407841071579168e-05 \n",
      "\n",
      "Epoch:  10446  Learning Rate:  2.909337712196247e-06  Varinance:  1.1391904735748655e-05 \n",
      "\n",
      "Epoch:  10447  Learning Rate:  2.90642982866814e-06  Varinance:  1.1375990662395142e-05 \n",
      "\n",
      "Epoch:  10448  Learning Rate:  2.903524851570104e-06  Varinance:  1.1360098820418784e-05 \n",
      "\n",
      "Epoch:  10449  Learning Rate:  2.900622777997156e-06  Varinance:  1.1344229178763183e-05 \n",
      "\n",
      "Epoch:  10450  Learning Rate:  2.8977236050472333e-06  Varinance:  1.1328381706415262e-05 \n",
      "\n",
      "Epoch:  10451  Learning Rate:  2.894827329821157e-06  Varinance:  1.1312556372405335e-05 \n",
      "\n",
      "Epoch:  10452  Learning Rate:  2.8919339494226466e-06  Varinance:  1.1296753145806954e-05 \n",
      "\n",
      "Epoch:  10453  Learning Rate:  2.8890434609583317e-06  Varinance:  1.1280971995736894e-05 \n",
      "\n",
      "Epoch:  10454  Learning Rate:  2.886155861537719e-06  Varinance:  1.1265212891355015e-05 \n",
      "\n",
      "Epoch:  10455  Learning Rate:  2.883271148273203e-06  Varinance:  1.1249475801864319e-05 \n",
      "\n",
      "Epoch:  10456  Learning Rate:  2.88038931828008e-06  Varinance:  1.1233760696510827e-05 \n",
      "\n",
      "Epoch:  10457  Learning Rate:  2.877510368676516e-06  Varinance:  1.121806754458347e-05 \n",
      "\n",
      "Epoch:  10458  Learning Rate:  2.8746342965835547e-06  Varinance:  1.1202396315414131e-05 \n",
      "\n",
      "Epoch:  10459  Learning Rate:  2.8717610991251354e-06  Varinance:  1.1186746978377527e-05 \n",
      "\n",
      "Epoch:  10460  Learning Rate:  2.868890773428054e-06  Varinance:  1.1171119502891166e-05 \n",
      "\n",
      "Epoch:  10461  Learning Rate:  2.86602331662198e-06  Varinance:  1.1155513858415226e-05 \n",
      "\n",
      "Epoch:  10462  Learning Rate:  2.863158725839467e-06  Varinance:  1.1139930014452603e-05 \n",
      "\n",
      "Epoch:  10463  Learning Rate:  2.8602969982159183e-06  Varinance:  1.1124367940548803e-05 \n",
      "\n",
      "Epoch:  10464  Learning Rate:  2.8574381308896057e-06  Varinance:  1.110882760629181e-05 \n",
      "\n",
      "Epoch:  10465  Learning Rate:  2.8545821210016576e-06  Varinance:  1.1093308981312152e-05 \n",
      "\n",
      "Epoch:  10466  Learning Rate:  2.851728965696073e-06  Varinance:  1.1077812035282787e-05 \n",
      "\n",
      "Epoch:  10467  Learning Rate:  2.8488786621196924e-06  Varinance:  1.1062336737918975e-05 \n",
      "\n",
      "Epoch:  10468  Learning Rate:  2.846031207422206e-06  Varinance:  1.1046883058978344e-05 \n",
      "\n",
      "Epoch:  10469  Learning Rate:  2.843186598756169e-06  Varinance:  1.1031450968260749e-05 \n",
      "\n",
      "Epoch:  10470  Learning Rate:  2.8403448332769675e-06  Varinance:  1.1016040435608256e-05 \n",
      "\n",
      "Epoch:  10471  Learning Rate:  2.8375059081428315e-06  Varinance:  1.1000651430904993e-05 \n",
      "\n",
      "Epoch:  10472  Learning Rate:  2.8346698205148448e-06  Varinance:  1.098528392407723e-05 \n",
      "\n",
      "Epoch:  10473  Learning Rate:  2.8318365675569148e-06  Varinance:  1.0969937885093237e-05 \n",
      "\n",
      "Epoch:  10474  Learning Rate:  2.8290061464357836e-06  Varinance:  1.0954613283963188e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10475  Learning Rate:  2.826178554321039e-06  Varinance:  1.0939310090739207e-05 \n",
      "\n",
      "Epoch:  10476  Learning Rate:  2.823353788385085e-06  Varinance:  1.092402827551523e-05 \n",
      "\n",
      "Epoch:  10477  Learning Rate:  2.820531845803149e-06  Varinance:  1.0908767808426996e-05 \n",
      "\n",
      "Epoch:  10478  Learning Rate:  2.8177127237532993e-06  Varinance:  1.0893528659651904e-05 \n",
      "\n",
      "Epoch:  10479  Learning Rate:  2.8148964194164084e-06  Varinance:  1.087831079940907e-05 \n",
      "\n",
      "Epoch:  10480  Learning Rate:  2.8120829299761707e-06  Varinance:  1.0863114197959214e-05 \n",
      "\n",
      "Epoch:  10481  Learning Rate:  2.809272252619093e-06  Varinance:  1.0847938825604544e-05 \n",
      "\n",
      "Epoch:  10482  Learning Rate:  2.8064643845345067e-06  Varinance:  1.083278465268881e-05 \n",
      "\n",
      "Epoch:  10483  Learning Rate:  2.803659322914539e-06  Varinance:  1.0817651649597174e-05 \n",
      "\n",
      "Epoch:  10484  Learning Rate:  2.8008570649541227e-06  Varinance:  1.0802539786756187e-05 \n",
      "\n",
      "Epoch:  10485  Learning Rate:  2.7980576078510093e-06  Varinance:  1.0787449034633654e-05 \n",
      "\n",
      "Epoch:  10486  Learning Rate:  2.7952609488057375e-06  Varinance:  1.0772379363738697e-05 \n",
      "\n",
      "Epoch:  10487  Learning Rate:  2.7924670850216424e-06  Varinance:  1.0757330744621629e-05 \n",
      "\n",
      "Epoch:  10488  Learning Rate:  2.78967601370487e-06  Varinance:  1.0742303147873852e-05 \n",
      "\n",
      "Epoch:  10489  Learning Rate:  2.786887732064344e-06  Varinance:  1.0727296544127906e-05 \n",
      "\n",
      "Epoch:  10490  Learning Rate:  2.7841022373117773e-06  Varinance:  1.0712310904057338e-05 \n",
      "\n",
      "Epoch:  10491  Learning Rate:  2.781319526661684e-06  Varinance:  1.0697346198376677e-05 \n",
      "\n",
      "Epoch:  10492  Learning Rate:  2.77853959733135e-06  Varinance:  1.0682402397841314e-05 \n",
      "\n",
      "Epoch:  10493  Learning Rate:  2.77576244654084e-06  Varinance:  1.0667479473247543e-05 \n",
      "\n",
      "Epoch:  10494  Learning Rate:  2.7729880715130123e-06  Varinance:  1.065257739543246e-05 \n",
      "\n",
      "Epoch:  10495  Learning Rate:  2.7702164694734877e-06  Varinance:  1.0637696135273839e-05 \n",
      "\n",
      "Epoch:  10496  Learning Rate:  2.767447637650663e-06  Varinance:  1.0622835663690198e-05 \n",
      "\n",
      "Epoch:  10497  Learning Rate:  2.764681573275702e-06  Varinance:  1.060799595164066e-05 \n",
      "\n",
      "Epoch:  10498  Learning Rate:  2.761918273582549e-06  Varinance:  1.0593176970124934e-05 \n",
      "\n",
      "Epoch:  10499  Learning Rate:  2.7591577358079003e-06  Varinance:  1.0578378690183188e-05 \n",
      "\n",
      "Epoch:  10500  Learning Rate:  2.756399957191212e-06  Varinance:  1.0563601082896098e-05 \n",
      "\n",
      "Epoch:  10501  Learning Rate:  2.753644934974716e-06  Varinance:  1.0548844119384745e-05 \n",
      "\n",
      "Epoch:  10502  Learning Rate:  2.750892666403384e-06  Varinance:  1.0534107770810494e-05 \n",
      "\n",
      "Epoch:  10503  Learning Rate:  2.748143148724943e-06  Varinance:  1.0519392008375048e-05 \n",
      "\n",
      "Epoch:  10504  Learning Rate:  2.7453963791898846e-06  Varinance:  1.050469680332033e-05 \n",
      "\n",
      "Epoch:  10505  Learning Rate:  2.7426523550514345e-06  Varinance:  1.049002212692845e-05 \n",
      "\n",
      "Epoch:  10506  Learning Rate:  2.7399110735655627e-06  Varinance:  1.0475367950521579e-05 \n",
      "\n",
      "Epoch:  10507  Learning Rate:  2.7371725319909974e-06  Varinance:  1.046073424546201e-05 \n",
      "\n",
      "Epoch:  10508  Learning Rate:  2.7344367275891927e-06  Varinance:  1.0446120983152041e-05 \n",
      "\n",
      "Epoch:  10509  Learning Rate:  2.7317036576243434e-06  Varinance:  1.043152813503386e-05 \n",
      "\n",
      "Epoch:  10510  Learning Rate:  2.728973319363374e-06  Varinance:  1.041695567258961e-05 \n",
      "\n",
      "Epoch:  10511  Learning Rate:  2.726245710075957e-06  Varinance:  1.0402403567341252e-05 \n",
      "\n",
      "Epoch:  10512  Learning Rate:  2.723520827034477e-06  Varinance:  1.0387871790850546e-05 \n",
      "\n",
      "Epoch:  10513  Learning Rate:  2.7207986675140454e-06  Varinance:  1.0373360314718929e-05 \n",
      "\n",
      "Epoch:  10514  Learning Rate:  2.718079228792514e-06  Varinance:  1.0358869110587556e-05 \n",
      "\n",
      "Epoch:  10515  Learning Rate:  2.7153625081504375e-06  Varinance:  1.0344398150137212e-05 \n",
      "\n",
      "Epoch:  10516  Learning Rate:  2.7126485028710904e-06  Varinance:  1.0329947405088173e-05 \n",
      "\n",
      "Epoch:  10517  Learning Rate:  2.709937210240477e-06  Varinance:  1.0315516847200286e-05 \n",
      "\n",
      "Epoch:  10518  Learning Rate:  2.7072286275473e-06  Varinance:  1.0301106448272828e-05 \n",
      "\n",
      "Epoch:  10519  Learning Rate:  2.7045227520829707e-06  Varinance:  1.0286716180144485e-05 \n",
      "\n",
      "Epoch:  10520  Learning Rate:  2.7018195811416246e-06  Varinance:  1.0272346014693234e-05 \n",
      "\n",
      "Epoch:  10521  Learning Rate:  2.6991191120200844e-06  Varinance:  1.0257995923836391e-05 \n",
      "\n",
      "Epoch:  10522  Learning Rate:  2.696421342017876e-06  Varinance:  1.02436658795305e-05 \n",
      "\n",
      "Epoch:  10523  Learning Rate:  2.6937262684372394e-06  Varinance:  1.0229355853771227e-05 \n",
      "\n",
      "Epoch:  10524  Learning Rate:  2.6910338885830956e-06  Varinance:  1.0215065818593417e-05 \n",
      "\n",
      "Epoch:  10525  Learning Rate:  2.688344199763065e-06  Varinance:  1.020079574607096e-05 \n",
      "\n",
      "Epoch:  10526  Learning Rate:  2.685657199287453e-06  Varinance:  1.0186545608316775e-05 \n",
      "\n",
      "Epoch:  10527  Learning Rate:  2.682972884469269e-06  Varinance:  1.0172315377482684e-05 \n",
      "\n",
      "Epoch:  10528  Learning Rate:  2.680291252624193e-06  Varinance:  1.015810502575947e-05 \n",
      "\n",
      "Epoch:  10529  Learning Rate:  2.6776123010705884e-06  Varinance:  1.014391452537676e-05 \n",
      "\n",
      "Epoch:  10530  Learning Rate:  2.6749360271295125e-06  Varinance:  1.0129743848602926e-05 \n",
      "\n",
      "Epoch:  10531  Learning Rate:  2.672262428124687e-06  Varinance:  1.0115592967745127e-05 \n",
      "\n",
      "Epoch:  10532  Learning Rate:  2.6695915013825076e-06  Varinance:  1.0101461855149215e-05 \n",
      "\n",
      "Epoch:  10533  Learning Rate:  2.6669232442320564e-06  Varinance:  1.0087350483199615e-05 \n",
      "\n",
      "Epoch:  10534  Learning Rate:  2.664257654005072e-06  Varinance:  1.0073258824319387e-05 \n",
      "\n",
      "Epoch:  10535  Learning Rate:  2.6615947280359587e-06  Varinance:  1.0059186850970092e-05 \n",
      "\n",
      "Epoch:  10536  Learning Rate:  2.6589344636618e-06  Varinance:  1.0045134535651785e-05 \n",
      "\n",
      "Epoch:  10537  Learning Rate:  2.6562768582223267e-06  Varinance:  1.0031101850902886e-05 \n",
      "\n",
      "Epoch:  10538  Learning Rate:  2.6536219090599276e-06  Varinance:  1.001708876930022e-05 \n",
      "\n",
      "Epoch:  10539  Learning Rate:  2.6509696135196643e-06  Varinance:  1.0003095263458931e-05 \n",
      "\n",
      "Epoch:  10540  Learning Rate:  2.648319968949235e-06  Varinance:  9.989121306032365e-06 \n",
      "\n",
      "Epoch:  10541  Learning Rate:  2.645672972698995e-06  Varinance:  9.975166869712116e-06 \n",
      "\n",
      "Epoch:  10542  Learning Rate:  2.6430286221219437e-06  Varinance:  9.961231927227916e-06 \n",
      "\n",
      "Epoch:  10543  Learning Rate:  2.64038691457374e-06  Varinance:  9.947316451347609e-06 \n",
      "\n",
      "Epoch:  10544  Learning Rate:  2.6377478474126704e-06  Varinance:  9.933420414877027e-06 \n",
      "\n",
      "Epoch:  10545  Learning Rate:  2.6351114179996634e-06  Varinance:  9.919543790660045e-06 \n",
      "\n",
      "Epoch:  10546  Learning Rate:  2.6324776236982987e-06  Varinance:  9.905686551578474e-06 \n",
      "\n",
      "Epoch:  10547  Learning Rate:  2.629846461874777e-06  Varinance:  9.891848670551954e-06 \n",
      "\n",
      "Epoch:  10548  Learning Rate:  2.627217929897932e-06  Varinance:  9.878030120538011e-06 \n",
      "\n",
      "Epoch:  10549  Learning Rate:  2.62459202513924e-06  Varinance:  9.864230874531928e-06 \n",
      "\n",
      "Epoch:  10550  Learning Rate:  2.6219687449727925e-06  Varinance:  9.85045090556673e-06 \n",
      "\n",
      "Epoch:  10551  Learning Rate:  2.6193480867753034e-06  Varinance:  9.836690186713066e-06 \n",
      "\n",
      "Epoch:  10552  Learning Rate:  2.616730047926124e-06  Varinance:  9.82294869107925e-06 \n",
      "\n",
      "Epoch:  10553  Learning Rate:  2.6141146258072106e-06  Varinance:  9.809226391811167e-06 \n",
      "\n",
      "Epoch:  10554  Learning Rate:  2.611501817803136e-06  Varinance:  9.795523262092163e-06 \n",
      "\n",
      "Epoch:  10555  Learning Rate:  2.6088916213011017e-06  Varinance:  9.781839275143098e-06 \n",
      "\n",
      "Epoch:  10556  Learning Rate:  2.6062840336909065e-06  Varinance:  9.768174404222221e-06 \n",
      "\n",
      "Epoch:  10557  Learning Rate:  2.6036790523649615e-06  Varinance:  9.754528622625166e-06 \n",
      "\n",
      "Epoch:  10558  Learning Rate:  2.6010766747182817e-06  Varinance:  9.740901903684804e-06 \n",
      "\n",
      "Epoch:  10559  Learning Rate:  2.5984768981484976e-06  Varinance:  9.727294220771327e-06 \n",
      "\n",
      "Epoch:  10560  Learning Rate:  2.5958797200558285e-06  Varinance:  9.713705547292116e-06 \n",
      "\n",
      "Epoch:  10561  Learning Rate:  2.593285137843091e-06  Varinance:  9.700135856691655e-06 \n",
      "\n",
      "Epoch:  10562  Learning Rate:  2.590693148915712e-06  Varinance:  9.686585122451574e-06 \n",
      "\n",
      "Epoch:  10563  Learning Rate:  2.588103750681698e-06  Varinance:  9.673053318090535e-06 \n",
      "\n",
      "Epoch:  10564  Learning Rate:  2.5855169405516456e-06  Varinance:  9.659540417164207e-06 \n",
      "\n",
      "Epoch:  10565  Learning Rate:  2.582932715938754e-06  Varinance:  9.646046393265149e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10566  Learning Rate:  2.580351074258793e-06  Varinance:  9.632571220022863e-06 \n",
      "\n",
      "Epoch:  10567  Learning Rate:  2.577772012930118e-06  Varinance:  9.619114871103693e-06 \n",
      "\n",
      "Epoch:  10568  Learning Rate:  2.575195529373674e-06  Varinance:  9.605677320210711e-06 \n",
      "\n",
      "Epoch:  10569  Learning Rate:  2.5726216210129746e-06  Varinance:  9.592258541083784e-06 \n",
      "\n",
      "Epoch:  10570  Learning Rate:  2.570050285274106e-06  Varinance:  9.57885850749944e-06 \n",
      "\n",
      "Epoch:  10571  Learning Rate:  2.567481519585741e-06  Varinance:  9.565477193270862e-06 \n",
      "\n",
      "Epoch:  10572  Learning Rate:  2.56491532137911e-06  Varinance:  9.552114572247763e-06 \n",
      "\n",
      "Epoch:  10573  Learning Rate:  2.562351688088014e-06  Varinance:  9.538770618316432e-06 \n",
      "\n",
      "Epoch:  10574  Learning Rate:  2.559790617148815e-06  Varinance:  9.52544530539965e-06 \n",
      "\n",
      "Epoch:  10575  Learning Rate:  2.557232106000451e-06  Varinance:  9.512138607456563e-06 \n",
      "\n",
      "Epoch:  10576  Learning Rate:  2.554676152084406e-06  Varinance:  9.498850498482757e-06 \n",
      "\n",
      "Epoch:  10577  Learning Rate:  2.552122752844722e-06  Varinance:  9.485580952510121e-06 \n",
      "\n",
      "Epoch:  10578  Learning Rate:  2.549571905728007e-06  Varinance:  9.47232994360684e-06 \n",
      "\n",
      "Epoch:  10579  Learning Rate:  2.547023608183411e-06  Varinance:  9.459097445877272e-06 \n",
      "\n",
      "Epoch:  10580  Learning Rate:  2.544477857662631e-06  Varinance:  9.445883433462005e-06 \n",
      "\n",
      "Epoch:  10581  Learning Rate:  2.541934651619925e-06  Varinance:  9.432687880537749e-06 \n",
      "\n",
      "Epoch:  10582  Learning Rate:  2.5393939875120823e-06  Varinance:  9.419510761317236e-06 \n",
      "\n",
      "Epoch:  10583  Learning Rate:  2.536855862798434e-06  Varinance:  9.406352050049277e-06 \n",
      "\n",
      "Epoch:  10584  Learning Rate:  2.5343202749408652e-06  Varinance:  9.393211721018636e-06 \n",
      "\n",
      "Epoch:  10585  Learning Rate:  2.5317872214037816e-06  Varinance:  9.380089748546018e-06 \n",
      "\n",
      "Epoch:  10586  Learning Rate:  2.5292566996541265e-06  Varinance:  9.36698610698795e-06 \n",
      "\n",
      "Epoch:  10587  Learning Rate:  2.5267287071613864e-06  Varinance:  9.353900770736834e-06 \n",
      "\n",
      "Epoch:  10588  Learning Rate:  2.524203241397564e-06  Varinance:  9.340833714220843e-06 \n",
      "\n",
      "Epoch:  10589  Learning Rate:  2.521680299837193e-06  Varinance:  9.327784911903826e-06 \n",
      "\n",
      "Epoch:  10590  Learning Rate:  2.5191598799573274e-06  Varinance:  9.31475433828535e-06 \n",
      "\n",
      "Epoch:  10591  Learning Rate:  2.5166419792375564e-06  Varinance:  9.301741967900593e-06 \n",
      "\n",
      "Epoch:  10592  Learning Rate:  2.5141265951599747e-06  Varinance:  9.28874777532032e-06 \n",
      "\n",
      "Epoch:  10593  Learning Rate:  2.511613725209193e-06  Varinance:  9.275771735150772e-06 \n",
      "\n",
      "Epoch:  10594  Learning Rate:  2.50910336687235e-06  Varinance:  9.262813822033714e-06 \n",
      "\n",
      "Epoch:  10595  Learning Rate:  2.506595517639083e-06  Varinance:  9.249874010646335e-06 \n",
      "\n",
      "Epoch:  10596  Learning Rate:  2.504090175001538e-06  Varinance:  9.236952275701146e-06 \n",
      "\n",
      "Epoch:  10597  Learning Rate:  2.501587336454381e-06  Varinance:  9.224048591946037e-06 \n",
      "\n",
      "Epoch:  10598  Learning Rate:  2.4990869994947696e-06  Varinance:  9.211162934164177e-06 \n",
      "\n",
      "Epoch:  10599  Learning Rate:  2.496589161622361e-06  Varinance:  9.198295277173906e-06 \n",
      "\n",
      "Epoch:  10600  Learning Rate:  2.494093820339327e-06  Varinance:  9.185445595828794e-06 \n",
      "\n",
      "Epoch:  10601  Learning Rate:  2.4916009731503204e-06  Varinance:  9.172613865017528e-06 \n",
      "\n",
      "Epoch:  10602  Learning Rate:  2.489110617562491e-06  Varinance:  9.159800059663883e-06 \n",
      "\n",
      "Epoch:  10603  Learning Rate:  2.4866227510854903e-06  Varinance:  9.147004154726619e-06 \n",
      "\n",
      "Epoch:  10604  Learning Rate:  2.4841373712314486e-06  Varinance:  9.134226125199527e-06 \n",
      "\n",
      "Epoch:  10605  Learning Rate:  2.4816544755149846e-06  Varinance:  9.121465946111331e-06 \n",
      "\n",
      "Epoch:  10606  Learning Rate:  2.4791740614531984e-06  Varinance:  9.108723592525589e-06 \n",
      "\n",
      "Epoch:  10607  Learning Rate:  2.476696126565685e-06  Varinance:  9.095999039540744e-06 \n",
      "\n",
      "Epoch:  10608  Learning Rate:  2.4742206683745046e-06  Varinance:  9.083292262290011e-06 \n",
      "\n",
      "Epoch:  10609  Learning Rate:  2.471747684404194e-06  Varinance:  9.070603235941353e-06 \n",
      "\n",
      "Epoch:  10610  Learning Rate:  2.4692771721817784e-06  Varinance:  9.057931935697383e-06 \n",
      "\n",
      "Epoch:  10611  Learning Rate:  2.466809129236741e-06  Varinance:  9.045278336795395e-06 \n",
      "\n",
      "Epoch:  10612  Learning Rate:  2.464343553101034e-06  Varinance:  9.032642414507281e-06 \n",
      "\n",
      "Epoch:  10613  Learning Rate:  2.4618804413090893e-06  Varinance:  9.020024144139426e-06 \n",
      "\n",
      "Epoch:  10614  Learning Rate:  2.4594197913977913e-06  Varinance:  9.00742350103276e-06 \n",
      "\n",
      "Epoch:  10615  Learning Rate:  2.4569616009064853e-06  Varinance:  8.994840460562647e-06 \n",
      "\n",
      "Epoch:  10616  Learning Rate:  2.4545058673769896e-06  Varinance:  8.982274998138864e-06 \n",
      "\n",
      "Epoch:  10617  Learning Rate:  2.4520525883535657e-06  Varinance:  8.969727089205494e-06 \n",
      "\n",
      "Epoch:  10618  Learning Rate:  2.4496017613829298e-06  Varinance:  8.957196709240967e-06 \n",
      "\n",
      "Epoch:  10619  Learning Rate:  2.447153384014264e-06  Varinance:  8.944683833757975e-06 \n",
      "\n",
      "Epoch:  10620  Learning Rate:  2.444707453799186e-06  Varinance:  8.932188438303366e-06 \n",
      "\n",
      "Epoch:  10621  Learning Rate:  2.4422639682917656e-06  Varinance:  8.919710498458197e-06 \n",
      "\n",
      "Epoch:  10622  Learning Rate:  2.439822925048513e-06  Varinance:  8.907249989837622e-06 \n",
      "\n",
      "Epoch:  10623  Learning Rate:  2.4373843216283925e-06  Varinance:  8.894806888090873e-06 \n",
      "\n",
      "Epoch:  10624  Learning Rate:  2.4349481555927973e-06  Varinance:  8.882381168901158e-06 \n",
      "\n",
      "Epoch:  10625  Learning Rate:  2.432514424505556e-06  Varinance:  8.869972807985692e-06 \n",
      "\n",
      "Epoch:  10626  Learning Rate:  2.430083125932946e-06  Varinance:  8.857581781095623e-06 \n",
      "\n",
      "Epoch:  10627  Learning Rate:  2.427654257443665e-06  Varinance:  8.845208064015918e-06 \n",
      "\n",
      "Epoch:  10628  Learning Rate:  2.4252278166088394e-06  Varinance:  8.832851632565424e-06 \n",
      "\n",
      "Epoch:  10629  Learning Rate:  2.4228038010020367e-06  Varinance:  8.82051246259675e-06 \n",
      "\n",
      "Epoch:  10630  Learning Rate:  2.420382208199237e-06  Varinance:  8.808190529996254e-06 \n",
      "\n",
      "Epoch:  10631  Learning Rate:  2.4179630357788424e-06  Varinance:  8.795885810683933e-06 \n",
      "\n",
      "Epoch:  10632  Learning Rate:  2.4155462813216896e-06  Varinance:  8.78359828061347e-06 \n",
      "\n",
      "Epoch:  10633  Learning Rate:  2.4131319424110196e-06  Varinance:  8.771327915772142e-06 \n",
      "\n",
      "Epoch:  10634  Learning Rate:  2.410720016632493e-06  Varinance:  8.75907469218072e-06 \n",
      "\n",
      "Epoch:  10635  Learning Rate:  2.4083105015741797e-06  Varinance:  8.74683858589352e-06 \n",
      "\n",
      "Epoch:  10636  Learning Rate:  2.405903394826573e-06  Varinance:  8.734619572998297e-06 \n",
      "\n",
      "Epoch:  10637  Learning Rate:  2.4034986939825614e-06  Varinance:  8.722417629616227e-06 \n",
      "\n",
      "Epoch:  10638  Learning Rate:  2.40109639663744e-06  Varinance:  8.710232731901792e-06 \n",
      "\n",
      "Epoch:  10639  Learning Rate:  2.3986965003889194e-06  Varinance:  8.698064856042834e-06 \n",
      "\n",
      "Epoch:  10640  Learning Rate:  2.396299002837099e-06  Varinance:  8.685913978260462e-06 \n",
      "\n",
      "Epoch:  10641  Learning Rate:  2.393903901584477e-06  Varinance:  8.673780074808954e-06 \n",
      "\n",
      "Epoch:  10642  Learning Rate:  2.3915111942359606e-06  Varinance:  8.661663121975808e-06 \n",
      "\n",
      "Epoch:  10643  Learning Rate:  2.389120878398837e-06  Varinance:  8.649563096081632e-06 \n",
      "\n",
      "Epoch:  10644  Learning Rate:  2.3867329516827875e-06  Varinance:  8.637479973480127e-06 \n",
      "\n",
      "Epoch:  10645  Learning Rate:  2.3843474116998926e-06  Varinance:  8.625413730557983e-06 \n",
      "\n",
      "Epoch:  10646  Learning Rate:  2.3819642560646076e-06  Varinance:  8.613364343734923e-06 \n",
      "\n",
      "Epoch:  10647  Learning Rate:  2.3795834823937734e-06  Varinance:  8.601331789463606e-06 \n",
      "\n",
      "Epoch:  10648  Learning Rate:  2.377205088306624e-06  Varinance:  8.589316044229546e-06 \n",
      "\n",
      "Epoch:  10649  Learning Rate:  2.3748290714247614e-06  Varinance:  8.577317084551149e-06 \n",
      "\n",
      "Epoch:  10650  Learning Rate:  2.372455429372168e-06  Varinance:  8.565334886979611e-06 \n",
      "\n",
      "Epoch:  10651  Learning Rate:  2.370084159775197e-06  Varinance:  8.553369428098895e-06 \n",
      "\n",
      "Epoch:  10652  Learning Rate:  2.3677152602625875e-06  Varinance:  8.541420684525635e-06 \n",
      "\n",
      "Epoch:  10653  Learning Rate:  2.365348728465436e-06  Varinance:  8.529488632909173e-06 \n",
      "\n",
      "Epoch:  10654  Learning Rate:  2.3629845620172055e-06  Varinance:  8.517573249931475e-06 \n",
      "\n",
      "Epoch:  10655  Learning Rate:  2.360622758553738e-06  Varinance:  8.505674512307032e-06 \n",
      "\n",
      "Epoch:  10656  Learning Rate:  2.3582633157132263e-06  Varinance:  8.493792396782909e-06 \n",
      "\n",
      "Epoch:  10657  Learning Rate:  2.3559062311362224e-06  Varinance:  8.481926880138643e-06 \n",
      "\n",
      "Epoch:  10658  Learning Rate:  2.3535515024656503e-06  Varinance:  8.470077939186223e-06 \n",
      "\n",
      "Epoch:  10659  Learning Rate:  2.3511991273467765e-06  Varinance:  8.458245550769982e-06 \n",
      "\n",
      "Epoch:  10660  Learning Rate:  2.348849103427222e-06  Varinance:  8.44642969176665e-06 \n",
      "\n",
      "Epoch:  10661  Learning Rate:  2.3465014283569706e-06  Varinance:  8.43463033908526e-06 \n",
      "\n",
      "Epoch:  10662  Learning Rate:  2.3441560997883433e-06  Varinance:  8.422847469667054e-06 \n",
      "\n",
      "Epoch:  10663  Learning Rate:  2.341813115376007e-06  Varinance:  8.411081060485531e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10664  Learning Rate:  2.3394724727769853e-06  Varinance:  8.399331088546362e-06 \n",
      "\n",
      "Epoch:  10665  Learning Rate:  2.3371341696506316e-06  Varinance:  8.387597530887289e-06 \n",
      "\n",
      "Epoch:  10666  Learning Rate:  2.334798203658642e-06  Varinance:  8.37588036457818e-06 \n",
      "\n",
      "Epoch:  10667  Learning Rate:  2.332464572465047e-06  Varinance:  8.36417956672092e-06 \n",
      "\n",
      "Epoch:  10668  Learning Rate:  2.3301332737362222e-06  Varinance:  8.3524951144494e-06 \n",
      "\n",
      "Epoch:  10669  Learning Rate:  2.3278043051408656e-06  Varinance:  8.340826984929399e-06 \n",
      "\n",
      "Epoch:  10670  Learning Rate:  2.325477664350004e-06  Varinance:  8.329175155358651e-06 \n",
      "\n",
      "Epoch:  10671  Learning Rate:  2.323153349037005e-06  Varinance:  8.317539602966739e-06 \n",
      "\n",
      "Epoch:  10672  Learning Rate:  2.3208313568775483e-06  Varinance:  8.30592030501501e-06 \n",
      "\n",
      "Epoch:  10673  Learning Rate:  2.318511685549638e-06  Varinance:  8.294317238796624e-06 \n",
      "\n",
      "Epoch:  10674  Learning Rate:  2.3161943327336103e-06  Varinance:  8.282730381636442e-06 \n",
      "\n",
      "Epoch:  10675  Learning Rate:  2.313879296112108e-06  Varinance:  8.27115971089102e-06 \n",
      "\n",
      "Epoch:  10676  Learning Rate:  2.311566573370091e-06  Varinance:  8.259605203948498e-06 \n",
      "\n",
      "Epoch:  10677  Learning Rate:  2.309256162194844e-06  Varinance:  8.248066838228653e-06 \n",
      "\n",
      "Epoch:  10678  Learning Rate:  2.3069480602759516e-06  Varinance:  8.236544591182804e-06 \n",
      "\n",
      "Epoch:  10679  Learning Rate:  2.304642265305308e-06  Varinance:  8.225038440293722e-06 \n",
      "\n",
      "Epoch:  10680  Learning Rate:  2.3023387749771256e-06  Varinance:  8.213548363075682e-06 \n",
      "\n",
      "Epoch:  10681  Learning Rate:  2.30003758698791e-06  Varinance:  8.202074337074355e-06 \n",
      "\n",
      "Epoch:  10682  Learning Rate:  2.297738699036473e-06  Varinance:  8.190616339866794e-06 \n",
      "\n",
      "Epoch:  10683  Learning Rate:  2.295442108823923e-06  Varinance:  8.179174349061332e-06 \n",
      "\n",
      "Epoch:  10684  Learning Rate:  2.293147814053676e-06  Varinance:  8.16774834229763e-06 \n",
      "\n",
      "Epoch:  10685  Learning Rate:  2.290855812431435e-06  Varinance:  8.156338297246576e-06 \n",
      "\n",
      "Epoch:  10686  Learning Rate:  2.2885661016651933e-06  Varinance:  8.144944191610217e-06 \n",
      "\n",
      "Epoch:  10687  Learning Rate:  2.2862786794652475e-06  Varinance:  8.133566003121789e-06 \n",
      "\n",
      "Epoch:  10688  Learning Rate:  2.283993543544172e-06  Varinance:  8.12220370954562e-06 \n",
      "\n",
      "Epoch:  10689  Learning Rate:  2.281710691616827e-06  Varinance:  8.110857288677115e-06 \n",
      "\n",
      "Epoch:  10690  Learning Rate:  2.279430121400367e-06  Varinance:  8.099526718342654e-06 \n",
      "\n",
      "Epoch:  10691  Learning Rate:  2.2771518306142183e-06  Varinance:  8.088211976399635e-06 \n",
      "\n",
      "Epoch:  10692  Learning Rate:  2.2748758169800865e-06  Varinance:  8.07691304073639e-06 \n",
      "\n",
      "Epoch:  10693  Learning Rate:  2.2726020782219647e-06  Varinance:  8.0656298892721e-06 \n",
      "\n",
      "Epoch:  10694  Learning Rate:  2.2703306120661107e-06  Varinance:  8.054362499956828e-06 \n",
      "\n",
      "Epoch:  10695  Learning Rate:  2.2680614162410543e-06  Varinance:  8.043110850771432e-06 \n",
      "\n",
      "Epoch:  10696  Learning Rate:  2.2657944884776066e-06  Varinance:  8.03187491972754e-06 \n",
      "\n",
      "Epoch:  10697  Learning Rate:  2.2635298265088364e-06  Varinance:  8.020654684867456e-06 \n",
      "\n",
      "Epoch:  10698  Learning Rate:  2.2612674280700812e-06  Varinance:  8.009450124264204e-06 \n",
      "\n",
      "Epoch:  10699  Learning Rate:  2.259007290898939e-06  Varinance:  7.998261216021433e-06 \n",
      "\n",
      "Epoch:  10700  Learning Rate:  2.2567494127352797e-06  Varinance:  7.98708793827334e-06 \n",
      "\n",
      "Epoch:  10701  Learning Rate:  2.254493791321221e-06  Varinance:  7.975930269184714e-06 \n",
      "\n",
      "Epoch:  10702  Learning Rate:  2.2522404244011378e-06  Varinance:  7.964788186950826e-06 \n",
      "\n",
      "Epoch:  10703  Learning Rate:  2.2499893097216705e-06  Varinance:  7.953661669797429e-06 \n",
      "\n",
      "Epoch:  10704  Learning Rate:  2.2477404450317004e-06  Varinance:  7.942550695980648e-06 \n",
      "\n",
      "Epoch:  10705  Learning Rate:  2.2454938280823587e-06  Varinance:  7.931455243787021e-06 \n",
      "\n",
      "Epoch:  10706  Learning Rate:  2.2432494566270363e-06  Varinance:  7.920375291533429e-06 \n",
      "\n",
      "Epoch:  10707  Learning Rate:  2.241007328421357e-06  Varinance:  7.909310817566992e-06 \n",
      "\n",
      "Epoch:  10708  Learning Rate:  2.23876744122319e-06  Varinance:  7.898261800265126e-06 \n",
      "\n",
      "Epoch:  10709  Learning Rate:  2.2365297927926536e-06  Varinance:  7.887228218035435e-06 \n",
      "\n",
      "Epoch:  10710  Learning Rate:  2.2342943808920967e-06  Varinance:  7.876210049315707e-06 \n",
      "\n",
      "Epoch:  10711  Learning Rate:  2.232061203286103e-06  Varinance:  7.865207272573803e-06 \n",
      "\n",
      "Epoch:  10712  Learning Rate:  2.2298302577415026e-06  Varinance:  7.854219866307706e-06 \n",
      "\n",
      "Epoch:  10713  Learning Rate:  2.227601542027346e-06  Varinance:  7.843247809045442e-06 \n",
      "\n",
      "Epoch:  10714  Learning Rate:  2.2253750539149162e-06  Varinance:  7.832291079344988e-06 \n",
      "\n",
      "Epoch:  10715  Learning Rate:  2.223150791177723e-06  Varinance:  7.821349655794314e-06 \n",
      "\n",
      "Epoch:  10716  Learning Rate:  2.2209287515915094e-06  Varinance:  7.810423517011288e-06 \n",
      "\n",
      "Epoch:  10717  Learning Rate:  2.2187089329342323e-06  Varinance:  7.799512641643666e-06 \n",
      "\n",
      "Epoch:  10718  Learning Rate:  2.2164913329860698e-06  Varinance:  7.788617008368989e-06 \n",
      "\n",
      "Epoch:  10719  Learning Rate:  2.2142759495294286e-06  Varinance:  7.777736595894624e-06 \n",
      "\n",
      "Epoch:  10720  Learning Rate:  2.2120627803489213e-06  Varinance:  7.766871382957688e-06 \n",
      "\n",
      "Epoch:  10721  Learning Rate:  2.2098518232313747e-06  Varinance:  7.756021348324951e-06 \n",
      "\n",
      "Epoch:  10722  Learning Rate:  2.2076430759658394e-06  Varinance:  7.745186470792898e-06 \n",
      "\n",
      "Epoch:  10723  Learning Rate:  2.2054365363435647e-06  Varinance:  7.734366729187612e-06 \n",
      "\n",
      "Epoch:  10724  Learning Rate:  2.2032322021580056e-06  Varinance:  7.723562102364773e-06 \n",
      "\n",
      "Epoch:  10725  Learning Rate:  2.2010300712048364e-06  Varinance:  7.712772569209553e-06 \n",
      "\n",
      "Epoch:  10726  Learning Rate:  2.1988301412819213e-06  Varinance:  7.701998108636671e-06 \n",
      "\n",
      "Epoch:  10727  Learning Rate:  2.1966324101893274e-06  Varinance:  7.69123869959029e-06 \n",
      "\n",
      "Epoch:  10728  Learning Rate:  2.1944368757293307e-06  Varinance:  7.680494321043956e-06 \n",
      "\n",
      "Epoch:  10729  Learning Rate:  2.192243535706392e-06  Varinance:  7.669764952000623e-06 \n",
      "\n",
      "Epoch:  10730  Learning Rate:  2.1900523879271726e-06  Varinance:  7.65905057149258e-06 \n",
      "\n",
      "Epoch:  10731  Learning Rate:  2.187863430200519e-06  Varinance:  7.648351158581365e-06 \n",
      "\n",
      "Epoch:  10732  Learning Rate:  2.185676660337482e-06  Varinance:  7.637666692357808e-06 \n",
      "\n",
      "Epoch:  10733  Learning Rate:  2.1834920761512873e-06  Varinance:  7.6269971519419315e-06 \n",
      "\n",
      "Epoch:  10734  Learning Rate:  2.181309675457347e-06  Varinance:  7.616342516482946e-06 \n",
      "\n",
      "Epoch:  10735  Learning Rate:  2.179129456073268e-06  Varinance:  7.605702765159146e-06 \n",
      "\n",
      "Epoch:  10736  Learning Rate:  2.1769514158188265e-06  Varinance:  7.595077877177951e-06 \n",
      "\n",
      "Epoch:  10737  Learning Rate:  2.1747755525159788e-06  Varinance:  7.584467831775834e-06 \n",
      "\n",
      "Epoch:  10738  Learning Rate:  2.172601863988868e-06  Varinance:  7.573872608218223e-06 \n",
      "\n",
      "Epoch:  10739  Learning Rate:  2.1704303480638023e-06  Varinance:  7.5632921857995635e-06 \n",
      "\n",
      "Epoch:  10740  Learning Rate:  2.168261002569262e-06  Varinance:  7.552726543843205e-06 \n",
      "\n",
      "Epoch:  10741  Learning Rate:  2.1660938253359085e-06  Varinance:  7.542175661701397e-06 \n",
      "\n",
      "Epoch:  10742  Learning Rate:  2.1639288141965615e-06  Varinance:  7.531639518755191e-06 \n",
      "\n",
      "Epoch:  10743  Learning Rate:  2.161765966986205e-06  Varinance:  7.521118094414487e-06 \n",
      "\n",
      "Epoch:  10744  Learning Rate:  2.1596052815419986e-06  Varinance:  7.5106113681179446e-06 \n",
      "\n",
      "Epoch:  10745  Learning Rate:  2.1574467557032544e-06  Varinance:  7.5001193193329075e-06 \n",
      "\n",
      "Epoch:  10746  Learning Rate:  2.155290387311446e-06  Varinance:  7.489641927555445e-06 \n",
      "\n",
      "Epoch:  10747  Learning Rate:  2.1531361742102e-06  Varinance:  7.479179172310254e-06 \n",
      "\n",
      "Epoch:  10748  Learning Rate:  2.1509841142453118e-06  Varinance:  7.46873103315065e-06 \n",
      "\n",
      "Epoch:  10749  Learning Rate:  2.1488342052647172e-06  Varinance:  7.458297489658468e-06 \n",
      "\n",
      "Epoch:  10750  Learning Rate:  2.146686445118503e-06  Varinance:  7.447878521444111e-06 \n",
      "\n",
      "Epoch:  10751  Learning Rate:  2.1445408316589166e-06  Varinance:  7.437474108146463e-06 \n",
      "\n",
      "Epoch:  10752  Learning Rate:  2.1423973627403402e-06  Varinance:  7.427084229432811e-06 \n",
      "\n",
      "Epoch:  10753  Learning Rate:  2.1402560362193017e-06  Varinance:  7.416708864998889e-06 \n",
      "\n",
      "Epoch:  10754  Learning Rate:  2.1381168499544816e-06  Varinance:  7.406347994568778e-06 \n",
      "\n",
      "Epoch:  10755  Learning Rate:  2.1359798018066895e-06  Varinance:  7.396001597894902e-06 \n",
      "\n",
      "Epoch:  10756  Learning Rate:  2.1338448896388736e-06  Varinance:  7.3856696547579265e-06 \n",
      "\n",
      "Epoch:  10757  Learning Rate:  2.131712111316129e-06  Varinance:  7.375352144966803e-06 \n",
      "\n",
      "Epoch:  10758  Learning Rate:  2.129581464705673e-06  Varinance:  7.365049048358689e-06 \n",
      "\n",
      "Epoch:  10759  Learning Rate:  2.1274529476768594e-06  Varinance:  7.35476034479887e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10760  Learning Rate:  2.125326558101167e-06  Varinance:  7.344486014180801e-06 \n",
      "\n",
      "Epoch:  10761  Learning Rate:  2.123202293852213e-06  Varinance:  7.334226036426004e-06 \n",
      "\n",
      "Epoch:  10762  Learning Rate:  2.1210801528057307e-06  Varinance:  7.323980391484074e-06 \n",
      "\n",
      "Epoch:  10763  Learning Rate:  2.1189601328395737e-06  Varinance:  7.313749059332567e-06 \n",
      "\n",
      "Epoch:  10764  Learning Rate:  2.11684223183373e-06  Varinance:  7.303532019977052e-06 \n",
      "\n",
      "Epoch:  10765  Learning Rate:  2.1147264476702946e-06  Varinance:  7.293329253451033e-06 \n",
      "\n",
      "Epoch:  10766  Learning Rate:  2.112612778233479e-06  Varinance:  7.2831407398158616e-06 \n",
      "\n",
      "Epoch:  10767  Learning Rate:  2.1105012214096218e-06  Varinance:  7.272966459160786e-06 \n",
      "\n",
      "Epoch:  10768  Learning Rate:  2.1083917750871617e-06  Varinance:  7.262806391602855e-06 \n",
      "\n",
      "Epoch:  10769  Learning Rate:  2.106284437156649e-06  Varinance:  7.252660517286904e-06 \n",
      "\n",
      "Epoch:  10770  Learning Rate:  2.104179205510752e-06  Varinance:  7.24252881638547e-06 \n",
      "\n",
      "Epoch:  10771  Learning Rate:  2.1020760780442367e-06  Varinance:  7.2324112690988256e-06 \n",
      "\n",
      "Epoch:  10772  Learning Rate:  2.09997505265397e-06  Varinance:  7.222307855654901e-06 \n",
      "\n",
      "Epoch:  10773  Learning Rate:  2.0978761272389355e-06  Varinance:  7.212218556309211e-06 \n",
      "\n",
      "Epoch:  10774  Learning Rate:  2.0957792997002026e-06  Varinance:  7.20214335134489e-06 \n",
      "\n",
      "Epoch:  10775  Learning Rate:  2.0936845679409443e-06  Varinance:  7.192082221072607e-06 \n",
      "\n",
      "Epoch:  10776  Learning Rate:  2.0915919298664244e-06  Varinance:  7.182035145830541e-06 \n",
      "\n",
      "Epoch:  10777  Learning Rate:  2.0895013833840127e-06  Varinance:  7.172002105984307e-06 \n",
      "\n",
      "Epoch:  10778  Learning Rate:  2.0874129264031583e-06  Varinance:  7.1619830819269845e-06 \n",
      "\n",
      "Epoch:  10779  Learning Rate:  2.0853265568354006e-06  Varinance:  7.151978054079044e-06 \n",
      "\n",
      "Epoch:  10780  Learning Rate:  2.0832422725943773e-06  Varinance:  7.141987002888265e-06 \n",
      "\n",
      "Epoch:  10781  Learning Rate:  2.0811600715958e-06  Varinance:  7.132009908829786e-06 \n",
      "\n",
      "Epoch:  10782  Learning Rate:  2.079079951757464e-06  Varinance:  7.122046752406002e-06 \n",
      "\n",
      "Epoch:  10783  Learning Rate:  2.077001910999257e-06  Varinance:  7.112097514146562e-06 \n",
      "\n",
      "Epoch:  10784  Learning Rate:  2.074925947243134e-06  Varinance:  7.102162174608277e-06 \n",
      "\n",
      "Epoch:  10785  Learning Rate:  2.0728520584131272e-06  Varinance:  7.092240714375153e-06 \n",
      "\n",
      "Epoch:  10786  Learning Rate:  2.0707802424353557e-06  Varinance:  7.082333114058323e-06 \n",
      "\n",
      "Epoch:  10787  Learning Rate:  2.068710497237999e-06  Varinance:  7.072439354295967e-06 \n",
      "\n",
      "Epoch:  10788  Learning Rate:  2.0666428207513082e-06  Varinance:  7.062559415753351e-06 \n",
      "\n",
      "Epoch:  10789  Learning Rate:  2.064577210907614e-06  Varinance:  7.052693279122736e-06 \n",
      "\n",
      "Epoch:  10790  Learning Rate:  2.0625136656413027e-06  Varinance:  7.042840925123369e-06 \n",
      "\n",
      "Epoch:  10791  Learning Rate:  2.060452182888829e-06  Varinance:  7.033002334501397e-06 \n",
      "\n",
      "Epoch:  10792  Learning Rate:  2.058392760588706e-06  Varinance:  7.0231774880298965e-06 \n",
      "\n",
      "Epoch:  10793  Learning Rate:  2.056335396681519e-06  Varinance:  7.0133663665088066e-06 \n",
      "\n",
      "Epoch:  10794  Learning Rate:  2.0542800891099005e-06  Varinance:  7.00356895076485e-06 \n",
      "\n",
      "Epoch:  10795  Learning Rate:  2.052226835818538e-06  Varinance:  6.9937852216515694e-06 \n",
      "\n",
      "Epoch:  10796  Learning Rate:  2.050175634754186e-06  Varinance:  6.984015160049256e-06 \n",
      "\n",
      "Epoch:  10797  Learning Rate:  2.04812648386564e-06  Varinance:  6.974258746864874e-06 \n",
      "\n",
      "Epoch:  10798  Learning Rate:  2.0460793811037444e-06  Varinance:  6.964515963032098e-06 \n",
      "\n",
      "Epoch:  10799  Learning Rate:  2.0440343244214046e-06  Varinance:  6.954786789511221e-06 \n",
      "\n",
      "Epoch:  10800  Learning Rate:  2.0419913117735592e-06  Varinance:  6.9450712072891475e-06 \n",
      "\n",
      "Epoch:  10801  Learning Rate:  2.0399503411171922e-06  Varinance:  6.935369197379309e-06 \n",
      "\n",
      "Epoch:  10802  Learning Rate:  2.03791141041134e-06  Varinance:  6.925680740821693e-06 \n",
      "\n",
      "Epoch:  10803  Learning Rate:  2.035874517617068e-06  Varinance:  6.916005818682777e-06 \n",
      "\n",
      "Epoch:  10804  Learning Rate:  2.0338396606974797e-06  Varinance:  6.90634441205545e-06 \n",
      "\n",
      "Epoch:  10805  Learning Rate:  2.031806837617725e-06  Varinance:  6.896696502059049e-06 \n",
      "\n",
      "Epoch:  10806  Learning Rate:  2.0297760463449776e-06  Varinance:  6.887062069839275e-06 \n",
      "\n",
      "Epoch:  10807  Learning Rate:  2.0277472848484455e-06  Varinance:  6.877441096568181e-06 \n",
      "\n",
      "Epoch:  10808  Learning Rate:  2.0257205510993634e-06  Varinance:  6.867833563444086e-06 \n",
      "\n",
      "Epoch:  10809  Learning Rate:  2.023695843071005e-06  Varinance:  6.858239451691606e-06 \n",
      "\n",
      "Epoch:  10810  Learning Rate:  2.0216731587386584e-06  Varinance:  6.848658742561592e-06 \n",
      "\n",
      "Epoch:  10811  Learning Rate:  2.019652496079635e-06  Varinance:  6.839091417331046e-06 \n",
      "\n",
      "Epoch:  10812  Learning Rate:  2.0176338530732805e-06  Varinance:  6.829537457303163e-06 \n",
      "\n",
      "Epoch:  10813  Learning Rate:  2.015617227700946e-06  Varinance:  6.819996843807245e-06 \n",
      "\n",
      "Epoch:  10814  Learning Rate:  2.0136026179460042e-06  Varinance:  6.810469558198688e-06 \n",
      "\n",
      "Epoch:  10815  Learning Rate:  2.011590021793852e-06  Varinance:  6.800955581858899e-06 \n",
      "\n",
      "Epoch:  10816  Learning Rate:  2.0095794372318893e-06  Varinance:  6.791454896195329e-06 \n",
      "\n",
      "Epoch:  10817  Learning Rate:  2.007570862249527e-06  Varinance:  6.781967482641401e-06 \n",
      "\n",
      "Epoch:  10818  Learning Rate:  2.005564294838198e-06  Varinance:  6.772493322656441e-06 \n",
      "\n",
      "Epoch:  10819  Learning Rate:  2.003559732991331e-06  Varinance:  6.763032397725712e-06 \n",
      "\n",
      "Epoch:  10820  Learning Rate:  2.001557174704361e-06  Varinance:  6.7535846893603285e-06 \n",
      "\n",
      "Epoch:  10821  Learning Rate:  1.999556617974736e-06  Varinance:  6.744150179097243e-06 \n",
      "\n",
      "Epoch:  10822  Learning Rate:  1.9975580608018946e-06  Varinance:  6.734728848499168e-06 \n",
      "\n",
      "Epoch:  10823  Learning Rate:  1.9955615011872815e-06  Varinance:  6.725320679154606e-06 \n",
      "\n",
      "Epoch:  10824  Learning Rate:  1.993566937134332e-06  Varinance:  6.715925652677781e-06 \n",
      "\n",
      "Epoch:  10825  Learning Rate:  1.991574366648489e-06  Varinance:  6.706543750708562e-06 \n",
      "\n",
      "Epoch:  10826  Learning Rate:  1.989583787737179e-06  Varinance:  6.69717495491251e-06 \n",
      "\n",
      "Epoch:  10827  Learning Rate:  1.9875951984098187e-06  Varinance:  6.687819246980777e-06 \n",
      "\n",
      "Epoch:  10828  Learning Rate:  1.985608596677826e-06  Varinance:  6.678476608630109e-06 \n",
      "\n",
      "Epoch:  10829  Learning Rate:  1.9836239805545958e-06  Varinance:  6.669147021602757e-06 \n",
      "\n",
      "Epoch:  10830  Learning Rate:  1.9816413480555076e-06  Varinance:  6.659830467666514e-06 \n",
      "\n",
      "Epoch:  10831  Learning Rate:  1.9796606971979363e-06  Varinance:  6.650526928614639e-06 \n",
      "\n",
      "Epoch:  10832  Learning Rate:  1.977682026001227e-06  Varinance:  6.6412363862657935e-06 \n",
      "\n",
      "Epoch:  10833  Learning Rate:  1.975705332486705e-06  Varinance:  6.63195882246407e-06 \n",
      "\n",
      "Epoch:  10834  Learning Rate:  1.9737306146776837e-06  Varinance:  6.622694219078916e-06 \n",
      "\n",
      "Epoch:  10835  Learning Rate:  1.9717578705994416e-06  Varinance:  6.613442558005115e-06 \n",
      "\n",
      "Epoch:  10836  Learning Rate:  1.969787098279231e-06  Varinance:  6.604203821162709e-06 \n",
      "\n",
      "Epoch:  10837  Learning Rate:  1.967818295746286e-06  Varinance:  6.594977990497032e-06 \n",
      "\n",
      "Epoch:  10838  Learning Rate:  1.965851461031801e-06  Varinance:  6.58576504797864e-06 \n",
      "\n",
      "Epoch:  10839  Learning Rate:  1.9638865921689407e-06  Varinance:  6.576564975603242e-06 \n",
      "\n",
      "Epoch:  10840  Learning Rate:  1.961923687192833e-06  Varinance:  6.567377755391728e-06 \n",
      "\n",
      "Epoch:  10841  Learning Rate:  1.9599627441405793e-06  Varinance:  6.558203369390099e-06 \n",
      "\n",
      "Epoch:  10842  Learning Rate:  1.958003761051233e-06  Varinance:  6.549041799669447e-06 \n",
      "\n",
      "Epoch:  10843  Learning Rate:  1.9560467359658075e-06  Varinance:  6.539893028325871e-06 \n",
      "\n",
      "Epoch:  10844  Learning Rate:  1.9540916669272843e-06  Varinance:  6.53075703748052e-06 \n",
      "\n",
      "Epoch:  10845  Learning Rate:  1.952138551980591e-06  Varinance:  6.521633809279517e-06 \n",
      "\n",
      "Epoch:  10846  Learning Rate:  1.950187389172609e-06  Varinance:  6.512523325893896e-06 \n",
      "\n",
      "Epoch:  10847  Learning Rate:  1.9482381765521823e-06  Varinance:  6.503425569519625e-06 \n",
      "\n",
      "Epoch:  10848  Learning Rate:  1.946290912170094e-06  Varinance:  6.494340522377537e-06 \n",
      "\n",
      "Epoch:  10849  Learning Rate:  1.944345594079077e-06  Varinance:  6.485268166713313e-06 \n",
      "\n",
      "Epoch:  10850  Learning Rate:  1.9424022203338193e-06  Varinance:  6.476208484797398e-06 \n",
      "\n",
      "Epoch:  10851  Learning Rate:  1.9404607889909442e-06  Varinance:  6.467161458925043e-06 \n",
      "\n",
      "Epoch:  10852  Learning Rate:  1.9385212981090162e-06  Varinance:  6.458127071416232e-06 \n",
      "\n",
      "Epoch:  10853  Learning Rate:  1.936583745748551e-06  Varinance:  6.4491053046156096e-06 \n",
      "\n",
      "Epoch:  10854  Learning Rate:  1.9346481299719932e-06  Varinance:  6.440096140892521e-06 \n",
      "\n",
      "Epoch:  10855  Learning Rate:  1.9327144488437266e-06  Varinance:  6.431099562640929e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10856  Learning Rate:  1.930782700430066e-06  Varinance:  6.422115552279403e-06 \n",
      "\n",
      "Epoch:  10857  Learning Rate:  1.9288528827992707e-06  Varinance:  6.413144092251038e-06 \n",
      "\n",
      "Epoch:  10858  Learning Rate:  1.926924994021519e-06  Varinance:  6.404185165023489e-06 \n",
      "\n",
      "Epoch:  10859  Learning Rate:  1.9249990321689184e-06  Varinance:  6.395238753088906e-06 \n",
      "\n",
      "Epoch:  10860  Learning Rate:  1.9230749953155133e-06  Varinance:  6.386304838963859e-06 \n",
      "\n",
      "Epoch:  10861  Learning Rate:  1.921152881537264e-06  Varinance:  6.37738340518938e-06 \n",
      "\n",
      "Epoch:  10862  Learning Rate:  1.919232688912053e-06  Varinance:  6.368474434330884e-06 \n",
      "\n",
      "Epoch:  10863  Learning Rate:  1.9173144155196942e-06  Varinance:  6.3595779089781155e-06 \n",
      "\n",
      "Epoch:  10864  Learning Rate:  1.915398059441911e-06  Varinance:  6.350693811745168e-06 \n",
      "\n",
      "Epoch:  10865  Learning Rate:  1.9134836187623433e-06  Varinance:  6.341822125270416e-06 \n",
      "\n",
      "Epoch:  10866  Learning Rate:  1.9115710915665574e-06  Varinance:  6.332962832216493e-06 \n",
      "\n",
      "Epoch:  10867  Learning Rate:  1.9096604759420218e-06  Varinance:  6.324115915270225e-06 \n",
      "\n",
      "Epoch:  10868  Learning Rate:  1.9077517699781185e-06  Varinance:  6.3152813571426526e-06 \n",
      "\n",
      "Epoch:  10869  Learning Rate:  1.905844971766147e-06  Varinance:  6.306459140568972e-06 \n",
      "\n",
      "Epoch:  10870  Learning Rate:  1.9039400793993065e-06  Varinance:  6.297649248308461e-06 \n",
      "\n",
      "Epoch:  10871  Learning Rate:  1.902037090972704e-06  Varinance:  6.288851663144517e-06 \n",
      "\n",
      "Epoch:  10872  Learning Rate:  1.9001360045833473e-06  Varinance:  6.280066367884578e-06 \n",
      "\n",
      "Epoch:  10873  Learning Rate:  1.8982368183301572e-06  Varinance:  6.27129334536011e-06 \n",
      "\n",
      "Epoch:  10874  Learning Rate:  1.8963395303139436e-06  Varinance:  6.2625325784265285e-06 \n",
      "\n",
      "Epoch:  10875  Learning Rate:  1.8944441386374148e-06  Varinance:  6.2537840499632325e-06 \n",
      "\n",
      "Epoch:  10876  Learning Rate:  1.8925506414051861e-06  Varinance:  6.24504774287354e-06 \n",
      "\n",
      "Epoch:  10877  Learning Rate:  1.8906590367237564e-06  Varinance:  6.236323640084618e-06 \n",
      "\n",
      "Epoch:  10878  Learning Rate:  1.8887693227015175e-06  Varinance:  6.227611724547516e-06 \n",
      "\n",
      "Epoch:  10879  Learning Rate:  1.886881497448762e-06  Varinance:  6.218911979237089e-06 \n",
      "\n",
      "Epoch:  10880  Learning Rate:  1.8849955590776615e-06  Varinance:  6.210224387151989e-06 \n",
      "\n",
      "Epoch:  10881  Learning Rate:  1.8831115057022737e-06  Varinance:  6.201548931314585e-06 \n",
      "\n",
      "Epoch:  10882  Learning Rate:  1.8812293354385518e-06  Varinance:  6.192885594770991e-06 \n",
      "\n",
      "Epoch:  10883  Learning Rate:  1.879349046404322e-06  Varinance:  6.184234360591012e-06 \n",
      "\n",
      "Epoch:  10884  Learning Rate:  1.8774706367192955e-06  Varinance:  6.175595211868066e-06 \n",
      "\n",
      "Epoch:  10885  Learning Rate:  1.875594104505059e-06  Varinance:  6.166968131719224e-06 \n",
      "\n",
      "Epoch:  10886  Learning Rate:  1.8737194478850862e-06  Varinance:  6.158353103285131e-06 \n",
      "\n",
      "Epoch:  10887  Learning Rate:  1.8718466649847176e-06  Varinance:  6.149750109729992e-06 \n",
      "\n",
      "Epoch:  10888  Learning Rate:  1.8699757539311665e-06  Varinance:  6.1411591342415034e-06 \n",
      "\n",
      "Epoch:  10889  Learning Rate:  1.8681067128535288e-06  Varinance:  6.132580160030875e-06 \n",
      "\n",
      "Epoch:  10890  Learning Rate:  1.8662395398827595e-06  Varinance:  6.124013170332774e-06 \n",
      "\n",
      "Epoch:  10891  Learning Rate:  1.8643742331516821e-06  Varinance:  6.115458148405252e-06 \n",
      "\n",
      "Epoch:  10892  Learning Rate:  1.8625107907949967e-06  Varinance:  6.106915077529786e-06 \n",
      "\n",
      "Epoch:  10893  Learning Rate:  1.8606492109492571e-06  Varinance:  6.098383941011193e-06 \n",
      "\n",
      "Epoch:  10894  Learning Rate:  1.8587894917528806e-06  Varinance:  6.089864722177624e-06 \n",
      "\n",
      "Epoch:  10895  Learning Rate:  1.8569316313461536e-06  Varinance:  6.081357404380487e-06 \n",
      "\n",
      "Epoch:  10896  Learning Rate:  1.8550756278712128e-06  Varinance:  6.072861970994484e-06 \n",
      "\n",
      "Epoch:  10897  Learning Rate:  1.8532214794720514e-06  Varinance:  6.064378405417538e-06 \n",
      "\n",
      "Epoch:  10898  Learning Rate:  1.851369184294527e-06  Varinance:  6.055906691070733e-06 \n",
      "\n",
      "Epoch:  10899  Learning Rate:  1.8495187404863414e-06  Varinance:  6.047446811398346e-06 \n",
      "\n",
      "Epoch:  10900  Learning Rate:  1.8476701461970502e-06  Varinance:  6.03899874986777e-06 \n",
      "\n",
      "Epoch:  10901  Learning Rate:  1.845823399578056e-06  Varinance:  6.030562489969505e-06 \n",
      "\n",
      "Epoch:  10902  Learning Rate:  1.8439784987826184e-06  Varinance:  6.0221380152170825e-06 \n",
      "\n",
      "Epoch:  10903  Learning Rate:  1.8421354419658332e-06  Varinance:  6.013725309147095e-06 \n",
      "\n",
      "Epoch:  10904  Learning Rate:  1.8402942272846404e-06  Varinance:  6.005324355319136e-06 \n",
      "\n",
      "Epoch:  10905  Learning Rate:  1.8384548528978312e-06  Varinance:  5.996935137315733e-06 \n",
      "\n",
      "Epoch:  10906  Learning Rate:  1.8366173169660284e-06  Varinance:  5.988557638742382e-06 \n",
      "\n",
      "Epoch:  10907  Learning Rate:  1.8347816176516922e-06  Varinance:  5.980191843227464e-06 \n",
      "\n",
      "Epoch:  10908  Learning Rate:  1.8329477531191299e-06  Varinance:  5.97183773442225e-06 \n",
      "\n",
      "Epoch:  10909  Learning Rate:  1.8311157215344732e-06  Varinance:  5.963495296000811e-06 \n",
      "\n",
      "Epoch:  10910  Learning Rate:  1.8292855210656877e-06  Varinance:  5.9551645116600605e-06 \n",
      "\n",
      "Epoch:  10911  Learning Rate:  1.8274571498825789e-06  Varinance:  5.946845365119687e-06 \n",
      "\n",
      "Epoch:  10912  Learning Rate:  1.8256306061567722e-06  Varinance:  5.938537840122086e-06 \n",
      "\n",
      "Epoch:  10913  Learning Rate:  1.8238058880617203e-06  Varinance:  5.930241920432401e-06 \n",
      "\n",
      "Epoch:  10914  Learning Rate:  1.821982993772712e-06  Varinance:  5.92195758983844e-06 \n",
      "\n",
      "Epoch:  10915  Learning Rate:  1.8201619214668494e-06  Varinance:  5.913684832150674e-06 \n",
      "\n",
      "Epoch:  10916  Learning Rate:  1.8183426693230598e-06  Varinance:  5.905423631202155e-06 \n",
      "\n",
      "Epoch:  10917  Learning Rate:  1.8165252355220879e-06  Varinance:  5.897173970848552e-06 \n",
      "\n",
      "Epoch:  10918  Learning Rate:  1.814709618246506e-06  Varinance:  5.888935834968089e-06 \n",
      "\n",
      "Epoch:  10919  Learning Rate:  1.8128958156806935e-06  Varinance:  5.880709207461477e-06 \n",
      "\n",
      "Epoch:  10920  Learning Rate:  1.8110838260108447e-06  Varinance:  5.87249407225195e-06 \n",
      "\n",
      "Epoch:  10921  Learning Rate:  1.809273647424976e-06  Varinance:  5.864290413285191e-06 \n",
      "\n",
      "Epoch:  10922  Learning Rate:  1.8074652781129056e-06  Varinance:  5.856098214529319e-06 \n",
      "\n",
      "Epoch:  10923  Learning Rate:  1.8056587162662607e-06  Varinance:  5.847917459974819e-06 \n",
      "\n",
      "Epoch:  10924  Learning Rate:  1.8038539600784855e-06  Varinance:  5.839748133634573e-06 \n",
      "\n",
      "Epoch:  10925  Learning Rate:  1.802051007744821e-06  Varinance:  5.831590219543793e-06 \n",
      "\n",
      "Epoch:  10926  Learning Rate:  1.800249857462311e-06  Varinance:  5.823443701759965e-06 \n",
      "\n",
      "Epoch:  10927  Learning Rate:  1.7984505074298116e-06  Varinance:  5.815308564362875e-06 \n",
      "\n",
      "Epoch:  10928  Learning Rate:  1.79665295584797e-06  Varinance:  5.807184791454553e-06 \n",
      "\n",
      "Epoch:  10929  Learning Rate:  1.7948572009192304e-06  Varinance:  5.799072367159202e-06 \n",
      "\n",
      "Epoch:  10930  Learning Rate:  1.7930632408478443e-06  Varinance:  5.790971275623235e-06 \n",
      "\n",
      "Epoch:  10931  Learning Rate:  1.791271073839849e-06  Varinance:  5.782881501015205e-06 \n",
      "\n",
      "Epoch:  10932  Learning Rate:  1.7894806981030761e-06  Varinance:  5.7748030275257875e-06 \n",
      "\n",
      "Epoch:  10933  Learning Rate:  1.7876921118471478e-06  Varinance:  5.766735839367713e-06 \n",
      "\n",
      "Epoch:  10934  Learning Rate:  1.7859053132834833e-06  Varinance:  5.7586799207758e-06 \n",
      "\n",
      "Epoch:  10935  Learning Rate:  1.7841203006252811e-06  Varinance:  5.750635256006884e-06 \n",
      "\n",
      "Epoch:  10936  Learning Rate:  1.7823370720875248e-06  Varinance:  5.742601829339769e-06 \n",
      "\n",
      "Epoch:  10937  Learning Rate:  1.7805556258869924e-06  Varinance:  5.734579625075249e-06 \n",
      "\n",
      "Epoch:  10938  Learning Rate:  1.7787759602422343e-06  Varinance:  5.726568627536037e-06 \n",
      "\n",
      "Epoch:  10939  Learning Rate:  1.7769980733735813e-06  Varinance:  5.7185688210667615e-06 \n",
      "\n",
      "Epoch:  10940  Learning Rate:  1.7752219635031534e-06  Varinance:  5.710580190033885e-06 \n",
      "\n",
      "Epoch:  10941  Learning Rate:  1.7734476288548363e-06  Varinance:  5.702602718825745e-06 \n",
      "\n",
      "Epoch:  10942  Learning Rate:  1.7716750676542931e-06  Varinance:  5.694636391852482e-06 \n",
      "\n",
      "Epoch:  10943  Learning Rate:  1.7699042781289682e-06  Varinance:  5.6866811935459915e-06 \n",
      "\n",
      "Epoch:  10944  Learning Rate:  1.7681352585080688e-06  Varinance:  5.678737108359943e-06 \n",
      "\n",
      "Epoch:  10945  Learning Rate:  1.7663680070225724e-06  Varinance:  5.670804120769715e-06 \n",
      "\n",
      "Epoch:  10946  Learning Rate:  1.7646025219052336e-06  Varinance:  5.662882215272382e-06 \n",
      "\n",
      "Epoch:  10947  Learning Rate:  1.7628388013905632e-06  Varinance:  5.654971376386646e-06 \n",
      "\n",
      "Epoch:  10948  Learning Rate:  1.7610768437148413e-06  Varinance:  5.647071588652868e-06 \n",
      "\n",
      "Epoch:  10949  Learning Rate:  1.7593166471161064e-06  Varinance:  5.639182836633003e-06 \n",
      "\n",
      "Epoch:  10950  Learning Rate:  1.757558209834169e-06  Varinance:  5.631305104910543e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10951  Learning Rate:  1.7558015301105876e-06  Varinance:  5.6234383780905465e-06 \n",
      "\n",
      "Epoch:  10952  Learning Rate:  1.7540466061886793e-06  Varinance:  5.6155826407995675e-06 \n",
      "\n",
      "Epoch:  10953  Learning Rate:  1.7522934363135268e-06  Varinance:  5.6077378776856475e-06 \n",
      "\n",
      "Epoch:  10954  Learning Rate:  1.7505420187319563e-06  Varinance:  5.599904073418244e-06 \n",
      "\n",
      "Epoch:  10955  Learning Rate:  1.7487923516925477e-06  Varinance:  5.592081212688259e-06 \n",
      "\n",
      "Epoch:  10956  Learning Rate:  1.7470444334456394e-06  Varinance:  5.584269280207983e-06 \n",
      "\n",
      "Epoch:  10957  Learning Rate:  1.74529826224331e-06  Varinance:  5.5764682607110295e-06 \n",
      "\n",
      "Epoch:  10958  Learning Rate:  1.7435538363393855e-06  Varinance:  5.568678138952373e-06 \n",
      "\n",
      "Epoch:  10959  Learning Rate:  1.7418111539894456e-06  Varinance:  5.560898899708273e-06 \n",
      "\n",
      "Epoch:  10960  Learning Rate:  1.7400702134508046e-06  Varinance:  5.553130527776263e-06 \n",
      "\n",
      "Epoch:  10961  Learning Rate:  1.7383310129825192e-06  Varinance:  5.545373007975088e-06 \n",
      "\n",
      "Epoch:  10962  Learning Rate:  1.736593550845395e-06  Varinance:  5.537626325144729e-06 \n",
      "\n",
      "Epoch:  10963  Learning Rate:  1.734857825301966e-06  Varinance:  5.529890464146345e-06 \n",
      "\n",
      "Epoch:  10964  Learning Rate:  1.7331238346165068e-06  Varinance:  5.522165409862212e-06 \n",
      "\n",
      "Epoch:  10965  Learning Rate:  1.7313915770550238e-06  Varinance:  5.514451147195759e-06 \n",
      "\n",
      "Epoch:  10966  Learning Rate:  1.7296610508852651e-06  Varinance:  5.506747661071489e-06 \n",
      "\n",
      "Epoch:  10967  Learning Rate:  1.7279322543767013e-06  Varinance:  5.499054936434979e-06 \n",
      "\n",
      "Epoch:  10968  Learning Rate:  1.726205185800533e-06  Varinance:  5.4913729582528055e-06 \n",
      "\n",
      "Epoch:  10969  Learning Rate:  1.7244798434296973e-06  Varinance:  5.483701711512574e-06 \n",
      "\n",
      "Epoch:  10970  Learning Rate:  1.7227562255388489e-06  Varinance:  5.4760411812228645e-06 \n",
      "\n",
      "Epoch:  10971  Learning Rate:  1.7210343304043665e-06  Varinance:  5.468391352413168e-06 \n",
      "\n",
      "Epoch:  10972  Learning Rate:  1.7193141563043609e-06  Varinance:  5.4607522101339205e-06 \n",
      "\n",
      "Epoch:  10973  Learning Rate:  1.7175957015186548e-06  Varinance:  5.453123739456429e-06 \n",
      "\n",
      "Epoch:  10974  Learning Rate:  1.7158789643287905e-06  Varinance:  5.445505925472868e-06 \n",
      "\n",
      "Epoch:  10975  Learning Rate:  1.7141639430180365e-06  Varinance:  5.437898753296206e-06 \n",
      "\n",
      "Epoch:  10976  Learning Rate:  1.7124506358713685e-06  Varinance:  5.430302208060239e-06 \n",
      "\n",
      "Epoch:  10977  Learning Rate:  1.7107390411754758e-06  Varinance:  5.4227162749195315e-06 \n",
      "\n",
      "Epoch:  10978  Learning Rate:  1.7090291572187698e-06  Varinance:  5.415140939049354e-06 \n",
      "\n",
      "Epoch:  10979  Learning Rate:  1.7073209822913637e-06  Varinance:  5.407576185645718e-06 \n",
      "\n",
      "Epoch:  10980  Learning Rate:  1.705614514685082e-06  Varinance:  5.400021999925307e-06 \n",
      "\n",
      "Epoch:  10981  Learning Rate:  1.7039097526934541e-06  Varinance:  5.392478367125463e-06 \n",
      "\n",
      "Epoch:  10982  Learning Rate:  1.7022066946117241e-06  Varinance:  5.384945272504124e-06 \n",
      "\n",
      "Epoch:  10983  Learning Rate:  1.7005053387368303e-06  Varinance:  5.377422701339852e-06 \n",
      "\n",
      "Epoch:  10984  Learning Rate:  1.6988056833674142e-06  Varinance:  5.36991063893177e-06 \n",
      "\n",
      "Epoch:  10985  Learning Rate:  1.6971077268038259e-06  Varinance:  5.362409070599514e-06 \n",
      "\n",
      "Epoch:  10986  Learning Rate:  1.6954114673481056e-06  Varinance:  5.354917981683253e-06 \n",
      "\n",
      "Epoch:  10987  Learning Rate:  1.6937169033039912e-06  Varinance:  5.347437357543627e-06 \n",
      "\n",
      "Epoch:  10988  Learning Rate:  1.6920240329769244e-06  Varinance:  5.339967183561737e-06 \n",
      "\n",
      "Epoch:  10989  Learning Rate:  1.6903328546740312e-06  Varinance:  5.332507445139077e-06 \n",
      "\n",
      "Epoch:  10990  Learning Rate:  1.6886433667041309e-06  Varinance:  5.325058127697561e-06 \n",
      "\n",
      "Epoch:  10991  Learning Rate:  1.6869555673777407e-06  Varinance:  5.31761921667947e-06 \n",
      "\n",
      "Epoch:  10992  Learning Rate:  1.6852694550070586e-06  Varinance:  5.310190697547394e-06 \n",
      "\n",
      "Epoch:  10993  Learning Rate:  1.683585027905969e-06  Varinance:  5.302772555784259e-06 \n",
      "\n",
      "Epoch:  10994  Learning Rate:  1.6819022843900504e-06  Varinance:  5.29536477689327e-06 \n",
      "\n",
      "Epoch:  10995  Learning Rate:  1.6802212227765568e-06  Varinance:  5.287967346397855e-06 \n",
      "\n",
      "Epoch:  10996  Learning Rate:  1.6785418413844256e-06  Varinance:  5.280580249841695e-06 \n",
      "\n",
      "Epoch:  10997  Learning Rate:  1.6768641385342725e-06  Varinance:  5.273203472788653e-06 \n",
      "\n",
      "Epoch:  10998  Learning Rate:  1.675188112548401e-06  Varinance:  5.265837000822775e-06 \n",
      "\n",
      "Epoch:  10999  Learning Rate:  1.6735137617507816e-06  Varinance:  5.25848081954821e-06 \n",
      "\n",
      "Epoch:  11000  Learning Rate:  1.6718410844670603e-06  Varinance:  5.25113491458925e-06 \n",
      "\n",
      "Epoch:  11001  Learning Rate:  1.670170079024566e-06  Varinance:  5.2437992715902686e-06 \n",
      "\n",
      "Epoch:  11002  Learning Rate:  1.6685007437522898e-06  Varinance:  5.236473876215663e-06 \n",
      "\n",
      "Epoch:  11003  Learning Rate:  1.6668330769808934e-06  Varinance:  5.229158714149891e-06 \n",
      "\n",
      "Epoch:  11004  Learning Rate:  1.665167077042716e-06  Varinance:  5.221853771097391e-06 \n",
      "\n",
      "Epoch:  11005  Learning Rate:  1.6635027422717538e-06  Varinance:  5.214559032782587e-06 \n",
      "\n",
      "Epoch:  11006  Learning Rate:  1.6618400710036703e-06  Varinance:  5.2072744849498165e-06 \n",
      "\n",
      "Epoch:  11007  Learning Rate:  1.6601790615757988e-06  Varinance:  5.200000113363358e-06 \n",
      "\n",
      "Epoch:  11008  Learning Rate:  1.6585197123271273e-06  Varinance:  5.192735903807378e-06 \n",
      "\n",
      "Epoch:  11009  Learning Rate:  1.6568620215983065e-06  Varinance:  5.1854818420858745e-06 \n",
      "\n",
      "Epoch:  11010  Learning Rate:  1.6552059877316423e-06  Varinance:  5.178237914022702e-06 \n",
      "\n",
      "Epoch:  11011  Learning Rate:  1.6535516090711068e-06  Varinance:  5.171004105461514e-06 \n",
      "\n",
      "Epoch:  11012  Learning Rate:  1.651898883962318e-06  Varinance:  5.163780402265744e-06 \n",
      "\n",
      "Epoch:  11013  Learning Rate:  1.650247810752548e-06  Varinance:  5.15656679031855e-06 \n",
      "\n",
      "Epoch:  11014  Learning Rate:  1.6485983877907293e-06  Varinance:  5.149363255522837e-06 \n",
      "\n",
      "Epoch:  11015  Learning Rate:  1.6469506134274357e-06  Varinance:  5.142169783801201e-06 \n",
      "\n",
      "Epoch:  11016  Learning Rate:  1.6453044860148897e-06  Varinance:  5.13498636109588e-06 \n",
      "\n",
      "Epoch:  11017  Learning Rate:  1.6436600039069697e-06  Varinance:  5.127812973368774e-06 \n",
      "\n",
      "Epoch:  11018  Learning Rate:  1.642017165459191e-06  Varinance:  5.120649606601389e-06 \n",
      "\n",
      "Epoch:  11019  Learning Rate:  1.6403759690287114e-06  Varinance:  5.113496246794817e-06 \n",
      "\n",
      "Epoch:  11020  Learning Rate:  1.6387364129743405e-06  Varinance:  5.106352879969686e-06 \n",
      "\n",
      "Epoch:  11021  Learning Rate:  1.637098495656519e-06  Varinance:  5.099219492166173e-06 \n",
      "\n",
      "Epoch:  11022  Learning Rate:  1.6354622154373268e-06  Varinance:  5.092096069443962e-06 \n",
      "\n",
      "Epoch:  11023  Learning Rate:  1.6338275706804893e-06  Varinance:  5.0849825978821805e-06 \n",
      "\n",
      "Epoch:  11024  Learning Rate:  1.6321945597513583e-06  Varinance:  5.077879063579432e-06 \n",
      "\n",
      "Epoch:  11025  Learning Rate:  1.6305631810169234e-06  Varinance:  5.070785452653729e-06 \n",
      "\n",
      "Epoch:  11026  Learning Rate:  1.6289334328458021e-06  Varinance:  5.063701751242487e-06 \n",
      "\n",
      "Epoch:  11027  Learning Rate:  1.6273053136082528e-06  Varinance:  5.056627945502459e-06 \n",
      "\n",
      "Epoch:  11028  Learning Rate:  1.6256788216761526e-06  Varinance:  5.049564021609765e-06 \n",
      "\n",
      "Epoch:  11029  Learning Rate:  1.6240539554230065e-06  Varinance:  5.042509965759835e-06 \n",
      "\n",
      "Epoch:  11030  Learning Rate:  1.6224307132239539e-06  Varinance:  5.035465764167356e-06 \n",
      "\n",
      "Epoch:  11031  Learning Rate:  1.62080909345575e-06  Varinance:  5.028431403066302e-06 \n",
      "\n",
      "Epoch:  11032  Learning Rate:  1.6191890944967716e-06  Varinance:  5.021406868709866e-06 \n",
      "\n",
      "Epoch:  11033  Learning Rate:  1.6175707147270256e-06  Varinance:  5.014392147370456e-06 \n",
      "\n",
      "Epoch:  11034  Learning Rate:  1.6159539525281294e-06  Varinance:  5.007387225339628e-06 \n",
      "\n",
      "Epoch:  11035  Learning Rate:  1.6143388062833172e-06  Varinance:  5.000392088928117e-06 \n",
      "\n",
      "Epoch:  11036  Learning Rate:  1.6127252743774485e-06  Varinance:  4.99340672446578e-06 \n",
      "\n",
      "Epoch:  11037  Learning Rate:  1.6111133551969887e-06  Varinance:  4.9864311183015436e-06 \n",
      "\n",
      "Epoch:  11038  Learning Rate:  1.6095030471300157e-06  Varinance:  4.979465256803433e-06 \n",
      "\n",
      "Epoch:  11039  Learning Rate:  1.6078943485662266e-06  Varinance:  4.972509126358507e-06 \n",
      "\n",
      "Epoch:  11040  Learning Rate:  1.60628725789692e-06  Varinance:  4.965562713372849e-06 \n",
      "\n",
      "Epoch:  11041  Learning Rate:  1.6046817735150053e-06  Varinance:  4.958626004271507e-06 \n",
      "\n",
      "Epoch:  11042  Learning Rate:  1.603077893814995e-06  Varinance:  4.95169898549852e-06 \n",
      "\n",
      "Epoch:  11043  Learning Rate:  1.6014756171930149e-06  Varinance:  4.944781643516862e-06 \n",
      "\n",
      "Epoch:  11044  Learning Rate:  1.5998749420467857e-06  Varinance:  4.937873964808393e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11045  Learning Rate:  1.5982758667756288e-06  Varinance:  4.930975935873883e-06 \n",
      "\n",
      "Epoch:  11046  Learning Rate:  1.5966783897804747e-06  Varinance:  4.924087543232954e-06 \n",
      "\n",
      "Epoch:  11047  Learning Rate:  1.5950825094638435e-06  Varinance:  4.9172087734240625e-06 \n",
      "\n",
      "Epoch:  11048  Learning Rate:  1.5934882242298518e-06  Varinance:  4.910339613004449e-06 \n",
      "\n",
      "Epoch:  11049  Learning Rate:  1.59189553248422e-06  Varinance:  4.903480048550158e-06 \n",
      "\n",
      "Epoch:  11050  Learning Rate:  1.590304432634253e-06  Varinance:  4.896630066655985e-06 \n",
      "\n",
      "Epoch:  11051  Learning Rate:  1.588714923088849e-06  Varinance:  4.889789653935429e-06 \n",
      "\n",
      "Epoch:  11052  Learning Rate:  1.587127002258503e-06  Varinance:  4.882958797020712e-06 \n",
      "\n",
      "Epoch:  11053  Learning Rate:  1.5855406685552913e-06  Varinance:  4.8761374825627255e-06 \n",
      "\n",
      "Epoch:  11054  Learning Rate:  1.5839559203928774e-06  Varinance:  4.869325697231014e-06 \n",
      "\n",
      "Epoch:  11055  Learning Rate:  1.5823727561865191e-06  Varinance:  4.862523427713722e-06 \n",
      "\n",
      "Epoch:  11056  Learning Rate:  1.5807911743530488e-06  Varinance:  4.855730660717613e-06 \n",
      "\n",
      "Epoch:  11057  Learning Rate:  1.5792111733108844e-06  Varinance:  4.848947382968021e-06 \n",
      "\n",
      "Epoch:  11058  Learning Rate:  1.5776327514800219e-06  Varinance:  4.842173581208801e-06 \n",
      "\n",
      "Epoch:  11059  Learning Rate:  1.5760559072820457e-06  Varinance:  4.83540924220235e-06 \n",
      "\n",
      "Epoch:  11060  Learning Rate:  1.5744806391401076e-06  Varinance:  4.828654352729557e-06 \n",
      "\n",
      "Epoch:  11061  Learning Rate:  1.5729069454789375e-06  Varinance:  4.821908899589754e-06 \n",
      "\n",
      "Epoch:  11062  Learning Rate:  1.5713348247248465e-06  Varinance:  4.815172869600736e-06 \n",
      "\n",
      "Epoch:  11063  Learning Rate:  1.569764275305711e-06  Varinance:  4.808446249598709e-06 \n",
      "\n",
      "Epoch:  11064  Learning Rate:  1.5681952956509794e-06  Varinance:  4.801729026438274e-06 \n",
      "\n",
      "Epoch:  11065  Learning Rate:  1.5666278841916762e-06  Varinance:  4.79502118699237e-06 \n",
      "\n",
      "Epoch:  11066  Learning Rate:  1.5650620393603883e-06  Varinance:  4.788322718152301e-06 \n",
      "\n",
      "Epoch:  11067  Learning Rate:  1.5634977595912673e-06  Varinance:  4.781633606827682e-06 \n",
      "\n",
      "Epoch:  11068  Learning Rate:  1.5619350433200387e-06  Varinance:  4.774953839946391e-06 \n",
      "\n",
      "Epoch:  11069  Learning Rate:  1.5603738889839836e-06  Varinance:  4.76828340445459e-06 \n",
      "\n",
      "Epoch:  11070  Learning Rate:  1.5588142950219452e-06  Varinance:  4.761622287316671e-06 \n",
      "\n",
      "Epoch:  11071  Learning Rate:  1.5572562598743342e-06  Varinance:  4.754970475515244e-06 \n",
      "\n",
      "Epoch:  11072  Learning Rate:  1.5556997819831127e-06  Varinance:  4.748327956051077e-06 \n",
      "\n",
      "Epoch:  11073  Learning Rate:  1.5541448597918029e-06  Varinance:  4.741694715943126e-06 \n",
      "\n",
      "Epoch:  11074  Learning Rate:  1.5525914917454797e-06  Varinance:  4.735070742228476e-06 \n",
      "\n",
      "Epoch:  11075  Learning Rate:  1.5510396762907803e-06  Varinance:  4.728456021962301e-06 \n",
      "\n",
      "Epoch:  11076  Learning Rate:  1.5494894118758869e-06  Varinance:  4.721850542217879e-06 \n",
      "\n",
      "Epoch:  11077  Learning Rate:  1.5479406969505313e-06  Varinance:  4.71525429008654e-06 \n",
      "\n",
      "Epoch:  11078  Learning Rate:  1.5463935299660043e-06  Varinance:  4.708667252677657e-06 \n",
      "\n",
      "Epoch:  11079  Learning Rate:  1.5448479093751366e-06  Varinance:  4.702089417118582e-06 \n",
      "\n",
      "Epoch:  11080  Learning Rate:  1.5433038336323039e-06  Varinance:  4.695520770554678e-06 \n",
      "\n",
      "Epoch:  11081  Learning Rate:  1.5417613011934362e-06  Varinance:  4.688961300149263e-06 \n",
      "\n",
      "Epoch:  11082  Learning Rate:  1.540220310515998e-06  Varinance:  4.682410993083561e-06 \n",
      "\n",
      "Epoch:  11083  Learning Rate:  1.538680860058996e-06  Varinance:  4.675869836556735e-06 \n",
      "\n",
      "Epoch:  11084  Learning Rate:  1.5371429482829855e-06  Varinance:  4.669337817785814e-06 \n",
      "\n",
      "Epoch:  11085  Learning Rate:  1.535606573650051e-06  Varinance:  4.662814924005699e-06 \n",
      "\n",
      "Epoch:  11086  Learning Rate:  1.5340717346238156e-06  Varinance:  4.6563011424690935e-06 \n",
      "\n",
      "Epoch:  11087  Learning Rate:  1.532538429669445e-06  Varinance:  4.649796460446537e-06 \n",
      "\n",
      "Epoch:  11088  Learning Rate:  1.531006657253632e-06  Varinance:  4.643300865226349e-06 \n",
      "\n",
      "Epoch:  11089  Learning Rate:  1.5294764158446037e-06  Varinance:  4.636814344114585e-06 \n",
      "\n",
      "Epoch:  11090  Learning Rate:  1.5279477039121162e-06  Varinance:  4.630336884435055e-06 \n",
      "\n",
      "Epoch:  11091  Learning Rate:  1.5264205199274629e-06  Varinance:  4.62386847352927e-06 \n",
      "\n",
      "Epoch:  11092  Learning Rate:  1.5248948623634561e-06  Varinance:  4.617409098756433e-06 \n",
      "\n",
      "Epoch:  11093  Learning Rate:  1.5233707296944365e-06  Varinance:  4.610958747493383e-06 \n",
      "\n",
      "Epoch:  11094  Learning Rate:  1.5218481203962762e-06  Varinance:  4.604517407134613e-06 \n",
      "\n",
      "Epoch:  11095  Learning Rate:  1.520327032946363e-06  Varinance:  4.5980850650922305e-06 \n",
      "\n",
      "Epoch:  11096  Learning Rate:  1.5188074658236066e-06  Varinance:  4.591661708795899e-06 \n",
      "\n",
      "Epoch:  11097  Learning Rate:  1.5172894175084454e-06  Varinance:  4.585247325692871e-06 \n",
      "\n",
      "Epoch:  11098  Learning Rate:  1.5157728864828283e-06  Varinance:  4.5788419032479225e-06 \n",
      "\n",
      "Epoch:  11099  Learning Rate:  1.5142578712302212e-06  Varinance:  4.572445428943351e-06 \n",
      "\n",
      "Epoch:  11100  Learning Rate:  1.512744370235614e-06  Varinance:  4.5660578902789175e-06 \n",
      "\n",
      "Epoch:  11101  Learning Rate:  1.5112323819855034e-06  Varinance:  4.559679274771867e-06 \n",
      "\n",
      "Epoch:  11102  Learning Rate:  1.509721904967898e-06  Varinance:  4.553309569956884e-06 \n",
      "\n",
      "Epoch:  11103  Learning Rate:  1.5082129376723259e-06  Varinance:  4.546948763386043e-06 \n",
      "\n",
      "Epoch:  11104  Learning Rate:  1.5067054785898173e-06  Varinance:  4.54059684262883e-06 \n",
      "\n",
      "Epoch:  11105  Learning Rate:  1.5051995262129127e-06  Varinance:  4.53425379527209e-06 \n",
      "\n",
      "Epoch:  11106  Learning Rate:  1.5036950790356573e-06  Varinance:  4.5279196089200144e-06 \n",
      "\n",
      "Epoch:  11107  Learning Rate:  1.5021921355536086e-06  Varinance:  4.521594271194091e-06 \n",
      "\n",
      "Epoch:  11108  Learning Rate:  1.500690694263821e-06  Varinance:  4.515277769733119e-06 \n",
      "\n",
      "Epoch:  11109  Learning Rate:  1.49919075366485e-06  Varinance:  4.508970092193169e-06 \n",
      "\n",
      "Epoch:  11110  Learning Rate:  1.49769231225676e-06  Varinance:  4.5026712262475315e-06 \n",
      "\n",
      "Epoch:  11111  Learning Rate:  1.4961953685411072e-06  Varinance:  4.496381159586739e-06 \n",
      "\n",
      "Epoch:  11112  Learning Rate:  1.4946999210209449e-06  Varinance:  4.490099879918513e-06 \n",
      "\n",
      "Epoch:  11113  Learning Rate:  1.4932059682008309e-06  Varinance:  4.483827374967756e-06 \n",
      "\n",
      "Epoch:  11114  Learning Rate:  1.4917135085868097e-06  Varinance:  4.477563632476493e-06 \n",
      "\n",
      "Epoch:  11115  Learning Rate:  1.4902225406864185e-06  Varinance:  4.471308640203898e-06 \n",
      "\n",
      "Epoch:  11116  Learning Rate:  1.4887330630086948e-06  Varinance:  4.465062385926242e-06 \n",
      "\n",
      "Epoch:  11117  Learning Rate:  1.4872450740641583e-06  Varinance:  4.4588248574368516e-06 \n",
      "\n",
      "Epoch:  11118  Learning Rate:  1.4857585723648174e-06  Varinance:  4.45259604254613e-06 \n",
      "\n",
      "Epoch:  11119  Learning Rate:  1.484273556424175e-06  Varinance:  4.446375929081497e-06 \n",
      "\n",
      "Epoch:  11120  Learning Rate:  1.4827900247572126e-06  Varinance:  4.440164504887388e-06 \n",
      "\n",
      "Epoch:  11121  Learning Rate:  1.4813079758803988e-06  Varinance:  4.433961757825196e-06 \n",
      "\n",
      "Epoch:  11122  Learning Rate:  1.4798274083116816e-06  Varinance:  4.427767675773291e-06 \n",
      "\n",
      "Epoch:  11123  Learning Rate:  1.4783483205704985e-06  Varinance:  4.421582246626982e-06 \n",
      "\n",
      "Epoch:  11124  Learning Rate:  1.4768707111777593e-06  Varinance:  4.415405458298459e-06 \n",
      "\n",
      "Epoch:  11125  Learning Rate:  1.4753945786558518e-06  Varinance:  4.409237298716826e-06 \n",
      "\n",
      "Epoch:  11126  Learning Rate:  1.4739199215286484e-06  Varinance:  4.403077755828045e-06 \n",
      "\n",
      "Epoch:  11127  Learning Rate:  1.4724467383214895e-06  Varinance:  4.3969268175949e-06 \n",
      "\n",
      "Epoch:  11128  Learning Rate:  1.4709750275611889e-06  Varinance:  4.390784471997007e-06 \n",
      "\n",
      "Epoch:  11129  Learning Rate:  1.4695047877760412e-06  Varinance:  4.384650707030772e-06 \n",
      "\n",
      "Epoch:  11130  Learning Rate:  1.4680360174958036e-06  Varinance:  4.378525510709372e-06 \n",
      "\n",
      "Epoch:  11131  Learning Rate:  1.4665687152517032e-06  Varinance:  4.372408871062712e-06 \n",
      "\n",
      "Epoch:  11132  Learning Rate:  1.465102879576443e-06  Varinance:  4.366300776137437e-06 \n",
      "\n",
      "Epoch:  11133  Learning Rate:  1.4636385090041842e-06  Varinance:  4.360201213996892e-06 \n",
      "\n",
      "Epoch:  11134  Learning Rate:  1.4621756020705565e-06  Varinance:  4.3541101727210745e-06 \n",
      "\n",
      "Epoch:  11135  Learning Rate:  1.4607141573126503e-06  Varinance:  4.348027640406656e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11136  Learning Rate:  1.4592541732690255e-06  Varinance:  4.341953605166931e-06 \n",
      "\n",
      "Epoch:  11137  Learning Rate:  1.4577956484796956e-06  Varinance:  4.335888055131803e-06 \n",
      "\n",
      "Epoch:  11138  Learning Rate:  1.4563385814861334e-06  Varinance:  4.329830978447738e-06 \n",
      "\n",
      "Epoch:  11139  Learning Rate:  1.4548829708312763e-06  Varinance:  4.323782363277783e-06 \n",
      "\n",
      "Epoch:  11140  Learning Rate:  1.4534288150595112e-06  Varinance:  4.317742197801521e-06 \n",
      "\n",
      "Epoch:  11141  Learning Rate:  1.45197611271668e-06  Varinance:  4.3117104702150225e-06 \n",
      "\n",
      "Epoch:  11142  Learning Rate:  1.450524862350085e-06  Varinance:  4.305687168730874e-06 \n",
      "\n",
      "Epoch:  11143  Learning Rate:  1.449075062508473e-06  Varinance:  4.299672281578119e-06 \n",
      "\n",
      "Epoch:  11144  Learning Rate:  1.4476267117420419e-06  Varinance:  4.293665797002252e-06 \n",
      "\n",
      "Epoch:  11145  Learning Rate:  1.4461798086024457e-06  Varinance:  4.287667703265166e-06 \n",
      "\n",
      "Epoch:  11146  Learning Rate:  1.4447343516427787e-06  Varinance:  4.281677988645176e-06 \n",
      "\n",
      "Epoch:  11147  Learning Rate:  1.4432903394175811e-06  Varinance:  4.275696641436969e-06 \n",
      "\n",
      "Epoch:  11148  Learning Rate:  1.4418477704828458e-06  Varinance:  4.269723649951561e-06 \n",
      "\n",
      "Epoch:  11149  Learning Rate:  1.440406643396001e-06  Varinance:  4.263759002516322e-06 \n",
      "\n",
      "Epoch:  11150  Learning Rate:  1.4389669567159198e-06  Varinance:  4.2578026874749215e-06 \n",
      "\n",
      "Epoch:  11151  Learning Rate:  1.4375287090029126e-06  Varinance:  4.251854693187316e-06 \n",
      "\n",
      "Epoch:  11152  Learning Rate:  1.4360918988187367e-06  Varinance:  4.245915008029704e-06 \n",
      "\n",
      "Epoch:  11153  Learning Rate:  1.4346565247265794e-06  Varinance:  4.2399836203945415e-06 \n",
      "\n",
      "Epoch:  11154  Learning Rate:  1.4332225852910638e-06  Varinance:  4.234060518690502e-06 \n",
      "\n",
      "Epoch:  11155  Learning Rate:  1.4317900790782554e-06  Varinance:  4.228145691342428e-06 \n",
      "\n",
      "Epoch:  11156  Learning Rate:  1.4303590046556455e-06  Varinance:  4.222239126791356e-06 \n",
      "\n",
      "Epoch:  11157  Learning Rate:  1.428929360592157e-06  Varinance:  4.21634081349446e-06 \n",
      "\n",
      "Epoch:  11158  Learning Rate:  1.4275011454581505e-06  Varinance:  4.21045073992505e-06 \n",
      "\n",
      "Epoch:  11159  Learning Rate:  1.4260743578254084e-06  Varinance:  4.204568894572512e-06 \n",
      "\n",
      "Epoch:  11160  Learning Rate:  1.4246489962671406e-06  Varinance:  4.198695265942335e-06 \n",
      "\n",
      "Epoch:  11161  Learning Rate:  1.4232250593579902e-06  Varinance:  4.192829842556069e-06 \n",
      "\n",
      "Epoch:  11162  Learning Rate:  1.4218025456740177e-06  Varinance:  4.186972612951274e-06 \n",
      "\n",
      "Epoch:  11163  Learning Rate:  1.420381453792707e-06  Varinance:  4.181123565681543e-06 \n",
      "\n",
      "Epoch:  11164  Learning Rate:  1.4189617822929709e-06  Varinance:  4.1752826893164564e-06 \n",
      "\n",
      "Epoch:  11165  Learning Rate:  1.4175435297551352e-06  Varinance:  4.169449972441566e-06 \n",
      "\n",
      "Epoch:  11166  Learning Rate:  1.4161266947609477e-06  Varinance:  4.163625403658347e-06 \n",
      "\n",
      "Epoch:  11167  Learning Rate:  1.4147112758935703e-06  Varinance:  4.157808971584223e-06 \n",
      "\n",
      "Epoch:  11168  Learning Rate:  1.4132972717375891e-06  Varinance:  4.152000664852517e-06 \n",
      "\n",
      "Epoch:  11169  Learning Rate:  1.4118846808789975e-06  Varinance:  4.146200472112408e-06 \n",
      "\n",
      "Epoch:  11170  Learning Rate:  1.4104735019052019e-06  Varinance:  4.140408382028955e-06 \n",
      "\n",
      "Epoch:  11171  Learning Rate:  1.4090637334050283e-06  Varinance:  4.134624383283044e-06 \n",
      "\n",
      "Epoch:  11172  Learning Rate:  1.4076553739687055e-06  Varinance:  4.1288484645713794e-06 \n",
      "\n",
      "Epoch:  11173  Learning Rate:  1.4062484221878714e-06  Varinance:  4.1230806146064355e-06 \n",
      "\n",
      "Epoch:  11174  Learning Rate:  1.4048428766555793e-06  Varinance:  4.117320822116476e-06 \n",
      "\n",
      "Epoch:  11175  Learning Rate:  1.4034387359662808e-06  Varinance:  4.111569075845512e-06 \n",
      "\n",
      "Epoch:  11176  Learning Rate:  1.4020359987158329e-06  Varinance:  4.105825364553255e-06 \n",
      "\n",
      "Epoch:  11177  Learning Rate:  1.4006346635015028e-06  Varinance:  4.10008967701514e-06 \n",
      "\n",
      "Epoch:  11178  Learning Rate:  1.399234728921953e-06  Varinance:  4.094362002022279e-06 \n",
      "\n",
      "Epoch:  11179  Learning Rate:  1.3978361935772465e-06  Varinance:  4.088642328381446e-06 \n",
      "\n",
      "Epoch:  11180  Learning Rate:  1.3964390560688522e-06  Varinance:  4.082930644915032e-06 \n",
      "\n",
      "Epoch:  11181  Learning Rate:  1.3950433149996306e-06  Varinance:  4.0772269404610625e-06 \n",
      "\n",
      "Epoch:  11182  Learning Rate:  1.39364896897384e-06  Varinance:  4.071531203873157e-06 \n",
      "\n",
      "Epoch:  11183  Learning Rate:  1.3922560165971323e-06  Varinance:  4.065843424020485e-06 \n",
      "\n",
      "Epoch:  11184  Learning Rate:  1.3908644564765597e-06  Varinance:  4.060163589787785e-06 \n",
      "\n",
      "Epoch:  11185  Learning Rate:  1.3894742872205593e-06  Varinance:  4.054491690075317e-06 \n",
      "\n",
      "Epoch:  11186  Learning Rate:  1.3880855074389593e-06  Varinance:  4.0488277137988556e-06 \n",
      "\n",
      "Epoch:  11187  Learning Rate:  1.3866981157429852e-06  Varinance:  4.043171649889637e-06 \n",
      "\n",
      "Epoch:  11188  Learning Rate:  1.3853121107452422e-06  Varinance:  4.037523487294379e-06 \n",
      "\n",
      "Epoch:  11189  Learning Rate:  1.383927491059723e-06  Varinance:  4.0318832149752456e-06 \n",
      "\n",
      "Epoch:  11190  Learning Rate:  1.3825442553018127e-06  Varinance:  4.026250821909793e-06 \n",
      "\n",
      "Epoch:  11191  Learning Rate:  1.381162402088273e-06  Varinance:  4.020626297091003e-06 \n",
      "\n",
      "Epoch:  11192  Learning Rate:  1.3797819300372476e-06  Varinance:  4.015009629527228e-06 \n",
      "\n",
      "Epoch:  11193  Learning Rate:  1.3784028377682701e-06  Varinance:  4.009400808242158e-06 \n",
      "\n",
      "Epoch:  11194  Learning Rate:  1.3770251239022452e-06  Varinance:  4.003799822274835e-06 \n",
      "\n",
      "Epoch:  11195  Learning Rate:  1.3756487870614563e-06  Varinance:  3.998206660679609e-06 \n",
      "\n",
      "Epoch:  11196  Learning Rate:  1.3742738258695716e-06  Varinance:  3.992621312526125e-06 \n",
      "\n",
      "Epoch:  11197  Learning Rate:  1.3729002389516274e-06  Varinance:  3.987043766899277e-06 \n",
      "\n",
      "Epoch:  11198  Learning Rate:  1.3715280249340363e-06  Varinance:  3.981474012899229e-06 \n",
      "\n",
      "Epoch:  11199  Learning Rate:  1.3701571824445823e-06  Varinance:  3.9759120396413705e-06 \n",
      "\n",
      "Epoch:  11200  Learning Rate:  1.3687877101124273e-06  Varinance:  3.970357836256276e-06 \n",
      "\n",
      "Epoch:  11201  Learning Rate:  1.3674196065680965e-06  Varinance:  3.964811391889724e-06 \n",
      "\n",
      "Epoch:  11202  Learning Rate:  1.3660528704434836e-06  Varinance:  3.959272695702653e-06 \n",
      "\n",
      "Epoch:  11203  Learning Rate:  1.3646875003718574e-06  Varinance:  3.953741736871144e-06 \n",
      "\n",
      "Epoch:  11204  Learning Rate:  1.3633234949878454e-06  Varinance:  3.948218504586382e-06 \n",
      "\n",
      "Epoch:  11205  Learning Rate:  1.3619608529274398e-06  Varinance:  3.942702988054672e-06 \n",
      "\n",
      "Epoch:  11206  Learning Rate:  1.3605995728280027e-06  Varinance:  3.937195176497397e-06 \n",
      "\n",
      "Epoch:  11207  Learning Rate:  1.359239653328252e-06  Varinance:  3.931695059150975e-06 \n",
      "\n",
      "Epoch:  11208  Learning Rate:  1.3578810930682654e-06  Varinance:  3.926202625266883e-06 \n",
      "\n",
      "Epoch:  11209  Learning Rate:  1.3565238906894875e-06  Varinance:  3.920717864111605e-06 \n",
      "\n",
      "Epoch:  11210  Learning Rate:  1.3551680448347134e-06  Varinance:  3.915240764966628e-06 \n",
      "\n",
      "Epoch:  11211  Learning Rate:  1.3538135541480945e-06  Varinance:  3.909771317128391e-06 \n",
      "\n",
      "Epoch:  11212  Learning Rate:  1.3524604172751452e-06  Varinance:  3.904309509908305e-06 \n",
      "\n",
      "Epoch:  11213  Learning Rate:  1.3511086328627255e-06  Varinance:  3.8988553326327135e-06 \n",
      "\n",
      "Epoch:  11214  Learning Rate:  1.3497581995590516e-06  Varinance:  3.89340877464285e-06 \n",
      "\n",
      "Epoch:  11215  Learning Rate:  1.3484091160136871e-06  Varinance:  3.887969825294859e-06 \n",
      "\n",
      "Epoch:  11216  Learning Rate:  1.3470613808775535e-06  Varinance:  3.882538473959746e-06 \n",
      "\n",
      "Epoch:  11217  Learning Rate:  1.345714992802913e-06  Varinance:  3.877114710023372e-06 \n",
      "\n",
      "Epoch:  11218  Learning Rate:  1.344369950443375e-06  Varinance:  3.8716985228864065e-06 \n",
      "\n",
      "Epoch:  11219  Learning Rate:  1.3430262524539021e-06  Varinance:  3.8662899019643456e-06 \n",
      "\n",
      "Epoch:  11220  Learning Rate:  1.3416838974907935e-06  Varinance:  3.860888836687469e-06 \n",
      "\n",
      "Epoch:  11221  Learning Rate:  1.3403428842116916e-06  Varinance:  3.855495316500806e-06 \n",
      "\n",
      "Epoch:  11222  Learning Rate:  1.3390032112755882e-06  Varinance:  3.850109330864148e-06 \n",
      "\n",
      "Epoch:  11223  Learning Rate:  1.3376648773428075e-06  Varinance:  3.844730869252004e-06 \n",
      "\n",
      "Epoch:  11224  Learning Rate:  1.3363278810750133e-06  Varinance:  3.839359921153596e-06 \n",
      "\n",
      "Epoch:  11225  Learning Rate:  1.334992221135214e-06  Varinance:  3.833996476072806e-06 \n",
      "\n",
      "Epoch:  11226  Learning Rate:  1.3336578961877467e-06  Varinance:  3.8286405235282e-06 \n",
      "\n",
      "Epoch:  11227  Learning Rate:  1.3323249048982849e-06  Varinance:  3.823292053052989e-06 \n",
      "\n",
      "Epoch:  11228  Learning Rate:  1.3309932459338411e-06  Varinance:  3.817951054194978e-06 \n",
      "\n",
      "Epoch:  11229  Learning Rate:  1.3296629179627542e-06  Varinance:  3.8126175165166026e-06 \n",
      "\n",
      "Epoch:  11230  Learning Rate:  1.328333919654696e-06  Varinance:  3.8072914295948667e-06 \n",
      "\n",
      "Epoch:  11231  Learning Rate:  1.3270062496806658e-06  Varinance:  3.801972783021343e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11232  Learning Rate:  1.3256799067129985e-06  Varinance:  3.796661566402124e-06 \n",
      "\n",
      "Epoch:  11233  Learning Rate:  1.324354889425348e-06  Varinance:  3.7913577693578437e-06 \n",
      "\n",
      "Epoch:  11234  Learning Rate:  1.3230311964926953e-06  Varinance:  3.7860613815236333e-06 \n",
      "\n",
      "Epoch:  11235  Learning Rate:  1.3217088265913514e-06  Varinance:  3.7807723925490862e-06 \n",
      "\n",
      "Epoch:  11236  Learning Rate:  1.3203877783989443e-06  Varinance:  3.7754907920982712e-06 \n",
      "\n",
      "Epoch:  11237  Learning Rate:  1.3190680505944233e-06  Varinance:  3.7702165698496933e-06 \n",
      "\n",
      "Epoch:  11238  Learning Rate:  1.3177496418580652e-06  Varinance:  3.76494971549628e-06 \n",
      "\n",
      "Epoch:  11239  Learning Rate:  1.3164325508714588e-06  Varinance:  3.759690218745339e-06 \n",
      "\n",
      "Epoch:  11240  Learning Rate:  1.3151167763175105e-06  Varinance:  3.7544380693185766e-06 \n",
      "\n",
      "Epoch:  11241  Learning Rate:  1.3138023168804505e-06  Varinance:  3.7491932569520554e-06 \n",
      "\n",
      "Epoch:  11242  Learning Rate:  1.312489171245817e-06  Varinance:  3.743955771396159e-06 \n",
      "\n",
      "Epoch:  11243  Learning Rate:  1.3111773381004615e-06  Varinance:  3.7387256024156075e-06 \n",
      "\n",
      "Epoch:  11244  Learning Rate:  1.309866816132556e-06  Varinance:  3.7335027397894137e-06 \n",
      "\n",
      "Epoch:  11245  Learning Rate:  1.3085576040315757e-06  Varinance:  3.728287173310875e-06 \n",
      "\n",
      "Epoch:  11246  Learning Rate:  1.3072497004883084e-06  Varinance:  3.723078892787527e-06 \n",
      "\n",
      "Epoch:  11247  Learning Rate:  1.3059431041948482e-06  Varinance:  3.7178778880411652e-06 \n",
      "\n",
      "Epoch:  11248  Learning Rate:  1.3046378138446035e-06  Varinance:  3.7126841489078e-06 \n",
      "\n",
      "Epoch:  11249  Learning Rate:  1.3033338281322813e-06  Varinance:  3.7074976652376257e-06 \n",
      "\n",
      "Epoch:  11250  Learning Rate:  1.3020311457538934e-06  Varinance:  3.702318426895031e-06 \n",
      "\n",
      "Epoch:  11251  Learning Rate:  1.3007297654067621e-06  Varinance:  3.6971464237585596e-06 \n",
      "\n",
      "Epoch:  11252  Learning Rate:  1.2994296857895048e-06  Varinance:  3.691981645720901e-06 \n",
      "\n",
      "Epoch:  11253  Learning Rate:  1.298130905602039e-06  Varinance:  3.6868240826888427e-06 \n",
      "\n",
      "Epoch:  11254  Learning Rate:  1.2968334235455894e-06  Varinance:  3.6816737245832934e-06 \n",
      "\n",
      "Epoch:  11255  Learning Rate:  1.2955372383226716e-06  Varinance:  3.676530561339241e-06 \n",
      "\n",
      "Epoch:  11256  Learning Rate:  1.2942423486370977e-06  Varinance:  3.6713945829057147e-06 \n",
      "\n",
      "Epoch:  11257  Learning Rate:  1.2929487531939823e-06  Varinance:  3.6662657792458047e-06 \n",
      "\n",
      "Epoch:  11258  Learning Rate:  1.291656450699728e-06  Varinance:  3.6611441403366215e-06 \n",
      "\n",
      "Epoch:  11259  Learning Rate:  1.290365439862032e-06  Varinance:  3.656029656169258e-06 \n",
      "\n",
      "Epoch:  11260  Learning Rate:  1.2890757193898814e-06  Varinance:  3.650922316748808e-06 \n",
      "\n",
      "Epoch:  11261  Learning Rate:  1.2877872879935594e-06  Varinance:  3.6458221120943223e-06 \n",
      "\n",
      "Epoch:  11262  Learning Rate:  1.2865001443846332e-06  Varinance:  3.6407290322388015e-06 \n",
      "\n",
      "Epoch:  11263  Learning Rate:  1.285214287275956e-06  Varinance:  3.6356430672291487e-06 \n",
      "\n",
      "Epoch:  11264  Learning Rate:  1.2839297153816756e-06  Varinance:  3.630564207126192e-06 \n",
      "\n",
      "Epoch:  11265  Learning Rate:  1.2826464274172175e-06  Varinance:  3.6254924420046436e-06 \n",
      "\n",
      "Epoch:  11266  Learning Rate:  1.2813644220992916e-06  Varinance:  3.6204277619530615e-06 \n",
      "\n",
      "Epoch:  11267  Learning Rate:  1.2800836981458965e-06  Varinance:  3.615370157073869e-06 \n",
      "\n",
      "Epoch:  11268  Learning Rate:  1.2788042542763065e-06  Varinance:  3.6103196174833097e-06 \n",
      "\n",
      "Epoch:  11269  Learning Rate:  1.2775260892110748e-06  Varinance:  3.6052761333114406e-06 \n",
      "\n",
      "Epoch:  11270  Learning Rate:  1.2762492016720415e-06  Varinance:  3.600239694702088e-06 \n",
      "\n",
      "Epoch:  11271  Learning Rate:  1.274973590382316e-06  Varinance:  3.5952102918128662e-06 \n",
      "\n",
      "Epoch:  11272  Learning Rate:  1.2736992540662848e-06  Varinance:  3.5901879148151366e-06 \n",
      "\n",
      "Epoch:  11273  Learning Rate:  1.272426191449616e-06  Varinance:  3.585172553893976e-06 \n",
      "\n",
      "Epoch:  11274  Learning Rate:  1.2711544012592449e-06  Varinance:  3.5801641992481865e-06 \n",
      "\n",
      "Epoch:  11275  Learning Rate:  1.2698838822233807e-06  Varinance:  3.57516284109026e-06 \n",
      "\n",
      "Epoch:  11276  Learning Rate:  1.2686146330715026e-06  Varinance:  3.5701684696463653e-06 \n",
      "\n",
      "Epoch:  11277  Learning Rate:  1.2673466525343653e-06  Varinance:  3.5651810751563058e-06 \n",
      "\n",
      "Epoch:  11278  Learning Rate:  1.2660799393439864e-06  Varinance:  3.5602006478735397e-06 \n",
      "\n",
      "Epoch:  11279  Learning Rate:  1.2648144922336498e-06  Varinance:  3.5552271780651394e-06 \n",
      "\n",
      "Epoch:  11280  Learning Rate:  1.2635503099379134e-06  Varinance:  3.5502606560117565e-06 \n",
      "\n",
      "Epoch:  11281  Learning Rate:  1.2622873911925919e-06  Varinance:  3.5453010720076375e-06 \n",
      "\n",
      "Epoch:  11282  Learning Rate:  1.2610257347347647e-06  Varinance:  3.540348416360583e-06 \n",
      "\n",
      "Epoch:  11283  Learning Rate:  1.2597653393027797e-06  Varinance:  3.5354026793919363e-06 \n",
      "\n",
      "Epoch:  11284  Learning Rate:  1.258506203636239e-06  Varinance:  3.530463851436547e-06 \n",
      "\n",
      "Epoch:  11285  Learning Rate:  1.2572483264760043e-06  Varinance:  3.5255319228427816e-06 \n",
      "\n",
      "Epoch:  11286  Learning Rate:  1.2559917065642033e-06  Varinance:  3.520606883972492e-06 \n",
      "\n",
      "Epoch:  11287  Learning Rate:  1.2547363426442134e-06  Varinance:  3.5156887252009735e-06 \n",
      "\n",
      "Epoch:  11288  Learning Rate:  1.2534822334606684e-06  Varinance:  3.5107774369169875e-06 \n",
      "\n",
      "Epoch:  11289  Learning Rate:  1.2522293777594637e-06  Varinance:  3.5058730095227138e-06 \n",
      "\n",
      "Epoch:  11290  Learning Rate:  1.2509777742877411e-06  Varinance:  3.5009754334337484e-06 \n",
      "\n",
      "Epoch:  11291  Learning Rate:  1.249727421793897e-06  Varinance:  3.4960846990790552e-06 \n",
      "\n",
      "Epoch:  11292  Learning Rate:  1.2484783190275766e-06  Varinance:  3.4912007969009903e-06 \n",
      "\n",
      "Epoch:  11293  Learning Rate:  1.2472304647396818e-06  Varinance:  3.486323717355257e-06 \n",
      "\n",
      "Epoch:  11294  Learning Rate:  1.2459838576823551e-06  Varinance:  3.4814534509108776e-06 \n",
      "\n",
      "Epoch:  11295  Learning Rate:  1.2447384966089881e-06  Varinance:  3.4765899880502045e-06 \n",
      "\n",
      "Epoch:  11296  Learning Rate:  1.2434943802742235e-06  Varinance:  3.471733319268881e-06 \n",
      "\n",
      "Epoch:  11297  Learning Rate:  1.242251507433943e-06  Varinance:  3.466883435075833e-06 \n",
      "\n",
      "Epoch:  11298  Learning Rate:  1.241009876845271e-06  Varinance:  3.4620403259932284e-06 \n",
      "\n",
      "Epoch:  11299  Learning Rate:  1.2397694872665815e-06  Varinance:  3.4572039825564904e-06 \n",
      "\n",
      "Epoch:  11300  Learning Rate:  1.2385303374574826e-06  Varinance:  3.452374395314267e-06 \n",
      "\n",
      "Epoch:  11301  Learning Rate:  1.2372924261788223e-06  Varinance:  3.4475515548283902e-06 \n",
      "\n",
      "Epoch:  11302  Learning Rate:  1.2360557521926933e-06  Varinance:  3.4427354516738944e-06 \n",
      "\n",
      "Epoch:  11303  Learning Rate:  1.2348203142624195e-06  Varinance:  3.437926076438974e-06 \n",
      "\n",
      "Epoch:  11304  Learning Rate:  1.2335861111525608e-06  Varinance:  3.4331234197249786e-06 \n",
      "\n",
      "Epoch:  11305  Learning Rate:  1.2323531416289182e-06  Varinance:  3.4283274721463685e-06 \n",
      "\n",
      "Epoch:  11306  Learning Rate:  1.23112140445852e-06  Varinance:  3.423538224330734e-06 \n",
      "\n",
      "Epoch:  11307  Learning Rate:  1.2298908984096288e-06  Varinance:  3.4187556669187573e-06 \n",
      "\n",
      "Epoch:  11308  Learning Rate:  1.2286616222517362e-06  Varinance:  3.4139797905641777e-06 \n",
      "\n",
      "Epoch:  11309  Learning Rate:  1.2274335747555707e-06  Varinance:  3.409210585933809e-06 \n",
      "\n",
      "Epoch:  11310  Learning Rate:  1.2262067546930821e-06  Varinance:  3.404448043707497e-06 \n",
      "\n",
      "Epoch:  11311  Learning Rate:  1.2249811608374481e-06  Varinance:  3.3996921545781135e-06 \n",
      "\n",
      "Epoch:  11312  Learning Rate:  1.2237567919630791e-06  Varinance:  3.3949429092515146e-06 \n",
      "\n",
      "Epoch:  11313  Learning Rate:  1.2225336468456042e-06  Varinance:  3.390200298446556e-06 \n",
      "\n",
      "Epoch:  11314  Learning Rate:  1.221311724261876e-06  Varinance:  3.3854643128950603e-06 \n",
      "\n",
      "Epoch:  11315  Learning Rate:  1.220091022989976e-06  Varinance:  3.3807349433417807e-06 \n",
      "\n",
      "Epoch:  11316  Learning Rate:  1.2188715418092003e-06  Varinance:  3.3760121805444153e-06 \n",
      "\n",
      "Epoch:  11317  Learning Rate:  1.217653279500066e-06  Varinance:  3.3712960152735678e-06 \n",
      "\n",
      "Epoch:  11318  Learning Rate:  1.2164362348443149e-06  Varinance:  3.366586438312742e-06 \n",
      "\n",
      "Epoch:  11319  Learning Rate:  1.2152204066249e-06  Varinance:  3.361883440458298e-06 \n",
      "\n",
      "Epoch:  11320  Learning Rate:  1.214005793625991e-06  Varinance:  3.3571870125194723e-06 \n",
      "\n",
      "Epoch:  11321  Learning Rate:  1.2127923946329786e-06  Varinance:  3.3524971453183385e-06 \n",
      "\n",
      "Epoch:  11322  Learning Rate:  1.211580208432462e-06  Varinance:  3.347813829689775e-06 \n",
      "\n",
      "Epoch:  11323  Learning Rate:  1.2103692338122548e-06  Varinance:  3.3431370564814798e-06 \n",
      "\n",
      "Epoch:  11324  Learning Rate:  1.2091594695613802e-06  Varinance:  3.33846681655394e-06 \n",
      "\n",
      "Epoch:  11325  Learning Rate:  1.2079509144700782e-06  Varinance:  3.3338031007803884e-06 \n",
      "\n",
      "Epoch:  11326  Learning Rate:  1.2067435673297912e-06  Varinance:  3.329145900046827e-06 \n",
      "\n",
      "Epoch:  11327  Learning Rate:  1.20553742693317e-06  Varinance:  3.324495205251984e-06 \n",
      "\n",
      "Epoch:  11328  Learning Rate:  1.2043324920740783e-06  Varinance:  3.319851007307307e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11329  Learning Rate:  1.2031287615475789e-06  Varinance:  3.315213297136923e-06 \n",
      "\n",
      "Epoch:  11330  Learning Rate:  1.2019262341499392e-06  Varinance:  3.3105820656776558e-06 \n",
      "\n",
      "Epoch:  11331  Learning Rate:  1.2007249086786362e-06  Varinance:  3.305957303878988e-06 \n",
      "\n",
      "Epoch:  11332  Learning Rate:  1.1995247839323418e-06  Varinance:  3.3013390027030296e-06 \n",
      "\n",
      "Epoch:  11333  Learning Rate:  1.198325858710929e-06  Varinance:  3.296727153124533e-06 \n",
      "\n",
      "Epoch:  11334  Learning Rate:  1.1971281318154772e-06  Varinance:  3.2921217461308544e-06 \n",
      "\n",
      "Epoch:  11335  Learning Rate:  1.1959316020482567e-06  Varinance:  3.2875227727219426e-06 \n",
      "\n",
      "Epoch:  11336  Learning Rate:  1.194736268212736e-06  Varinance:  3.282930223910306e-06 \n",
      "\n",
      "Epoch:  11337  Learning Rate:  1.193542129113585e-06  Varinance:  3.278344090721022e-06 \n",
      "\n",
      "Epoch:  11338  Learning Rate:  1.1923491835566626e-06  Varinance:  3.2737643641917083e-06 \n",
      "\n",
      "Epoch:  11339  Learning Rate:  1.1911574303490232e-06  Varinance:  3.2691910353724833e-06 \n",
      "\n",
      "Epoch:  11340  Learning Rate:  1.1899668682989113e-06  Varinance:  3.2646240953259865e-06 \n",
      "\n",
      "Epoch:  11341  Learning Rate:  1.188777496215769e-06  Varinance:  3.2600635351273365e-06 \n",
      "\n",
      "Epoch:  11342  Learning Rate:  1.1875893129102219e-06  Varinance:  3.255509345864126e-06 \n",
      "\n",
      "Epoch:  11343  Learning Rate:  1.1864023171940845e-06  Varinance:  3.2509615186363795e-06 \n",
      "\n",
      "Epoch:  11344  Learning Rate:  1.1852165078803653e-06  Varinance:  3.246420044556573e-06 \n",
      "\n",
      "Epoch:  11345  Learning Rate:  1.1840318837832528e-06  Varinance:  3.2418849147495966e-06 \n",
      "\n",
      "Epoch:  11346  Learning Rate:  1.1828484437181209e-06  Varinance:  3.2373561203527233e-06 \n",
      "\n",
      "Epoch:  11347  Learning Rate:  1.181666186501533e-06  Varinance:  3.232833652515623e-06 \n",
      "\n",
      "Epoch:  11348  Learning Rate:  1.1804851109512302e-06  Varinance:  3.2283175024003224e-06 \n",
      "\n",
      "Epoch:  11349  Learning Rate:  1.1793052158861348e-06  Varinance:  3.223807661181203e-06 \n",
      "\n",
      "Epoch:  11350  Learning Rate:  1.1781265001263555e-06  Varinance:  3.2193041200449554e-06 \n",
      "\n",
      "Epoch:  11351  Learning Rate:  1.1769489624931746e-06  Varinance:  3.214806870190601e-06 \n",
      "\n",
      "Epoch:  11352  Learning Rate:  1.175772601809052e-06  Varinance:  3.210315902829455e-06 \n",
      "\n",
      "Epoch:  11353  Learning Rate:  1.1745974168976314e-06  Varinance:  3.2058312091850944e-06 \n",
      "\n",
      "Epoch:  11354  Learning Rate:  1.1734234065837256e-06  Varinance:  3.201352780493371e-06 \n",
      "\n",
      "Epoch:  11355  Learning Rate:  1.1722505696933241e-06  Varinance:  3.1968806080023766e-06 \n",
      "\n",
      "Epoch:  11356  Learning Rate:  1.171078905053588e-06  Varinance:  3.1924146829724326e-06 \n",
      "\n",
      "Epoch:  11357  Learning Rate:  1.1699084114928564e-06  Varinance:  3.1879549966760556e-06 \n",
      "\n",
      "Epoch:  11358  Learning Rate:  1.168739087840634e-06  Varinance:  3.183501540397967e-06 \n",
      "\n",
      "Epoch:  11359  Learning Rate:  1.1675709329275947e-06  Varinance:  3.179054305435067e-06 \n",
      "\n",
      "Epoch:  11360  Learning Rate:  1.1664039455855877e-06  Varinance:  3.174613283096394e-06 \n",
      "\n",
      "Epoch:  11361  Learning Rate:  1.1652381246476235e-06  Varinance:  3.170178464703145e-06 \n",
      "\n",
      "Epoch:  11362  Learning Rate:  1.164073468947879e-06  Varinance:  3.1657498415886365e-06 \n",
      "\n",
      "Epoch:  11363  Learning Rate:  1.1629099773217025e-06  Varinance:  3.161327405098297e-06 \n",
      "\n",
      "Epoch:  11364  Learning Rate:  1.1617476486056e-06  Varinance:  3.1569111465896266e-06 \n",
      "\n",
      "Epoch:  11365  Learning Rate:  1.1605864816372412e-06  Varinance:  3.1525010574322183e-06 \n",
      "\n",
      "Epoch:  11366  Learning Rate:  1.1594264752554627e-06  Varinance:  3.1480971290077203e-06 \n",
      "\n",
      "Epoch:  11367  Learning Rate:  1.1582676283002561e-06  Varinance:  3.143699352709802e-06 \n",
      "\n",
      "Epoch:  11368  Learning Rate:  1.1571099396127723e-06  Varinance:  3.1393077199441752e-06 \n",
      "\n",
      "Epoch:  11369  Learning Rate:  1.1559534080353264e-06  Varinance:  3.1349222221285493e-06 \n",
      "\n",
      "Epoch:  11370  Learning Rate:  1.154798032411385e-06  Varinance:  3.13054285069263e-06 \n",
      "\n",
      "Epoch:  11371  Learning Rate:  1.1536438115855724e-06  Varinance:  3.1261695970780786e-06 \n",
      "\n",
      "Epoch:  11372  Learning Rate:  1.1524907444036653e-06  Varinance:  3.1218024527385284e-06 \n",
      "\n",
      "Epoch:  11373  Learning Rate:  1.1513388297126006e-06  Varinance:  3.11744140913955e-06 \n",
      "\n",
      "Epoch:  11374  Learning Rate:  1.1501880663604618e-06  Varinance:  3.1130864577586245e-06 \n",
      "\n",
      "Epoch:  11375  Learning Rate:  1.1490384531964828e-06  Varinance:  3.10873759008515e-06 \n",
      "\n",
      "Epoch:  11376  Learning Rate:  1.1478899890710551e-06  Varinance:  3.1043947976204097e-06 \n",
      "\n",
      "Epoch:  11377  Learning Rate:  1.1467426728357119e-06  Varinance:  3.1000580718775675e-06 \n",
      "\n",
      "Epoch:  11378  Learning Rate:  1.1455965033431352e-06  Varinance:  3.095727404381622e-06 \n",
      "\n",
      "Epoch:  11379  Learning Rate:  1.1444514794471594e-06  Varinance:  3.0914027866694308e-06 \n",
      "\n",
      "Epoch:  11380  Learning Rate:  1.1433076000027583e-06  Varinance:  3.0870842102896736e-06 \n",
      "\n",
      "Epoch:  11381  Learning Rate:  1.1421648638660505e-06  Varinance:  3.082771666802819e-06 \n",
      "\n",
      "Epoch:  11382  Learning Rate:  1.141023269894304e-06  Varinance:  3.0784651477811415e-06 \n",
      "\n",
      "Epoch:  11383  Learning Rate:  1.1398828169459221e-06  Varinance:  3.074164644808686e-06 \n",
      "\n",
      "Epoch:  11384  Learning Rate:  1.1387435038804521e-06  Varinance:  3.069870149481256e-06 \n",
      "\n",
      "Epoch:  11385  Learning Rate:  1.1376053295585791e-06  Varinance:  3.065581653406381e-06 \n",
      "\n",
      "Epoch:  11386  Learning Rate:  1.1364682928421324e-06  Varinance:  3.061299148203331e-06 \n",
      "\n",
      "Epoch:  11387  Learning Rate:  1.1353323925940735e-06  Varinance:  3.057022625503082e-06 \n",
      "\n",
      "Epoch:  11388  Learning Rate:  1.1341976276784995e-06  Varinance:  3.0527520769482877e-06 \n",
      "\n",
      "Epoch:  11389  Learning Rate:  1.13306399696065e-06  Varinance:  3.0484874941932907e-06 \n",
      "\n",
      "Epoch:  11390  Learning Rate:  1.1319314993068914e-06  Varinance:  3.044228868904092e-06 \n",
      "\n",
      "Epoch:  11391  Learning Rate:  1.1308001335847248e-06  Varinance:  3.039976192758321e-06 \n",
      "\n",
      "Epoch:  11392  Learning Rate:  1.1296698986627879e-06  Varinance:  3.035729457445246e-06 \n",
      "\n",
      "Epoch:  11393  Learning Rate:  1.1285407934108437e-06  Varinance:  3.0314886546657415e-06 \n",
      "\n",
      "Epoch:  11394  Learning Rate:  1.127412816699785e-06  Varinance:  3.0272537761322827e-06 \n",
      "\n",
      "Epoch:  11395  Learning Rate:  1.1262859674016392e-06  Varinance:  3.0230248135689027e-06 \n",
      "\n",
      "Epoch:  11396  Learning Rate:  1.1251602443895545e-06  Varinance:  3.018801758711214e-06 \n",
      "\n",
      "Epoch:  11397  Learning Rate:  1.1240356465378059e-06  Varinance:  3.014584603306374e-06 \n",
      "\n",
      "Epoch:  11398  Learning Rate:  1.1229121727217995e-06  Varinance:  3.0103733391130526e-06 \n",
      "\n",
      "Epoch:  11399  Learning Rate:  1.1217898218180594e-06  Varinance:  3.006167957901448e-06 \n",
      "\n",
      "Epoch:  11400  Learning Rate:  1.1206685927042345e-06  Varinance:  3.0019684514532505e-06 \n",
      "\n",
      "Epoch:  11401  Learning Rate:  1.119548484259094e-06  Varinance:  2.9977748115616368e-06 \n",
      "\n",
      "Epoch:  11402  Learning Rate:  1.118429495362533e-06  Varinance:  2.993587030031231e-06 \n",
      "\n",
      "Epoch:  11403  Learning Rate:  1.1173116248955602e-06  Varinance:  2.9894050986781233e-06 \n",
      "\n",
      "Epoch:  11404  Learning Rate:  1.1161948717403038e-06  Varinance:  2.985229009329836e-06 \n",
      "\n",
      "Epoch:  11405  Learning Rate:  1.115079234780014e-06  Varinance:  2.9810587538252904e-06 \n",
      "\n",
      "Epoch:  11406  Learning Rate:  1.113964712899052e-06  Varinance:  2.9768943240148273e-06 \n",
      "\n",
      "Epoch:  11407  Learning Rate:  1.1128513049828936e-06  Varinance:  2.9727357117601646e-06 \n",
      "\n",
      "Epoch:  11408  Learning Rate:  1.1117390099181348e-06  Varinance:  2.9685829089343964e-06 \n",
      "\n",
      "Epoch:  11409  Learning Rate:  1.110627826592479e-06  Varinance:  2.9644359074219523e-06 \n",
      "\n",
      "Epoch:  11410  Learning Rate:  1.1095177538947398e-06  Varinance:  2.9602946991186157e-06 \n",
      "\n",
      "Epoch:  11411  Learning Rate:  1.1084087907148492e-06  Varinance:  2.956159275931491e-06 \n",
      "\n",
      "Epoch:  11412  Learning Rate:  1.107300935943842e-06  Varinance:  2.9520296297789722e-06 \n",
      "\n",
      "Epoch:  11413  Learning Rate:  1.1061941884738607e-06  Varinance:  2.947905752590759e-06 \n",
      "\n",
      "Epoch:  11414  Learning Rate:  1.105088547198162e-06  Varinance:  2.943787636307821e-06 \n",
      "\n",
      "Epoch:  11415  Learning Rate:  1.1039840110111025e-06  Varinance:  2.9396752728823878e-06 \n",
      "\n",
      "Epoch:  11416  Learning Rate:  1.1028805788081462e-06  Varinance:  2.935568654277919e-06 \n",
      "\n",
      "Epoch:  11417  Learning Rate:  1.1017782494858587e-06  Varinance:  2.9314677724691154e-06 \n",
      "\n",
      "Epoch:  11418  Learning Rate:  1.1006770219419145e-06  Varinance:  2.9273726194418895e-06 \n",
      "\n",
      "Epoch:  11419  Learning Rate:  1.0995768950750837e-06  Varinance:  2.9232831871933316e-06 \n",
      "\n",
      "Epoch:  11420  Learning Rate:  1.0984778677852379e-06  Varinance:  2.9191994677317304e-06 \n",
      "\n",
      "Epoch:  11421  Learning Rate:  1.0973799389733532e-06  Varinance:  2.9151214530765307e-06 \n",
      "\n",
      "Epoch:  11422  Learning Rate:  1.096283107541499e-06  Varinance:  2.9110491352583326e-06 \n",
      "\n",
      "Epoch:  11423  Learning Rate:  1.0951873723928419e-06  Varinance:  2.906982506318854e-06 \n",
      "\n",
      "Epoch:  11424  Learning Rate:  1.09409273243165e-06  Varinance:  2.9029215583109435e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11425  Learning Rate:  1.0929991865632822e-06  Varinance:  2.8988662832985546e-06 \n",
      "\n",
      "Epoch:  11426  Learning Rate:  1.0919067336941897e-06  Varinance:  2.8948166733567097e-06 \n",
      "\n",
      "Epoch:  11427  Learning Rate:  1.090815372731924e-06  Varinance:  2.8907727205715173e-06 \n",
      "\n",
      "Epoch:  11428  Learning Rate:  1.089725102585122e-06  Varinance:  2.8867344170401384e-06 \n",
      "\n",
      "Epoch:  11429  Learning Rate:  1.0886359221635114e-06  Varinance:  2.882701754870776e-06 \n",
      "\n",
      "Epoch:  11430  Learning Rate:  1.0875478303779156e-06  Varinance:  2.878674726182645e-06 \n",
      "\n",
      "Epoch:  11431  Learning Rate:  1.0864608261402408e-06  Varinance:  2.8746533231059833e-06 \n",
      "\n",
      "Epoch:  11432  Learning Rate:  1.0853749083634826e-06  Varinance:  2.8706375377820234e-06 \n",
      "\n",
      "Epoch:  11433  Learning Rate:  1.0842900759617215e-06  Varinance:  2.8666273623629597e-06 \n",
      "\n",
      "Epoch:  11434  Learning Rate:  1.0832063278501285e-06  Varinance:  2.8626227890119664e-06 \n",
      "\n",
      "Epoch:  11435  Learning Rate:  1.0821236629449536e-06  Varinance:  2.85862380990316e-06 \n",
      "\n",
      "Epoch:  11436  Learning Rate:  1.08104208016353e-06  Varinance:  2.854630417221593e-06 \n",
      "\n",
      "Epoch:  11437  Learning Rate:  1.0799615784242786e-06  Varinance:  2.8506426031632228e-06 \n",
      "\n",
      "Epoch:  11438  Learning Rate:  1.0788821566466953e-06  Varinance:  2.846660359934921e-06 \n",
      "\n",
      "Epoch:  11439  Learning Rate:  1.077803813751357e-06  Varinance:  2.842683679754448e-06 \n",
      "\n",
      "Epoch:  11440  Learning Rate:  1.076726548659924e-06  Varinance:  2.8387125548504203e-06 \n",
      "\n",
      "Epoch:  11441  Learning Rate:  1.0756503602951294e-06  Varinance:  2.834746977462324e-06 \n",
      "\n",
      "Epoch:  11442  Learning Rate:  1.0745752475807827e-06  Varinance:  2.830786939840484e-06 \n",
      "\n",
      "Epoch:  11443  Learning Rate:  1.0735012094417753e-06  Varinance:  2.8268324342460537e-06 \n",
      "\n",
      "Epoch:  11444  Learning Rate:  1.0724282448040666e-06  Varinance:  2.8228834529509844e-06 \n",
      "\n",
      "Epoch:  11445  Learning Rate:  1.0713563525946903e-06  Varinance:  2.818939988238037e-06 \n",
      "\n",
      "Epoch:  11446  Learning Rate:  1.0702855317417578e-06  Varinance:  2.815002032400754e-06 \n",
      "\n",
      "Epoch:  11447  Learning Rate:  1.0692157811744463e-06  Varinance:  2.8110695777434223e-06 \n",
      "\n",
      "Epoch:  11448  Learning Rate:  1.0681470998230048e-06  Varinance:  2.8071426165811207e-06 \n",
      "\n",
      "Epoch:  11449  Learning Rate:  1.0670794866187504e-06  Varinance:  2.8032211412396125e-06 \n",
      "\n",
      "Epoch:  11450  Learning Rate:  1.0660129404940734e-06  Varinance:  2.799305144055418e-06 \n",
      "\n",
      "Epoch:  11451  Learning Rate:  1.0649474603824258e-06  Varinance:  2.7953946173757512e-06 \n",
      "\n",
      "Epoch:  11452  Learning Rate:  1.0638830452183254e-06  Varinance:  2.7914895535585183e-06 \n",
      "\n",
      "Epoch:  11453  Learning Rate:  1.0628196939373607e-06  Varinance:  2.787589944972301e-06 \n",
      "\n",
      "Epoch:  11454  Learning Rate:  1.0617574054761785e-06  Varinance:  2.783695783996341e-06 \n",
      "\n",
      "Epoch:  11455  Learning Rate:  1.0606961787724885e-06  Varinance:  2.7798070630205367e-06 \n",
      "\n",
      "Epoch:  11456  Learning Rate:  1.0596360127650677e-06  Varinance:  2.7759237744453873e-06 \n",
      "\n",
      "Epoch:  11457  Learning Rate:  1.0585769063937475e-06  Varinance:  2.772045910682038e-06 \n",
      "\n",
      "Epoch:  11458  Learning Rate:  1.0575188585994203e-06  Varinance:  2.7681734641522257e-06 \n",
      "\n",
      "Epoch:  11459  Learning Rate:  1.0564618683240417e-06  Varinance:  2.7643064272882738e-06 \n",
      "\n",
      "Epoch:  11460  Learning Rate:  1.0554059345106194e-06  Varinance:  2.7604447925330774e-06 \n",
      "\n",
      "Epoch:  11461  Learning Rate:  1.0543510561032177e-06  Varinance:  2.7565885523400884e-06 \n",
      "\n",
      "Epoch:  11462  Learning Rate:  1.053297232046962e-06  Varinance:  2.7527376991733105e-06 \n",
      "\n",
      "Epoch:  11463  Learning Rate:  1.0522444612880258e-06  Varinance:  2.748892225507247e-06 \n",
      "\n",
      "Epoch:  11464  Learning Rate:  1.051192742773639e-06  Varinance:  2.745052123826942e-06 \n",
      "\n",
      "Epoch:  11465  Learning Rate:  1.0501420754520803e-06  Varinance:  2.741217386627928e-06 \n",
      "\n",
      "Epoch:  11466  Learning Rate:  1.0490924582726866e-06  Varinance:  2.737388006416221e-06 \n",
      "\n",
      "Epoch:  11467  Learning Rate:  1.0480438901858385e-06  Varinance:  2.7335639757083074e-06 \n",
      "\n",
      "Epoch:  11468  Learning Rate:  1.0469963701429663e-06  Varinance:  2.7297452870311246e-06 \n",
      "\n",
      "Epoch:  11469  Learning Rate:  1.0459498970965531e-06  Varinance:  2.7259319329220625e-06 \n",
      "\n",
      "Epoch:  11470  Learning Rate:  1.0449044700001244e-06  Varinance:  2.7221239059289054e-06 \n",
      "\n",
      "Epoch:  11471  Learning Rate:  1.0438600878082507e-06  Varinance:  2.718321198609877e-06 \n",
      "\n",
      "Epoch:  11472  Learning Rate:  1.0428167494765537e-06  Varinance:  2.7145238035335876e-06 \n",
      "\n",
      "Epoch:  11473  Learning Rate:  1.0417744539616931e-06  Varinance:  2.710731713279029e-06 \n",
      "\n",
      "Epoch:  11474  Learning Rate:  1.0407332002213714e-06  Varinance:  2.7069449204355598e-06 \n",
      "\n",
      "Epoch:  11475  Learning Rate:  1.0396929872143386e-06  Varinance:  2.7031634176028905e-06 \n",
      "\n",
      "Epoch:  11476  Learning Rate:  1.0386538139003797e-06  Varinance:  2.6993871973910793e-06 \n",
      "\n",
      "Epoch:  11477  Learning Rate:  1.0376156792403192e-06  Varinance:  2.6956162524204804e-06 \n",
      "\n",
      "Epoch:  11478  Learning Rate:  1.0365785821960263e-06  Varinance:  2.6918505753217836e-06 \n",
      "\n",
      "Epoch:  11479  Learning Rate:  1.0355425217304023e-06  Varinance:  2.6880901587359656e-06 \n",
      "\n",
      "Epoch:  11480  Learning Rate:  1.0345074968073858e-06  Varinance:  2.6843349953142825e-06 \n",
      "\n",
      "Epoch:  11481  Learning Rate:  1.0334735063919507e-06  Varinance:  2.6805850777182566e-06 \n",
      "\n",
      "Epoch:  11482  Learning Rate:  1.03244054945011e-06  Varinance:  2.6768403986196615e-06 \n",
      "\n",
      "Epoch:  11483  Learning Rate:  1.0314086249489049e-06  Varinance:  2.6731009507005184e-06 \n",
      "\n",
      "Epoch:  11484  Learning Rate:  1.0303777318564087e-06  Varinance:  2.6693667266530417e-06 \n",
      "\n",
      "Epoch:  11485  Learning Rate:  1.029347869141732e-06  Varinance:  2.665637719179684e-06 \n",
      "\n",
      "Epoch:  11486  Learning Rate:  1.0283190357750105e-06  Varinance:  2.6619139209930826e-06 \n",
      "\n",
      "Epoch:  11487  Learning Rate:  1.0272912307274084e-06  Varinance:  2.6581953248160546e-06 \n",
      "\n",
      "Epoch:  11488  Learning Rate:  1.0262644529711246e-06  Varinance:  2.6544819233815836e-06 \n",
      "\n",
      "Epoch:  11489  Learning Rate:  1.0252387014793792e-06  Varinance:  2.6507737094328037e-06 \n",
      "\n",
      "Epoch:  11490  Learning Rate:  1.0242139752264187e-06  Varinance:  2.647070675722998e-06 \n",
      "\n",
      "Epoch:  11491  Learning Rate:  1.0231902731875208e-06  Varinance:  2.6433728150155436e-06 \n",
      "\n",
      "Epoch:  11492  Learning Rate:  1.0221675943389814e-06  Varinance:  2.639680120083955e-06 \n",
      "\n",
      "Epoch:  11493  Learning Rate:  1.0211459376581198e-06  Varinance:  2.6359925837118327e-06 \n",
      "\n",
      "Epoch:  11494  Learning Rate:  1.0201253021232825e-06  Varinance:  2.632310198692858e-06 \n",
      "\n",
      "Epoch:  11495  Learning Rate:  1.0191056867138324e-06  Varinance:  2.6286329578307794e-06 \n",
      "\n",
      "Epoch:  11496  Learning Rate:  1.0180870904101541e-06  Varinance:  2.624960853939408e-06 \n",
      "\n",
      "Epoch:  11497  Learning Rate:  1.017069512193649e-06  Varinance:  2.621293879842565e-06 \n",
      "\n",
      "Epoch:  11498  Learning Rate:  1.0160529510467429e-06  Varinance:  2.6176320283741246e-06 \n",
      "\n",
      "Epoch:  11499  Learning Rate:  1.0150374059528725e-06  Varinance:  2.6139752923779635e-06 \n",
      "\n",
      "Epoch:  11500  Learning Rate:  1.0140228758964906e-06  Varinance:  2.6103236647079536e-06 \n",
      "\n",
      "Epoch:  11501  Learning Rate:  1.0130093598630711e-06  Varinance:  2.606677138227951e-06 \n",
      "\n",
      "Epoch:  11502  Learning Rate:  1.0119968568390959e-06  Varinance:  2.6030357058117804e-06 \n",
      "\n",
      "Epoch:  11503  Learning Rate:  1.01098536581206e-06  Varinance:  2.5993993603432295e-06 \n",
      "\n",
      "Epoch:  11504  Learning Rate:  1.009974885770476e-06  Varinance:  2.595768094716002e-06 \n",
      "\n",
      "Epoch:  11505  Learning Rate:  1.0089654157038617e-06  Varinance:  2.5921419018337536e-06 \n",
      "\n",
      "Epoch:  11506  Learning Rate:  1.0079569546027457e-06  Varinance:  2.5885207746100463e-06 \n",
      "\n",
      "Epoch:  11507  Learning Rate:  1.00694950145867e-06  Varinance:  2.584904705968341e-06 \n",
      "\n",
      "Epoch:  11508  Learning Rate:  1.0059430552641796e-06  Varinance:  2.581293688841983e-06 \n",
      "\n",
      "Epoch:  11509  Learning Rate:  1.0049376150128284e-06  Varinance:  2.5776877161741917e-06 \n",
      "\n",
      "Epoch:  11510  Learning Rate:  1.003933179699174e-06  Varinance:  2.574086780918052e-06 \n",
      "\n",
      "Epoch:  11511  Learning Rate:  1.002929748318785e-06  Varinance:  2.570490876036466e-06 \n",
      "\n",
      "Epoch:  11512  Learning Rate:  1.0019273198682276e-06  Varinance:  2.5668999945021945e-06 \n",
      "\n",
      "Epoch:  11513  Learning Rate:  1.000925893345072e-06  Varinance:  2.563314129297805e-06 \n",
      "\n",
      "Epoch:  11514  Learning Rate:  9.999254677478948e-07  Varinance:  2.5597332734156684e-06 \n",
      "\n",
      "Epoch:  11515  Learning Rate:  9.989260420762689e-07  Varinance:  2.556157419857946e-06 \n",
      "\n",
      "Epoch:  11516  Learning Rate:  9.979276153307663e-07  Varinance:  2.552586561636573e-06 \n",
      "\n",
      "Epoch:  11517  Learning Rate:  9.969301865129642e-07  Varinance:  2.5490206917732567e-06 \n",
      "\n",
      "Epoch:  11518  Learning Rate:  9.959337546254315e-07  Varinance:  2.545459803299426e-06 \n",
      "\n",
      "Epoch:  11519  Learning Rate:  9.949383186717347e-07  Varinance:  2.5419038892562713e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11520  Learning Rate:  9.939438776564412e-07  Varinance:  2.5383529426946957e-06 \n",
      "\n",
      "Epoch:  11521  Learning Rate:  9.929504305851082e-07  Varinance:  2.534806956675309e-06 \n",
      "\n",
      "Epoch:  11522  Learning Rate:  9.919579764642867e-07  Varinance:  2.5312659242684157e-06 \n",
      "\n",
      "Epoch:  11523  Learning Rate:  9.909665143015264e-07  Varinance:  2.5277298385540006e-06 \n",
      "\n",
      "Epoch:  11524  Learning Rate:  9.899760431053625e-07  Varinance:  2.524198692621725e-06 \n",
      "\n",
      "Epoch:  11525  Learning Rate:  9.889865618853246e-07  Varinance:  2.5206724795708757e-06 \n",
      "\n",
      "Epoch:  11526  Learning Rate:  9.87998069651929e-07  Varinance:  2.5171511925104083e-06 \n",
      "\n",
      "Epoch:  11527  Learning Rate:  9.870105654166874e-07  Varinance:  2.5136348245588936e-06 \n",
      "\n",
      "Epoch:  11528  Learning Rate:  9.860240481920933e-07  Varinance:  2.5101233688445172e-06 \n",
      "\n",
      "Epoch:  11529  Learning Rate:  9.850385169916279e-07  Varinance:  2.506616818505064e-06 \n",
      "\n",
      "Epoch:  11530  Learning Rate:  9.840539708297631e-07  Varinance:  2.5031151666879042e-06 \n",
      "\n",
      "Epoch:  11531  Learning Rate:  9.830704087219516e-07  Varinance:  2.499618406549992e-06 \n",
      "\n",
      "Epoch:  11532  Learning Rate:  9.820878296846287e-07  Varinance:  2.4961265312578118e-06 \n",
      "\n",
      "Epoch:  11533  Learning Rate:  9.811062327352188e-07  Varinance:  2.492639533987423e-06 \n",
      "\n",
      "Epoch:  11534  Learning Rate:  9.801256168921236e-07  Varinance:  2.489157407924407e-06 \n",
      "\n",
      "Epoch:  11535  Learning Rate:  9.791459811747254e-07  Varinance:  2.4856801462638664e-06 \n",
      "\n",
      "Epoch:  11536  Learning Rate:  9.781673246033916e-07  Varinance:  2.482207742210409e-06 \n",
      "\n",
      "Epoch:  11537  Learning Rate:  9.771896461994638e-07  Varinance:  2.4787401889781356e-06 \n",
      "\n",
      "Epoch:  11538  Learning Rate:  9.762129449852622e-07  Varinance:  2.4752774797906358e-06 \n",
      "\n",
      "Epoch:  11539  Learning Rate:  9.752372199840884e-07  Varinance:  2.4718196078809387e-06 \n",
      "\n",
      "Epoch:  11540  Learning Rate:  9.74262470220216e-07  Varinance:  2.4683665664915536e-06 \n",
      "\n",
      "Epoch:  11541  Learning Rate:  9.732886947188948e-07  Varinance:  2.464918348874421e-06 \n",
      "\n",
      "Epoch:  11542  Learning Rate:  9.72315892506348e-07  Varinance:  2.4614749482909074e-06 \n",
      "\n",
      "Epoch:  11543  Learning Rate:  9.713440626097761e-07  Varinance:  2.4580363580117932e-06 \n",
      "\n",
      "Epoch:  11544  Learning Rate:  9.70373204057348e-07  Varinance:  2.4546025713172603e-06 \n",
      "\n",
      "Epoch:  11545  Learning Rate:  9.694033158782032e-07  Varinance:  2.451173581496885e-06 \n",
      "\n",
      "Epoch:  11546  Learning Rate:  9.684343971024565e-07  Varinance:  2.447749381849593e-06 \n",
      "\n",
      "Epoch:  11547  Learning Rate:  9.674664467611877e-07  Varinance:  2.444329965683696e-06 \n",
      "\n",
      "Epoch:  11548  Learning Rate:  9.664994638864446e-07  Varinance:  2.440915326316847e-06 \n",
      "\n",
      "Epoch:  11549  Learning Rate:  9.65533447511248e-07  Varinance:  2.4375054570760324e-06 \n",
      "\n",
      "Epoch:  11550  Learning Rate:  9.645683966695787e-07  Varinance:  2.434100351297561e-06 \n",
      "\n",
      "Epoch:  11551  Learning Rate:  9.63604310396385e-07  Varinance:  2.4307000023270497e-06 \n",
      "\n",
      "Epoch:  11552  Learning Rate:  9.626411877275838e-07  Varinance:  2.427304403519422e-06 \n",
      "\n",
      "Epoch:  11553  Learning Rate:  9.616790277000506e-07  Varinance:  2.423913548238856e-06 \n",
      "\n",
      "Epoch:  11554  Learning Rate:  9.607178293516237e-07  Varinance:  2.4205274298588277e-06 \n",
      "\n",
      "Epoch:  11555  Learning Rate:  9.597575917211074e-07  Varinance:  2.417146041762061e-06 \n",
      "\n",
      "Epoch:  11556  Learning Rate:  9.58798313848263e-07  Varinance:  2.4137693773405223e-06 \n",
      "\n",
      "Epoch:  11557  Learning Rate:  9.578399947738124e-07  Varinance:  2.4103974299954117e-06 \n",
      "\n",
      "Epoch:  11558  Learning Rate:  9.568826335394347e-07  Varinance:  2.407030193137146e-06 \n",
      "\n",
      "Epoch:  11559  Learning Rate:  9.559262291877722e-07  Varinance:  2.4036676601853553e-06 \n",
      "\n",
      "Epoch:  11560  Learning Rate:  9.549707807624182e-07  Varinance:  2.400309824568839e-06 \n",
      "\n",
      "Epoch:  11561  Learning Rate:  9.540162873079232e-07  Varinance:  2.3969566797256e-06 \n",
      "\n",
      "Epoch:  11562  Learning Rate:  9.530627478697964e-07  Varinance:  2.3936082191027994e-06 \n",
      "\n",
      "Epoch:  11563  Learning Rate:  9.521101614944969e-07  Varinance:  2.390264436156754e-06 \n",
      "\n",
      "Epoch:  11564  Learning Rate:  9.511585272294367e-07  Varinance:  2.3869253243529196e-06 \n",
      "\n",
      "Epoch:  11565  Learning Rate:  9.502078441229845e-07  Varinance:  2.3835908771658827e-06 \n",
      "\n",
      "Epoch:  11566  Learning Rate:  9.492581112244557e-07  Varinance:  2.3802610880793524e-06 \n",
      "\n",
      "Epoch:  11567  Learning Rate:  9.483093275841157e-07  Varinance:  2.3769359505861165e-06 \n",
      "\n",
      "Epoch:  11568  Learning Rate:  9.473614922531838e-07  Varinance:  2.373615458188078e-06 \n",
      "\n",
      "Epoch:  11569  Learning Rate:  9.464146042838232e-07  Varinance:  2.3702996043962096e-06 \n",
      "\n",
      "Epoch:  11570  Learning Rate:  9.45468662729144e-07  Varinance:  2.3669883827305477e-06 \n",
      "\n",
      "Epoch:  11571  Learning Rate:  9.44523666643208e-07  Varinance:  2.3636817867201823e-06 \n",
      "\n",
      "Epoch:  11572  Learning Rate:  9.435796150810174e-07  Varinance:  2.3603798099032423e-06 \n",
      "\n",
      "Epoch:  11573  Learning Rate:  9.426365070985205e-07  Varinance:  2.3570824458268926e-06 \n",
      "\n",
      "Epoch:  11574  Learning Rate:  9.416943417526074e-07  Varinance:  2.3537896880472866e-06 \n",
      "\n",
      "Epoch:  11575  Learning Rate:  9.407531181011164e-07  Varinance:  2.3505015301296045e-06 \n",
      "\n",
      "Epoch:  11576  Learning Rate:  9.398128352028219e-07  Varinance:  2.347217965648009e-06 \n",
      "\n",
      "Epoch:  11577  Learning Rate:  9.388734921174392e-07  Varinance:  2.343938988185637e-06 \n",
      "\n",
      "Epoch:  11578  Learning Rate:  9.379350879056285e-07  Varinance:  2.3406645913345914e-06 \n",
      "\n",
      "Epoch:  11579  Learning Rate:  9.369976216289839e-07  Varinance:  2.3373947686959264e-06 \n",
      "\n",
      "Epoch:  11580  Learning Rate:  9.360610923500373e-07  Varinance:  2.334129513879642e-06 \n",
      "\n",
      "Epoch:  11581  Learning Rate:  9.351254991322629e-07  Varinance:  2.3308688205046427e-06 \n",
      "\n",
      "Epoch:  11582  Learning Rate:  9.341908410400655e-07  Varinance:  2.3276126821987694e-06 \n",
      "\n",
      "Epoch:  11583  Learning Rate:  9.332571171387851e-07  Varinance:  2.3243610925987583e-06 \n",
      "\n",
      "Epoch:  11584  Learning Rate:  9.323243264947015e-07  Varinance:  2.3211140453502334e-06 \n",
      "\n",
      "Epoch:  11585  Learning Rate:  9.313924681750219e-07  Varinance:  2.3178715341076966e-06 \n",
      "\n",
      "Epoch:  11586  Learning Rate:  9.304615412478866e-07  Varinance:  2.3146335525345134e-06 \n",
      "\n",
      "Epoch:  11587  Learning Rate:  9.295315447823718e-07  Varinance:  2.3114000943029106e-06 \n",
      "\n",
      "Epoch:  11588  Learning Rate:  9.28602477848479e-07  Varinance:  2.3081711530939284e-06 \n",
      "\n",
      "Epoch:  11589  Learning Rate:  9.276743395171416e-07  Varinance:  2.304946722597461e-06 \n",
      "\n",
      "Epoch:  11590  Learning Rate:  9.267471288602194e-07  Varinance:  2.3017267965122077e-06 \n",
      "\n",
      "Epoch:  11591  Learning Rate:  9.258208449505048e-07  Varinance:  2.2985113685456716e-06 \n",
      "\n",
      "Epoch:  11592  Learning Rate:  9.248954868617124e-07  Varinance:  2.295300432414145e-06 \n",
      "\n",
      "Epoch:  11593  Learning Rate:  9.239710536684822e-07  Varinance:  2.2920939818427e-06 \n",
      "\n",
      "Epoch:  11594  Learning Rate:  9.230475444463845e-07  Varinance:  2.288892010565181e-06 \n",
      "\n",
      "Epoch:  11595  Learning Rate:  9.221249582719081e-07  Varinance:  2.285694512324163e-06 \n",
      "\n",
      "Epoch:  11596  Learning Rate:  9.21203294222465e-07  Varinance:  2.282501480870985e-06 \n",
      "\n",
      "Epoch:  11597  Learning Rate:  9.202825513763946e-07  Varinance:  2.279312909965709e-06 \n",
      "\n",
      "Epoch:  11598  Learning Rate:  9.193627288129525e-07  Varinance:  2.276128793377113e-06 \n",
      "\n",
      "Epoch:  11599  Learning Rate:  9.18443825612314e-07  Varinance:  2.2729491248826793e-06 \n",
      "\n",
      "Epoch:  11600  Learning Rate:  9.175258408555792e-07  Varinance:  2.2697738982685842e-06 \n",
      "\n",
      "Epoch:  11601  Learning Rate:  9.166087736247618e-07  Varinance:  2.266603107329691e-06 \n",
      "\n",
      "Epoch:  11602  Learning Rate:  9.156926230027928e-07  Varinance:  2.263436745869508e-06 \n",
      "\n",
      "Epoch:  11603  Learning Rate:  9.147773880735247e-07  Varinance:  2.260274807700225e-06 \n",
      "\n",
      "Epoch:  11604  Learning Rate:  9.138630679217209e-07  Varinance:  2.2571172866426653e-06 \n",
      "\n",
      "Epoch:  11605  Learning Rate:  9.129496616330612e-07  Varinance:  2.253964176526286e-06 \n",
      "\n",
      "Epoch:  11606  Learning Rate:  9.120371682941378e-07  Varinance:  2.250815471189165e-06 \n",
      "\n",
      "Epoch:  11607  Learning Rate:  9.111255869924599e-07  Varinance:  2.2476711644779855e-06 \n",
      "\n",
      "Epoch:  11608  Learning Rate:  9.102149168164453e-07  Varinance:  2.2445312502480374e-06 \n",
      "\n",
      "Epoch:  11609  Learning Rate:  9.093051568554215e-07  Varinance:  2.2413957223631694e-06 \n",
      "\n",
      "Epoch:  11610  Learning Rate:  9.083963061996321e-07  Varinance:  2.238264574695826e-06 \n",
      "\n",
      "Epoch:  11611  Learning Rate:  9.074883639402245e-07  Varinance:  2.2351378011270035e-06 \n",
      "\n",
      "Epoch:  11612  Learning Rate:  9.065813291692548e-07  Varinance:  2.2320153955462473e-06 \n",
      "\n",
      "Epoch:  11613  Learning Rate:  9.056752009796915e-07  Varinance:  2.2288973518516376e-06 \n",
      "\n",
      "Epoch:  11614  Learning Rate:  9.047699784654047e-07  Varinance:  2.22578366394978e-06 \n",
      "\n",
      "Epoch:  11615  Learning Rate:  9.038656607211701e-07  Varinance:  2.2226743257557984e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11616  Learning Rate:  9.029622468426731e-07  Varinance:  2.219569331193296e-06 \n",
      "\n",
      "Epoch:  11617  Learning Rate:  9.020597359264983e-07  Varinance:  2.2164686741943855e-06 \n",
      "\n",
      "Epoch:  11618  Learning Rate:  9.01158127070133e-07  Varinance:  2.21337234869965e-06 \n",
      "\n",
      "Epoch:  11619  Learning Rate:  9.002574193719714e-07  Varinance:  2.210280348658137e-06 \n",
      "\n",
      "Epoch:  11620  Learning Rate:  8.993576119313041e-07  Varinance:  2.2071926680273476e-06 \n",
      "\n",
      "Epoch:  11621  Learning Rate:  8.98458703848324e-07  Varinance:  2.2041093007732217e-06 \n",
      "\n",
      "Epoch:  11622  Learning Rate:  8.975606942241207e-07  Varinance:  2.2010302408701394e-06 \n",
      "\n",
      "Epoch:  11623  Learning Rate:  8.966635821606881e-07  Varinance:  2.1979554823008733e-06 \n",
      "\n",
      "Epoch:  11624  Learning Rate:  8.957673667609125e-07  Varinance:  2.194885019056625e-06 \n",
      "\n",
      "Epoch:  11625  Learning Rate:  8.948720471285766e-07  Varinance:  2.1918188451369834e-06 \n",
      "\n",
      "Epoch:  11626  Learning Rate:  8.939776223683641e-07  Varinance:  2.188756954549919e-06 \n",
      "\n",
      "Epoch:  11627  Learning Rate:  8.930840915858483e-07  Varinance:  2.185699341311773e-06 \n",
      "\n",
      "Epoch:  11628  Learning Rate:  8.92191453887497e-07  Varinance:  2.1826459994472543e-06 \n",
      "\n",
      "Epoch:  11629  Learning Rate:  8.912997083806756e-07  Varinance:  2.1795969229893937e-06 \n",
      "\n",
      "Epoch:  11630  Learning Rate:  8.904088541736367e-07  Varinance:  2.1765521059795837e-06 \n",
      "\n",
      "Epoch:  11631  Learning Rate:  8.895188903755247e-07  Varinance:  2.1735115424675306e-06 \n",
      "\n",
      "Epoch:  11632  Learning Rate:  8.886298160963788e-07  Varinance:  2.1704752265112538e-06 \n",
      "\n",
      "Epoch:  11633  Learning Rate:  8.877416304471229e-07  Varinance:  2.167443152177074e-06 \n",
      "\n",
      "Epoch:  11634  Learning Rate:  8.868543325395716e-07  Varinance:  2.1644153135396006e-06 \n",
      "\n",
      "Epoch:  11635  Learning Rate:  8.859679214864251e-07  Varinance:  2.1613917046817283e-06 \n",
      "\n",
      "Epoch:  11636  Learning Rate:  8.850823964012756e-07  Varinance:  2.158372319694594e-06 \n",
      "\n",
      "Epoch:  11637  Learning Rate:  8.841977563985962e-07  Varinance:  2.155357152677614e-06 \n",
      "\n",
      "Epoch:  11638  Learning Rate:  8.833140005937452e-07  Varinance:  2.1523461977384373e-06 \n",
      "\n",
      "Epoch:  11639  Learning Rate:  8.824311281029699e-07  Varinance:  2.149339448992946e-06 \n",
      "\n",
      "Epoch:  11640  Learning Rate:  8.815491380433966e-07  Varinance:  2.146336900565242e-06 \n",
      "\n",
      "Epoch:  11641  Learning Rate:  8.806680295330328e-07  Varinance:  2.1433385465876356e-06 \n",
      "\n",
      "Epoch:  11642  Learning Rate:  8.797878016907737e-07  Varinance:  2.1403443812006405e-06 \n",
      "\n",
      "Epoch:  11643  Learning Rate:  8.789084536363896e-07  Varinance:  2.1373543985529347e-06 \n",
      "\n",
      "Epoch:  11644  Learning Rate:  8.78029984490531e-07  Varinance:  2.134368592801392e-06 \n",
      "\n",
      "Epoch:  11645  Learning Rate:  8.771523933747314e-07  Varinance:  2.1313869581110413e-06 \n",
      "\n",
      "Epoch:  11646  Learning Rate:  8.762756794113983e-07  Varinance:  2.1284094886550634e-06 \n",
      "\n",
      "Epoch:  11647  Learning Rate:  8.753998417238162e-07  Varinance:  2.1254361786147784e-06 \n",
      "\n",
      "Epoch:  11648  Learning Rate:  8.745248794361502e-07  Varinance:  2.1224670221796354e-06 \n",
      "\n",
      "Epoch:  11649  Learning Rate:  8.736507916734365e-07  Varinance:  2.119502013547207e-06 \n",
      "\n",
      "Epoch:  11650  Learning Rate:  8.727775775615873e-07  Varinance:  2.1165411469231507e-06 \n",
      "\n",
      "Epoch:  11651  Learning Rate:  8.719052362273868e-07  Varinance:  2.1135844165212393e-06 \n",
      "\n",
      "Epoch:  11652  Learning Rate:  8.710337667984968e-07  Varinance:  2.1106318165633226e-06 \n",
      "\n",
      "Epoch:  11653  Learning Rate:  8.701631684034462e-07  Varinance:  2.1076833412793216e-06 \n",
      "\n",
      "Epoch:  11654  Learning Rate:  8.692934401716349e-07  Varinance:  2.104738984907218e-06 \n",
      "\n",
      "Epoch:  11655  Learning Rate:  8.684245812333378e-07  Varinance:  2.101798741693043e-06 \n",
      "\n",
      "Epoch:  11656  Learning Rate:  8.675565907196944e-07  Varinance:  2.0988626058908734e-06 \n",
      "\n",
      "Epoch:  11657  Learning Rate:  8.666894677627122e-07  Varinance:  2.0959305717627898e-06 \n",
      "\n",
      "Epoch:  11658  Learning Rate:  8.658232114952719e-07  Varinance:  2.093002633578911e-06 \n",
      "\n",
      "Epoch:  11659  Learning Rate:  8.64957821051115e-07  Varinance:  2.0900787856173537e-06 \n",
      "\n",
      "Epoch:  11660  Learning Rate:  8.640932955648497e-07  Varinance:  2.0871590221642277e-06 \n",
      "\n",
      "Epoch:  11661  Learning Rate:  8.632296341719537e-07  Varinance:  2.084243337513623e-06 \n",
      "\n",
      "Epoch:  11662  Learning Rate:  8.623668360087636e-07  Varinance:  2.0813317259676035e-06 \n",
      "\n",
      "Epoch:  11663  Learning Rate:  8.615049002124798e-07  Varinance:  2.0784241818361975e-06 \n",
      "\n",
      "Epoch:  11664  Learning Rate:  8.606438259211696e-07  Varinance:  2.0755206994373617e-06 \n",
      "\n",
      "Epoch:  11665  Learning Rate:  8.597836122737573e-07  Varinance:  2.0726212730970126e-06 \n",
      "\n",
      "Epoch:  11666  Learning Rate:  8.589242584100286e-07  Varinance:  2.069725897148984e-06 \n",
      "\n",
      "Epoch:  11667  Learning Rate:  8.580657634706285e-07  Varinance:  2.066834565935027e-06 \n",
      "\n",
      "Epoch:  11668  Learning Rate:  8.572081265970648e-07  Varinance:  2.0639472738047958e-06 \n",
      "\n",
      "Epoch:  11669  Learning Rate:  8.563513469316993e-07  Varinance:  2.0610640151158394e-06 \n",
      "\n",
      "Epoch:  11670  Learning Rate:  8.554954236177504e-07  Varinance:  2.0581847842335947e-06 \n",
      "\n",
      "Epoch:  11671  Learning Rate:  8.54640355799298e-07  Varinance:  2.055309575531349e-06 \n",
      "\n",
      "Epoch:  11672  Learning Rate:  8.537861426212725e-07  Varinance:  2.0524383833902713e-06 \n",
      "\n",
      "Epoch:  11673  Learning Rate:  8.529327832294595e-07  Varinance:  2.0495712021993735e-06 \n",
      "\n",
      "Epoch:  11674  Learning Rate:  8.520802767705022e-07  Varinance:  2.0467080263555047e-06 \n",
      "\n",
      "Epoch:  11675  Learning Rate:  8.512286223918927e-07  Varinance:  2.0438488502633423e-06 \n",
      "\n",
      "Epoch:  11676  Learning Rate:  8.503778192419749e-07  Varinance:  2.0409936683353792e-06 \n",
      "\n",
      "Epoch:  11677  Learning Rate:  8.495278664699489e-07  Varinance:  2.0381424749919223e-06 \n",
      "\n",
      "Epoch:  11678  Learning Rate:  8.4867876322586e-07  Varinance:  2.03529526466105e-06 \n",
      "\n",
      "Epoch:  11679  Learning Rate:  8.478305086606036e-07  Varinance:  2.0324520317786475e-06 \n",
      "\n",
      "Epoch:  11680  Learning Rate:  8.469831019259281e-07  Varinance:  2.0296127707883653e-06 \n",
      "\n",
      "Epoch:  11681  Learning Rate:  8.46136542174425e-07  Varinance:  2.0267774761416153e-06 \n",
      "\n",
      "Epoch:  11682  Learning Rate:  8.452908285595347e-07  Varinance:  2.0239461422975615e-06 \n",
      "\n",
      "Epoch:  11683  Learning Rate:  8.444459602355417e-07  Varinance:  2.0211187637231065e-06 \n",
      "\n",
      "Epoch:  11684  Learning Rate:  8.43601936357581e-07  Varinance:  2.018295334892892e-06 \n",
      "\n",
      "Epoch:  11685  Learning Rate:  8.42758756081627e-07  Varinance:  2.0154758502892553e-06 \n",
      "\n",
      "Epoch:  11686  Learning Rate:  8.419164185644976e-07  Varinance:  2.012660304402264e-06 \n",
      "\n",
      "Epoch:  11687  Learning Rate:  8.410749229638584e-07  Varinance:  2.0098486917296743e-06 \n",
      "\n",
      "Epoch:  11688  Learning Rate:  8.402342684382124e-07  Varinance:  2.0070410067769307e-06 \n",
      "\n",
      "Epoch:  11689  Learning Rate:  8.393944541469034e-07  Varinance:  2.0042372440571525e-06 \n",
      "\n",
      "Epoch:  11690  Learning Rate:  8.385554792501199e-07  Varinance:  2.0014373980911237e-06 \n",
      "\n",
      "Epoch:  11691  Learning Rate:  8.377173429088856e-07  Varinance:  1.9986414634072903e-06 \n",
      "\n",
      "Epoch:  11692  Learning Rate:  8.368800442850625e-07  Varinance:  1.99584943454172e-06 \n",
      "\n",
      "Epoch:  11693  Learning Rate:  8.360435825413549e-07  Varinance:  1.993061306038135e-06 \n",
      "\n",
      "Epoch:  11694  Learning Rate:  8.352079568412995e-07  Varinance:  1.9902770724478726e-06 \n",
      "\n",
      "Epoch:  11695  Learning Rate:  8.343731663492691e-07  Varinance:  1.9874967283298817e-06 \n",
      "\n",
      "Epoch:  11696  Learning Rate:  8.33539210230476e-07  Varinance:  1.984720268250712e-06 \n",
      "\n",
      "Epoch:  11697  Learning Rate:  8.327060876509625e-07  Varinance:  1.9819476867845037e-06 \n",
      "\n",
      "Epoch:  11698  Learning Rate:  8.318737977776063e-07  Varinance:  1.9791789785129837e-06 \n",
      "\n",
      "Epoch:  11699  Learning Rate:  8.310423397781155e-07  Varinance:  1.9764141380254267e-06 \n",
      "\n",
      "Epoch:  11700  Learning Rate:  8.302117128210352e-07  Varinance:  1.973653159918688e-06 \n",
      "\n",
      "Epoch:  11701  Learning Rate:  8.293819160757371e-07  Varinance:  1.9708960387971625e-06 \n",
      "\n",
      "Epoch:  11702  Learning Rate:  8.285529487124227e-07  Varinance:  1.9681427692727834e-06 \n",
      "\n",
      "Epoch:  11703  Learning Rate:  8.277248099021275e-07  Varinance:  1.9653933459650104e-06 \n",
      "\n",
      "Epoch:  11704  Learning Rate:  8.268974988167111e-07  Varinance:  1.9626477635008207e-06 \n",
      "\n",
      "Epoch:  11705  Learning Rate:  8.260710146288609e-07  Varinance:  1.9599060165147024e-06 \n",
      "\n",
      "Epoch:  11706  Learning Rate:  8.252453565120957e-07  Varinance:  1.9571680996486206e-06 \n",
      "\n",
      "Epoch:  11707  Learning Rate:  8.244205236407559e-07  Varinance:  1.9544340075520437e-06 \n",
      "\n",
      "Epoch:  11708  Learning Rate:  8.235965151900069e-07  Varinance:  1.9517037348819096e-06 \n",
      "\n",
      "Epoch:  11709  Learning Rate:  8.227733303358431e-07  Varinance:  1.9489772763026195e-06 \n",
      "\n",
      "Epoch:  11710  Learning Rate:  8.219509682550782e-07  Varinance:  1.9462546264860286e-06 \n",
      "\n",
      "Epoch:  11711  Learning Rate:  8.211294281253487e-07  Varinance:  1.9435357801114343e-06 \n",
      "\n",
      "Epoch:  11712  Learning Rate:  8.203087091251172e-07  Varinance:  1.9408207318655745e-06 \n",
      "\n",
      "Epoch:  11713  Learning Rate:  8.194888104336632e-07  Varinance:  1.9381094764425893e-06 \n",
      "\n",
      "Epoch:  11714  Learning Rate:  8.186697312310878e-07  Varinance:  1.935402008544051e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11715  Learning Rate:  8.178514706983106e-07  Varinance:  1.9326983228789265e-06 \n",
      "\n",
      "Epoch:  11716  Learning Rate:  8.170340280170735e-07  Varinance:  1.9299984141635745e-06 \n",
      "\n",
      "Epoch:  11717  Learning Rate:  8.162174023699325e-07  Varinance:  1.9273022771217346e-06 \n",
      "\n",
      "Epoch:  11718  Learning Rate:  8.154015929402605e-07  Varinance:  1.924609906484517e-06 \n",
      "\n",
      "Epoch:  11719  Learning Rate:  8.14586598912251e-07  Varinance:  1.9219212969903996e-06 \n",
      "\n",
      "Epoch:  11720  Learning Rate:  8.137724194709081e-07  Varinance:  1.9192364433851894e-06 \n",
      "\n",
      "Epoch:  11721  Learning Rate:  8.12959053802051e-07  Varinance:  1.9165553404220543e-06 \n",
      "\n",
      "Epoch:  11722  Learning Rate:  8.121465010923171e-07  Varinance:  1.9138779828614846e-06 \n",
      "\n",
      "Epoch:  11723  Learning Rate:  8.113347605291519e-07  Varinance:  1.9112043654712905e-06 \n",
      "\n",
      "Epoch:  11724  Learning Rate:  8.105238313008133e-07  Varinance:  1.908534483026591e-06 \n",
      "\n",
      "Epoch:  11725  Learning Rate:  8.097137125963751e-07  Varinance:  1.9058683303098041e-06 \n",
      "\n",
      "Epoch:  11726  Learning Rate:  8.089044036057168e-07  Varinance:  1.9032059021106433e-06 \n",
      "\n",
      "Epoch:  11727  Learning Rate:  8.080959035195282e-07  Varinance:  1.9005471932260801e-06 \n",
      "\n",
      "Epoch:  11728  Learning Rate:  8.07288211529312e-07  Varinance:  1.897892198460376e-06 \n",
      "\n",
      "Epoch:  11729  Learning Rate:  8.064813268273744e-07  Varinance:  1.8952409126250424e-06 \n",
      "\n",
      "Epoch:  11730  Learning Rate:  8.05675248606831e-07  Varinance:  1.89259333053884e-06 \n",
      "\n",
      "Epoch:  11731  Learning Rate:  8.048699760616019e-07  Varinance:  1.8899494470277664e-06 \n",
      "\n",
      "Epoch:  11732  Learning Rate:  8.040655083864173e-07  Varinance:  1.8873092569250483e-06 \n",
      "\n",
      "Epoch:  11733  Learning Rate:  8.032618447768081e-07  Varinance:  1.884672755071136e-06 \n",
      "\n",
      "Epoch:  11734  Learning Rate:  8.024589844291091e-07  Varinance:  1.8820399363136677e-06 \n",
      "\n",
      "Epoch:  11735  Learning Rate:  8.016569265404629e-07  Varinance:  1.8794107955074994e-06 \n",
      "\n",
      "Epoch:  11736  Learning Rate:  8.008556703088101e-07  Varinance:  1.876785327514668e-06 \n",
      "\n",
      "Epoch:  11737  Learning Rate:  8.000552149328929e-07  Varinance:  1.8741635272043881e-06 \n",
      "\n",
      "Epoch:  11738  Learning Rate:  7.992555596122587e-07  Varinance:  1.8715453894530413e-06 \n",
      "\n",
      "Epoch:  11739  Learning Rate:  7.984567035472509e-07  Varinance:  1.868930909144167e-06 \n",
      "\n",
      "Epoch:  11740  Learning Rate:  7.976586459390114e-07  Varinance:  1.8663200811684589e-06 \n",
      "\n",
      "Epoch:  11741  Learning Rate:  7.968613859894859e-07  Varinance:  1.8637129004237285e-06 \n",
      "\n",
      "Epoch:  11742  Learning Rate:  7.960649229014128e-07  Varinance:  1.8611093618149336e-06 \n",
      "\n",
      "Epoch:  11743  Learning Rate:  7.952692558783276e-07  Varinance:  1.8585094602541445e-06 \n",
      "\n",
      "Epoch:  11744  Learning Rate:  7.94474384124566e-07  Varinance:  1.8559131906605384e-06 \n",
      "\n",
      "Epoch:  11745  Learning Rate:  7.936803068452547e-07  Varinance:  1.8533205479603899e-06 \n",
      "\n",
      "Epoch:  11746  Learning Rate:  7.928870232463162e-07  Varinance:  1.8507315270870621e-06 \n",
      "\n",
      "Epoch:  11747  Learning Rate:  7.920945325344656e-07  Varinance:  1.8481461229810022e-06 \n",
      "\n",
      "Epoch:  11748  Learning Rate:  7.913028339172152e-07  Varinance:  1.8455643305897053e-06 \n",
      "\n",
      "Epoch:  11749  Learning Rate:  7.905119266028644e-07  Varinance:  1.8429861448677458e-06 \n",
      "\n",
      "Epoch:  11750  Learning Rate:  7.897218098005051e-07  Varinance:  1.8404115607767379e-06 \n",
      "\n",
      "Epoch:  11751  Learning Rate:  7.889324827200223e-07  Varinance:  1.8378405732853357e-06 \n",
      "\n",
      "Epoch:  11752  Learning Rate:  7.881439445720883e-07  Varinance:  1.8352731773692213e-06 \n",
      "\n",
      "Epoch:  11753  Learning Rate:  7.87356194568163e-07  Varinance:  1.8327093680110952e-06 \n",
      "\n",
      "Epoch:  11754  Learning Rate:  7.865692319204993e-07  Varinance:  1.8301491402006748e-06 \n",
      "\n",
      "Epoch:  11755  Learning Rate:  7.857830558421331e-07  Varinance:  1.8275924889346555e-06 \n",
      "\n",
      "Epoch:  11756  Learning Rate:  7.849976655468868e-07  Varinance:  1.8250394092167426e-06 \n",
      "\n",
      "Epoch:  11757  Learning Rate:  7.842130602493728e-07  Varinance:  1.8224898960576142e-06 \n",
      "\n",
      "Epoch:  11758  Learning Rate:  7.834292391649846e-07  Varinance:  1.8199439444749186e-06 \n",
      "\n",
      "Epoch:  11759  Learning Rate:  7.826462015099008e-07  Varinance:  1.8174015494932637e-06 \n",
      "\n",
      "Epoch:  11760  Learning Rate:  7.818639465010822e-07  Varinance:  1.8148627061442148e-06 \n",
      "\n",
      "Epoch:  11761  Learning Rate:  7.810824733562767e-07  Varinance:  1.812327409466259e-06 \n",
      "\n",
      "Epoch:  11762  Learning Rate:  7.803017812940097e-07  Varinance:  1.8097956545048324e-06 \n",
      "\n",
      "Epoch:  11763  Learning Rate:  7.795218695335876e-07  Varinance:  1.8072674363122874e-06 \n",
      "\n",
      "Epoch:  11764  Learning Rate:  7.787427372951013e-07  Varinance:  1.804742749947887e-06 \n",
      "\n",
      "Epoch:  11765  Learning Rate:  7.779643837994173e-07  Varinance:  1.8022215904777976e-06 \n",
      "\n",
      "Epoch:  11766  Learning Rate:  7.771868082681805e-07  Varinance:  1.7997039529750762e-06 \n",
      "\n",
      "Epoch:  11767  Learning Rate:  7.764100099238182e-07  Varinance:  1.7971898325196705e-06 \n",
      "\n",
      "Epoch:  11768  Learning Rate:  7.756339879895304e-07  Varinance:  1.7946792241983808e-06 \n",
      "\n",
      "Epoch:  11769  Learning Rate:  7.74858741689294e-07  Varinance:  1.7921721231048916e-06 \n",
      "\n",
      "Epoch:  11770  Learning Rate:  7.740842702478651e-07  Varinance:  1.7896685243397343e-06 \n",
      "\n",
      "Epoch:  11771  Learning Rate:  7.733105728907709e-07  Varinance:  1.7871684230102838e-06 \n",
      "\n",
      "Epoch:  11772  Learning Rate:  7.725376488443128e-07  Varinance:  1.7846718142307511e-06 \n",
      "\n",
      "Epoch:  11773  Learning Rate:  7.717654973355692e-07  Varinance:  1.7821786931221722e-06 \n",
      "\n",
      "Epoch:  11774  Learning Rate:  7.709941175923873e-07  Varinance:  1.7796890548124045e-06 \n",
      "\n",
      "Epoch:  11775  Learning Rate:  7.702235088433872e-07  Varinance:  1.7772028944360935e-06 \n",
      "\n",
      "Epoch:  11776  Learning Rate:  7.694536703179588e-07  Varinance:  1.7747202071346997e-06 \n",
      "\n",
      "Epoch:  11777  Learning Rate:  7.686846012462662e-07  Varinance:  1.7722409880564645e-06 \n",
      "\n",
      "Epoch:  11778  Learning Rate:  7.679163008592388e-07  Varinance:  1.7697652323564076e-06 \n",
      "\n",
      "Epoch:  11779  Learning Rate:  7.671487683885751e-07  Varinance:  1.7672929351963167e-06 \n",
      "\n",
      "Epoch:  11780  Learning Rate:  7.663820030667451e-07  Varinance:  1.7648240917447383e-06 \n",
      "\n",
      "Epoch:  11781  Learning Rate:  7.656160041269817e-07  Varinance:  1.7623586971769738e-06 \n",
      "\n",
      "Epoch:  11782  Learning Rate:  7.648507708032851e-07  Varinance:  1.759896746675047e-06 \n",
      "\n",
      "Epoch:  11783  Learning Rate:  7.640863023304244e-07  Varinance:  1.7574382354277307e-06 \n",
      "\n",
      "Epoch:  11784  Learning Rate:  7.633225979439297e-07  Varinance:  1.7549831586305117e-06 \n",
      "\n",
      "Epoch:  11785  Learning Rate:  7.625596568800952e-07  Varinance:  1.7525315114855892e-06 \n",
      "\n",
      "Epoch:  11786  Learning Rate:  7.617974783759825e-07  Varinance:  1.7500832892018647e-06 \n",
      "\n",
      "Epoch:  11787  Learning Rate:  7.610360616694116e-07  Varinance:  1.7476384869949328e-06 \n",
      "\n",
      "Epoch:  11788  Learning Rate:  7.602754059989645e-07  Varinance:  1.7451971000870776e-06 \n",
      "\n",
      "Epoch:  11789  Learning Rate:  7.59515510603988e-07  Varinance:  1.7427591237072394e-06 \n",
      "\n",
      "Epoch:  11790  Learning Rate:  7.587563747245854e-07  Varinance:  1.7403245530910411e-06 \n",
      "\n",
      "Epoch:  11791  Learning Rate:  7.579979976016208e-07  Varinance:  1.7378933834807564e-06 \n",
      "\n",
      "Epoch:  11792  Learning Rate:  7.572403784767157e-07  Varinance:  1.7354656101253043e-06 \n",
      "\n",
      "Epoch:  11793  Learning Rate:  7.564835165922535e-07  Varinance:  1.7330412282802413e-06 \n",
      "\n",
      "Epoch:  11794  Learning Rate:  7.557274111913708e-07  Varinance:  1.7306202332077522e-06 \n",
      "\n",
      "Epoch:  11795  Learning Rate:  7.54972061517961e-07  Varinance:  1.7282026201766451e-06 \n",
      "\n",
      "Epoch:  11796  Learning Rate:  7.54217466816677e-07  Varinance:  1.7257883844623208e-06 \n",
      "\n",
      "Epoch:  11797  Learning Rate:  7.534636263329226e-07  Varinance:  1.7233775213467975e-06 \n",
      "\n",
      "Epoch:  11798  Learning Rate:  7.527105393128561e-07  Varinance:  1.7209700261186776e-06 \n",
      "\n",
      "Epoch:  11799  Learning Rate:  7.51958205003393e-07  Varinance:  1.7185658940731467e-06 \n",
      "\n",
      "Epoch:  11800  Learning Rate:  7.512066226521974e-07  Varinance:  1.7161651205119612e-06 \n",
      "\n",
      "Epoch:  11801  Learning Rate:  7.504557915076858e-07  Varinance:  1.7137677007434422e-06 \n",
      "\n",
      "Epoch:  11802  Learning Rate:  7.497057108190296e-07  Varinance:  1.7113736300824702e-06 \n",
      "\n",
      "Epoch:  11803  Learning Rate:  7.489563798361467e-07  Varinance:  1.7089829038504524e-06 \n",
      "\n",
      "Epoch:  11804  Learning Rate:  7.482077978097047e-07  Varinance:  1.7065955173753504e-06 \n",
      "\n",
      "Epoch:  11805  Learning Rate:  7.474599639911241e-07  Varinance:  1.7042114659916461e-06 \n",
      "\n",
      "Epoch:  11806  Learning Rate:  7.467128776325699e-07  Varinance:  1.7018307450403393e-06 \n",
      "\n",
      "Epoch:  11807  Learning Rate:  7.459665379869556e-07  Varinance:  1.6994533498689375e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11808  Learning Rate:  7.4522094430794e-07  Varinance:  1.6970792758314485e-06 \n",
      "\n",
      "Epoch:  11809  Learning Rate:  7.444760958499322e-07  Varinance:  1.694708518288376e-06 \n",
      "\n",
      "Epoch:  11810  Learning Rate:  7.437319918680822e-07  Varinance:  1.6923410726066865e-06 \n",
      "\n",
      "Epoch:  11811  Learning Rate:  7.429886316182848e-07  Varinance:  1.6899769341598373e-06 \n",
      "\n",
      "Epoch:  11812  Learning Rate:  7.422460143571823e-07  Varinance:  1.6876160983277426e-06 \n",
      "\n",
      "Epoch:  11813  Learning Rate:  7.415041393421559e-07  Varinance:  1.6852585604967704e-06 \n",
      "\n",
      "Epoch:  11814  Learning Rate:  7.407630058313295e-07  Varinance:  1.6829043160597342e-06 \n",
      "\n",
      "Epoch:  11815  Learning Rate:  7.400226130835719e-07  Varinance:  1.6805533604158835e-06 \n",
      "\n",
      "Epoch:  11816  Learning Rate:  7.392829603584889e-07  Varinance:  1.6782056889709005e-06 \n",
      "\n",
      "Epoch:  11817  Learning Rate:  7.385440469164267e-07  Varinance:  1.6758612971368678e-06 \n",
      "\n",
      "Epoch:  11818  Learning Rate:  7.378058720184742e-07  Varinance:  1.6735201803322951e-06 \n",
      "\n",
      "Epoch:  11819  Learning Rate:  7.370684349264552e-07  Varinance:  1.6711823339820868e-06 \n",
      "\n",
      "Epoch:  11820  Learning Rate:  7.363317349029314e-07  Varinance:  1.6688477535175375e-06 \n",
      "\n",
      "Epoch:  11821  Learning Rate:  7.355957712112049e-07  Varinance:  1.6665164343763246e-06 \n",
      "\n",
      "Epoch:  11822  Learning Rate:  7.348605431153111e-07  Varinance:  1.664188372002499e-06 \n",
      "\n",
      "Epoch:  11823  Learning Rate:  7.341260498800216e-07  Varinance:  1.6618635618464824e-06 \n",
      "\n",
      "Epoch:  11824  Learning Rate:  7.333922907708418e-07  Varinance:  1.6595419993650332e-06 \n",
      "\n",
      "Epoch:  11825  Learning Rate:  7.326592650540153e-07  Varinance:  1.6572236800212755e-06 \n",
      "\n",
      "Epoch:  11826  Learning Rate:  7.319269719965149e-07  Varinance:  1.6549085992846645e-06 \n",
      "\n",
      "Epoch:  11827  Learning Rate:  7.311954108660461e-07  Varinance:  1.6525967526309848e-06 \n",
      "\n",
      "Epoch:  11828  Learning Rate:  7.304645809310504e-07  Varinance:  1.6502881355423414e-06 \n",
      "\n",
      "Epoch:  11829  Learning Rate:  7.297344814606966e-07  Varinance:  1.6479827435071499e-06 \n",
      "\n",
      "Epoch:  11830  Learning Rate:  7.290051117248837e-07  Varinance:  1.645680572020135e-06 \n",
      "\n",
      "Epoch:  11831  Learning Rate:  7.282764709942446e-07  Varinance:  1.6433816165822973e-06 \n",
      "\n",
      "Epoch:  11832  Learning Rate:  7.275485585401372e-07  Varinance:  1.6410858727009393e-06 \n",
      "\n",
      "Epoch:  11833  Learning Rate:  7.268213736346477e-07  Varinance:  1.6387933358896348e-06 \n",
      "\n",
      "Epoch:  11834  Learning Rate:  7.260949155505937e-07  Varinance:  1.6365040016682244e-06 \n",
      "\n",
      "Epoch:  11835  Learning Rate:  7.253691835615157e-07  Varinance:  1.6342178655628073e-06 \n",
      "\n",
      "Epoch:  11836  Learning Rate:  7.246441769416804e-07  Varinance:  1.6319349231057332e-06 \n",
      "\n",
      "Epoch:  11837  Learning Rate:  7.239198949660838e-07  Varinance:  1.6296551698355969e-06 \n",
      "\n",
      "Epoch:  11838  Learning Rate:  7.231963369104424e-07  Varinance:  1.627378601297211e-06 \n",
      "\n",
      "Epoch:  11839  Learning Rate:  7.224735020511981e-07  Varinance:  1.625105213041627e-06 \n",
      "\n",
      "Epoch:  11840  Learning Rate:  7.21751389665515e-07  Varinance:  1.6228350006261067e-06 \n",
      "\n",
      "Epoch:  11841  Learning Rate:  7.210299990312829e-07  Varinance:  1.6205679596141179e-06 \n",
      "\n",
      "Epoch:  11842  Learning Rate:  7.2030932942711e-07  Varinance:  1.6183040855753257e-06 \n",
      "\n",
      "Epoch:  11843  Learning Rate:  7.19589380132325e-07  Varinance:  1.6160433740855856e-06 \n",
      "\n",
      "Epoch:  11844  Learning Rate:  7.188701504269816e-07  Varinance:  1.613785820726937e-06 \n",
      "\n",
      "Epoch:  11845  Learning Rate:  7.181516395918485e-07  Varinance:  1.6115314210875757e-06 \n",
      "\n",
      "Epoch:  11846  Learning Rate:  7.174338469084136e-07  Varinance:  1.6092801707618772e-06 \n",
      "\n",
      "Epoch:  11847  Learning Rate:  7.167167716588866e-07  Varinance:  1.6070320653503656e-06 \n",
      "\n",
      "Epoch:  11848  Learning Rate:  7.16000413126191e-07  Varinance:  1.6047871004597118e-06 \n",
      "\n",
      "Epoch:  11849  Learning Rate:  7.152847705939669e-07  Varinance:  1.6025452717027217e-06 \n",
      "\n",
      "Epoch:  11850  Learning Rate:  7.145698433465743e-07  Varinance:  1.600306574698333e-06 \n",
      "\n",
      "Epoch:  11851  Learning Rate:  7.138556306690846e-07  Varinance:  1.598071005071607e-06 \n",
      "\n",
      "Epoch:  11852  Learning Rate:  7.131421318472837e-07  Varinance:  1.595838558453701e-06 \n",
      "\n",
      "Epoch:  11853  Learning Rate:  7.124293461676755e-07  Varinance:  1.5936092304818911e-06 \n",
      "\n",
      "Epoch:  11854  Learning Rate:  7.117172729174727e-07  Varinance:  1.5913830167995433e-06 \n",
      "\n",
      "Epoch:  11855  Learning Rate:  7.110059113846022e-07  Varinance:  1.5891599130561095e-06 \n",
      "\n",
      "Epoch:  11856  Learning Rate:  7.102952608577011e-07  Varinance:  1.586939914907119e-06 \n",
      "\n",
      "Epoch:  11857  Learning Rate:  7.095853206261213e-07  Varinance:  1.5847230180141703e-06 \n",
      "\n",
      "Epoch:  11858  Learning Rate:  7.088760899799212e-07  Varinance:  1.5825092180449276e-06 \n",
      "\n",
      "Epoch:  11859  Learning Rate:  7.081675682098689e-07  Varinance:  1.580298510673091e-06 \n",
      "\n",
      "Epoch:  11860  Learning Rate:  7.07459754607445e-07  Varinance:  1.578090891578421e-06 \n",
      "\n",
      "Epoch:  11861  Learning Rate:  7.067526484648349e-07  Varinance:  1.5758863564467079e-06 \n",
      "\n",
      "Epoch:  11862  Learning Rate:  7.060462490749307e-07  Varinance:  1.5736849009697675e-06 \n",
      "\n",
      "Epoch:  11863  Learning Rate:  7.053405557313358e-07  Varinance:  1.571486520845436e-06 \n",
      "\n",
      "Epoch:  11864  Learning Rate:  7.046355677283553e-07  Varinance:  1.569291211777558e-06 \n",
      "\n",
      "Epoch:  11865  Learning Rate:  7.039312843610001e-07  Varinance:  1.5670989694759857e-06 \n",
      "\n",
      "Epoch:  11866  Learning Rate:  7.032277049249891e-07  Varinance:  1.5649097896565482e-06 \n",
      "\n",
      "Epoch:  11867  Learning Rate:  7.025248287167416e-07  Varinance:  1.5627236680410754e-06 \n",
      "\n",
      "Epoch:  11868  Learning Rate:  7.018226550333802e-07  Varinance:  1.5605406003573687e-06 \n",
      "\n",
      "Epoch:  11869  Learning Rate:  7.011211831727335e-07  Varinance:  1.5583605823391974e-06 \n",
      "\n",
      "Epoch:  11870  Learning Rate:  7.004204124333284e-07  Varinance:  1.5561836097262905e-06 \n",
      "\n",
      "Epoch:  11871  Learning Rate:  6.997203421143941e-07  Varinance:  1.5540096782643283e-06 \n",
      "\n",
      "Epoch:  11872  Learning Rate:  6.99020971515859e-07  Varinance:  1.55183878370494e-06 \n",
      "\n",
      "Epoch:  11873  Learning Rate:  6.983222999383549e-07  Varinance:  1.5496709218056734e-06 \n",
      "\n",
      "Epoch:  11874  Learning Rate:  6.97624326683209e-07  Varinance:  1.5475060883300183e-06 \n",
      "\n",
      "Epoch:  11875  Learning Rate:  6.969270510524466e-07  Varinance:  1.545344279047378e-06 \n",
      "\n",
      "Epoch:  11876  Learning Rate:  6.962304723487946e-07  Varinance:  1.5431854897330662e-06 \n",
      "\n",
      "Epoch:  11877  Learning Rate:  6.955345898756729e-07  Varinance:  1.5410297161682976e-06 \n",
      "\n",
      "Epoch:  11878  Learning Rate:  6.948394029371979e-07  Varinance:  1.5388769541401806e-06 \n",
      "\n",
      "Epoch:  11879  Learning Rate:  6.94144910838185e-07  Varinance:  1.5367271994417143e-06 \n",
      "\n",
      "Epoch:  11880  Learning Rate:  6.934511128841406e-07  Varinance:  1.5345804478717585e-06 \n",
      "\n",
      "Epoch:  11881  Learning Rate:  6.927580083812657e-07  Varinance:  1.5324366952350582e-06 \n",
      "\n",
      "Epoch:  11882  Learning Rate:  6.920655966364583e-07  Varinance:  1.530295937342214e-06 \n",
      "\n",
      "Epoch:  11883  Learning Rate:  6.913738769573052e-07  Varinance:  1.5281581700096775e-06 \n",
      "\n",
      "Epoch:  11884  Learning Rate:  6.906828486520865e-07  Varinance:  1.5260233890597466e-06 \n",
      "\n",
      "Epoch:  11885  Learning Rate:  6.899925110297729e-07  Varinance:  1.5238915903205536e-06 \n",
      "\n",
      "Epoch:  11886  Learning Rate:  6.893028634000289e-07  Varinance:  1.5217627696260653e-06 \n",
      "\n",
      "Epoch:  11887  Learning Rate:  6.886139050732059e-07  Varinance:  1.5196369228160512e-06 \n",
      "\n",
      "Epoch:  11888  Learning Rate:  6.879256353603441e-07  Varinance:  1.5175140457361094e-06 \n",
      "\n",
      "Epoch:  11889  Learning Rate:  6.872380535731762e-07  Varinance:  1.5153941342376358e-06 \n",
      "\n",
      "Epoch:  11890  Learning Rate:  6.865511590241191e-07  Varinance:  1.5132771841778212e-06 \n",
      "\n",
      "Epoch:  11891  Learning Rate:  6.858649510262772e-07  Varinance:  1.5111631914196453e-06 \n",
      "\n",
      "Epoch:  11892  Learning Rate:  6.851794288934446e-07  Varinance:  1.509052151831871e-06 \n",
      "\n",
      "Epoch:  11893  Learning Rate:  6.844945919400979e-07  Varinance:  1.5069440612890163e-06 \n",
      "\n",
      "Epoch:  11894  Learning Rate:  6.838104394813989e-07  Varinance:  1.50483891567138e-06 \n",
      "\n",
      "Epoch:  11895  Learning Rate:  6.831269708331978e-07  Varinance:  1.5027367108650085e-06 \n",
      "\n",
      "Epoch:  11896  Learning Rate:  6.824441853120243e-07  Varinance:  1.5006374427616967e-06 \n",
      "\n",
      "Epoch:  11897  Learning Rate:  6.817620822350918e-07  Varinance:  1.498541107258978e-06 \n",
      "\n",
      "Epoch:  11898  Learning Rate:  6.810806609202997e-07  Varinance:  1.496447700260117e-06 \n",
      "\n",
      "Epoch:  11899  Learning Rate:  6.803999206862251e-07  Varinance:  1.4943572176741063e-06 \n",
      "\n",
      "Epoch:  11900  Learning Rate:  6.797198608521279e-07  Varinance:  1.4922696554156376e-06 \n",
      "\n",
      "Epoch:  11901  Learning Rate:  6.79040480737947e-07  Varinance:  1.4901850094051262e-06 \n",
      "\n",
      "Epoch:  11902  Learning Rate:  6.783617796643047e-07  Varinance:  1.4881032755686798e-06 \n",
      "\n",
      "Epoch:  11903  Learning Rate:  6.776837569524986e-07  Varinance:  1.4860244498380986e-06 \n",
      "\n",
      "Epoch:  11904  Learning Rate:  6.770064119245048e-07  Varinance:  1.4839485281508652e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11905  Learning Rate:  6.763297439029804e-07  Varinance:  1.4818755064501373e-06 \n",
      "\n",
      "Epoch:  11906  Learning Rate:  6.756537522112563e-07  Varinance:  1.4798053806847455e-06 \n",
      "\n",
      "Epoch:  11907  Learning Rate:  6.749784361733395e-07  Varinance:  1.4777381468091637e-06 \n",
      "\n",
      "Epoch:  11908  Learning Rate:  6.743037951139162e-07  Varinance:  1.475673800783533e-06 \n",
      "\n",
      "Epoch:  11909  Learning Rate:  6.736298283583445e-07  Varinance:  1.473612338573633e-06 \n",
      "\n",
      "Epoch:  11910  Learning Rate:  6.72956535232656e-07  Varinance:  1.4715537561508786e-06 \n",
      "\n",
      "Epoch:  11911  Learning Rate:  6.722839150635598e-07  Varinance:  1.469498049492313e-06 \n",
      "\n",
      "Epoch:  11912  Learning Rate:  6.716119671784349e-07  Varinance:  1.467445214580599e-06 \n",
      "\n",
      "Epoch:  11913  Learning Rate:  6.709406909053319e-07  Varinance:  1.4653952474040165e-06 \n",
      "\n",
      "Epoch:  11914  Learning Rate:  6.702700855729768e-07  Varinance:  1.4633481439564343e-06 \n",
      "\n",
      "Epoch:  11915  Learning Rate:  6.696001505107632e-07  Varinance:  1.4613039002373334e-06 \n",
      "\n",
      "Epoch:  11916  Learning Rate:  6.689308850487559e-07  Varinance:  1.4592625122517777e-06 \n",
      "\n",
      "Epoch:  11917  Learning Rate:  6.682622885176883e-07  Varinance:  1.4572239760104123e-06 \n",
      "\n",
      "Epoch:  11918  Learning Rate:  6.67594360248966e-07  Varinance:  1.4551882875294547e-06 \n",
      "\n",
      "Epoch:  11919  Learning Rate:  6.669270995746597e-07  Varinance:  1.4531554428306884e-06 \n",
      "\n",
      "Epoch:  11920  Learning Rate:  6.662605058275072e-07  Varinance:  1.451125437941459e-06 \n",
      "\n",
      "Epoch:  11921  Learning Rate:  6.655945783409174e-07  Varinance:  1.4490982688946462e-06 \n",
      "\n",
      "Epoch:  11922  Learning Rate:  6.649293164489613e-07  Varinance:  1.4470739317286873e-06 \n",
      "\n",
      "Epoch:  11923  Learning Rate:  6.642647194863758e-07  Varinance:  1.4450524224875485e-06 \n",
      "\n",
      "Epoch:  11924  Learning Rate:  6.636007867885665e-07  Varinance:  1.4430337372207227e-06 \n",
      "\n",
      "Epoch:  11925  Learning Rate:  6.629375176915992e-07  Varinance:  1.4410178719832212e-06 \n",
      "\n",
      "Epoch:  11926  Learning Rate:  6.622749115322037e-07  Varinance:  1.4390048228355663e-06 \n",
      "\n",
      "Epoch:  11927  Learning Rate:  6.61612967647776e-07  Varinance:  1.4369945858437887e-06 \n",
      "\n",
      "Epoch:  11928  Learning Rate:  6.609516853763711e-07  Varinance:  1.4349871570794e-06 \n",
      "\n",
      "Epoch:  11929  Learning Rate:  6.602910640567056e-07  Varinance:  1.4329825326194138e-06 \n",
      "\n",
      "Epoch:  11930  Learning Rate:  6.596311030281603e-07  Varinance:  1.4309807085463204e-06 \n",
      "\n",
      "Epoch:  11931  Learning Rate:  6.58971801630773e-07  Varinance:  1.4289816809480817e-06 \n",
      "\n",
      "Epoch:  11932  Learning Rate:  6.583131592052422e-07  Varinance:  1.4269854459181247e-06 \n",
      "\n",
      "Epoch:  11933  Learning Rate:  6.576551750929244e-07  Varinance:  1.4249919995553339e-06 \n",
      "\n",
      "Epoch:  11934  Learning Rate:  6.569978486358375e-07  Varinance:  1.4230013379640487e-06 \n",
      "\n",
      "Epoch:  11935  Learning Rate:  6.563411791766542e-07  Varinance:  1.421013457254035e-06 \n",
      "\n",
      "Epoch:  11936  Learning Rate:  6.556851660587033e-07  Varinance:  1.4190283535405088e-06 \n",
      "\n",
      "Epoch:  11937  Learning Rate:  6.550298086259745e-07  Varinance:  1.4170460229441078e-06 \n",
      "\n",
      "Epoch:  11938  Learning Rate:  6.543751062231088e-07  Varinance:  1.4150664615908891e-06 \n",
      "\n",
      "Epoch:  11939  Learning Rate:  6.537210581954028e-07  Varinance:  1.4130896656123216e-06 \n",
      "\n",
      "Epoch:  11940  Learning Rate:  6.530676638888106e-07  Varinance:  1.4111156311452778e-06 \n",
      "\n",
      "Epoch:  11941  Learning Rate:  6.524149226499367e-07  Varinance:  1.4091443543320326e-06 \n",
      "\n",
      "Epoch:  11942  Learning Rate:  6.517628338260386e-07  Varinance:  1.4071758313202347e-06 \n",
      "\n",
      "Epoch:  11943  Learning Rate:  6.511113967650298e-07  Varinance:  1.4052100582629296e-06 \n",
      "\n",
      "Epoch:  11944  Learning Rate:  6.504606108154721e-07  Varinance:  1.4032470313185316e-06 \n",
      "\n",
      "Epoch:  11945  Learning Rate:  6.498104753265783e-07  Varinance:  1.4012867466508214e-06 \n",
      "\n",
      "Epoch:  11946  Learning Rate:  6.491609896482151e-07  Varinance:  1.3993292004289391e-06 \n",
      "\n",
      "Epoch:  11947  Learning Rate:  6.485121531308956e-07  Varinance:  1.3973743888273763e-06 \n",
      "\n",
      "Epoch:  11948  Learning Rate:  6.478639651257832e-07  Varinance:  1.3954223080259735e-06 \n",
      "\n",
      "Epoch:  11949  Learning Rate:  6.472164249846889e-07  Varinance:  1.3934729542098933e-06 \n",
      "\n",
      "Epoch:  11950  Learning Rate:  6.465695320600746e-07  Varinance:  1.391526323569642e-06 \n",
      "\n",
      "Epoch:  11951  Learning Rate:  6.459232857050463e-07  Varinance:  1.3895824123010424e-06 \n",
      "\n",
      "Epoch:  11952  Learning Rate:  6.452776852733564e-07  Varinance:  1.3876412166052326e-06 \n",
      "\n",
      "Epoch:  11953  Learning Rate:  6.446327301194066e-07  Varinance:  1.3857027326886567e-06 \n",
      "\n",
      "Epoch:  11954  Learning Rate:  6.439884195982407e-07  Varinance:  1.3837669567630583e-06 \n",
      "\n",
      "Epoch:  11955  Learning Rate:  6.433447530655469e-07  Varinance:  1.3818338850454785e-06 \n",
      "\n",
      "Epoch:  11956  Learning Rate:  6.427017298776608e-07  Varinance:  1.379903513758228e-06 \n",
      "\n",
      "Epoch:  11957  Learning Rate:  6.420593493915583e-07  Varinance:  1.377975839128909e-06 \n",
      "\n",
      "Epoch:  11958  Learning Rate:  6.414176109648575e-07  Varinance:  1.3760508573903898e-06 \n",
      "\n",
      "Epoch:  11959  Learning Rate:  6.407765139558223e-07  Varinance:  1.3741285647808004e-06 \n",
      "\n",
      "Epoch:  11960  Learning Rate:  6.401360577233544e-07  Varinance:  1.3722089575435264e-06 \n",
      "\n",
      "Epoch:  11961  Learning Rate:  6.394962416269964e-07  Varinance:  1.3702920319272008e-06 \n",
      "\n",
      "Epoch:  11962  Learning Rate:  6.388570650269345e-07  Varinance:  1.3683777841857026e-06 \n",
      "\n",
      "Epoch:  11963  Learning Rate:  6.382185272839909e-07  Varinance:  1.3664662105781292e-06 \n",
      "\n",
      "Epoch:  11964  Learning Rate:  6.375806277596278e-07  Varinance:  1.364557307368818e-06 \n",
      "\n",
      "Epoch:  11965  Learning Rate:  6.369433658159444e-07  Varinance:  1.3626510708273206e-06 \n",
      "\n",
      "Epoch:  11966  Learning Rate:  6.363067408156809e-07  Varinance:  1.3607474972284e-06 \n",
      "\n",
      "Epoch:  11967  Learning Rate:  6.356707521222115e-07  Varinance:  1.358846582852023e-06 \n",
      "\n",
      "Epoch:  11968  Learning Rate:  6.350353990995459e-07  Varinance:  1.356948323983353e-06 \n",
      "\n",
      "Epoch:  11969  Learning Rate:  6.344006811123335e-07  Varinance:  1.355052716912748e-06 \n",
      "\n",
      "Epoch:  11970  Learning Rate:  6.33766597525855e-07  Varinance:  1.3531597579357336e-06 \n",
      "\n",
      "Epoch:  11971  Learning Rate:  6.331331477060258e-07  Varinance:  1.351269443353025e-06 \n",
      "\n",
      "Epoch:  11972  Learning Rate:  6.325003310193981e-07  Varinance:  1.3493817694705005e-06 \n",
      "\n",
      "Epoch:  11973  Learning Rate:  6.318681468331542e-07  Varinance:  1.3474967325991985e-06 \n",
      "\n",
      "Epoch:  11974  Learning Rate:  6.312365945151086e-07  Varinance:  1.3456143290553107e-06 \n",
      "\n",
      "Epoch:  11975  Learning Rate:  6.306056734337114e-07  Varinance:  1.3437345551601757e-06 \n",
      "\n",
      "Epoch:  11976  Learning Rate:  6.299753829580401e-07  Varinance:  1.3418574072402753e-06 \n",
      "\n",
      "Epoch:  11977  Learning Rate:  6.293457224578031e-07  Varinance:  1.3399828816272083e-06 \n",
      "\n",
      "Epoch:  11978  Learning Rate:  6.287166913033421e-07  Varinance:  1.3381109746577136e-06 \n",
      "\n",
      "Epoch:  11979  Learning Rate:  6.280882888656248e-07  Varinance:  1.336241682673642e-06 \n",
      "\n",
      "Epoch:  11980  Learning Rate:  6.274605145162488e-07  Varinance:  1.3343750020219543e-06 \n",
      "\n",
      "Epoch:  11981  Learning Rate:  6.268333676274385e-07  Varinance:  1.3325109290547153e-06 \n",
      "\n",
      "Epoch:  11982  Learning Rate:  6.26206847572049e-07  Varinance:  1.3306494601290851e-06 \n",
      "\n",
      "Epoch:  11983  Learning Rate:  6.255809537235594e-07  Varinance:  1.328790591607318e-06 \n",
      "\n",
      "Epoch:  11984  Learning Rate:  6.249556854560745e-07  Varinance:  1.3269343198567353e-06 \n",
      "\n",
      "Epoch:  11985  Learning Rate:  6.243310421443283e-07  Varinance:  1.325080641249748e-06 \n",
      "\n",
      "Epoch:  11986  Learning Rate:  6.237070231636762e-07  Varinance:  1.3232295521638294e-06 \n",
      "\n",
      "Epoch:  11987  Learning Rate:  6.230836278900981e-07  Varinance:  1.3213810489815134e-06 \n",
      "\n",
      "Epoch:  11988  Learning Rate:  6.22460855700201e-07  Varinance:  1.3195351280903875e-06 \n",
      "\n",
      "Epoch:  11989  Learning Rate:  6.218387059712115e-07  Varinance:  1.3176917858830855e-06 \n",
      "\n",
      "Epoch:  11990  Learning Rate:  6.212171780809786e-07  Varinance:  1.3158510187572855e-06 \n",
      "\n",
      "Epoch:  11991  Learning Rate:  6.205962714079767e-07  Varinance:  1.3140128231156834e-06 \n",
      "\n",
      "Epoch:  11992  Learning Rate:  6.199759853312979e-07  Varinance:  1.3121771953660149e-06 \n",
      "\n",
      "Epoch:  11993  Learning Rate:  6.19356319230655e-07  Varinance:  1.310344131921029e-06 \n",
      "\n",
      "Epoch:  11994  Learning Rate:  6.18737272486384e-07  Varinance:  1.3085136291984861e-06 \n",
      "\n",
      "Epoch:  11995  Learning Rate:  6.181188444794372e-07  Varinance:  1.3066856836211508e-06 \n",
      "\n",
      "Epoch:  11996  Learning Rate:  6.175010345913863e-07  Varinance:  1.3048602916167849e-06 \n",
      "\n",
      "Epoch:  11997  Learning Rate:  6.168838422044203e-07  Varinance:  1.3030374496181452e-06 \n",
      "\n",
      "Epoch:  11998  Learning Rate:  6.162672667013491e-07  Varinance:  1.3012171540629582e-06 \n",
      "\n",
      "Epoch:  11999  Learning Rate:  6.156513074655958e-07  Varinance:  1.29939940139394e-06 \n",
      "\n",
      "Epoch:  12000  Learning Rate:  6.150359638812003e-07  Varinance:  1.2975841880587721e-06 \n",
      "\n",
      "Epoch:  12001  Learning Rate:  6.14421235332821e-07  Varinance:  1.295771510510098e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12002  Learning Rate:  6.138071212057283e-07  Varinance:  1.2939613652055166e-06 \n",
      "\n",
      "Epoch:  12003  Learning Rate:  6.131936208858068e-07  Varinance:  1.2921537486075766e-06 \n",
      "\n",
      "Epoch:  12004  Learning Rate:  6.125807337595583e-07  Varinance:  1.2903486571837712e-06 \n",
      "\n",
      "Epoch:  12005  Learning Rate:  6.119684592140947e-07  Varinance:  1.2885460874065161e-06 \n",
      "\n",
      "Epoch:  12006  Learning Rate:  6.113567966371403e-07  Varinance:  1.2867460357531678e-06 \n",
      "\n",
      "Epoch:  12007  Learning Rate:  6.107457454170344e-07  Varinance:  1.2849484987059998e-06 \n",
      "\n",
      "Epoch:  12008  Learning Rate:  6.10135304942725e-07  Varinance:  1.283153472752199e-06 \n",
      "\n",
      "Epoch:  12009  Learning Rate:  6.095254746037711e-07  Varinance:  1.2813609543838602e-06 \n",
      "\n",
      "Epoch:  12010  Learning Rate:  6.089162537903418e-07  Varinance:  1.2795709400979787e-06 \n",
      "\n",
      "Epoch:  12011  Learning Rate:  6.083076418932181e-07  Varinance:  1.277783426396447e-06 \n",
      "\n",
      "Epoch:  12012  Learning Rate:  6.076996383037869e-07  Varinance:  1.2759984097860322e-06 \n",
      "\n",
      "Epoch:  12013  Learning Rate:  6.070922424140435e-07  Varinance:  1.2742158867783933e-06 \n",
      "\n",
      "Epoch:  12014  Learning Rate:  6.064854536165942e-07  Varinance:  1.272435853890059e-06 \n",
      "\n",
      "Epoch:  12015  Learning Rate:  6.058792713046492e-07  Varinance:  1.270658307642424e-06 \n",
      "\n",
      "Epoch:  12016  Learning Rate:  6.052736948720248e-07  Varinance:  1.268883244561742e-06 \n",
      "\n",
      "Epoch:  12017  Learning Rate:  6.046687237131467e-07  Varinance:  1.26711066117912e-06 \n",
      "\n",
      "Epoch:  12018  Learning Rate:  6.040643572230429e-07  Varinance:  1.2653405540305153e-06 \n",
      "\n",
      "Epoch:  12019  Learning Rate:  6.034605947973455e-07  Varinance:  1.2635729196567108e-06 \n",
      "\n",
      "Epoch:  12020  Learning Rate:  6.028574358322942e-07  Varinance:  1.261807754603335e-06 \n",
      "\n",
      "Epoch:  12021  Learning Rate:  6.022548797247291e-07  Varinance:  1.2600450554208378e-06 \n",
      "\n",
      "Epoch:  12022  Learning Rate:  6.016529258720927e-07  Varinance:  1.258284818664488e-06 \n",
      "\n",
      "Epoch:  12023  Learning Rate:  6.010515736724336e-07  Varinance:  1.2565270408943666e-06 \n",
      "\n",
      "Epoch:  12024  Learning Rate:  6.00450822524398e-07  Varinance:  1.2547717186753645e-06 \n",
      "\n",
      "Epoch:  12025  Learning Rate:  5.998506718272351e-07  Varinance:  1.2530188485771578e-06 \n",
      "\n",
      "Epoch:  12026  Learning Rate:  5.99251120980793e-07  Varinance:  1.2512684271742278e-06 \n",
      "\n",
      "Epoch:  12027  Learning Rate:  5.986521693855227e-07  Varinance:  1.2495204510458374e-06 \n",
      "\n",
      "Epoch:  12028  Learning Rate:  5.980538164424718e-07  Varinance:  1.2477749167760277e-06 \n",
      "\n",
      "Epoch:  12029  Learning Rate:  5.974560615532861e-07  Varinance:  1.2460318209536113e-06 \n",
      "\n",
      "Epoch:  12030  Learning Rate:  5.968589041202128e-07  Varinance:  1.244291160172167e-06 \n",
      "\n",
      "Epoch:  12031  Learning Rate:  5.962623435460934e-07  Varinance:  1.242552931030036e-06 \n",
      "\n",
      "Epoch:  12032  Learning Rate:  5.956663792343662e-07  Varinance:  1.2408171301302986e-06 \n",
      "\n",
      "Epoch:  12033  Learning Rate:  5.950710105890688e-07  Varinance:  1.2390837540807937e-06 \n",
      "\n",
      "Epoch:  12034  Learning Rate:  5.944762370148316e-07  Varinance:  1.237352799494094e-06 \n",
      "\n",
      "Epoch:  12035  Learning Rate:  5.9388205791688e-07  Varinance:  1.235624262987505e-06 \n",
      "\n",
      "Epoch:  12036  Learning Rate:  5.932884727010368e-07  Varinance:  1.2338981411830577e-06 \n",
      "\n",
      "Epoch:  12037  Learning Rate:  5.926954807737158e-07  Varinance:  1.2321744307075012e-06 \n",
      "\n",
      "Epoch:  12038  Learning Rate:  5.921030815419238e-07  Varinance:  1.230453128192302e-06 \n",
      "\n",
      "Epoch:  12039  Learning Rate:  5.915112744132638e-07  Varinance:  1.228734230273619e-06 \n",
      "\n",
      "Epoch:  12040  Learning Rate:  5.909200587959275e-07  Varinance:  1.2270177335923235e-06 \n",
      "\n",
      "Epoch:  12041  Learning Rate:  5.903294340986992e-07  Varinance:  1.2253036347939748e-06 \n",
      "\n",
      "Epoch:  12042  Learning Rate:  5.897393997309531e-07  Varinance:  1.2235919305288183e-06 \n",
      "\n",
      "Epoch:  12043  Learning Rate:  5.891499551026571e-07  Varinance:  1.2218826174517788e-06 \n",
      "\n",
      "Epoch:  12044  Learning Rate:  5.885610996243651e-07  Varinance:  1.2201756922224543e-06 \n",
      "\n",
      "Epoch:  12045  Learning Rate:  5.879728327072209e-07  Varinance:  1.2184711515051134e-06 \n",
      "\n",
      "Epoch:  12046  Learning Rate:  5.873851537629593e-07  Varinance:  1.2167689919686713e-06 \n",
      "\n",
      "Epoch:  12047  Learning Rate:  5.867980622039006e-07  Varinance:  1.2150692102867104e-06 \n",
      "\n",
      "Epoch:  12048  Learning Rate:  5.862115574429518e-07  Varinance:  1.2133718031374548e-06 \n",
      "\n",
      "Epoch:  12049  Learning Rate:  5.856256388936104e-07  Varinance:  1.2116767672037695e-06 \n",
      "\n",
      "Epoch:  12050  Learning Rate:  5.850403059699568e-07  Varinance:  1.209984099173153e-06 \n",
      "\n",
      "Epoch:  12051  Learning Rate:  5.844555580866567e-07  Varinance:  1.208293795737732e-06 \n",
      "\n",
      "Epoch:  12052  Learning Rate:  5.838713946589644e-07  Varinance:  1.2066058535942575e-06 \n",
      "\n",
      "Epoch:  12053  Learning Rate:  5.832878151027156e-07  Varinance:  1.2049202694440827e-06 \n",
      "\n",
      "Epoch:  12054  Learning Rate:  5.827048188343294e-07  Varinance:  1.2032370399931819e-06 \n",
      "\n",
      "Epoch:  12055  Learning Rate:  5.821224052708116e-07  Varinance:  1.2015561619521264e-06 \n",
      "\n",
      "Epoch:  12056  Learning Rate:  5.815405738297476e-07  Varinance:  1.1998776320360829e-06 \n",
      "\n",
      "Epoch:  12057  Learning Rate:  5.809593239293059e-07  Varinance:  1.1982014469648067e-06 \n",
      "\n",
      "Epoch:  12058  Learning Rate:  5.803786549882355e-07  Varinance:  1.1965276034626356e-06 \n",
      "\n",
      "Epoch:  12059  Learning Rate:  5.797985664258695e-07  Varinance:  1.194856098258488e-06 \n",
      "\n",
      "Epoch:  12060  Learning Rate:  5.792190576621182e-07  Varinance:  1.1931869280858382e-06 \n",
      "\n",
      "Epoch:  12061  Learning Rate:  5.786401281174718e-07  Varinance:  1.1915200896827374e-06 \n",
      "\n",
      "Epoch:  12062  Learning Rate:  5.780617772130028e-07  Varinance:  1.189855579791789e-06 \n",
      "\n",
      "Epoch:  12063  Learning Rate:  5.774840043703592e-07  Varinance:  1.1881933951601468e-06 \n",
      "\n",
      "Epoch:  12064  Learning Rate:  5.769068090117669e-07  Varinance:  1.1865335325395086e-06 \n",
      "\n",
      "Epoch:  12065  Learning Rate:  5.76330190560033e-07  Varinance:  1.1848759886861105e-06 \n",
      "\n",
      "Epoch:  12066  Learning Rate:  5.757541484385375e-07  Varinance:  1.1832207603607237e-06 \n",
      "\n",
      "Epoch:  12067  Learning Rate:  5.751786820712375e-07  Varinance:  1.181567844328632e-06 \n",
      "\n",
      "Epoch:  12068  Learning Rate:  5.746037908826684e-07  Varinance:  1.1799172373596507e-06 \n",
      "\n",
      "Epoch:  12069  Learning Rate:  5.740294742979382e-07  Varinance:  1.178268936228103e-06 \n",
      "\n",
      "Epoch:  12070  Learning Rate:  5.73455731742729e-07  Varinance:  1.1766229377128189e-06 \n",
      "\n",
      "Epoch:  12071  Learning Rate:  5.728825626433005e-07  Varinance:  1.1749792385971278e-06 \n",
      "\n",
      "Epoch:  12072  Learning Rate:  5.723099664264822e-07  Varinance:  1.1733378356688526e-06 \n",
      "\n",
      "Epoch:  12073  Learning Rate:  5.717379425196781e-07  Varinance:  1.1716987257203078e-06 \n",
      "\n",
      "Epoch:  12074  Learning Rate:  5.711664903508631e-07  Varinance:  1.1700619055482766e-06 \n",
      "\n",
      "Epoch:  12075  Learning Rate:  5.705956093485871e-07  Varinance:  1.168427371954029e-06 \n",
      "\n",
      "Epoch:  12076  Learning Rate:  5.70025298941968e-07  Varinance:  1.1667951217433002e-06 \n",
      "\n",
      "Epoch:  12077  Learning Rate:  5.694555585606944e-07  Varinance:  1.1651651517262867e-06 \n",
      "\n",
      "Epoch:  12078  Learning Rate:  5.688863876350277e-07  Varinance:  1.1635374587176416e-06 \n",
      "\n",
      "Epoch:  12079  Learning Rate:  5.683177855957962e-07  Varinance:  1.1619120395364675e-06 \n",
      "\n",
      "Epoch:  12080  Learning Rate:  5.677497518743965e-07  Varinance:  1.160288891006315e-06 \n",
      "\n",
      "Epoch:  12081  Learning Rate:  5.67182285902797e-07  Varinance:  1.1586680099551598e-06 \n",
      "\n",
      "Epoch:  12082  Learning Rate:  5.666153871135307e-07  Varinance:  1.1570493932154206e-06 \n",
      "\n",
      "Epoch:  12083  Learning Rate:  5.660490549396978e-07  Varinance:  1.1554330376239377e-06 \n",
      "\n",
      "Epoch:  12084  Learning Rate:  5.654832888149678e-07  Varinance:  1.1538189400219695e-06 \n",
      "\n",
      "Epoch:  12085  Learning Rate:  5.64918088173574e-07  Varinance:  1.152207097255188e-06 \n",
      "\n",
      "Epoch:  12086  Learning Rate:  5.643534524503143e-07  Varinance:  1.1505975061736704e-06 \n",
      "\n",
      "Epoch:  12087  Learning Rate:  5.637893810805552e-07  Varinance:  1.1489901636318994e-06 \n",
      "\n",
      "Epoch:  12088  Learning Rate:  5.632258735002241e-07  Varinance:  1.1473850664887391e-06 \n",
      "\n",
      "Epoch:  12089  Learning Rate:  5.626629291458135e-07  Varinance:  1.1457822116074544e-06 \n",
      "\n",
      "Epoch:  12090  Learning Rate:  5.621005474543778e-07  Varinance:  1.144181595855687e-06 \n",
      "\n",
      "Epoch:  12091  Learning Rate:  5.615387278635374e-07  Varinance:  1.1425832161054554e-06 \n",
      "\n",
      "Epoch:  12092  Learning Rate:  5.609774698114717e-07  Varinance:  1.1409870692331475e-06 \n",
      "\n",
      "Epoch:  12093  Learning Rate:  5.604167727369216e-07  Varinance:  1.1393931521195144e-06 \n",
      "\n",
      "Epoch:  12094  Learning Rate:  5.598566360791919e-07  Varinance:  1.1378014616496692e-06 \n",
      "\n",
      "Epoch:  12095  Learning Rate:  5.59297059278145e-07  Varinance:  1.1362119947130642e-06 \n",
      "\n",
      "Epoch:  12096  Learning Rate:  5.587380417742029e-07  Varinance:  1.1346247482035087e-06 \n",
      "\n",
      "Epoch:  12097  Learning Rate:  5.581795830083501e-07  Varinance:  1.1330397190191476e-06 \n",
      "\n",
      "Epoch:  12098  Learning Rate:  5.576216824221269e-07  Varinance:  1.131456904062459e-06 \n",
      "\n",
      "Epoch:  12099  Learning Rate:  5.570643394576315e-07  Varinance:  1.1298763002402477e-06 \n",
      "\n",
      "Epoch:  12100  Learning Rate:  5.565075535575232e-07  Varinance:  1.1282979044636403e-06 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12101  Learning Rate:  5.559513241650146e-07  Varinance:  1.1267217136480819e-06 \n",
      "\n",
      "Epoch:  12102  Learning Rate:  5.553956507238756e-07  Varinance:  1.1251477247133145e-06 \n",
      "\n",
      "Epoch:  12103  Learning Rate:  5.548405326784346e-07  Varinance:  1.1235759345833957e-06 \n",
      "\n",
      "Epoch:  12104  Learning Rate:  5.542859694735725e-07  Varinance:  1.1220063401866752e-06 \n",
      "\n",
      "Epoch:  12105  Learning Rate:  5.537319605547261e-07  Varinance:  1.1204389384557949e-06 \n",
      "\n",
      "Epoch:  12106  Learning Rate:  5.531785053678854e-07  Varinance:  1.1188737263276804e-06 \n",
      "\n",
      "Epoch:  12107  Learning Rate:  5.526256033595971e-07  Varinance:  1.117310700743537e-06 \n",
      "\n",
      "Epoch:  12108  Learning Rate:  5.520732539769583e-07  Varinance:  1.1157498586488473e-06 \n",
      "\n",
      "Epoch:  12109  Learning Rate:  5.515214566676184e-07  Varinance:  1.1141911969933483e-06 \n",
      "\n",
      "Epoch:  12110  Learning Rate:  5.509702108797821e-07  Varinance:  1.11263471273105e-06 \n",
      "\n",
      "Epoch:  12111  Learning Rate:  5.504195160622027e-07  Varinance:  1.1110804028202147e-06 \n",
      "\n",
      "Epoch:  12112  Learning Rate:  5.498693716641842e-07  Varinance:  1.1095282642233524e-06 \n",
      "\n",
      "Epoch:  12113  Learning Rate:  5.493197771355842e-07  Varinance:  1.1079782939072178e-06 \n",
      "\n",
      "Epoch:  12114  Learning Rate:  5.48770731926807e-07  Varinance:  1.1064304888428013e-06 \n",
      "\n",
      "Epoch:  12115  Learning Rate:  5.482222354888066e-07  Varinance:  1.10488484600533e-06 \n",
      "\n",
      "Epoch:  12116  Learning Rate:  5.476742872730883e-07  Varinance:  1.1033413623742446e-06 \n",
      "\n",
      "Epoch:  12117  Learning Rate:  5.47126886731703e-07  Varinance:  1.1018000349332163e-06 \n",
      "\n",
      "Epoch:  12118  Learning Rate:  5.465800333172489e-07  Varinance:  1.100260860670127e-06 \n",
      "\n",
      "Epoch:  12119  Learning Rate:  5.460337264828747e-07  Varinance:  1.0987238365770656e-06 \n",
      "\n",
      "Epoch:  12120  Learning Rate:  5.454879656822725e-07  Varinance:  1.0971889596503239e-06 \n",
      "\n",
      "Epoch:  12121  Learning Rate:  5.449427503696814e-07  Varinance:  1.0956562268903887e-06 \n",
      "\n",
      "Epoch:  12122  Learning Rate:  5.443980799998852e-07  Varinance:  1.094125635301942e-06 \n",
      "\n",
      "Epoch:  12123  Learning Rate:  5.438539540282152e-07  Varinance:  1.0925971818938378e-06 \n",
      "\n",
      "Epoch:  12124  Learning Rate:  5.433103719105446e-07  Varinance:  1.0910708636791202e-06 \n",
      "\n",
      "Epoch:  12125  Learning Rate:  5.427673331032904e-07  Varinance:  1.0895466776750024e-06 \n",
      "\n",
      "Epoch:  12126  Learning Rate:  5.422248370634153e-07  Varinance:  1.0880246209028644e-06 \n",
      "\n",
      "Epoch:  12127  Learning Rate:  5.416828832484225e-07  Varinance:  1.0865046903882473e-06 \n",
      "\n",
      "Epoch:  12128  Learning Rate:  5.411414711163571e-07  Varinance:  1.084986883160847e-06 \n",
      "\n",
      "Epoch:  12129  Learning Rate:  5.406006001258089e-07  Varinance:  1.0834711962545129e-06 \n",
      "\n",
      "Epoch:  12130  Learning Rate:  5.400602697359058e-07  Varinance:  1.0819576267072268e-06 \n",
      "\n",
      "Epoch:  12131  Learning Rate:  5.395204794063167e-07  Varinance:  1.0804461715611195e-06 \n",
      "\n",
      "Epoch:  12132  Learning Rate:  5.389812285972527e-07  Varinance:  1.07893682786245e-06 \n",
      "\n",
      "Epoch:  12133  Learning Rate:  5.384425167694623e-07  Varinance:  1.0774295926616035e-06 \n",
      "\n",
      "Epoch:  12134  Learning Rate:  5.379043433842336e-07  Varinance:  1.0759244630130859e-06 \n",
      "\n",
      "Epoch:  12135  Learning Rate:  5.37366707903392e-07  Varinance:  1.0744214359755177e-06 \n",
      "\n",
      "Epoch:  12136  Learning Rate:  5.368296097893042e-07  Varinance:  1.0729205086116327e-06 \n",
      "\n",
      "Epoch:  12137  Learning Rate:  5.362930485048709e-07  Varinance:  1.0714216779882554e-06 \n",
      "\n",
      "Epoch:  12138  Learning Rate:  5.357570235135298e-07  Varinance:  1.0699249411763208e-06 \n",
      "\n",
      "Epoch:  12139  Learning Rate:  5.352215342792577e-07  Varinance:  1.0684302952508502e-06 \n",
      "\n",
      "Epoch:  12140  Learning Rate:  5.346865802665646e-07  Varinance:  1.0669377372909525e-06 \n",
      "\n",
      "Epoch:  12141  Learning Rate:  5.341521609404953e-07  Varinance:  1.0654472643798163e-06 \n",
      "\n",
      "Epoch:  12142  Learning Rate:  5.336182757666326e-07  Varinance:  1.0639588736047043e-06 \n",
      "\n",
      "Epoch:  12143  Learning Rate:  5.330849242110899e-07  Varinance:  1.0624725620569532e-06 \n",
      "\n",
      "Epoch:  12144  Learning Rate:  5.325521057405151e-07  Varinance:  1.0609883268319508e-06 \n",
      "\n",
      "Epoch:  12145  Learning Rate:  5.320198198220912e-07  Varinance:  1.0595061650291542e-06 \n",
      "\n",
      "Epoch:  12146  Learning Rate:  5.314880659235315e-07  Varinance:  1.0580260737520684e-06 \n",
      "\n",
      "Epoch:  12147  Learning Rate:  5.309568435130811e-07  Varinance:  1.056548050108245e-06 \n",
      "\n",
      "Epoch:  12148  Learning Rate:  5.304261520595192e-07  Varinance:  1.055072091209276e-06 \n",
      "\n",
      "Epoch:  12149  Learning Rate:  5.298959910321539e-07  Varinance:  1.0535981941707887e-06 \n",
      "\n",
      "Epoch:  12150  Learning Rate:  5.293663599008237e-07  Varinance:  1.0521263561124431e-06 \n",
      "\n",
      "Epoch:  12151  Learning Rate:  5.288372581358963e-07  Varinance:  1.0506565741579123e-06 \n",
      "\n",
      "Epoch:  12152  Learning Rate:  5.283086852082723e-07  Varinance:  1.0491888454348982e-06 \n",
      "\n",
      "Epoch:  12153  Learning Rate:  5.277806405893776e-07  Varinance:  1.0477231670751113e-06 \n",
      "\n",
      "Epoch:  12154  Learning Rate:  5.272531237511664e-07  Varinance:  1.04625953621427e-06 \n",
      "\n",
      "Epoch:  12155  Learning Rate:  5.267261341661238e-07  Varinance:  1.0447979499920924e-06 \n",
      "\n",
      "Epoch:  12156  Learning Rate:  5.261996713072593e-07  Varinance:  1.0433384055522978e-06 \n",
      "\n",
      "Epoch:  12157  Learning Rate:  5.25673734648109e-07  Varinance:  1.0418809000425832e-06 \n",
      "\n",
      "Epoch:  12158  Learning Rate:  5.251483236627382e-07  Varinance:  1.0404254306146418e-06 \n",
      "\n",
      "Epoch:  12159  Learning Rate:  5.246234378257347e-07  Varinance:  1.0389719944241422e-06 \n",
      "\n",
      "Epoch:  12160  Learning Rate:  5.240990766122118e-07  Varinance:  1.0375205886307259e-06 \n",
      "\n",
      "Epoch:  12161  Learning Rate:  5.235752394978102e-07  Varinance:  1.0360712103980027e-06 \n",
      "\n",
      "Epoch:  12162  Learning Rate:  5.230519259586917e-07  Varinance:  1.0346238568935447e-06 \n",
      "\n",
      "Epoch:  12163  Learning Rate:  5.225291354715419e-07  Varinance:  1.0331785252888844e-06 \n",
      "\n",
      "Epoch:  12164  Learning Rate:  5.220068675135719e-07  Varinance:  1.0317352127594945e-06 \n",
      "\n",
      "Epoch:  12165  Learning Rate:  5.214851215625129e-07  Varinance:  1.0302939164848045e-06 \n",
      "\n",
      "Epoch:  12166  Learning Rate:  5.209638970966191e-07  Varinance:  1.0288546336481806e-06 \n",
      "\n",
      "Epoch:  12167  Learning Rate:  5.204431935946648e-07  Varinance:  1.0274173614369234e-06 \n",
      "\n",
      "Epoch:  12168  Learning Rate:  5.199230105359483e-07  Varinance:  1.0259820970422633e-06 \n",
      "\n",
      "Epoch:  12169  Learning Rate:  5.194033474002857e-07  Varinance:  1.0245488376593539e-06 \n",
      "\n",
      "Epoch:  12170  Learning Rate:  5.18884203668013e-07  Varinance:  1.023117580487271e-06 \n",
      "\n",
      "Epoch:  12171  Learning Rate:  5.18365578819988e-07  Varinance:  1.021688322728992e-06 \n",
      "\n",
      "Epoch:  12172  Learning Rate:  5.178474723375851e-07  Varinance:  1.020261061591413e-06 \n",
      "\n",
      "Epoch:  12173  Learning Rate:  5.173298837026966e-07  Varinance:  1.0188357942853276e-06 \n",
      "\n",
      "Epoch:  12174  Learning Rate:  5.168128123977359e-07  Varinance:  1.017412518025427e-06 \n",
      "\n",
      "Epoch:  12175  Learning Rate:  5.162962579056308e-07  Varinance:  1.015991230030292e-06 \n",
      "\n",
      "Epoch:  12176  Learning Rate:  5.157802197098256e-07  Varinance:  1.0145719275223901e-06 \n",
      "\n",
      "Epoch:  12177  Learning Rate:  5.15264697294284e-07  Varinance:  1.0131546077280717e-06 \n",
      "\n",
      "Epoch:  12178  Learning Rate:  5.147496901434827e-07  Varinance:  1.0117392678775516e-06 \n",
      "\n",
      "Epoch:  12179  Learning Rate:  5.142351977424134e-07  Varinance:  1.010325905204924e-06 \n",
      "\n",
      "Epoch:  12180  Learning Rate:  5.137212195765858e-07  Varinance:  1.0089145169481449e-06 \n",
      "\n",
      "Epoch:  12181  Learning Rate:  5.132077551320204e-07  Varinance:  1.0075051003490265e-06 \n",
      "\n",
      "Epoch:  12182  Learning Rate:  5.12694803895253e-07  Varinance:  1.0060976526532362e-06 \n",
      "\n",
      "Epoch:  12183  Learning Rate:  5.121823653533314e-07  Varinance:  1.0046921711102877e-06 \n",
      "\n",
      "Epoch:  12184  Learning Rate:  5.116704389938186e-07  Varinance:  1.0032886529735415e-06 \n",
      "\n",
      "Epoch:  12185  Learning Rate:  5.111590243047874e-07  Varinance:  1.0018870955001835e-06 \n",
      "\n",
      "Epoch:  12186  Learning Rate:  5.106481207748223e-07  Varinance:  1.000487495951243e-06 \n",
      "\n",
      "Epoch:  12187  Learning Rate:  5.101377278930214e-07  Varinance:  9.990898515915708e-07 \n",
      "\n",
      "Epoch:  12188  Learning Rate:  5.096278451489909e-07  Varinance:  9.976941596898394e-07 \n",
      "\n",
      "Epoch:  12189  Learning Rate:  5.091184720328471e-07  Varinance:  9.963004175185367e-07 \n",
      "\n",
      "Epoch:  12190  Learning Rate:  5.086096080352187e-07  Varinance:  9.949086223539606e-07 \n",
      "\n",
      "Epoch:  12191  Learning Rate:  5.081012526472408e-07  Varinance:  9.935187714762177e-07 \n",
      "\n",
      "Epoch:  12192  Learning Rate:  5.075934053605569e-07  Varinance:  9.921308621692034e-07 \n",
      "\n",
      "Epoch:  12193  Learning Rate:  5.070860656673215e-07  Varinance:  9.907448917206184e-07 \n",
      "\n",
      "Epoch:  12194  Learning Rate:  5.065792330601941e-07  Varinance:  9.89360857421948e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12195  Learning Rate:  5.06072907032341e-07  Varinance:  9.879787565684622e-07 \n",
      "\n",
      "Epoch:  12196  Learning Rate:  5.05567087077438e-07  Varinance:  9.865985864592087e-07 \n",
      "\n",
      "Epoch:  12197  Learning Rate:  5.050617726896643e-07  Varinance:  9.852203443970086e-07 \n",
      "\n",
      "Epoch:  12198  Learning Rate:  5.045569633637053e-07  Varinance:  9.838440276884546e-07 \n",
      "\n",
      "Epoch:  12199  Learning Rate:  5.040526585947508e-07  Varinance:  9.824696336438907e-07 \n",
      "\n",
      "Epoch:  12200  Learning Rate:  5.035488578784979e-07  Varinance:  9.810971595774298e-07 \n",
      "\n",
      "Epoch:  12201  Learning Rate:  5.030455607111448e-07  Varinance:  9.797266028069326e-07 \n",
      "\n",
      "Epoch:  12202  Learning Rate:  5.025427665893935e-07  Varinance:  9.78357960654007e-07 \n",
      "\n",
      "Epoch:  12203  Learning Rate:  5.020404750104514e-07  Varinance:  9.769912304440025e-07 \n",
      "\n",
      "Epoch:  12204  Learning Rate:  5.015386854720262e-07  Varinance:  9.756264095060048e-07 \n",
      "\n",
      "Epoch:  12205  Learning Rate:  5.010373974723275e-07  Varinance:  9.742634951728343e-07 \n",
      "\n",
      "Epoch:  12206  Learning Rate:  5.005366105100688e-07  Varinance:  9.729024847810272e-07 \n",
      "\n",
      "Epoch:  12207  Learning Rate:  5.000363240844623e-07  Varinance:  9.715433756708503e-07 \n",
      "\n",
      "Epoch:  12208  Learning Rate:  4.995365376952207e-07  Varinance:  9.701861651862834e-07 \n",
      "\n",
      "Epoch:  12209  Learning Rate:  4.990372508425594e-07  Varinance:  9.688308506750158e-07 \n",
      "\n",
      "Epoch:  12210  Learning Rate:  4.985384630271904e-07  Varinance:  9.674774294884424e-07 \n",
      "\n",
      "Epoch:  12211  Learning Rate:  4.980401737503252e-07  Varinance:  9.66125898981658e-07 \n",
      "\n",
      "Epoch:  12212  Learning Rate:  4.97542382513676e-07  Varinance:  9.647762565134552e-07 \n",
      "\n",
      "Epoch:  12213  Learning Rate:  4.97045088819451e-07  Varinance:  9.634284994463073e-07 \n",
      "\n",
      "Epoch:  12214  Learning Rate:  4.96548292170356e-07  Varinance:  9.620826251463807e-07 \n",
      "\n",
      "Epoch:  12215  Learning Rate:  4.960519920695937e-07  Varinance:  9.607386309835193e-07 \n",
      "\n",
      "Epoch:  12216  Learning Rate:  4.955561880208658e-07  Varinance:  9.593965143312402e-07 \n",
      "\n",
      "Epoch:  12217  Learning Rate:  4.950608795283672e-07  Varinance:  9.580562725667298e-07 \n",
      "\n",
      "Epoch:  12218  Learning Rate:  4.945660660967885e-07  Varinance:  9.56717903070839e-07 \n",
      "\n",
      "Epoch:  12219  Learning Rate:  4.940717472313179e-07  Varinance:  9.553814032280801e-07 \n",
      "\n",
      "Epoch:  12220  Learning Rate:  4.935779224376358e-07  Varinance:  9.5404677042661e-07 \n",
      "\n",
      "Epoch:  12221  Learning Rate:  4.930845912219163e-07  Varinance:  9.527140020582434e-07 \n",
      "\n",
      "Epoch:  12222  Learning Rate:  4.9259175309083e-07  Varinance:  9.513830955184356e-07 \n",
      "\n",
      "Epoch:  12223  Learning Rate:  4.92099407551538e-07  Varinance:  9.500540482062806e-07 \n",
      "\n",
      "Epoch:  12224  Learning Rate:  4.916075541116935e-07  Varinance:  9.487268575245054e-07 \n",
      "\n",
      "Epoch:  12225  Learning Rate:  4.911161922794451e-07  Varinance:  9.474015208794653e-07 \n",
      "\n",
      "Epoch:  12226  Learning Rate:  4.906253215634298e-07  Varinance:  9.460780356811424e-07 \n",
      "\n",
      "Epoch:  12227  Learning Rate:  4.901349414727761e-07  Varinance:  9.447563993431268e-07 \n",
      "\n",
      "Epoch:  12228  Learning Rate:  4.896450515171056e-07  Varinance:  9.434366092826317e-07 \n",
      "\n",
      "Epoch:  12229  Learning Rate:  4.891556512065274e-07  Varinance:  9.421186629204751e-07 \n",
      "\n",
      "Epoch:  12230  Learning Rate:  4.886667400516411e-07  Varinance:  9.408025576810781e-07 \n",
      "\n",
      "Epoch:  12231  Learning Rate:  4.881783175635349e-07  Varinance:  9.394882909924596e-07 \n",
      "\n",
      "Epoch:  12232  Learning Rate:  4.876903832537876e-07  Varinance:  9.381758602862317e-07 \n",
      "\n",
      "Epoch:  12233  Learning Rate:  4.872029366344643e-07  Varinance:  9.368652629975973e-07 \n",
      "\n",
      "Epoch:  12234  Learning Rate:  4.867159772181174e-07  Varinance:  9.355564965653327e-07 \n",
      "\n",
      "Epoch:  12235  Learning Rate:  4.862295045177891e-07  Varinance:  9.342495584318021e-07 \n",
      "\n",
      "Epoch:  12236  Learning Rate:  4.857435180470058e-07  Varinance:  9.329444460429394e-07 \n",
      "\n",
      "Epoch:  12237  Learning Rate:  4.852580173197802e-07  Varinance:  9.316411568482458e-07 \n",
      "\n",
      "Epoch:  12238  Learning Rate:  4.847730018506132e-07  Varinance:  9.303396883007858e-07 \n",
      "\n",
      "Epoch:  12239  Learning Rate:  4.842884711544885e-07  Varinance:  9.290400378571824e-07 \n",
      "\n",
      "Epoch:  12240  Learning Rate:  4.838044247468745e-07  Varinance:  9.277422029776143e-07 \n",
      "\n",
      "Epoch:  12241  Learning Rate:  4.833208621437263e-07  Varinance:  9.264461811257984e-07 \n",
      "\n",
      "Epoch:  12242  Learning Rate:  4.828377828614806e-07  Varinance:  9.251519697690045e-07 \n",
      "\n",
      "Epoch:  12243  Learning Rate:  4.823551864170571e-07  Varinance:  9.238595663780379e-07 \n",
      "\n",
      "Epoch:  12244  Learning Rate:  4.818730723278611e-07  Varinance:  9.225689684272362e-07 \n",
      "\n",
      "Epoch:  12245  Learning Rate:  4.813914401117776e-07  Varinance:  9.212801733944657e-07 \n",
      "\n",
      "Epoch:  12246  Learning Rate:  4.809102892871742e-07  Varinance:  9.199931787611163e-07 \n",
      "\n",
      "Epoch:  12247  Learning Rate:  4.804296193728994e-07  Varinance:  9.187079820120991e-07 \n",
      "\n",
      "Epoch:  12248  Learning Rate:  4.799494298882849e-07  Varinance:  9.174245806358287e-07 \n",
      "\n",
      "Epoch:  12249  Learning Rate:  4.794697203531403e-07  Varinance:  9.16142972124239e-07 \n",
      "\n",
      "Epoch:  12250  Learning Rate:  4.789904902877551e-07  Varinance:  9.148631539727635e-07 \n",
      "\n",
      "Epoch:  12251  Learning Rate:  4.785117392129009e-07  Varinance:  9.135851236803347e-07 \n",
      "\n",
      "Epoch:  12252  Learning Rate:  4.780334666498259e-07  Varinance:  9.123088787493792e-07 \n",
      "\n",
      "Epoch:  12253  Learning Rate:  4.775556721202565e-07  Varinance:  9.110344166858122e-07 \n",
      "\n",
      "Epoch:  12254  Learning Rate:  4.770783551463998e-07  Varinance:  9.09761734999037e-07 \n",
      "\n",
      "Epoch:  12255  Learning Rate:  4.7660151525093807e-07  Varinance:  9.084908312019256e-07 \n",
      "\n",
      "Epoch:  12256  Learning Rate:  4.7612515195703045e-07  Varinance:  9.072217028108346e-07 \n",
      "\n",
      "Epoch:  12257  Learning Rate:  4.756492647883153e-07  Varinance:  9.05954347345587e-07 \n",
      "\n",
      "Epoch:  12258  Learning Rate:  4.7517385326890457e-07  Varinance:  9.046887623294704e-07 \n",
      "\n",
      "Epoch:  12259  Learning Rate:  4.7469891692338674e-07  Varinance:  9.034249452892321e-07 \n",
      "\n",
      "Epoch:  12260  Learning Rate:  4.742244552768245e-07  Varinance:  9.021628937550746e-07 \n",
      "\n",
      "Epoch:  12261  Learning Rate:  4.737504678547579e-07  Varinance:  9.009026052606539e-07 \n",
      "\n",
      "Epoch:  12262  Learning Rate:  4.7327695418319864e-07  Varinance:  8.996440773430619e-07 \n",
      "\n",
      "Epoch:  12263  Learning Rate:  4.728039137886322e-07  Varinance:  8.983873075428402e-07 \n",
      "\n",
      "Epoch:  12264  Learning Rate:  4.7233134619801974e-07  Varinance:  8.971322934039633e-07 \n",
      "\n",
      "Epoch:  12265  Learning Rate:  4.7185925093879295e-07  Varinance:  8.958790324738368e-07 \n",
      "\n",
      "Epoch:  12266  Learning Rate:  4.713876275388555e-07  Varinance:  8.946275223032923e-07 \n",
      "\n",
      "Epoch:  12267  Learning Rate:  4.709164755265857e-07  Varinance:  8.933777604465829e-07 \n",
      "\n",
      "Epoch:  12268  Learning Rate:  4.704457944308306e-07  Varinance:  8.921297444613813e-07 \n",
      "\n",
      "Epoch:  12269  Learning Rate:  4.6997558378090847e-07  Varinance:  8.908834719087627e-07 \n",
      "\n",
      "Epoch:  12270  Learning Rate:  4.6950584310661e-07  Varinance:  8.896389403532188e-07 \n",
      "\n",
      "Epoch:  12271  Learning Rate:  4.6903657193819373e-07  Varinance:  8.883961473626406e-07 \n",
      "\n",
      "Epoch:  12272  Learning Rate:  4.685677698063878e-07  Varinance:  8.871550905083164e-07 \n",
      "\n",
      "Epoch:  12273  Learning Rate:  4.6809943624239143e-07  Varinance:  8.859157673649277e-07 \n",
      "\n",
      "Epoch:  12274  Learning Rate:  4.676315707778703e-07  Varinance:  8.846781755105438e-07 \n",
      "\n",
      "Epoch:  12275  Learning Rate:  4.6716417294495896e-07  Varinance:  8.834423125266207e-07 \n",
      "\n",
      "Epoch:  12276  Learning Rate:  4.666972422762587e-07  Varinance:  8.822081759979831e-07 \n",
      "\n",
      "Epoch:  12277  Learning Rate:  4.662307783048404e-07  Varinance:  8.809757635128397e-07 \n",
      "\n",
      "Epoch:  12278  Learning Rate:  4.6576478056423934e-07  Varinance:  8.797450726627648e-07 \n",
      "\n",
      "Epoch:  12279  Learning Rate:  4.652992485884567e-07  Varinance:  8.785161010426972e-07 \n",
      "\n",
      "Epoch:  12280  Learning Rate:  4.6483418191196224e-07  Varinance:  8.77288846250936e-07 \n",
      "\n",
      "Epoch:  12281  Learning Rate:  4.6436958006968856e-07  Varinance:  8.760633058891346e-07 \n",
      "\n",
      "Epoch:  12282  Learning Rate:  4.6390544259703277e-07  Varinance:  8.748394775623004e-07 \n",
      "\n",
      "Epoch:  12283  Learning Rate:  4.63441769029859e-07  Varinance:  8.73617358878777e-07 \n",
      "\n",
      "Epoch:  12284  Learning Rate:  4.6297855890449295e-07  Varinance:  8.723969474502587e-07 \n",
      "\n",
      "Epoch:  12285  Learning Rate:  4.6251581175772353e-07  Varinance:  8.711782408917728e-07 \n",
      "\n",
      "Epoch:  12286  Learning Rate:  4.620535271268052e-07  Varinance:  8.699612368216782e-07 \n",
      "\n",
      "Epoch:  12287  Learning Rate:  4.615917045494526e-07  Varinance:  8.687459328616609e-07 \n",
      "\n",
      "Epoch:  12288  Learning Rate:  4.6113034356384207e-07  Varinance:  8.67532326636733e-07 \n",
      "\n",
      "Epoch:  12289  Learning Rate:  4.606694437086145e-07  Varinance:  8.663204157752141e-07 \n",
      "\n",
      "Epoch:  12290  Learning Rate:  4.602090045228689e-07  Varinance:  8.651101979087472e-07 \n",
      "\n",
      "Epoch:  12291  Learning Rate:  4.5974902554616624e-07  Varinance:  8.639016706722799e-07 \n",
      "\n",
      "Epoch:  12292  Learning Rate:  4.5928950631852657e-07  Varinance:  8.626948317040642e-07 \n",
      "\n",
      "Epoch:  12293  Learning Rate:  4.5883044638043237e-07  Varinance:  8.614896786456514e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12294  Learning Rate:  4.5837184527282276e-07  Varinance:  8.60286209141887e-07 \n",
      "\n",
      "Epoch:  12295  Learning Rate:  4.579137025370958e-07  Varinance:  8.590844208409103e-07 \n",
      "\n",
      "Epoch:  12296  Learning Rate:  4.5745601771511034e-07  Varinance:  8.578843113941361e-07 \n",
      "\n",
      "Epoch:  12297  Learning Rate:  4.5699879034918074e-07  Varinance:  8.5668587845627e-07 \n",
      "\n",
      "Epoch:  12298  Learning Rate:  4.5654201998207875e-07  Varinance:  8.554891196852901e-07 \n",
      "\n",
      "Epoch:  12299  Learning Rate:  4.5608570615703556e-07  Varinance:  8.542940327424472e-07 \n",
      "\n",
      "Epoch:  12300  Learning Rate:  4.5562984841773663e-07  Varinance:  8.531006152922583e-07 \n",
      "\n",
      "Epoch:  12301  Learning Rate:  4.551744463083231e-07  Varinance:  8.519088650025035e-07 \n",
      "\n",
      "Epoch:  12302  Learning Rate:  4.5471949937339475e-07  Varinance:  8.507187795442236e-07 \n",
      "\n",
      "Epoch:  12303  Learning Rate:  4.5426500715800367e-07  Varinance:  8.495303565917042e-07 \n",
      "\n",
      "Epoch:  12304  Learning Rate:  4.5381096920765683e-07  Varinance:  8.483435938224885e-07 \n",
      "\n",
      "Epoch:  12305  Learning Rate:  4.533573850683178e-07  Varinance:  8.471584889173617e-07 \n",
      "\n",
      "Epoch:  12306  Learning Rate:  4.529042542864016e-07  Varinance:  8.45975039560348e-07 \n",
      "\n",
      "Epoch:  12307  Learning Rate:  4.5245157640877736e-07  Varinance:  8.447932434387077e-07 \n",
      "\n",
      "Epoch:  12308  Learning Rate:  4.519993509827665e-07  Varinance:  8.436130982429311e-07 \n",
      "\n",
      "Epoch:  12309  Learning Rate:  4.515475775561451e-07  Varinance:  8.424346016667389e-07 \n",
      "\n",
      "Epoch:  12310  Learning Rate:  4.5109625567713887e-07  Varinance:  8.412577514070635e-07 \n",
      "\n",
      "Epoch:  12311  Learning Rate:  4.506453848944251e-07  Varinance:  8.400825451640644e-07 \n",
      "\n",
      "Epoch:  12312  Learning Rate:  4.501949647571345e-07  Varinance:  8.389089806411103e-07 \n",
      "\n",
      "Epoch:  12313  Learning Rate:  4.497449948148463e-07  Varinance:  8.377370555447787e-07 \n",
      "\n",
      "Epoch:  12314  Learning Rate:  4.492954746175896e-07  Varinance:  8.365667675848503e-07 \n",
      "\n",
      "Epoch:  12315  Learning Rate:  4.488464037158457e-07  Varinance:  8.35398114474306e-07 \n",
      "\n",
      "Epoch:  12316  Learning Rate:  4.483977816605429e-07  Varinance:  8.342310939293239e-07 \n",
      "\n",
      "Epoch:  12317  Learning Rate:  4.4794960800305836e-07  Varinance:  8.330657036692638e-07 \n",
      "\n",
      "Epoch:  12318  Learning Rate:  4.4750188229521994e-07  Varinance:  8.319019414166806e-07 \n",
      "\n",
      "Epoch:  12319  Learning Rate:  4.470546040893011e-07  Varinance:  8.307398048973073e-07 \n",
      "\n",
      "Epoch:  12320  Learning Rate:  4.4660777293802283e-07  Varinance:  8.295792918400543e-07 \n",
      "\n",
      "Epoch:  12321  Learning Rate:  4.4616138839455554e-07  Varinance:  8.284203999770044e-07 \n",
      "\n",
      "Epoch:  12322  Learning Rate:  4.4571545001251376e-07  Varinance:  8.272631270434089e-07 \n",
      "\n",
      "Epoch:  12323  Learning Rate:  4.4526995734595914e-07  Varinance:  8.261074707776853e-07 \n",
      "\n",
      "Epoch:  12324  Learning Rate:  4.4482490994939815e-07  Varinance:  8.249534289214024e-07 \n",
      "\n",
      "Epoch:  12325  Learning Rate:  4.44380307377785e-07  Varinance:  8.238009992192921e-07 \n",
      "\n",
      "Epoch:  12326  Learning Rate:  4.4393614918651633e-07  Varinance:  8.226501794192342e-07 \n",
      "\n",
      "Epoch:  12327  Learning Rate:  4.4349243493143295e-07  Varinance:  8.215009672722544e-07 \n",
      "\n",
      "Epoch:  12328  Learning Rate:  4.4304916416882237e-07  Varinance:  8.203533605325204e-07 \n",
      "\n",
      "Epoch:  12329  Learning Rate:  4.426063364554128e-07  Varinance:  8.192073569573371e-07 \n",
      "\n",
      "Epoch:  12330  Learning Rate:  4.4216395134837573e-07  Varinance:  8.180629543071452e-07 \n",
      "\n",
      "Epoch:  12331  Learning Rate:  4.417220084053277e-07  Varinance:  8.169201503455056e-07 \n",
      "\n",
      "Epoch:  12332  Learning Rate:  4.4128050718432494e-07  Varinance:  8.157789428391115e-07 \n",
      "\n",
      "Epoch:  12333  Learning Rate:  4.4083944724386526e-07  Varinance:  8.146393295577737e-07 \n",
      "\n",
      "Epoch:  12334  Learning Rate:  4.4039882814289047e-07  Varinance:  8.135013082744185e-07 \n",
      "\n",
      "Epoch:  12335  Learning Rate:  4.399586494407804e-07  Varinance:  8.123648767650823e-07 \n",
      "\n",
      "Epoch:  12336  Learning Rate:  4.395189106973557e-07  Varinance:  8.112300328089098e-07 \n",
      "\n",
      "Epoch:  12337  Learning Rate:  4.39079611472879e-07  Varinance:  8.1009677418815e-07 \n",
      "\n",
      "Epoch:  12338  Learning Rate:  4.386407513280505e-07  Varinance:  8.089650986881418e-07 \n",
      "\n",
      "Epoch:  12339  Learning Rate:  4.382023298240099e-07  Varinance:  8.078350040973263e-07 \n",
      "\n",
      "Epoch:  12340  Learning Rate:  4.3776434652233474e-07  Varinance:  8.067064882072312e-07 \n",
      "\n",
      "Epoch:  12341  Learning Rate:  4.3732680098504346e-07  Varinance:  8.055795488124698e-07 \n",
      "\n",
      "Epoch:  12342  Learning Rate:  4.368896927745896e-07  Varinance:  8.044541837107355e-07 \n",
      "\n",
      "Epoch:  12343  Learning Rate:  4.3645302145386404e-07  Varinance:  8.03330390702799e-07 \n",
      "\n",
      "Epoch:  12344  Learning Rate:  4.3601678658619725e-07  Varinance:  8.022081675925056e-07 \n",
      "\n",
      "Epoch:  12345  Learning Rate:  4.355809877353532e-07  Varinance:  8.010875121867601e-07 \n",
      "\n",
      "Epoch:  12346  Learning Rate:  4.3514562446553247e-07  Varinance:  7.999684222955394e-07 \n",
      "\n",
      "Epoch:  12347  Learning Rate:  4.3471069634137327e-07  Varinance:  7.988508957318776e-07 \n",
      "\n",
      "Epoch:  12348  Learning Rate:  4.342762029279467e-07  Varinance:  7.977349303118628e-07 \n",
      "\n",
      "Epoch:  12349  Learning Rate:  4.3384214379075837e-07  Varinance:  7.966205238546349e-07 \n",
      "\n",
      "Epoch:  12350  Learning Rate:  4.3340851849575085e-07  Varinance:  7.955076741823801e-07 \n",
      "\n",
      "Epoch:  12351  Learning Rate:  4.3297532660929784e-07  Varinance:  7.943963791203295e-07 \n",
      "\n",
      "Epoch:  12352  Learning Rate:  4.3254256769820685e-07  Varinance:  7.932866364967439e-07 \n",
      "\n",
      "Epoch:  12353  Learning Rate:  4.3211024132972033e-07  Varinance:  7.921784441429265e-07 \n",
      "\n",
      "Epoch:  12354  Learning Rate:  4.316783470715111e-07  Varinance:  7.910717998932077e-07 \n",
      "\n",
      "Epoch:  12355  Learning Rate:  4.3124688449168497e-07  Varinance:  7.899667015849422e-07 \n",
      "\n",
      "Epoch:  12356  Learning Rate:  4.308158531587784e-07  Varinance:  7.888631470585069e-07 \n",
      "\n",
      "Epoch:  12357  Learning Rate:  4.3038525264176183e-07  Varinance:  7.877611341572949e-07 \n",
      "\n",
      "Epoch:  12358  Learning Rate:  4.2995508251003366e-07  Varinance:  7.866606607277152e-07 \n",
      "\n",
      "Epoch:  12359  Learning Rate:  4.2952534233342314e-07  Varinance:  7.855617246191766e-07 \n",
      "\n",
      "Epoch:  12360  Learning Rate:  4.290960316821914e-07  Varinance:  7.844643236841012e-07 \n",
      "\n",
      "Epoch:  12361  Learning Rate:  4.286671501270272e-07  Varinance:  7.833684557779074e-07 \n",
      "\n",
      "Epoch:  12362  Learning Rate:  4.28238697239048e-07  Varinance:  7.822741187590104e-07 \n",
      "\n",
      "Epoch:  12363  Learning Rate:  4.2781067258980256e-07  Varinance:  7.811813104888167e-07 \n",
      "\n",
      "Epoch:  12364  Learning Rate:  4.2738307575126533e-07  Varinance:  7.800900288317202e-07 \n",
      "\n",
      "Epoch:  12365  Learning Rate:  4.2695590629583873e-07  Varinance:  7.790002716551012e-07 \n",
      "\n",
      "Epoch:  12366  Learning Rate:  4.2652916379635473e-07  Varinance:  7.77912036829311e-07 \n",
      "\n",
      "Epoch:  12367  Learning Rate:  4.2610284782607003e-07  Varinance:  7.768253222276839e-07 \n",
      "\n",
      "Epoch:  12368  Learning Rate:  4.2567695795866804e-07  Varinance:  7.757401257265226e-07 \n",
      "\n",
      "Epoch:  12369  Learning Rate:  4.252514937682601e-07  Varinance:  7.746564452050962e-07 \n",
      "\n",
      "Epoch:  12370  Learning Rate:  4.248264548293815e-07  Varinance:  7.735742785456366e-07 \n",
      "\n",
      "Epoch:  12371  Learning Rate:  4.2440184071699303e-07  Varinance:  7.724936236333343e-07 \n",
      "\n",
      "Epoch:  12372  Learning Rate:  4.239776510064799e-07  Varinance:  7.714144783563367e-07 \n",
      "\n",
      "Epoch:  12373  Learning Rate:  4.2355388527365386e-07  Varinance:  7.703368406057329e-07 \n",
      "\n",
      "Epoch:  12374  Learning Rate:  4.2313054309474843e-07  Varinance:  7.692607082755665e-07 \n",
      "\n",
      "Epoch:  12375  Learning Rate:  4.2270762404642054e-07  Varinance:  7.681860792628207e-07 \n",
      "\n",
      "Epoch:  12376  Learning Rate:  4.2228512770575273e-07  Varinance:  7.671129514674158e-07 \n",
      "\n",
      "Epoch:  12377  Learning Rate:  4.218630536502478e-07  Varinance:  7.660413227922063e-07 \n",
      "\n",
      "Epoch:  12378  Learning Rate:  4.2144140145783094e-07  Varinance:  7.649711911429763e-07 \n",
      "\n",
      "Epoch:  12379  Learning Rate:  4.2102017070685146e-07  Varinance:  7.639025544284381e-07 \n",
      "\n",
      "Epoch:  12380  Learning Rate:  4.205993609760776e-07  Varinance:  7.62835410560217e-07 \n",
      "\n",
      "Epoch:  12381  Learning Rate:  4.201789718446992e-07  Varinance:  7.617697574528647e-07 \n",
      "\n",
      "Epoch:  12382  Learning Rate:  4.197590028923283e-07  Varinance:  7.607055930238425e-07 \n",
      "\n",
      "Epoch:  12383  Learning Rate:  4.1933945369899533e-07  Varinance:  7.596429151935214e-07 \n",
      "\n",
      "Epoch:  12384  Learning Rate:  4.189203238451509e-07  Varinance:  7.585817218851777e-07 \n",
      "\n",
      "Epoch:  12385  Learning Rate:  4.185016129116646e-07  Varinance:  7.575220110249884e-07 \n",
      "\n",
      "Epoch:  12386  Learning Rate:  4.180833204798268e-07  Varinance:  7.564637805420306e-07 \n",
      "\n",
      "Epoch:  12387  Learning Rate:  4.176654461313443e-07  Varinance:  7.554070283682659e-07 \n",
      "\n",
      "Epoch:  12388  Learning Rate:  4.17247989448342e-07  Varinance:  7.543517524385534e-07 \n",
      "\n",
      "Epoch:  12389  Learning Rate:  4.1683095001336466e-07  Varinance:  7.532979506906345e-07 \n",
      "\n",
      "Epoch:  12390  Learning Rate:  4.1641432740937206e-07  Varinance:  7.52245621065131e-07 \n",
      "\n",
      "Epoch:  12391  Learning Rate:  4.1599812121974087e-07  Varinance:  7.511947615055418e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12392  Learning Rate:  4.155823310282663e-07  Varinance:  7.501453699582389e-07 \n",
      "\n",
      "Epoch:  12393  Learning Rate:  4.151669564191573e-07  Varinance:  7.490974443724656e-07 \n",
      "\n",
      "Epoch:  12394  Learning Rate:  4.147519969770387e-07  Varinance:  7.480509827003219e-07 \n",
      "\n",
      "Epoch:  12395  Learning Rate:  4.1433745228695233e-07  Varinance:  7.470059828967769e-07 \n",
      "\n",
      "Epoch:  12396  Learning Rate:  4.1392332193435275e-07  Varinance:  7.459624429196537e-07 \n",
      "\n",
      "Epoch:  12397  Learning Rate:  4.1350960550510893e-07  Varinance:  7.449203607296283e-07 \n",
      "\n",
      "Epoch:  12398  Learning Rate:  4.130963025855057e-07  Varinance:  7.438797342902255e-07 \n",
      "\n",
      "Epoch:  12399  Learning Rate:  4.1268341276223955e-07  Varinance:  7.428405615678153e-07 \n",
      "\n",
      "Epoch:  12400  Learning Rate:  4.1227093562242054e-07  Varinance:  7.418028405316107e-07 \n",
      "\n",
      "Epoch:  12401  Learning Rate:  4.118588707535708e-07  Varinance:  7.40766569153654e-07 \n",
      "\n",
      "Epoch:  12402  Learning Rate:  4.114472177436268e-07  Varinance:  7.397317454088285e-07 \n",
      "\n",
      "Epoch:  12403  Learning Rate:  4.1103597618093494e-07  Varinance:  7.38698367274844e-07 \n",
      "\n",
      "Epoch:  12404  Learning Rate:  4.1062514565425273e-07  Varinance:  7.376664327322347e-07 \n",
      "\n",
      "Epoch:  12405  Learning Rate:  4.102147257527511e-07  Varinance:  7.366359397643567e-07 \n",
      "\n",
      "Epoch:  12406  Learning Rate:  4.098047160660094e-07  Varinance:  7.356068863573828e-07 \n",
      "\n",
      "Epoch:  12407  Learning Rate:  4.0939511618401724e-07  Varinance:  7.345792705003018e-07 \n",
      "\n",
      "Epoch:  12408  Learning Rate:  4.0898592569717604e-07  Varinance:  7.33553090184904e-07 \n",
      "\n",
      "Epoch:  12409  Learning Rate:  4.085771441962947e-07  Varinance:  7.325283434057929e-07 \n",
      "\n",
      "Epoch:  12410  Learning Rate:  4.081687712725908e-07  Varinance:  7.315050281603708e-07 \n",
      "\n",
      "Epoch:  12411  Learning Rate:  4.07760806517693e-07  Varinance:  7.304831424488377e-07 \n",
      "\n",
      "Epoch:  12412  Learning Rate:  4.073532495236356e-07  Varinance:  7.294626842741871e-07 \n",
      "\n",
      "Epoch:  12413  Learning Rate:  4.06946099882861e-07  Varinance:  7.284436516422023e-07 \n",
      "\n",
      "Epoch:  12414  Learning Rate:  4.065393571882209e-07  Varinance:  7.27426042561455e-07 \n",
      "\n",
      "Epoch:  12415  Learning Rate:  4.0613302103297187e-07  Varinance:  7.264098550432911e-07 \n",
      "\n",
      "Epoch:  12416  Learning Rate:  4.0572709101077776e-07  Varinance:  7.253950871018425e-07 \n",
      "\n",
      "Epoch:  12417  Learning Rate:  4.0532156671570763e-07  Varinance:  7.243817367540126e-07 \n",
      "\n",
      "Epoch:  12418  Learning Rate:  4.0491644774223886e-07  Varinance:  7.233698020194748e-07 \n",
      "\n",
      "Epoch:  12419  Learning Rate:  4.045117336852515e-07  Varinance:  7.223592809206695e-07 \n",
      "\n",
      "Epoch:  12420  Learning Rate:  4.041074241400308e-07  Varinance:  7.213501714828019e-07 \n",
      "\n",
      "Epoch:  12421  Learning Rate:  4.0370351870226866e-07  Varinance:  7.203424717338281e-07 \n",
      "\n",
      "Epoch:  12422  Learning Rate:  4.033000169680589e-07  Varinance:  7.19336179704467e-07 \n",
      "\n",
      "Epoch:  12423  Learning Rate:  4.02896918533899e-07  Varinance:  7.183312934281863e-07 \n",
      "\n",
      "Epoch:  12424  Learning Rate:  4.0249422299669177e-07  Varinance:  7.173278109411999e-07 \n",
      "\n",
      "Epoch:  12425  Learning Rate:  4.020919299537413e-07  Varinance:  7.163257302824659e-07 \n",
      "\n",
      "Epoch:  12426  Learning Rate:  4.016900390027534e-07  Varinance:  7.153250494936816e-07 \n",
      "\n",
      "Epoch:  12427  Learning Rate:  4.012885497418388e-07  Varinance:  7.143257666192826e-07 \n",
      "\n",
      "Epoch:  12428  Learning Rate:  4.0088746176950734e-07  Varinance:  7.133278797064285e-07 \n",
      "\n",
      "Epoch:  12429  Learning Rate:  4.004867746846704e-07  Varinance:  7.123313868050148e-07 \n",
      "\n",
      "Epoch:  12430  Learning Rate:  4.0008648808664215e-07  Varinance:  7.113362859676586e-07 \n",
      "\n",
      "Epoch:  12431  Learning Rate:  3.996866015751353e-07  Varinance:  7.103425752496975e-07 \n",
      "\n",
      "Epoch:  12432  Learning Rate:  3.9928711475026344e-07  Varinance:  7.093502527091856e-07 \n",
      "\n",
      "Epoch:  12433  Learning Rate:  3.9888802721253887e-07  Varinance:  7.083593164068899e-07 \n",
      "\n",
      "Epoch:  12434  Learning Rate:  3.984893385628754e-07  Varinance:  7.07369764406289e-07 \n",
      "\n",
      "Epoch:  12435  Learning Rate:  3.980910484025837e-07  Varinance:  7.063815947735592e-07 \n",
      "\n",
      "Epoch:  12436  Learning Rate:  3.97693156333373e-07  Varinance:  7.053948055775854e-07 \n",
      "\n",
      "Epoch:  12437  Learning Rate:  3.972956619573523e-07  Varinance:  7.044093948899485e-07 \n",
      "\n",
      "Epoch:  12438  Learning Rate:  3.9689856487702685e-07  Varinance:  7.034253607849226e-07 \n",
      "\n",
      "Epoch:  12439  Learning Rate:  3.9650186469529845e-07  Varinance:  7.024427013394723e-07 \n",
      "\n",
      "Epoch:  12440  Learning Rate:  3.961055610154686e-07  Varinance:  7.014614146332486e-07 \n",
      "\n",
      "Epoch:  12441  Learning Rate:  3.9570965344123286e-07  Varinance:  7.004814987485876e-07 \n",
      "\n",
      "Epoch:  12442  Learning Rate:  3.953141415766827e-07  Varinance:  6.995029517704969e-07 \n",
      "\n",
      "Epoch:  12443  Learning Rate:  3.949190250263078e-07  Varinance:  6.985257717866667e-07 \n",
      "\n",
      "Epoch:  12444  Learning Rate:  3.9452430339499085e-07  Varinance:  6.97549956887456e-07 \n",
      "\n",
      "Epoch:  12445  Learning Rate:  3.9412997628800943e-07  Varinance:  6.965755051658915e-07 \n",
      "\n",
      "Epoch:  12446  Learning Rate:  3.9373604331103783e-07  Varinance:  6.956024147176644e-07 \n",
      "\n",
      "Epoch:  12447  Learning Rate:  3.9334250407014246e-07  Varinance:  6.946306836411255e-07 \n",
      "\n",
      "Epoch:  12448  Learning Rate:  3.929493581717838e-07  Varinance:  6.936603100372847e-07 \n",
      "\n",
      "Epoch:  12449  Learning Rate:  3.925566052228155e-07  Varinance:  6.926912920097979e-07 \n",
      "\n",
      "Epoch:  12450  Learning Rate:  3.921642448304857e-07  Varinance:  6.917236276649767e-07 \n",
      "\n",
      "Epoch:  12451  Learning Rate:  3.917722766024335e-07  Varinance:  6.90757315111776e-07 \n",
      "\n",
      "Epoch:  12452  Learning Rate:  3.9138070014668985e-07  Varinance:  6.897923524617928e-07 \n",
      "\n",
      "Epoch:  12453  Learning Rate:  3.9098951507167966e-07  Varinance:  6.888287378292615e-07 \n",
      "\n",
      "Epoch:  12454  Learning Rate:  3.9059872098621706e-07  Varinance:  6.878664693310513e-07 \n",
      "\n",
      "Epoch:  12455  Learning Rate:  3.902083174995074e-07  Varinance:  6.869055450866642e-07 \n",
      "\n",
      "Epoch:  12456  Learning Rate:  3.8981830422114836e-07  Varinance:  6.859459632182221e-07 \n",
      "\n",
      "Epoch:  12457  Learning Rate:  3.894286807611261e-07  Varinance:  6.849877218504773e-07 \n",
      "\n",
      "Epoch:  12458  Learning Rate:  3.890394467298163e-07  Varinance:  6.840308191107998e-07 \n",
      "\n",
      "Epoch:  12459  Learning Rate:  3.8865060173798633e-07  Varinance:  6.830752531291747e-07 \n",
      "\n",
      "Epoch:  12460  Learning Rate:  3.8826214539679053e-07  Varinance:  6.821210220382006e-07 \n",
      "\n",
      "Epoch:  12461  Learning Rate:  3.8787407731777183e-07  Varinance:  6.811681239730839e-07 \n",
      "\n",
      "Epoch:  12462  Learning Rate:  3.8748639711286337e-07  Varinance:  6.802165570716386e-07 \n",
      "\n",
      "Epoch:  12463  Learning Rate:  3.8709910439438436e-07  Varinance:  6.792663194742733e-07 \n",
      "\n",
      "Epoch:  12464  Learning Rate:  3.8671219877504196e-07  Varinance:  6.783174093240012e-07 \n",
      "\n",
      "Epoch:  12465  Learning Rate:  3.8632567986793e-07  Varinance:  6.773698247664274e-07 \n",
      "\n",
      "Epoch:  12466  Learning Rate:  3.859395472865307e-07  Varinance:  6.764235639497474e-07 \n",
      "\n",
      "Epoch:  12467  Learning Rate:  3.855538006447108e-07  Varinance:  6.754786250247438e-07 \n",
      "\n",
      "Epoch:  12468  Learning Rate:  3.8516843955672305e-07  Varinance:  6.745350061447825e-07 \n",
      "\n",
      "Epoch:  12469  Learning Rate:  3.847834636372076e-07  Varinance:  6.735927054658111e-07 \n",
      "\n",
      "Epoch:  12470  Learning Rate:  3.8439887250118787e-07  Varinance:  6.726517211463464e-07 \n",
      "\n",
      "Epoch:  12471  Learning Rate:  3.84014665764072e-07  Varinance:  6.717120513474849e-07 \n",
      "\n",
      "Epoch:  12472  Learning Rate:  3.836308430416546e-07  Varinance:  6.707736942328895e-07 \n",
      "\n",
      "Epoch:  12473  Learning Rate:  3.8324740395011214e-07  Varinance:  6.698366479687884e-07 \n",
      "\n",
      "Epoch:  12474  Learning Rate:  3.8286434810600493e-07  Varinance:  6.689009107239714e-07 \n",
      "\n",
      "Epoch:  12475  Learning Rate:  3.8248167512627843e-07  Varinance:  6.679664806697866e-07 \n",
      "\n",
      "Epoch:  12476  Learning Rate:  3.8209938462825895e-07  Varinance:  6.67033355980139e-07 \n",
      "\n",
      "Epoch:  12477  Learning Rate:  3.8171747622965515e-07  Varinance:  6.661015348314775e-07 \n",
      "\n",
      "Epoch:  12478  Learning Rate:  3.813359495485602e-07  Varinance:  6.651710154028053e-07 \n",
      "\n",
      "Epoch:  12479  Learning Rate:  3.809548042034465e-07  Varinance:  6.642417958756676e-07 \n",
      "\n",
      "Epoch:  12480  Learning Rate:  3.8057403981316874e-07  Varinance:  6.633138744341493e-07 \n",
      "\n",
      "Epoch:  12481  Learning Rate:  3.801936559969619e-07  Varinance:  6.623872492648724e-07 \n",
      "\n",
      "Epoch:  12482  Learning Rate:  3.798136523744434e-07  Varinance:  6.614619185569923e-07 \n",
      "\n",
      "Epoch:  12483  Learning Rate:  3.794340285656089e-07  Varinance:  6.605378805021961e-07 \n",
      "\n",
      "Epoch:  12484  Learning Rate:  3.790547841908339e-07  Varinance:  6.596151332946901e-07 \n",
      "\n",
      "Epoch:  12485  Learning Rate:  3.7867591887087535e-07  Varinance:  6.586936751312103e-07 \n",
      "\n",
      "Epoch:  12486  Learning Rate:  3.7829743222686723e-07  Varinance:  6.577735042110096e-07 \n",
      "\n",
      "Epoch:  12487  Learning Rate:  3.779193238803222e-07  Varinance:  6.568546187358561e-07 \n",
      "\n",
      "Epoch:  12488  Learning Rate:  3.7754159345313323e-07  Varinance:  6.559370169100302e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12489  Learning Rate:  3.7716424056756914e-07  Varinance:  6.550206969403211e-07 \n",
      "\n",
      "Epoch:  12490  Learning Rate:  3.7678726484627644e-07  Varinance:  6.541056570360248e-07 \n",
      "\n",
      "Epoch:  12491  Learning Rate:  3.764106659122806e-07  Varinance:  6.531918954089325e-07 \n",
      "\n",
      "Epoch:  12492  Learning Rate:  3.760344433889821e-07  Varinance:  6.522794102733398e-07 \n",
      "\n",
      "Epoch:  12493  Learning Rate:  3.756585969001576e-07  Varinance:  6.51368199846035e-07 \n",
      "\n",
      "Epoch:  12494  Learning Rate:  3.75283126069962e-07  Varinance:  6.504582623462974e-07 \n",
      "\n",
      "Epoch:  12495  Learning Rate:  3.749080305229237e-07  Varinance:  6.495495959958939e-07 \n",
      "\n",
      "Epoch:  12496  Learning Rate:  3.7453330988394716e-07  Varinance:  6.486421990190753e-07 \n",
      "\n",
      "Epoch:  12497  Learning Rate:  3.741589637783111e-07  Varinance:  6.477360696425755e-07 \n",
      "\n",
      "Epoch:  12498  Learning Rate:  3.7378499183167063e-07  Varinance:  6.46831206095599e-07 \n",
      "\n",
      "Epoch:  12499  Learning Rate:  3.7341139367005317e-07  Varinance:  6.459276066098305e-07 \n",
      "\n",
      "Epoch:  12500  Learning Rate:  3.730381689198598e-07  Varinance:  6.450252694194227e-07 \n",
      "\n",
      "Epoch:  12501  Learning Rate:  3.726653172078671e-07  Varinance:  6.441241927609955e-07 \n",
      "\n",
      "Epoch:  12502  Learning Rate:  3.722928381612227e-07  Varinance:  6.432243748736316e-07 \n",
      "\n",
      "Epoch:  12503  Learning Rate:  3.719207314074468e-07  Varinance:  6.423258139988742e-07 \n",
      "\n",
      "Epoch:  12504  Learning Rate:  3.71548996574434e-07  Varinance:  6.414285083807251e-07 \n",
      "\n",
      "Epoch:  12505  Learning Rate:  3.7117763329044867e-07  Varinance:  6.405324562656321e-07 \n",
      "\n",
      "Epoch:  12506  Learning Rate:  3.708066411841269e-07  Varinance:  6.396376559024997e-07 \n",
      "\n",
      "Epoch:  12507  Learning Rate:  3.7043601988447793e-07  Varinance:  6.387441055426763e-07 \n",
      "\n",
      "Epoch:  12508  Learning Rate:  3.700657690208797e-07  Varinance:  6.378518034399532e-07 \n",
      "\n",
      "Epoch:  12509  Learning Rate:  3.696958882230813e-07  Varinance:  6.369607478505608e-07 \n",
      "\n",
      "Epoch:  12510  Learning Rate:  3.6932637712120133e-07  Varinance:  6.360709370331657e-07 \n",
      "\n",
      "Epoch:  12511  Learning Rate:  3.689572353457299e-07  Varinance:  6.351823692488695e-07 \n",
      "\n",
      "Epoch:  12512  Learning Rate:  3.685884625275245e-07  Varinance:  6.342950427611958e-07 \n",
      "\n",
      "Epoch:  12513  Learning Rate:  3.6822005829781175e-07  Varinance:  6.334089558361011e-07 \n",
      "\n",
      "Epoch:  12514  Learning Rate:  3.6785202228818864e-07  Varinance:  6.325241067419618e-07 \n",
      "\n",
      "Epoch:  12515  Learning Rate:  3.674843541306185e-07  Varinance:  6.316404937495735e-07 \n",
      "\n",
      "Epoch:  12516  Learning Rate:  3.671170534574324e-07  Varinance:  6.307581151321474e-07 \n",
      "\n",
      "Epoch:  12517  Learning Rate:  3.6675011990133097e-07  Varinance:  6.298769691653067e-07 \n",
      "\n",
      "Epoch:  12518  Learning Rate:  3.663835530953801e-07  Varinance:  6.289970541270863e-07 \n",
      "\n",
      "Epoch:  12519  Learning Rate:  3.6601735267301217e-07  Varinance:  6.281183682979194e-07 \n",
      "\n",
      "Epoch:  12520  Learning Rate:  3.65651518268028e-07  Varinance:  6.272409099606481e-07 \n",
      "\n",
      "Epoch:  12521  Learning Rate:  3.6528604951459266e-07  Varinance:  6.263646774005114e-07 \n",
      "\n",
      "Epoch:  12522  Learning Rate:  3.6492094604723654e-07  Varinance:  6.254896689051435e-07 \n",
      "\n",
      "Epoch:  12523  Learning Rate:  3.645562075008576e-07  Varinance:  6.246158827645709e-07 \n",
      "\n",
      "Epoch:  12524  Learning Rate:  3.641918335107165e-07  Varinance:  6.237433172712086e-07 \n",
      "\n",
      "Epoch:  12525  Learning Rate:  3.638278237124393e-07  Varinance:  6.228719707198594e-07 \n",
      "\n",
      "Epoch:  12526  Learning Rate:  3.6346417774201543e-07  Varinance:  6.220018414077017e-07 \n",
      "\n",
      "Epoch:  12527  Learning Rate:  3.631008952358003e-07  Varinance:  6.21132927634299e-07 \n",
      "\n",
      "Epoch:  12528  Learning Rate:  3.627379758305106e-07  Varinance:  6.202652277015885e-07 \n",
      "\n",
      "Epoch:  12529  Learning Rate:  3.6237541916322637e-07  Varinance:  6.193987399138788e-07 \n",
      "\n",
      "Epoch:  12530  Learning Rate:  3.620132248713921e-07  Varinance:  6.185334625778482e-07 \n",
      "\n",
      "Epoch:  12531  Learning Rate:  3.616513925928129e-07  Varinance:  6.176693940025399e-07 \n",
      "\n",
      "Epoch:  12532  Learning Rate:  3.6128992196565574e-07  Varinance:  6.168065324993617e-07 \n",
      "\n",
      "Epoch:  12533  Learning Rate:  3.6092881262845135e-07  Varinance:  6.159448763820735e-07 \n",
      "\n",
      "Epoch:  12534  Learning Rate:  3.605680642200896e-07  Varinance:  6.150844239667979e-07 \n",
      "\n",
      "Epoch:  12535  Learning Rate:  3.6020767637982155e-07  Varinance:  6.142251735720071e-07 \n",
      "\n",
      "Epoch:  12536  Learning Rate:  3.5984764874726054e-07  Varinance:  6.133671235185225e-07 \n",
      "\n",
      "Epoch:  12537  Learning Rate:  3.594879809623782e-07  Varinance:  6.125102721295115e-07 \n",
      "\n",
      "Epoch:  12538  Learning Rate:  3.5912867266550623e-07  Varinance:  6.116546177304837e-07 \n",
      "\n",
      "Epoch:  12539  Learning Rate:  3.587697234973374e-07  Varinance:  6.1080015864929e-07 \n",
      "\n",
      "Epoch:  12540  Learning Rate:  3.5841113309892204e-07  Varinance:  6.09946893216111e-07 \n",
      "\n",
      "Epoch:  12541  Learning Rate:  3.580529011116696e-07  Varinance:  6.090948197634663e-07 \n",
      "\n",
      "Epoch:  12542  Learning Rate:  3.5769502717734746e-07  Varinance:  6.082439366262029e-07 \n",
      "\n",
      "Epoch:  12543  Learning Rate:  3.5733751093808297e-07  Varinance:  6.073942421414935e-07 \n",
      "\n",
      "Epoch:  12544  Learning Rate:  3.5698035203635917e-07  Varinance:  6.065457346488344e-07 \n",
      "\n",
      "Epoch:  12545  Learning Rate:  3.566235501150165e-07  Varinance:  6.056984124900411e-07 \n",
      "\n",
      "Epoch:  12546  Learning Rate:  3.562671048172544e-07  Varinance:  6.048522740092477e-07 \n",
      "\n",
      "Epoch:  12547  Learning Rate:  3.5591101578662673e-07  Varinance:  6.040073175528954e-07 \n",
      "\n",
      "Epoch:  12548  Learning Rate:  3.5555528266704387e-07  Varinance:  6.031635414697412e-07 \n",
      "\n",
      "Epoch:  12549  Learning Rate:  3.55199905102774e-07  Varinance:  6.023209441108471e-07 \n",
      "\n",
      "Epoch:  12550  Learning Rate:  3.5484488273843876e-07  Varinance:  6.014795238295786e-07 \n",
      "\n",
      "Epoch:  12551  Learning Rate:  3.544902152190152e-07  Varinance:  6.006392789816014e-07 \n",
      "\n",
      "Epoch:  12552  Learning Rate:  3.541359021898371e-07  Varinance:  5.998002079248781e-07 \n",
      "\n",
      "Epoch:  12553  Learning Rate:  3.5378194329659064e-07  Varinance:  5.989623090196678e-07 \n",
      "\n",
      "Epoch:  12554  Learning Rate:  3.5342833818531637e-07  Varinance:  5.981255806285132e-07 \n",
      "\n",
      "Epoch:  12555  Learning Rate:  3.530750865024103e-07  Varinance:  5.972900211162515e-07 \n",
      "\n",
      "Epoch:  12556  Learning Rate:  3.527221878946202e-07  Varinance:  5.964556288500014e-07 \n",
      "\n",
      "Epoch:  12557  Learning Rate:  3.5236964200904737e-07  Varinance:  5.956224021991632e-07 \n",
      "\n",
      "Epoch:  12558  Learning Rate:  3.520174484931453e-07  Varinance:  5.947903395354149e-07 \n",
      "\n",
      "Epoch:  12559  Learning Rate:  3.5166560699472174e-07  Varinance:  5.939594392327114e-07 \n",
      "\n",
      "Epoch:  12560  Learning Rate:  3.5131411716193443e-07  Varinance:  5.931296996672726e-07 \n",
      "\n",
      "Epoch:  12561  Learning Rate:  3.509629786432929e-07  Varinance:  5.923011192175931e-07 \n",
      "\n",
      "Epoch:  12562  Learning Rate:  3.506121910876599e-07  Varinance:  5.914736962644308e-07 \n",
      "\n",
      "Epoch:  12563  Learning Rate:  3.502617541442472e-07  Varinance:  5.906474291908053e-07 \n",
      "\n",
      "Epoch:  12564  Learning Rate:  3.4991166746261726e-07  Varinance:  5.898223163819957e-07 \n",
      "\n",
      "Epoch:  12565  Learning Rate:  3.495619306926845e-07  Varinance:  5.889983562255358e-07 \n",
      "\n",
      "Epoch:  12566  Learning Rate:  3.4921254348471164e-07  Varinance:  5.881755471112151e-07 \n",
      "\n",
      "Epoch:  12567  Learning Rate:  3.488635054893107e-07  Varinance:  5.873538874310653e-07 \n",
      "\n",
      "Epoch:  12568  Learning Rate:  3.4851481635744493e-07  Varinance:  5.865333755793714e-07 \n",
      "\n",
      "Epoch:  12569  Learning Rate:  3.481664757404246e-07  Varinance:  5.857140099526591e-07 \n",
      "\n",
      "Epoch:  12570  Learning Rate:  3.4781848328990836e-07  Varinance:  5.84895788949694e-07 \n",
      "\n",
      "Epoch:  12571  Learning Rate:  3.4747083865790504e-07  Varinance:  5.840787109714788e-07 \n",
      "\n",
      "Epoch:  12572  Learning Rate:  3.4712354149676935e-07  Varinance:  5.832627744212497e-07 \n",
      "\n",
      "Epoch:  12573  Learning Rate:  3.46776591459204e-07  Varinance:  5.824479777044757e-07 \n",
      "\n",
      "Epoch:  12574  Learning Rate:  3.4642998819825846e-07  Varinance:  5.816343192288472e-07 \n",
      "\n",
      "Epoch:  12575  Learning Rate:  3.460837313673306e-07  Varinance:  5.80821797404285e-07 \n",
      "\n",
      "Epoch:  12576  Learning Rate:  3.457378206201629e-07  Varinance:  5.800104106429293e-07 \n",
      "\n",
      "Epoch:  12577  Learning Rate:  3.453922556108441e-07  Varinance:  5.792001573591384e-07 \n",
      "\n",
      "Epoch:  12578  Learning Rate:  3.450470359938103e-07  Varinance:  5.783910359694858e-07 \n",
      "\n",
      "Epoch:  12579  Learning Rate:  3.4470216142384117e-07  Varinance:  5.775830448927566e-07 \n",
      "\n",
      "Epoch:  12580  Learning Rate:  3.4435763155606165e-07  Varinance:  5.767761825499474e-07 \n",
      "\n",
      "Epoch:  12581  Learning Rate:  3.4401344604594294e-07  Varinance:  5.75970447364254e-07 \n",
      "\n",
      "Epoch:  12582  Learning Rate:  3.4366960454929896e-07  Varinance:  5.751658377610814e-07 \n",
      "\n",
      "Epoch:  12583  Learning Rate:  3.433261067222876e-07  Varinance:  5.743623521680321e-07 \n",
      "\n",
      "Epoch:  12584  Learning Rate:  3.429829522214121e-07  Varinance:  5.735599890149049e-07 \n",
      "\n",
      "Epoch:  12585  Learning Rate:  3.4264014070351744e-07  Varinance:  5.727587467336927e-07 \n",
      "\n",
      "Epoch:  12586  Learning Rate:  3.422976718257915e-07  Varinance:  5.719586237585784e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12587  Learning Rate:  3.4195554524576646e-07  Varinance:  5.711596185259344e-07 \n",
      "\n",
      "Epoch:  12588  Learning Rate:  3.4161376062131516e-07  Varinance:  5.703617294743113e-07 \n",
      "\n",
      "Epoch:  12589  Learning Rate:  3.412723176106529e-07  Varinance:  5.695649550444474e-07 \n",
      "\n",
      "Epoch:  12590  Learning Rate:  3.409312158723362e-07  Varinance:  5.687692936792566e-07 \n",
      "\n",
      "Epoch:  12591  Learning Rate:  3.405904550652643e-07  Varinance:  5.679747438238288e-07 \n",
      "\n",
      "Epoch:  12592  Learning Rate:  3.4025003484867583e-07  Varinance:  5.671813039254252e-07 \n",
      "\n",
      "Epoch:  12593  Learning Rate:  3.3990995488215e-07  Varinance:  5.663889724334766e-07 \n",
      "\n",
      "Epoch:  12594  Learning Rate:  3.39570214825608e-07  Varinance:  5.655977477995821e-07 \n",
      "\n",
      "Epoch:  12595  Learning Rate:  3.392308143393091e-07  Varinance:  5.648076284774973e-07 \n",
      "\n",
      "Epoch:  12596  Learning Rate:  3.388917530838522e-07  Varinance:  5.640186129231441e-07 \n",
      "\n",
      "Epoch:  12597  Learning Rate:  3.385530307201772e-07  Varinance:  5.632306995945996e-07 \n",
      "\n",
      "Epoch:  12598  Learning Rate:  3.382146469095612e-07  Varinance:  5.624438869520946e-07 \n",
      "\n",
      "Epoch:  12599  Learning Rate:  3.378766013136197e-07  Varinance:  5.616581734580112e-07 \n",
      "\n",
      "Epoch:  12600  Learning Rate:  3.375388935943082e-07  Varinance:  5.608735575768792e-07 \n",
      "\n",
      "Epoch:  12601  Learning Rate:  3.3720152341391847e-07  Varinance:  5.600900377753756e-07 \n",
      "\n",
      "Epoch:  12602  Learning Rate:  3.3686449043507963e-07  Varinance:  5.593076125223133e-07 \n",
      "\n",
      "Epoch:  12603  Learning Rate:  3.365277943207599e-07  Varinance:  5.585262802886504e-07 \n",
      "\n",
      "Epoch:  12604  Learning Rate:  3.3619143473426254e-07  Varinance:  5.577460395474786e-07 \n",
      "\n",
      "Epoch:  12605  Learning Rate:  3.3585541133922793e-07  Varinance:  5.56966888774023e-07 \n",
      "\n",
      "Epoch:  12606  Learning Rate:  3.3551972379963206e-07  Varinance:  5.561888264456387e-07 \n",
      "\n",
      "Epoch:  12607  Learning Rate:  3.351843717797885e-07  Varinance:  5.55411851041808e-07 \n",
      "\n",
      "Epoch:  12608  Learning Rate:  3.3484935494434475e-07  Varinance:  5.546359610441387e-07 \n",
      "\n",
      "Epoch:  12609  Learning Rate:  3.3451467295828316e-07  Varinance:  5.538611549363547e-07 \n",
      "\n",
      "Epoch:  12610  Learning Rate:  3.3418032548692304e-07  Varinance:  5.530874312043032e-07 \n",
      "\n",
      "Epoch:  12611  Learning Rate:  3.3384631219591624e-07  Varinance:  5.52314788335945e-07 \n",
      "\n",
      "Epoch:  12612  Learning Rate:  3.3351263275124887e-07  Varinance:  5.515432248213532e-07 \n",
      "\n",
      "Epoch:  12613  Learning Rate:  3.3317928681924267e-07  Varinance:  5.507727391527103e-07 \n",
      "\n",
      "Epoch:  12614  Learning Rate:  3.32846274066551e-07  Varinance:  5.500033298243048e-07 \n",
      "\n",
      "Epoch:  12615  Learning Rate:  3.3251359416016055e-07  Varinance:  5.492349953325309e-07 \n",
      "\n",
      "Epoch:  12616  Learning Rate:  3.3218124676739255e-07  Varinance:  5.484677341758772e-07 \n",
      "\n",
      "Epoch:  12617  Learning Rate:  3.3184923155589905e-07  Varinance:  5.477015448549359e-07 \n",
      "\n",
      "Epoch:  12618  Learning Rate:  3.315175481936642e-07  Varinance:  5.46936425872392e-07 \n",
      "\n",
      "Epoch:  12619  Learning Rate:  3.3118619634900564e-07  Varinance:  5.461723757330217e-07 \n",
      "\n",
      "Epoch:  12620  Learning Rate:  3.308551756905711e-07  Varinance:  5.454093929436904e-07 \n",
      "\n",
      "Epoch:  12621  Learning Rate:  3.305244858873398e-07  Varinance:  5.446474760133491e-07 \n",
      "\n",
      "Epoch:  12622  Learning Rate:  3.3019412660862134e-07  Varinance:  5.43886623453034e-07 \n",
      "\n",
      "Epoch:  12623  Learning Rate:  3.2986409752405765e-07  Varinance:  5.431268337758551e-07 \n",
      "\n",
      "Epoch:  12624  Learning Rate:  3.295343983036189e-07  Varinance:  5.423681054970058e-07 \n",
      "\n",
      "Epoch:  12625  Learning Rate:  3.292050286176054e-07  Varinance:  5.416104371337514e-07 \n",
      "\n",
      "Epoch:  12626  Learning Rate:  3.2887598813664846e-07  Varinance:  5.408538272054286e-07 \n",
      "\n",
      "Epoch:  12627  Learning Rate:  3.2854727653170707e-07  Varinance:  5.400982742334426e-07 \n",
      "\n",
      "Epoch:  12628  Learning Rate:  3.2821889347406906e-07  Varinance:  5.393437767412643e-07 \n",
      "\n",
      "Epoch:  12629  Learning Rate:  3.2789083863535245e-07  Varinance:  5.385903332544288e-07 \n",
      "\n",
      "Epoch:  12630  Learning Rate:  3.2756311168750177e-07  Varinance:  5.378379423005255e-07 \n",
      "\n",
      "Epoch:  12631  Learning Rate:  3.2723571230278947e-07  Varinance:  5.370866024092066e-07 \n",
      "\n",
      "Epoch:  12632  Learning Rate:  3.2690864015381737e-07  Varinance:  5.36336312112176e-07 \n",
      "\n",
      "Epoch:  12633  Learning Rate:  3.265818949135127e-07  Varinance:  5.35587069943189e-07 \n",
      "\n",
      "Epoch:  12634  Learning Rate:  3.262554762551301e-07  Varinance:  5.34838874438049e-07 \n",
      "\n",
      "Epoch:  12635  Learning Rate:  3.2592938385225035e-07  Varinance:  5.340917241346052e-07 \n",
      "\n",
      "Epoch:  12636  Learning Rate:  3.256036173787822e-07  Varinance:  5.333456175727509e-07 \n",
      "\n",
      "Epoch:  12637  Learning Rate:  3.252781765089586e-07  Varinance:  5.326005532944136e-07 \n",
      "\n",
      "Epoch:  12638  Learning Rate:  3.2495306091733805e-07  Varinance:  5.318565298435633e-07 \n",
      "\n",
      "Epoch:  12639  Learning Rate:  3.2462827027880607e-07  Varinance:  5.311135457662023e-07 \n",
      "\n",
      "Epoch:  12640  Learning Rate:  3.243038042685713e-07  Varinance:  5.303715996103637e-07 \n",
      "\n",
      "Epoch:  12641  Learning Rate:  3.239796625621674e-07  Varinance:  5.296306899261093e-07 \n",
      "\n",
      "Epoch:  12642  Learning Rate:  3.2365584483545357e-07  Varinance:  5.288908152655263e-07 \n",
      "\n",
      "Epoch:  12643  Learning Rate:  3.2333235076461153e-07  Varinance:  5.281519741827264e-07 \n",
      "\n",
      "Epoch:  12644  Learning Rate:  3.2300918002614666e-07  Varinance:  5.274141652338357e-07 \n",
      "\n",
      "Epoch:  12645  Learning Rate:  3.2268633229688936e-07  Varinance:  5.266773869770025e-07 \n",
      "\n",
      "Epoch:  12646  Learning Rate:  3.2236380725399117e-07  Varinance:  5.259416379723882e-07 \n",
      "\n",
      "Epoch:  12647  Learning Rate:  3.2204160457492656e-07  Varinance:  5.252069167821649e-07 \n",
      "\n",
      "Epoch:  12648  Learning Rate:  3.2171972393749384e-07  Varinance:  5.244732219705136e-07 \n",
      "\n",
      "Epoch:  12649  Learning Rate:  3.2139816501981196e-07  Varinance:  5.237405521036211e-07 \n",
      "\n",
      "Epoch:  12650  Learning Rate:  3.2107692750032186e-07  Varinance:  5.230089057496791e-07 \n",
      "\n",
      "Epoch:  12651  Learning Rate:  3.207560110577855e-07  Varinance:  5.222782814788737e-07 \n",
      "\n",
      "Epoch:  12652  Learning Rate:  3.2043541537128744e-07  Varinance:  5.215486778633941e-07 \n",
      "\n",
      "Epoch:  12653  Learning Rate:  3.2011514012023144e-07  Varinance:  5.208200934774221e-07 \n",
      "\n",
      "Epoch:  12654  Learning Rate:  3.197951849843417e-07  Varinance:  5.200925268971317e-07 \n",
      "\n",
      "Epoch:  12655  Learning Rate:  3.1947554964366415e-07  Varinance:  5.193659767006854e-07 \n",
      "\n",
      "Epoch:  12656  Learning Rate:  3.1915623377856283e-07  Varinance:  5.186404414682325e-07 \n",
      "\n",
      "Epoch:  12657  Learning Rate:  3.188372370697214e-07  Varinance:  5.179159197819069e-07 \n",
      "\n",
      "Epoch:  12658  Learning Rate:  3.185185591981441e-07  Varinance:  5.171924102258187e-07 \n",
      "\n",
      "Epoch:  12659  Learning Rate:  3.182001998451526e-07  Varinance:  5.164699113860605e-07 \n",
      "\n",
      "Epoch:  12660  Learning Rate:  3.1788215869238684e-07  Varinance:  5.157484218506988e-07 \n",
      "\n",
      "Epoch:  12661  Learning Rate:  3.175644354218069e-07  Varinance:  5.150279402097722e-07 \n",
      "\n",
      "Epoch:  12662  Learning Rate:  3.1724702971568877e-07  Varinance:  5.143084650552893e-07 \n",
      "\n",
      "Epoch:  12663  Learning Rate:  3.169299412566263e-07  Varinance:  5.135899949812254e-07 \n",
      "\n",
      "Epoch:  12664  Learning Rate:  3.16613169727532e-07  Varinance:  5.128725285835217e-07 \n",
      "\n",
      "Epoch:  12665  Learning Rate:  3.1629671481163384e-07  Varinance:  5.121560644600756e-07 \n",
      "\n",
      "Epoch:  12666  Learning Rate:  3.159805761924769e-07  Varinance:  5.114406012107484e-07 \n",
      "\n",
      "Epoch:  12667  Learning Rate:  3.156647535539218e-07  Varinance:  5.107261374373557e-07 \n",
      "\n",
      "Epoch:  12668  Learning Rate:  3.153492465801472e-07  Varinance:  5.100126717436663e-07 \n",
      "\n",
      "Epoch:  12669  Learning Rate:  3.1503405495564547e-07  Varinance:  5.093002027353993e-07 \n",
      "\n",
      "Epoch:  12670  Learning Rate:  3.1471917836522434e-07  Varinance:  5.085887290202219e-07 \n",
      "\n",
      "Epoch:  12671  Learning Rate:  3.144046164940084e-07  Varinance:  5.078782492077478e-07 \n",
      "\n",
      "Epoch:  12672  Learning Rate:  3.140903690274351e-07  Varinance:  5.071687619095277e-07 \n",
      "\n",
      "Epoch:  12673  Learning Rate:  3.137764356512565e-07  Varinance:  5.064602657390576e-07 \n",
      "\n",
      "Epoch:  12674  Learning Rate:  3.1346281605154026e-07  Varinance:  5.057527593117682e-07 \n",
      "\n",
      "Epoch:  12675  Learning Rate:  3.1314950991466613e-07  Varinance:  5.050462412450246e-07 \n",
      "\n",
      "Epoch:  12676  Learning Rate:  3.1283651692732754e-07  Varinance:  5.043407101581231e-07 \n",
      "\n",
      "Epoch:  12677  Learning Rate:  3.125238367765325e-07  Varinance:  5.036361646722893e-07 \n",
      "\n",
      "Epoch:  12678  Learning Rate:  3.122114691496002e-07  Varinance:  5.029326034106766e-07 \n",
      "\n",
      "Epoch:  12679  Learning Rate:  3.1189941373416255e-07  Varinance:  5.022300249983558e-07 \n",
      "\n",
      "Epoch:  12680  Learning Rate:  3.115876702181652e-07  Varinance:  5.015284280623247e-07 \n",
      "\n",
      "Epoch:  12681  Learning Rate:  3.11276238289864e-07  Varinance:  5.008278112314966e-07 \n",
      "\n",
      "Epoch:  12682  Learning Rate:  3.109651176378271e-07  Varinance:  5.001281731367004e-07 \n",
      "\n",
      "Epoch:  12683  Learning Rate:  3.1065430795093313e-07  Varinance:  4.994295124106779e-07 \n",
      "\n",
      "Epoch:  12684  Learning Rate:  3.1034380891837356e-07  Varinance:  4.98731827688081e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12685  Learning Rate:  3.1003362022964877e-07  Varinance:  4.9803511760547e-07 \n",
      "\n",
      "Epoch:  12686  Learning Rate:  3.097237415745695e-07  Varinance:  4.973393808013052e-07 \n",
      "\n",
      "Epoch:  12687  Learning Rate:  3.0941417264325816e-07  Varinance:  4.966446159159541e-07 \n",
      "\n",
      "Epoch:  12688  Learning Rate:  3.091049131261453e-07  Varinance:  4.959508215916817e-07 \n",
      "\n",
      "Epoch:  12689  Learning Rate:  3.087959627139707e-07  Varinance:  4.952579964726498e-07 \n",
      "\n",
      "Epoch:  12690  Learning Rate:  3.0848732109778514e-07  Varinance:  4.945661392049141e-07 \n",
      "\n",
      "Epoch:  12691  Learning Rate:  3.081789879689464e-07  Varinance:  4.938752484364237e-07 \n",
      "\n",
      "Epoch:  12692  Learning Rate:  3.0787096301912067e-07  Varinance:  4.93185322817011e-07 \n",
      "\n",
      "Epoch:  12693  Learning Rate:  3.0756324594028424e-07  Varinance:  4.924963609983999e-07 \n",
      "\n",
      "Epoch:  12694  Learning Rate:  3.0725583642471936e-07  Varinance:  4.91808361634196e-07 \n",
      "\n",
      "Epoch:  12695  Learning Rate:  3.06948734165016e-07  Varinance:  4.911213233798856e-07 \n",
      "\n",
      "Epoch:  12696  Learning Rate:  3.066419388540729e-07  Varinance:  4.904352448928338e-07 \n",
      "\n",
      "Epoch:  12697  Learning Rate:  3.063354501850942e-07  Varinance:  4.897501248322805e-07 \n",
      "\n",
      "Epoch:  12698  Learning Rate:  3.060292678515912e-07  Varinance:  4.89065961859341e-07 \n",
      "\n",
      "Epoch:  12699  Learning Rate:  3.0572339154738105e-07  Varinance:  4.883827546369954e-07 \n",
      "\n",
      "Epoch:  12700  Learning Rate:  3.0541782096658844e-07  Varinance:  4.87700501830097e-07 \n",
      "\n",
      "Epoch:  12701  Learning Rate:  3.0511255580364227e-07  Varinance:  4.870192021053623e-07 \n",
      "\n",
      "Epoch:  12702  Learning Rate:  3.048075957532768e-07  Varinance:  4.863388541313705e-07 \n",
      "\n",
      "Epoch:  12703  Learning Rate:  3.0450294051053294e-07  Varinance:  4.856594565785606e-07 \n",
      "\n",
      "Epoch:  12704  Learning Rate:  3.0419858977075504e-07  Varinance:  4.849810081192293e-07 \n",
      "\n",
      "Epoch:  12705  Learning Rate:  3.038945432295917e-07  Varinance:  4.843035074275292e-07 \n",
      "\n",
      "Epoch:  12706  Learning Rate:  3.035908005829975e-07  Varinance:  4.836269531794604e-07 \n",
      "\n",
      "Epoch:  12707  Learning Rate:  3.032873615272291e-07  Varinance:  4.829513440528775e-07 \n",
      "\n",
      "Epoch:  12708  Learning Rate:  3.0298422575884703e-07  Varinance:  4.822766787274805e-07 \n",
      "\n",
      "Epoch:  12709  Learning Rate:  3.0268139297471647e-07  Varinance:  4.816029558848137e-07 \n",
      "\n",
      "Epoch:  12710  Learning Rate:  3.0237886287200415e-07  Varinance:  4.809301742082633e-07 \n",
      "\n",
      "Epoch:  12711  Learning Rate:  3.020766351481793e-07  Varinance:  4.802583323830546e-07 \n",
      "\n",
      "Epoch:  12712  Learning Rate:  3.0177470950101535e-07  Varinance:  4.795874290962516e-07 \n",
      "\n",
      "Epoch:  12713  Learning Rate:  3.0147308562858604e-07  Varinance:  4.789174630367471e-07 \n",
      "\n",
      "Epoch:  12714  Learning Rate:  3.0117176322926745e-07  Varinance:  4.782484328952705e-07 \n",
      "\n",
      "Epoch:  12715  Learning Rate:  3.0087074200173675e-07  Varinance:  4.775803373643788e-07 \n",
      "\n",
      "Epoch:  12716  Learning Rate:  3.005700216449736e-07  Varinance:  4.76913175138455e-07 \n",
      "\n",
      "Epoch:  12717  Learning Rate:  3.0026960185825704e-07  Varinance:  4.762469449137067e-07 \n",
      "\n",
      "Epoch:  12718  Learning Rate:  2.9996948234116696e-07  Varinance:  4.75581645388162e-07 \n",
      "\n",
      "Epoch:  12719  Learning Rate:  2.996696627935847e-07  Varinance:  4.749172752616701e-07 \n",
      "\n",
      "Epoch:  12720  Learning Rate:  2.993701429156902e-07  Varinance:  4.7425383323589125e-07 \n",
      "\n",
      "Epoch:  12721  Learning Rate:  2.9907092240796304e-07  Varinance:  4.7359131801430444e-07 \n",
      "\n",
      "Epoch:  12722  Learning Rate:  2.987720009711838e-07  Varinance:  4.7292972830219807e-07 \n",
      "\n",
      "Epoch:  12723  Learning Rate:  2.984733783064304e-07  Varinance:  4.722690628066694e-07 \n",
      "\n",
      "Epoch:  12724  Learning Rate:  2.981750541150796e-07  Varinance:  4.716093202366219e-07 \n",
      "\n",
      "Epoch:  12725  Learning Rate:  2.9787702809880834e-07  Varinance:  4.7095049930276216e-07 \n",
      "\n",
      "Epoch:  12726  Learning Rate:  2.9757929995958996e-07  Varinance:  4.702925987176002e-07 \n",
      "\n",
      "Epoch:  12727  Learning Rate:  2.9728186939969587e-07  Varinance:  4.6963561719543903e-07 \n",
      "\n",
      "Epoch:  12728  Learning Rate:  2.969847361216964e-07  Varinance:  4.689795534523832e-07 \n",
      "\n",
      "Epoch:  12729  Learning Rate:  2.966878998284578e-07  Varinance:  4.683244062063288e-07 \n",
      "\n",
      "Epoch:  12730  Learning Rate:  2.963913602231439e-07  Varinance:  4.676701741769633e-07 \n",
      "\n",
      "Epoch:  12731  Learning Rate:  2.960951170092142e-07  Varinance:  4.670168560857623e-07 \n",
      "\n",
      "Epoch:  12732  Learning Rate:  2.9579916989042686e-07  Varinance:  4.663644506559878e-07 \n",
      "\n",
      "Epoch:  12733  Learning Rate:  2.95503518570834e-07  Varinance:  4.657129566126869e-07 \n",
      "\n",
      "Epoch:  12734  Learning Rate:  2.952081627547838e-07  Varinance:  4.6506237268268276e-07 \n",
      "\n",
      "Epoch:  12735  Learning Rate:  2.949131021469215e-07  Varinance:  4.644126975945822e-07 \n",
      "\n",
      "Epoch:  12736  Learning Rate:  2.946183364521859e-07  Varinance:  4.6376393007876626e-07 \n",
      "\n",
      "Epoch:  12737  Learning Rate:  2.9432386537581083e-07  Varinance:  4.631160688673898e-07 \n",
      "\n",
      "Epoch:  12738  Learning Rate:  2.9402968862332617e-07  Varinance:  4.624691126943787e-07 \n",
      "\n",
      "Epoch:  12739  Learning Rate:  2.937358059005546e-07  Varinance:  4.6182306029542765e-07 \n",
      "\n",
      "Epoch:  12740  Learning Rate:  2.934422169136129e-07  Varinance:  4.6117791040799893e-07 \n",
      "\n",
      "Epoch:  12741  Learning Rate:  2.931489213689131e-07  Varinance:  4.605336617713138e-07 \n",
      "\n",
      "Epoch:  12742  Learning Rate:  2.9285591897315907e-07  Varinance:  4.598903131263597e-07 \n",
      "\n",
      "Epoch:  12743  Learning Rate:  2.9256320943334795e-07  Varinance:  4.59247863215881e-07 \n",
      "\n",
      "Epoch:  12744  Learning Rate:  2.922707924567711e-07  Varinance:  4.586063107843787e-07 \n",
      "\n",
      "Epoch:  12745  Learning Rate:  2.919786677510111e-07  Varinance:  4.5796565457810753e-07 \n",
      "\n",
      "Epoch:  12746  Learning Rate:  2.916868350239432e-07  Varinance:  4.5732589334507377e-07 \n",
      "\n",
      "Epoch:  12747  Learning Rate:  2.913952939837341e-07  Varinance:  4.5668702583503415e-07 \n",
      "\n",
      "Epoch:  12748  Learning Rate:  2.911040443388438e-07  Varinance:  4.5604905079948725e-07 \n",
      "\n",
      "Epoch:  12749  Learning Rate:  2.9081308579802205e-07  Varinance:  4.554119669916804e-07 \n",
      "\n",
      "Epoch:  12750  Learning Rate:  2.9052241807030984e-07  Varinance:  4.5477577316660124e-07 \n",
      "\n",
      "Epoch:  12751  Learning Rate:  2.902320408650404e-07  Varinance:  4.541404680809765e-07 \n",
      "\n",
      "Epoch:  12752  Learning Rate:  2.8994195389183607e-07  Varinance:  4.5350605049326964e-07 \n",
      "\n",
      "Epoch:  12753  Learning Rate:  2.8965215686060923e-07  Varinance:  4.5287251916367867e-07 \n",
      "\n",
      "Epoch:  12754  Learning Rate:  2.893626494815639e-07  Varinance:  4.5223987285413515e-07 \n",
      "\n",
      "Epoch:  12755  Learning Rate:  2.890734314651922e-07  Varinance:  4.5160811032829523e-07 \n",
      "\n",
      "Epoch:  12756  Learning Rate:  2.8878450252227553e-07  Varinance:  4.509772303515471e-07 \n",
      "\n",
      "Epoch:  12757  Learning Rate:  2.8849586236388593e-07  Varinance:  4.50347231691002e-07 \n",
      "\n",
      "Epoch:  12758  Learning Rate:  2.882075107013828e-07  Varinance:  4.497181131154935e-07 \n",
      "\n",
      "Epoch:  12759  Learning Rate:  2.8791944724641433e-07  Varinance:  4.490898733955751e-07 \n",
      "\n",
      "Epoch:  12760  Learning Rate:  2.876316717109166e-07  Varinance:  4.4846251130351783e-07 \n",
      "\n",
      "Epoch:  12761  Learning Rate:  2.873441838071151e-07  Varinance:  4.4783602561330913e-07 \n",
      "\n",
      "Epoch:  12762  Learning Rate:  2.870569832475213e-07  Varinance:  4.472104151006447e-07 \n",
      "\n",
      "Epoch:  12763  Learning Rate:  2.867700697449342e-07  Varinance:  4.4658567854293517e-07 \n",
      "\n",
      "Epoch:  12764  Learning Rate:  2.8648344301244117e-07  Varinance:  4.459618147192974e-07 \n",
      "\n",
      "Epoch:  12765  Learning Rate:  2.861971027634151e-07  Varinance:  4.45338822410554e-07 \n",
      "\n",
      "Epoch:  12766  Learning Rate:  2.8591104871151513e-07  Varinance:  4.447167003992306e-07 \n",
      "\n",
      "Epoch:  12767  Learning Rate:  2.856252805706882e-07  Varinance:  4.440954474695535e-07 \n",
      "\n",
      "Epoch:  12768  Learning Rate:  2.8533979805516566e-07  Varinance:  4.4347506240744914e-07 \n",
      "\n",
      "Epoch:  12769  Learning Rate:  2.850546008794644e-07  Varinance:  4.428555440005353e-07 \n",
      "\n",
      "Epoch:  12770  Learning Rate:  2.847696887583883e-07  Varinance:  4.422368910381278e-07 \n",
      "\n",
      "Epoch:  12771  Learning Rate:  2.844850614070247e-07  Varinance:  4.416191023112325e-07 \n",
      "\n",
      "Epoch:  12772  Learning Rate:  2.842007185407457e-07  Varinance:  4.410021766125441e-07 \n",
      "\n",
      "Epoch:  12773  Learning Rate:  2.8391665987520947e-07  Varinance:  4.403861127364437e-07 \n",
      "\n",
      "Epoch:  12774  Learning Rate:  2.836328851263567e-07  Varinance:  4.3977090947899686e-07 \n",
      "\n",
      "Epoch:  12775  Learning Rate:  2.8334939401041277e-07  Varinance:  4.391565656379525e-07 \n",
      "\n",
      "Epoch:  12776  Learning Rate:  2.830661862438859e-07  Varinance:  4.3854308001273404e-07 \n",
      "\n",
      "Epoch:  12777  Learning Rate:  2.827832615435694e-07  Varinance:  4.3793045140444717e-07 \n",
      "\n",
      "Epoch:  12778  Learning Rate:  2.82500619626538e-07  Varinance:  4.3731867861587053e-07 \n",
      "\n",
      "Epoch:  12779  Learning Rate:  2.8221826021014923e-07  Varinance:  4.367077604514555e-07 \n",
      "\n",
      "Epoch:  12780  Learning Rate:  2.819361830120447e-07  Varinance:  4.3609769571732335e-07 \n",
      "\n",
      "Epoch:  12781  Learning Rate:  2.8165438775014674e-07  Varinance:  4.3548848322126337e-07 \n",
      "\n",
      "Epoch:  12782  Learning Rate:  2.8137287414265944e-07  Varinance:  4.348801217727316e-07 \n",
      "\n",
      "Epoch:  12783  Learning Rate:  2.810916419080703e-07  Varinance:  4.3427261018284284e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12784  Learning Rate:  2.808106907651464e-07  Varinance:  4.3366594726437715e-07 \n",
      "\n",
      "Epoch:  12785  Learning Rate:  2.805300204329362e-07  Varinance:  4.330601318317719e-07 \n",
      "\n",
      "Epoch:  12786  Learning Rate:  2.802496306307703e-07  Varinance:  4.3245516270112016e-07 \n",
      "\n",
      "Epoch:  12787  Learning Rate:  2.7996952107825843e-07  Varinance:  4.318510386901693e-07 \n",
      "\n",
      "Epoch:  12788  Learning Rate:  2.796896914952904e-07  Varinance:  4.3124775861831787e-07 \n",
      "\n",
      "Epoch:  12789  Learning Rate:  2.794101416020378e-07  Varinance:  4.306453213066156e-07 \n",
      "\n",
      "Epoch:  12790  Learning Rate:  2.7913087111894997e-07  Varinance:  4.300437255777543e-07 \n",
      "\n",
      "Epoch:  12791  Learning Rate:  2.7885187976675654e-07  Varinance:  4.294429702560752e-07 \n",
      "\n",
      "Epoch:  12792  Learning Rate:  2.7857316726646565e-07  Varinance:  4.288430541675602e-07 \n",
      "\n",
      "Epoch:  12793  Learning Rate:  2.782947333393657e-07  Varinance:  4.282439761398313e-07 \n",
      "\n",
      "Epoch:  12794  Learning Rate:  2.780165777070223e-07  Varinance:  4.2764573500214843e-07 \n",
      "\n",
      "Epoch:  12795  Learning Rate:  2.777387000912793e-07  Varinance:  4.2704832958540673e-07 \n",
      "\n",
      "Epoch:  12796  Learning Rate:  2.7746110021425997e-07  Varinance:  4.264517587221363e-07 \n",
      "\n",
      "Epoch:  12797  Learning Rate:  2.7718377779836406e-07  Varinance:  4.258560212464934e-07 \n",
      "\n",
      "Epoch:  12798  Learning Rate:  2.769067325662685e-07  Varinance:  4.2526111599426766e-07 \n",
      "\n",
      "Epoch:  12799  Learning Rate:  2.7662996424092903e-07  Varinance:  4.246670418028735e-07 \n",
      "\n",
      "Epoch:  12800  Learning Rate:  2.7635347254557694e-07  Varinance:  4.2407379751134926e-07 \n",
      "\n",
      "Epoch:  12801  Learning Rate:  2.7607725720371986e-07  Varinance:  4.234813819603553e-07 \n",
      "\n",
      "Epoch:  12802  Learning Rate:  2.758013179391435e-07  Varinance:  4.2288979399217155e-07 \n",
      "\n",
      "Epoch:  12803  Learning Rate:  2.7552565447590815e-07  Varinance:  4.2229903245069665e-07 \n",
      "\n",
      "Epoch:  12804  Learning Rate:  2.752502665383497e-07  Varinance:  4.2170909618143974e-07 \n",
      "\n",
      "Epoch:  12805  Learning Rate:  2.7497515385108117e-07  Varinance:  4.2111998403152725e-07 \n",
      "\n",
      "Epoch:  12806  Learning Rate:  2.747003161389894e-07  Varinance:  4.205316948496946e-07 \n",
      "\n",
      "Epoch:  12807  Learning Rate:  2.744257531272367e-07  Varinance:  4.1994422748628577e-07 \n",
      "\n",
      "Epoch:  12808  Learning Rate:  2.7415146454125947e-07  Varinance:  4.193575807932503e-07 \n",
      "\n",
      "Epoch:  12809  Learning Rate:  2.7387745010677017e-07  Varinance:  4.18771753624142e-07 \n",
      "\n",
      "Epoch:  12810  Learning Rate:  2.736037095497538e-07  Varinance:  4.1818674483411724e-07 \n",
      "\n",
      "Epoch:  12811  Learning Rate:  2.733302425964693e-07  Varinance:  4.176025532799276e-07 \n",
      "\n",
      "Epoch:  12812  Learning Rate:  2.7305704897345057e-07  Varinance:  4.170191778199261e-07 \n",
      "\n",
      "Epoch:  12813  Learning Rate:  2.7278412840750366e-07  Varinance:  4.1643661731405885e-07 \n",
      "\n",
      "Epoch:  12814  Learning Rate:  2.7251148062570733e-07  Varinance:  4.158548706238651e-07 \n",
      "\n",
      "Epoch:  12815  Learning Rate:  2.7223910535541486e-07  Varinance:  4.1527393661247393e-07 \n",
      "\n",
      "Epoch:  12816  Learning Rate:  2.7196700232425046e-07  Varinance:  4.14693814144603e-07 \n",
      "\n",
      "Epoch:  12817  Learning Rate:  2.716951712601105e-07  Varinance:  4.141145020865572e-07 \n",
      "\n",
      "Epoch:  12818  Learning Rate:  2.7142361189116495e-07  Varinance:  4.1353599930622065e-07 \n",
      "\n",
      "Epoch:  12819  Learning Rate:  2.7115232394585395e-07  Varinance:  4.1295830467306366e-07 \n",
      "\n",
      "Epoch:  12820  Learning Rate:  2.70881307152889e-07  Varinance:  4.123814170581342e-07 \n",
      "\n",
      "Epoch:  12821  Learning Rate:  2.7061056124125424e-07  Varinance:  4.1180533533405735e-07 \n",
      "\n",
      "Epoch:  12822  Learning Rate:  2.7034008594020325e-07  Varinance:  4.112300583750332e-07 \n",
      "\n",
      "Epoch:  12823  Learning Rate:  2.7006988097926074e-07  Varinance:  4.106555850568359e-07 \n",
      "\n",
      "Epoch:  12824  Learning Rate:  2.6979994608822124e-07  Varinance:  4.100819142568057e-07 \n",
      "\n",
      "Epoch:  12825  Learning Rate:  2.695302809971508e-07  Varinance:  4.095090448538557e-07 \n",
      "\n",
      "Epoch:  12826  Learning Rate:  2.692608854363838e-07  Varinance:  4.0893697572846347e-07 \n",
      "\n",
      "Epoch:  12827  Learning Rate:  2.6899175913652424e-07  Varinance:  4.0836570576267056e-07 \n",
      "\n",
      "Epoch:  12828  Learning Rate:  2.687229018284467e-07  Varinance:  4.0779523384008045e-07 \n",
      "\n",
      "Epoch:  12829  Learning Rate:  2.684543132432933e-07  Varinance:  4.0722555884585584e-07 \n",
      "\n",
      "Epoch:  12830  Learning Rate:  2.681859931124751e-07  Varinance:  4.066566796667186e-07 \n",
      "\n",
      "Epoch:  12831  Learning Rate:  2.6791794116767285e-07  Varinance:  4.0608859519094133e-07 \n",
      "\n",
      "Epoch:  12832  Learning Rate:  2.676501571408341e-07  Varinance:  4.05521304308354e-07 \n",
      "\n",
      "Epoch:  12833  Learning Rate:  2.673826407641743e-07  Varinance:  4.0495480591033595e-07 \n",
      "\n",
      "Epoch:  12834  Learning Rate:  2.67115391770178e-07  Varinance:  4.043890988898154e-07 \n",
      "\n",
      "Epoch:  12835  Learning Rate:  2.6684840989159576e-07  Varinance:  4.0382418214126683e-07 \n",
      "\n",
      "Epoch:  12836  Learning Rate:  2.6658169486144516e-07  Varinance:  4.032600545607094e-07 \n",
      "\n",
      "Epoch:  12837  Learning Rate:  2.663152464130121e-07  Varinance:  4.0269671504570576e-07 \n",
      "\n",
      "Epoch:  12838  Learning Rate:  2.660490642798477e-07  Varinance:  4.0213416249535426e-07 \n",
      "\n",
      "Epoch:  12839  Learning Rate:  2.657831481957697e-07  Varinance:  4.0157239581029567e-07 \n",
      "\n",
      "Epoch:  12840  Learning Rate:  2.6551749789486155e-07  Varinance:  4.010114138927049e-07 \n",
      "\n",
      "Epoch:  12841  Learning Rate:  2.6525211311147394e-07  Varinance:  4.004512156462907e-07 \n",
      "\n",
      "Epoch:  12842  Learning Rate:  2.6498699358022153e-07  Varinance:  3.9989179997629296e-07 \n",
      "\n",
      "Epoch:  12843  Learning Rate:  2.647221390359843e-07  Varinance:  3.9933316578948125e-07 \n",
      "\n",
      "Epoch:  12844  Learning Rate:  2.6445754921390867e-07  Varinance:  3.987753119941536e-07 \n",
      "\n",
      "Epoch:  12845  Learning Rate:  2.641932238494043e-07  Varinance:  3.982182375001289e-07 \n",
      "\n",
      "Epoch:  12846  Learning Rate:  2.639291626781453e-07  Varinance:  3.976619412187531e-07 \n",
      "\n",
      "Epoch:  12847  Learning Rate:  2.636653654360714e-07  Varinance:  3.9710642206289175e-07 \n",
      "\n",
      "Epoch:  12848  Learning Rate:  2.63401831859385e-07  Varinance:  3.9655167894692903e-07 \n",
      "\n",
      "Epoch:  12849  Learning Rate:  2.6313856168455184e-07  Varinance:  3.9599771078676555e-07 \n",
      "\n",
      "Epoch:  12850  Learning Rate:  2.628755546483028e-07  Varinance:  3.954445164998165e-07 \n",
      "\n",
      "Epoch:  12851  Learning Rate:  2.626128104876304e-07  Varinance:  3.948920950050107e-07 \n",
      "\n",
      "Epoch:  12852  Learning Rate:  2.623503289397898e-07  Varinance:  3.943404452227832e-07 \n",
      "\n",
      "Epoch:  12853  Learning Rate:  2.6208810974230046e-07  Varinance:  3.937895660750811e-07 \n",
      "\n",
      "Epoch:  12854  Learning Rate:  2.6182615263294275e-07  Varinance:  3.932394564853563e-07 \n",
      "\n",
      "Epoch:  12855  Learning Rate:  2.615644573497595e-07  Varinance:  3.9269011537856443e-07 \n",
      "\n",
      "Epoch:  12856  Learning Rate:  2.6130302363105495e-07  Varinance:  3.9214154168116303e-07 \n",
      "\n",
      "Epoch:  12857  Learning Rate:  2.610418512153962e-07  Varinance:  3.915937343211093e-07 \n",
      "\n",
      "Epoch:  12858  Learning Rate:  2.6078093984161046e-07  Varinance:  3.910466922278595e-07 \n",
      "\n",
      "Epoch:  12859  Learning Rate:  2.6052028924878585e-07  Varinance:  3.905004143323611e-07 \n",
      "\n",
      "Epoch:  12860  Learning Rate:  2.602598991762726e-07  Varinance:  3.8995489956705923e-07 \n",
      "\n",
      "Epoch:  12861  Learning Rate:  2.599997693636803e-07  Varinance:  3.894101468658891e-07 \n",
      "\n",
      "Epoch:  12862  Learning Rate:  2.597398995508785e-07  Varinance:  3.88866155164275e-07 \n",
      "\n",
      "Epoch:  12863  Learning Rate:  2.5948028947799836e-07  Varinance:  3.8832292339912837e-07 \n",
      "\n",
      "Epoch:  12864  Learning Rate:  2.5922093888542935e-07  Varinance:  3.877804505088458e-07 \n",
      "\n",
      "Epoch:  12865  Learning Rate:  2.5896184751382036e-07  Varinance:  3.872387354333084e-07 \n",
      "\n",
      "Epoch:  12866  Learning Rate:  2.587030151040809e-07  Varinance:  3.8669777711387397e-07 \n",
      "\n",
      "Epoch:  12867  Learning Rate:  2.5844444139737814e-07  Varinance:  3.8615757449338346e-07 \n",
      "\n",
      "Epoch:  12868  Learning Rate:  2.5818612613513785e-07  Varinance:  3.856181265161529e-07 \n",
      "\n",
      "Epoch:  12869  Learning Rate:  2.579280690590456e-07  Varinance:  3.8507943212797345e-07 \n",
      "\n",
      "Epoch:  12870  Learning Rate:  2.57670269911044e-07  Varinance:  3.845414902761088e-07 \n",
      "\n",
      "Epoch:  12871  Learning Rate:  2.5741272843333377e-07  Varinance:  3.840042999092934e-07 \n",
      "\n",
      "Epoch:  12872  Learning Rate:  2.571554443683729e-07  Varinance:  3.834678599777314e-07 \n",
      "\n",
      "Epoch:  12873  Learning Rate:  2.568984174588783e-07  Varinance:  3.829321694330896e-07 \n",
      "\n",
      "Epoch:  12874  Learning Rate:  2.5664164744782266e-07  Varinance:  3.8239722722850337e-07 \n",
      "\n",
      "Epoch:  12875  Learning Rate:  2.5638513407843526e-07  Varinance:  3.818630323185689e-07 \n",
      "\n",
      "Epoch:  12876  Learning Rate:  2.561288770942039e-07  Varinance:  3.813295836593431e-07 \n",
      "\n",
      "Epoch:  12877  Learning Rate:  2.558728762388709e-07  Varinance:  3.807968802083412e-07 \n",
      "\n",
      "Epoch:  12878  Learning Rate:  2.55617131256435e-07  Varinance:  3.8026492092453434e-07 \n",
      "\n",
      "Epoch:  12879  Learning Rate:  2.553616418911521e-07  Varinance:  3.7973370476834974e-07 \n",
      "\n",
      "Epoch:  12880  Learning Rate:  2.5510640788753245e-07  Varinance:  3.7920323070166253e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12881  Learning Rate:  2.548514289903414e-07  Varinance:  3.786734976878024e-07 \n",
      "\n",
      "Epoch:  12882  Learning Rate:  2.545967049446011e-07  Varinance:  3.7814450469154565e-07 \n",
      "\n",
      "Epoch:  12883  Learning Rate:  2.543422354955869e-07  Varinance:  3.7761625067911477e-07 \n",
      "\n",
      "Epoch:  12884  Learning Rate:  2.5408802038882945e-07  Varinance:  3.7708873461817657e-07 \n",
      "\n",
      "Epoch:  12885  Learning Rate:  2.538340593701131e-07  Varinance:  3.7656195547783964e-07 \n",
      "\n",
      "Epoch:  12886  Learning Rate:  2.535803521854777e-07  Varinance:  3.7603591222865444e-07 \n",
      "\n",
      "Epoch:  12887  Learning Rate:  2.533268985812156e-07  Varinance:  3.7551060384260526e-07 \n",
      "\n",
      "Epoch:  12888  Learning Rate:  2.530736983038728e-07  Varinance:  3.749860292931166e-07 \n",
      "\n",
      "Epoch:  12889  Learning Rate:  2.528207511002498e-07  Varinance:  3.7446218755504563e-07 \n",
      "\n",
      "Epoch:  12890  Learning Rate:  2.52568056717399e-07  Varinance:  3.7393907760468164e-07 \n",
      "\n",
      "Epoch:  12891  Learning Rate:  2.5231561490262546e-07  Varinance:  3.7341669841974416e-07 \n",
      "\n",
      "Epoch:  12892  Learning Rate:  2.5206342540348836e-07  Varinance:  3.728950489793806e-07 \n",
      "\n",
      "Epoch:  12893  Learning Rate:  2.5181148796779763e-07  Varinance:  3.723741282641658e-07 \n",
      "\n",
      "Epoch:  12894  Learning Rate:  2.515598023436154e-07  Varinance:  3.7185393525609487e-07 \n",
      "\n",
      "Epoch:  12895  Learning Rate:  2.5130836827925697e-07  Varinance:  3.713344689385889e-07 \n",
      "\n",
      "Epoch:  12896  Learning Rate:  2.510571855232877e-07  Varinance:  3.708157282964878e-07 \n",
      "\n",
      "Epoch:  12897  Learning Rate:  2.508062538245245e-07  Varinance:  3.702977123160497e-07 \n",
      "\n",
      "Epoch:  12898  Learning Rate:  2.5055557293203643e-07  Varinance:  3.6978041998494866e-07 \n",
      "\n",
      "Epoch:  12899  Learning Rate:  2.5030514259514216e-07  Varinance:  3.692638502922732e-07 \n",
      "\n",
      "Epoch:  12900  Learning Rate:  2.5005496256341137e-07  Varinance:  3.687480022285253e-07 \n",
      "\n",
      "Epoch:  12901  Learning Rate:  2.498050325866635e-07  Varinance:  3.68232874785613e-07 \n",
      "\n",
      "Epoch:  12902  Learning Rate:  2.4955535241496953e-07  Varinance:  3.6771846695685686e-07 \n",
      "\n",
      "Epoch:  12903  Learning Rate:  2.4930592179864876e-07  Varinance:  3.672047777369822e-07 \n",
      "\n",
      "Epoch:  12904  Learning Rate:  2.4905674048827004e-07  Varinance:  3.666918061221188e-07 \n",
      "\n",
      "Epoch:  12905  Learning Rate:  2.488078082346531e-07  Varinance:  3.6617955110979874e-07 \n",
      "\n",
      "Epoch:  12906  Learning Rate:  2.485591247888651e-07  Varinance:  3.6566801169895454e-07 \n",
      "\n",
      "Epoch:  12907  Learning Rate:  2.4831068990222216e-07  Varinance:  3.651571868899185e-07 \n",
      "\n",
      "Epoch:  12908  Learning Rate:  2.4806250332629026e-07  Varinance:  3.6464707568441544e-07 \n",
      "\n",
      "Epoch:  12909  Learning Rate:  2.4781456481288236e-07  Varinance:  3.6413767708556867e-07 \n",
      "\n",
      "Epoch:  12910  Learning Rate:  2.475668741140595e-07  Varinance:  3.636289900978929e-07 \n",
      "\n",
      "Epoch:  12911  Learning Rate:  2.473194309821318e-07  Varinance:  3.631210137272933e-07 \n",
      "\n",
      "Epoch:  12912  Learning Rate:  2.470722351696557e-07  Varinance:  3.626137469810639e-07 \n",
      "\n",
      "Epoch:  12913  Learning Rate:  2.468252864294349e-07  Varinance:  3.621071888678855e-07 \n",
      "\n",
      "Epoch:  12914  Learning Rate:  2.4657858451452156e-07  Varinance:  3.6160133839782495e-07 \n",
      "\n",
      "Epoch:  12915  Learning Rate:  2.4633212917821325e-07  Varinance:  3.6109619458232815e-07 \n",
      "\n",
      "Epoch:  12916  Learning Rate:  2.4608592017405465e-07  Varinance:  3.60591756434226e-07 \n",
      "\n",
      "Epoch:  12917  Learning Rate:  2.4583995725583634e-07  Varinance:  3.6008802296772685e-07 \n",
      "\n",
      "Epoch:  12918  Learning Rate:  2.4559424017759617e-07  Varinance:  3.595849931984164e-07 \n",
      "\n",
      "Epoch:  12919  Learning Rate:  2.453487686936167e-07  Varinance:  3.590826661432555e-07 \n",
      "\n",
      "Epoch:  12920  Learning Rate:  2.451035425584258e-07  Varinance:  3.585810408205782e-07 \n",
      "\n",
      "Epoch:  12921  Learning Rate:  2.4485856152679843e-07  Varinance:  3.5808011625009123e-07 \n",
      "\n",
      "Epoch:  12922  Learning Rate:  2.44613825353753e-07  Varinance:  3.575798914528669e-07 \n",
      "\n",
      "Epoch:  12923  Learning Rate:  2.443693337945528e-07  Varinance:  3.5708036545134894e-07 \n",
      "\n",
      "Epoch:  12924  Learning Rate:  2.4412508660470727e-07  Varinance:  3.565815372693453e-07 \n",
      "\n",
      "Epoch:  12925  Learning Rate:  2.438810835399687e-07  Varinance:  3.560834059320277e-07 \n",
      "\n",
      "Epoch:  12926  Learning Rate:  2.436373243563335e-07  Varinance:  3.5558597046592965e-07 \n",
      "\n",
      "Epoch:  12927  Learning Rate:  2.433938088100434e-07  Varinance:  3.550892298989446e-07 \n",
      "\n",
      "Epoch:  12928  Learning Rate:  2.4315053665758237e-07  Varinance:  3.545931832603253e-07 \n",
      "\n",
      "Epoch:  12929  Learning Rate:  2.429075076556779e-07  Varinance:  3.540978295806766e-07 \n",
      "\n",
      "Epoch:  12930  Learning Rate:  2.426647215613017e-07  Varinance:  3.536031678919615e-07 \n",
      "\n",
      "Epoch:  12931  Learning Rate:  2.4242217813166734e-07  Varinance:  3.5310919722749406e-07 \n",
      "\n",
      "Epoch:  12932  Learning Rate:  2.4217987712423124e-07  Varinance:  3.5261591662193876e-07 \n",
      "\n",
      "Epoch:  12933  Learning Rate:  2.41937818296692e-07  Varinance:  3.5212332511130854e-07 \n",
      "\n",
      "Epoch:  12934  Learning Rate:  2.4169600140699176e-07  Varinance:  3.5163142173296307e-07 \n",
      "\n",
      "Epoch:  12935  Learning Rate:  2.41454426213313e-07  Varinance:  3.51140205525608e-07 \n",
      "\n",
      "Epoch:  12936  Learning Rate:  2.412130924740802e-07  Varinance:  3.5064967552928825e-07 \n",
      "\n",
      "Epoch:  12937  Learning Rate:  2.4097199994796033e-07  Varinance:  3.5015983078539327e-07 \n",
      "\n",
      "Epoch:  12938  Learning Rate:  2.407311483938605e-07  Varinance:  3.496706703366506e-07 \n",
      "\n",
      "Epoch:  12939  Learning Rate:  2.404905375709287e-07  Varinance:  3.4918219322712497e-07 \n",
      "\n",
      "Epoch:  12940  Learning Rate:  2.4025016723855494e-07  Varinance:  3.4869439850221653e-07 \n",
      "\n",
      "Epoch:  12941  Learning Rate:  2.400100371563685e-07  Varinance:  3.4820728520865907e-07 \n",
      "\n",
      "Epoch:  12942  Learning Rate:  2.397701470842387e-07  Varinance:  3.477208523945191e-07 \n",
      "\n",
      "Epoch:  12943  Learning Rate:  2.3953049678227646e-07  Varinance:  3.4723509910918947e-07 \n",
      "\n",
      "Epoch:  12944  Learning Rate:  2.3929108601083095e-07  Varinance:  3.467500244033944e-07 \n",
      "\n",
      "Epoch:  12945  Learning Rate:  2.3905191453049093e-07  Varinance:  3.462656273291833e-07 \n",
      "\n",
      "Epoch:  12946  Learning Rate:  2.388129821020858e-07  Varinance:  3.4578190693992964e-07 \n",
      "\n",
      "Epoch:  12947  Learning Rate:  2.385742884866827e-07  Varinance:  3.452988622903294e-07 \n",
      "\n",
      "Epoch:  12948  Learning Rate:  2.3833583344558793e-07  Varinance:  3.4481649243639886e-07 \n",
      "\n",
      "Epoch:  12949  Learning Rate:  2.3809761674034608e-07  Varinance:  3.443347964354746e-07 \n",
      "\n",
      "Epoch:  12950  Learning Rate:  2.378596381327412e-07  Varinance:  3.438537733462062e-07 \n",
      "\n",
      "Epoch:  12951  Learning Rate:  2.3762189738479431e-07  Varinance:  3.433734222285619e-07 \n",
      "\n",
      "Epoch:  12952  Learning Rate:  2.3738439425876415e-07  Varinance:  3.4289374214382206e-07 \n",
      "\n",
      "Epoch:  12953  Learning Rate:  2.371471285171485e-07  Varinance:  3.424147321545783e-07 \n",
      "\n",
      "Epoch:  12954  Learning Rate:  2.3691009992268107e-07  Varinance:  3.419363913247316e-07 \n",
      "\n",
      "Epoch:  12955  Learning Rate:  2.366733082383329e-07  Varinance:  3.414587187194923e-07 \n",
      "\n",
      "Epoch:  12956  Learning Rate:  2.3643675322731314e-07  Varinance:  3.409817134053725e-07 \n",
      "\n",
      "Epoch:  12957  Learning Rate:  2.3620043465306632e-07  Varinance:  3.4050537445019216e-07 \n",
      "\n",
      "Epoch:  12958  Learning Rate:  2.359643522792734e-07  Varinance:  3.400297009230724e-07 \n",
      "\n",
      "Epoch:  12959  Learning Rate:  2.357285058698528e-07  Varinance:  3.395546918944345e-07 \n",
      "\n",
      "Epoch:  12960  Learning Rate:  2.3549289518895777e-07  Varinance:  3.3908034643599854e-07 \n",
      "\n",
      "Epoch:  12961  Learning Rate:  2.352575200009771e-07  Varinance:  3.386066636207812e-07 \n",
      "\n",
      "Epoch:  12962  Learning Rate:  2.3502238007053646e-07  Varinance:  3.381336425230954e-07 \n",
      "\n",
      "Epoch:  12963  Learning Rate:  2.347874751624955e-07  Varinance:  3.376612822185436e-07 \n",
      "\n",
      "Epoch:  12964  Learning Rate:  2.3455280504194926e-07  Varinance:  3.3718958178402326e-07 \n",
      "\n",
      "Epoch:  12965  Learning Rate:  2.3431836947422718e-07  Varinance:  3.3671854029772e-07 \n",
      "\n",
      "Epoch:  12966  Learning Rate:  2.3408416822489452e-07  Varinance:  3.362481568391075e-07 \n",
      "\n",
      "Epoch:  12967  Learning Rate:  2.3385020105974958e-07  Varinance:  3.357784304889451e-07 \n",
      "\n",
      "Epoch:  12968  Learning Rate:  2.3361646774482476e-07  Varinance:  3.3530936032927634e-07 \n",
      "\n",
      "Epoch:  12969  Learning Rate:  2.333829680463876e-07  Varinance:  3.348409454434284e-07 \n",
      "\n",
      "Epoch:  12970  Learning Rate:  2.3314970173093794e-07  Varinance:  3.3437318491600536e-07 \n",
      "\n",
      "Epoch:  12971  Learning Rate:  2.32916668565209e-07  Varinance:  3.3390607783289376e-07 \n",
      "\n",
      "Epoch:  12972  Learning Rate:  2.3268386831616849e-07  Varinance:  3.334396232812557e-07 \n",
      "\n",
      "Epoch:  12973  Learning Rate:  2.3245130075101565e-07  Varinance:  3.3297382034952876e-07 \n",
      "\n",
      "Epoch:  12974  Learning Rate:  2.3221896563718252e-07  Varinance:  3.3250866812742376e-07 \n",
      "\n",
      "Epoch:  12975  Learning Rate:  2.3198686274233478e-07  Varinance:  3.320441657059232e-07 \n",
      "\n",
      "Epoch:  12976  Learning Rate:  2.317549918343691e-07  Varinance:  3.3158031217728064e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12977  Learning Rate:  2.3152335268141424e-07  Varinance:  3.311171066350142e-07 \n",
      "\n",
      "Epoch:  12978  Learning Rate:  2.3129194505183168e-07  Varinance:  3.3065454817391185e-07 \n",
      "\n",
      "Epoch:  12979  Learning Rate:  2.310607687142135e-07  Varinance:  3.3019263589002484e-07 \n",
      "\n",
      "Epoch:  12980  Learning Rate:  2.3082982343738328e-07  Varinance:  3.2973136888066737e-07 \n",
      "\n",
      "Epoch:  12981  Learning Rate:  2.305991089903953e-07  Varinance:  3.292707462444146e-07 \n",
      "\n",
      "Epoch:  12982  Learning Rate:  2.3036862514253596e-07  Varinance:  3.2881076708110083e-07 \n",
      "\n",
      "Epoch:  12983  Learning Rate:  2.301383716633209e-07  Varinance:  3.2835143049181914e-07 \n",
      "\n",
      "Epoch:  12984  Learning Rate:  2.2990834832249637e-07  Varinance:  3.2789273557891486e-07 \n",
      "\n",
      "Epoch:  12985  Learning Rate:  2.296785548900397e-07  Varinance:  3.274346814459908e-07 \n",
      "\n",
      "Epoch:  12986  Learning Rate:  2.2944899113615702e-07  Varinance:  3.269772671979008e-07 \n",
      "\n",
      "Epoch:  12987  Learning Rate:  2.292196568312842e-07  Varinance:  3.265204919407491e-07 \n",
      "\n",
      "Epoch:  12988  Learning Rate:  2.2899055174608774e-07  Varinance:  3.2606435478188893e-07 \n",
      "\n",
      "Epoch:  12989  Learning Rate:  2.287616756514621e-07  Varinance:  3.256088548299203e-07 \n",
      "\n",
      "Epoch:  12990  Learning Rate:  2.2853302831853078e-07  Varinance:  3.251539911946896e-07 \n",
      "\n",
      "Epoch:  12991  Learning Rate:  2.2830460951864718e-07  Varinance:  3.246997629872835e-07 \n",
      "\n",
      "Epoch:  12992  Learning Rate:  2.2807641902339219e-07  Varinance:  3.2424616932003375e-07 \n",
      "\n",
      "Epoch:  12993  Learning Rate:  2.278484566045748e-07  Varinance:  3.23793209306511e-07 \n",
      "\n",
      "Epoch:  12994  Learning Rate:  2.276207220342334e-07  Varinance:  3.233408820615242e-07 \n",
      "\n",
      "Epoch:  12995  Learning Rate:  2.2739321508463298e-07  Varinance:  3.228891867011189e-07 \n",
      "\n",
      "Epoch:  12996  Learning Rate:  2.2716593552826662e-07  Varinance:  3.2243812234257547e-07 \n",
      "\n",
      "Epoch:  12997  Learning Rate:  2.2693888313785434e-07  Varinance:  3.219876881044087e-07 \n",
      "\n",
      "Epoch:  12998  Learning Rate:  2.2671205768634446e-07  Varinance:  3.215378831063611e-07 \n",
      "\n",
      "Epoch:  12999  Learning Rate:  2.264854589469112e-07  Varinance:  3.210887064694085e-07 \n",
      "\n",
      "Epoch:  13000  Learning Rate:  2.2625908669295533e-07  Varinance:  3.2064015731575343e-07 \n",
      "\n",
      "Epoch:  13001  Learning Rate:  2.2603294069810543e-07  Varinance:  3.201922347688248e-07 \n",
      "\n",
      "Epoch:  13002  Learning Rate:  2.2580702073621507e-07  Varinance:  3.197449379532759e-07 \n",
      "\n",
      "Epoch:  13003  Learning Rate:  2.2558132658136385e-07  Varinance:  3.19298265994983e-07 \n",
      "\n",
      "Epoch:  13004  Learning Rate:  2.253558580078584e-07  Varinance:  3.1885221802104456e-07 \n",
      "\n",
      "Epoch:  13005  Learning Rate:  2.2513061479022976e-07  Varinance:  3.1840679315977494e-07 \n",
      "\n",
      "Epoch:  13006  Learning Rate:  2.2490559670323426e-07  Varinance:  3.1796199054070977e-07 \n",
      "\n",
      "Epoch:  13007  Learning Rate:  2.246808035218546e-07  Varinance:  3.1751780929459953e-07 \n",
      "\n",
      "Epoch:  13008  Learning Rate:  2.2445623502129717e-07  Varinance:  3.1707424855340895e-07 \n",
      "\n",
      "Epoch:  13009  Learning Rate:  2.2423189097699352e-07  Varinance:  3.166313074503154e-07 \n",
      "\n",
      "Epoch:  13010  Learning Rate:  2.2400777116459906e-07  Varinance:  3.161889851197072e-07 \n",
      "\n",
      "Epoch:  13011  Learning Rate:  2.2378387535999492e-07  Varinance:  3.157472806971829e-07 \n",
      "\n",
      "Epoch:  13012  Learning Rate:  2.235602033392847e-07  Varinance:  3.153061933195454e-07 \n",
      "\n",
      "Epoch:  13013  Learning Rate:  2.2333675487879612e-07  Varinance:  3.1486572212480676e-07 \n",
      "\n",
      "Epoch:  13014  Learning Rate:  2.231135297550814e-07  Varinance:  3.1442586625218196e-07 \n",
      "\n",
      "Epoch:  13015  Learning Rate:  2.2289052774491504e-07  Varinance:  3.1398662484208866e-07 \n",
      "\n",
      "Epoch:  13016  Learning Rate:  2.2266774862529458e-07  Varinance:  3.135479970361451e-07 \n",
      "\n",
      "Epoch:  13017  Learning Rate:  2.224451921734417e-07  Varinance:  3.131099819771689e-07 \n",
      "\n",
      "Epoch:  13018  Learning Rate:  2.222228581667995e-07  Varinance:  3.12672578809176e-07 \n",
      "\n",
      "Epoch:  13019  Learning Rate:  2.2200074638303368e-07  Varinance:  3.12235786677375e-07 \n",
      "\n",
      "Epoch:  13020  Learning Rate:  2.2177885660003305e-07  Varinance:  3.1179960472817185e-07 \n",
      "\n",
      "Epoch:  13021  Learning Rate:  2.2155718859590755e-07  Varinance:  3.1136403210916377e-07 \n",
      "\n",
      "Epoch:  13022  Learning Rate:  2.213357421489887e-07  Varinance:  3.1092906796913883e-07 \n",
      "\n",
      "Epoch:  13023  Learning Rate:  2.2111451703783085e-07  Varinance:  3.1049471145807416e-07 \n",
      "\n",
      "Epoch:  13024  Learning Rate:  2.2089351304120844e-07  Varinance:  3.100609617271345e-07 \n",
      "\n",
      "Epoch:  13025  Learning Rate:  2.206727299381175e-07  Varinance:  3.096278179286712e-07 \n",
      "\n",
      "Epoch:  13026  Learning Rate:  2.2045216750777449e-07  Varinance:  3.0919527921621666e-07 \n",
      "\n",
      "Epoch:  13027  Learning Rate:  2.2023182552961774e-07  Varinance:  3.0876334474448905e-07 \n",
      "\n",
      "Epoch:  13028  Learning Rate:  2.2001170378330487e-07  Varinance:  3.0833201366938616e-07 \n",
      "\n",
      "Epoch:  13029  Learning Rate:  2.1979180204871375e-07  Varinance:  3.079012851479851e-07 \n",
      "\n",
      "Epoch:  13030  Learning Rate:  2.1957212010594335e-07  Varinance:  3.0747115833854033e-07 \n",
      "\n",
      "Epoch:  13031  Learning Rate:  2.1935265773531138e-07  Varinance:  3.070416324004824e-07 \n",
      "\n",
      "Epoch:  13032  Learning Rate:  2.1913341471735504e-07  Varinance:  3.06612706494417e-07 \n",
      "\n",
      "Epoch:  13033  Learning Rate:  2.1891439083283205e-07  Varinance:  3.061843797821193e-07 \n",
      "\n",
      "Epoch:  13034  Learning Rate:  2.1869558586271816e-07  Varinance:  3.057566514265387e-07 \n",
      "\n",
      "Epoch:  13035  Learning Rate:  2.1847699958820795e-07  Varinance:  3.0532952059179275e-07 \n",
      "\n",
      "Epoch:  13036  Learning Rate:  2.182586317907159e-07  Varinance:  3.0490298644316675e-07 \n",
      "\n",
      "Epoch:  13037  Learning Rate:  2.1804048225187386e-07  Varinance:  3.044770481471121e-07 \n",
      "\n",
      "Epoch:  13038  Learning Rate:  2.1782255075353185e-07  Varinance:  3.0405170487124455e-07 \n",
      "\n",
      "Epoch:  13039  Learning Rate:  2.1760483707775915e-07  Varinance:  3.036269557843438e-07 \n",
      "\n",
      "Epoch:  13040  Learning Rate:  2.1738734100684162e-07  Varinance:  3.032028000563475e-07 \n",
      "\n",
      "Epoch:  13041  Learning Rate:  2.1717006232328327e-07  Varinance:  3.027792368583561e-07 \n",
      "\n",
      "Epoch:  13042  Learning Rate:  2.169530008098049e-07  Varinance:  3.0235626536262684e-07 \n",
      "\n",
      "Epoch:  13043  Learning Rate:  2.167361562493458e-07  Varinance:  3.0193388474257336e-07 \n",
      "\n",
      "Epoch:  13044  Learning Rate:  2.1651952842506103e-07  Varinance:  3.015120941727641e-07 \n",
      "\n",
      "Epoch:  13045  Learning Rate:  2.1630311712032236e-07  Varinance:  3.010908928289204e-07 \n",
      "\n",
      "Epoch:  13046  Learning Rate:  2.1608692211871921e-07  Varinance:  3.006702798879162e-07 \n",
      "\n",
      "Epoch:  13047  Learning Rate:  2.158709432040562e-07  Varinance:  3.0025025452777233e-07 \n",
      "\n",
      "Epoch:  13048  Learning Rate:  2.1565518016035398e-07  Varinance:  2.9983081592766085e-07 \n",
      "\n",
      "Epoch:  13049  Learning Rate:  2.154396327718503e-07  Varinance:  2.9941196326789945e-07 \n",
      "\n",
      "Epoch:  13050  Learning Rate:  2.152243008229973e-07  Varinance:  2.9899369572995104e-07 \n",
      "\n",
      "Epoch:  13051  Learning Rate:  2.1500918409846273e-07  Varinance:  2.9857601249642177e-07 \n",
      "\n",
      "Epoch:  13052  Learning Rate:  2.1479428238313053e-07  Varinance:  2.981589127510599e-07 \n",
      "\n",
      "Epoch:  13053  Learning Rate:  2.145795954620986e-07  Varinance:  2.9774239567875484e-07 \n",
      "\n",
      "Epoch:  13054  Learning Rate:  2.1436512312067961e-07  Varinance:  2.973264604655316e-07 \n",
      "\n",
      "Epoch:  13055  Learning Rate:  2.1415086514440205e-07  Varinance:  2.9691110629855536e-07 \n",
      "\n",
      "Epoch:  13056  Learning Rate:  2.1393682131900744e-07  Varinance:  2.9649633236612594e-07 \n",
      "\n",
      "Epoch:  13057  Learning Rate:  2.13722991430452e-07  Varinance:  2.9608213785767683e-07 \n",
      "\n",
      "Epoch:  13058  Learning Rate:  2.135093752649054e-07  Varinance:  2.956685219637741e-07 \n",
      "\n",
      "Epoch:  13059  Learning Rate:  2.1329597260875224e-07  Varinance:  2.9525548387611427e-07 \n",
      "\n",
      "Epoch:  13060  Learning Rate:  2.130827832485895e-07  Varinance:  2.948430227875243e-07 \n",
      "\n",
      "Epoch:  13061  Learning Rate:  2.128698069712273e-07  Varinance:  2.944311378919556e-07 \n",
      "\n",
      "Epoch:  13062  Learning Rate:  2.1265704356369026e-07  Varinance:  2.940198283844887e-07 \n",
      "\n",
      "Epoch:  13063  Learning Rate:  2.1244449281321447e-07  Varinance:  2.9360909346132743e-07 \n",
      "\n",
      "Epoch:  13064  Learning Rate:  2.1223215450724885e-07  Varinance:  2.931989323197986e-07 \n",
      "\n",
      "Epoch:  13065  Learning Rate:  2.120200284334558e-07  Varinance:  2.9278934415835034e-07 \n",
      "\n",
      "Epoch:  13066  Learning Rate:  2.118081143797088e-07  Varinance:  2.9238032817655047e-07 \n",
      "\n",
      "Epoch:  13067  Learning Rate:  2.1159641213409354e-07  Varinance:  2.919718835750859e-07 \n",
      "\n",
      "Epoch:  13068  Learning Rate:  2.1138492148490838e-07  Varinance:  2.915640095557574e-07 \n",
      "\n",
      "Epoch:  13069  Learning Rate:  2.111736422206623e-07  Varinance:  2.9115670532148355e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13070  Learning Rate:  2.1096257413007567e-07  Varinance:  2.9074997007629557e-07 \n",
      "\n",
      "Epoch:  13071  Learning Rate:  2.1075171700208112e-07  Varinance:  2.903438030253364e-07 \n",
      "\n",
      "Epoch:  13072  Learning Rate:  2.1054107062582116e-07  Varinance:  2.899382033748598e-07 \n",
      "\n",
      "Epoch:  13073  Learning Rate:  2.1033063479064935e-07  Varinance:  2.895331703322278e-07 \n",
      "\n",
      "Epoch:  13074  Learning Rate:  2.1012040928612954e-07  Varinance:  2.8912870310591126e-07 \n",
      "\n",
      "Epoch:  13075  Learning Rate:  2.0991039390203683e-07  Varinance:  2.8872480090548336e-07 \n",
      "\n",
      "Epoch:  13076  Learning Rate:  2.0970058842835557e-07  Varinance:  2.883214629416247e-07 \n",
      "\n",
      "Epoch:  13077  Learning Rate:  2.0949099265527978e-07  Varinance:  2.8791868842611756e-07 \n",
      "\n",
      "Epoch:  13078  Learning Rate:  2.092816063732145e-07  Varinance:  2.8751647657184507e-07 \n",
      "\n",
      "Epoch:  13079  Learning Rate:  2.0907242937277306e-07  Varinance:  2.8711482659279026e-07 \n",
      "\n",
      "Epoch:  13080  Learning Rate:  2.08863461444778e-07  Varinance:  2.8671373770403394e-07 \n",
      "\n",
      "Epoch:  13081  Learning Rate:  2.0865470238026222e-07  Varinance:  2.8631320912175465e-07 \n",
      "\n",
      "Epoch:  13082  Learning Rate:  2.0844615197046617e-07  Varinance:  2.8591324006322264e-07 \n",
      "\n",
      "Epoch:  13083  Learning Rate:  2.0823781000683906e-07  Varinance:  2.8551382974680485e-07 \n",
      "\n",
      "Epoch:  13084  Learning Rate:  2.0802967628103974e-07  Varinance:  2.85114977391959e-07 \n",
      "\n",
      "Epoch:  13085  Learning Rate:  2.0782175058493402e-07  Varinance:  2.8471668221923315e-07 \n",
      "\n",
      "Epoch:  13086  Learning Rate:  2.0761403271059577e-07  Varinance:  2.843189434502644e-07 \n",
      "\n",
      "Epoch:  13087  Learning Rate:  2.0740652245030798e-07  Varinance:  2.839217603077781e-07 \n",
      "\n",
      "Epoch:  13088  Learning Rate:  2.071992195965599e-07  Varinance:  2.835251320155823e-07 \n",
      "\n",
      "Epoch:  13089  Learning Rate:  2.0699212394204866e-07  Varinance:  2.8312905779857265e-07 \n",
      "\n",
      "Epoch:  13090  Learning Rate:  2.067852352796783e-07  Varinance:  2.8273353688272634e-07 \n",
      "\n",
      "Epoch:  13091  Learning Rate:  2.0657855340256075e-07  Varinance:  2.8233856849510184e-07 \n",
      "\n",
      "Epoch:  13092  Learning Rate:  2.0637207810401385e-07  Varinance:  2.819441518638376e-07 \n",
      "\n",
      "Epoch:  13093  Learning Rate:  2.0616580917756192e-07  Varinance:  2.8155028621815017e-07 \n",
      "\n",
      "Epoch:  13094  Learning Rate:  2.0595974641693667e-07  Varinance:  2.811569707883339e-07 \n",
      "\n",
      "Epoch:  13095  Learning Rate:  2.0575388961607504e-07  Varinance:  2.807642048057554e-07 \n",
      "\n",
      "Epoch:  13096  Learning Rate:  2.0554823856911977e-07  Varinance:  2.8037198750285797e-07 \n",
      "\n",
      "Epoch:  13097  Learning Rate:  2.0534279307042055e-07  Varinance:  2.799803181131563e-07 \n",
      "\n",
      "Epoch:  13098  Learning Rate:  2.0513755291453153e-07  Varinance:  2.7958919587123556e-07 \n",
      "\n",
      "Epoch:  13099  Learning Rate:  2.0493251789621215e-07  Varinance:  2.7919862001275053e-07 \n",
      "\n",
      "Epoch:  13100  Learning Rate:  2.0472768781042814e-07  Varinance:  2.7880858977442355e-07 \n",
      "\n",
      "Epoch:  13101  Learning Rate:  2.0452306245234897e-07  Varinance:  2.784191043940442e-07 \n",
      "\n",
      "Epoch:  13102  Learning Rate:  2.0431864161734894e-07  Varinance:  2.780301631104639e-07 \n",
      "\n",
      "Epoch:  13103  Learning Rate:  2.0411442510100794e-07  Varinance:  2.776417651636004e-07 \n",
      "\n",
      "Epoch:  13104  Learning Rate:  2.03910412699109e-07  Varinance:  2.7725390979443216e-07 \n",
      "\n",
      "Epoch:  13105  Learning Rate:  2.037066042076398e-07  Varinance:  2.768665962449982e-07 \n",
      "\n",
      "Epoch:  13106  Learning Rate:  2.035029994227914e-07  Varinance:  2.76479823758396e-07 \n",
      "\n",
      "Epoch:  13107  Learning Rate:  2.0329959814095973e-07  Varinance:  2.7609359157878077e-07 \n",
      "\n",
      "Epoch:  13108  Learning Rate:  2.0309640015874316e-07  Varinance:  2.7570789895136435e-07 \n",
      "\n",
      "Epoch:  13109  Learning Rate:  2.0289340527294332e-07  Varinance:  2.7532274512241014e-07 \n",
      "\n",
      "Epoch:  13110  Learning Rate:  2.02690613280566e-07  Varinance:  2.749381293392374e-07 \n",
      "\n",
      "Epoch:  13111  Learning Rate:  2.0248802397881887e-07  Varinance:  2.74554050850216e-07 \n",
      "\n",
      "Epoch:  13112  Learning Rate:  2.0228563716511218e-07  Varinance:  2.741705089047656e-07 \n",
      "\n",
      "Epoch:  13113  Learning Rate:  2.0208345263705993e-07  Varinance:  2.7378750275335455e-07 \n",
      "\n",
      "Epoch:  13114  Learning Rate:  2.0188147019247716e-07  Varinance:  2.734050316474982e-07 \n",
      "\n",
      "Epoch:  13115  Learning Rate:  2.0167968962938106e-07  Varinance:  2.730230948397584e-07 \n",
      "\n",
      "Epoch:  13116  Learning Rate:  2.0147811074599169e-07  Varinance:  2.7264169158373856e-07 \n",
      "\n",
      "Epoch:  13117  Learning Rate:  2.0127673334072988e-07  Varinance:  2.722608211340872e-07 \n",
      "\n",
      "Epoch:  13118  Learning Rate:  2.0107555721221783e-07  Varinance:  2.718804827464935e-07 \n",
      "\n",
      "Epoch:  13119  Learning Rate:  2.0087458215928012e-07  Varinance:  2.7150067567768626e-07 \n",
      "\n",
      "Epoch:  13120  Learning Rate:  2.0067380798094134e-07  Varinance:  2.711213991854325e-07 \n",
      "\n",
      "Epoch:  13121  Learning Rate:  2.004732344764272e-07  Varinance:  2.7074265252853637e-07 \n",
      "\n",
      "Epoch:  13122  Learning Rate:  2.0027286144516392e-07  Varinance:  2.7036443496683817e-07 \n",
      "\n",
      "Epoch:  13123  Learning Rate:  2.0007268868677913e-07  Varinance:  2.699867457612094e-07 \n",
      "\n",
      "Epoch:  13124  Learning Rate:  1.998727160010997e-07  Varinance:  2.6960958417355696e-07 \n",
      "\n",
      "Epoch:  13125  Learning Rate:  1.9967294318815256e-07  Varinance:  2.692329494668179e-07 \n",
      "\n",
      "Epoch:  13126  Learning Rate:  1.994733700481656e-07  Varinance:  2.6885684090495884e-07 \n",
      "\n",
      "Epoch:  13127  Learning Rate:  1.9927399638156531e-07  Varinance:  2.684812577529747e-07 \n",
      "\n",
      "Epoch:  13128  Learning Rate:  1.9907482198897763e-07  Varinance:  2.6810619927688714e-07 \n",
      "\n",
      "Epoch:  13129  Learning Rate:  1.9887584667122894e-07  Varinance:  2.6773166474374414e-07 \n",
      "\n",
      "Epoch:  13130  Learning Rate:  1.9867707022934348e-07  Varinance:  2.6735765342161466e-07 \n",
      "\n",
      "Epoch:  13131  Learning Rate:  1.9847849246454442e-07  Varinance:  2.6698416457959314e-07 \n",
      "\n",
      "Epoch:  13132  Learning Rate:  1.9828011317825473e-07  Varinance:  2.6661119748779386e-07 \n",
      "\n",
      "Epoch:  13133  Learning Rate:  1.9808193217209479e-07  Varinance:  2.6623875141735103e-07 \n",
      "\n",
      "Epoch:  13134  Learning Rate:  1.9788394924788348e-07  Varinance:  2.658668256404168e-07 \n",
      "\n",
      "Epoch:  13135  Learning Rate:  1.9768616420763755e-07  Varinance:  2.6549541943016033e-07 \n",
      "\n",
      "Epoch:  13136  Learning Rate:  1.9748857685357268e-07  Varinance:  2.651245320607669e-07 \n",
      "\n",
      "Epoch:  13137  Learning Rate:  1.972911869881011e-07  Varinance:  2.6475416280743293e-07 \n",
      "\n",
      "Epoch:  13138  Learning Rate:  1.9709399441383257e-07  Varinance:  2.643843109463702e-07 \n",
      "\n",
      "Epoch:  13139  Learning Rate:  1.968969989335753e-07  Varinance:  2.6401497575480073e-07 \n",
      "\n",
      "Epoch:  13140  Learning Rate:  1.9670020035033335e-07  Varinance:  2.636461565109562e-07 \n",
      "\n",
      "Epoch:  13141  Learning Rate:  1.9650359846730773e-07  Varinance:  2.632778524940765e-07 \n",
      "\n",
      "Epoch:  13142  Learning Rate:  1.9630719308789737e-07  Varinance:  2.6291006298440843e-07 \n",
      "\n",
      "Epoch:  13143  Learning Rate:  1.9611098401569644e-07  Varinance:  2.6254278726320524e-07 \n",
      "\n",
      "Epoch:  13144  Learning Rate:  1.9591497105449555e-07  Varinance:  2.621760246127214e-07 \n",
      "\n",
      "Epoch:  13145  Learning Rate:  1.9571915400828232e-07  Varinance:  2.6180977431621684e-07 \n",
      "\n",
      "Epoch:  13146  Learning Rate:  1.9552353268123945e-07  Varinance:  2.6144403565795186e-07 \n",
      "\n",
      "Epoch:  13147  Learning Rate:  1.9532810687774522e-07  Varinance:  2.610788079231866e-07 \n",
      "\n",
      "Epoch:  13148  Learning Rate:  1.9513287640237446e-07  Varinance:  2.607140903981796e-07 \n",
      "\n",
      "Epoch:  13149  Learning Rate:  1.949378410598964e-07  Varinance:  2.6034988237018666e-07 \n",
      "\n",
      "Epoch:  13150  Learning Rate:  1.947430006552756e-07  Varinance:  2.599861831274599e-07 \n",
      "\n",
      "Epoch:  13151  Learning Rate:  1.9454835499367136e-07  Varinance:  2.5962299195924315e-07 \n",
      "\n",
      "Epoch:  13152  Learning Rate:  1.9435390388043867e-07  Varinance:  2.5926030815577583e-07 \n",
      "\n",
      "Epoch:  13153  Learning Rate:  1.9415964712112606e-07  Varinance:  2.5889813100828813e-07 \n",
      "\n",
      "Epoch:  13154  Learning Rate:  1.9396558452147642e-07  Varinance:  2.5853645980900006e-07 \n",
      "\n",
      "Epoch:  13155  Learning Rate:  1.9377171588742778e-07  Varinance:  2.5817529385112065e-07 \n",
      "\n",
      "Epoch:  13156  Learning Rate:  1.935780410251112e-07  Varinance:  2.5781463242884603e-07 \n",
      "\n",
      "Epoch:  13157  Learning Rate:  1.933845597408514e-07  Varinance:  2.574544748373595e-07 \n",
      "\n",
      "Epoch:  13158  Learning Rate:  1.9319127184116783e-07  Varinance:  2.57094820372826e-07 \n",
      "\n",
      "Epoch:  13159  Learning Rate:  1.929981771327722e-07  Varinance:  2.567356683323966e-07 \n",
      "\n",
      "Epoch:  13160  Learning Rate:  1.9280527542256944e-07  Varinance:  2.563770180142032e-07 \n",
      "\n",
      "Epoch:  13161  Learning Rate:  1.9261256651765851e-07  Varinance:  2.560188687173583e-07 \n",
      "\n",
      "Epoch:  13162  Learning Rate:  1.9242005022533016e-07  Varinance:  2.556612197419533e-07 \n",
      "\n",
      "Epoch:  13163  Learning Rate:  1.922277263530677e-07  Varinance:  2.5530407038905754e-07 \n",
      "\n",
      "Epoch:  13164  Learning Rate:  1.9203559470854795e-07  Varinance:  2.5494741996071774e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13165  Learning Rate:  1.9184365509963895e-07  Varinance:  2.5459126775995256e-07 \n",
      "\n",
      "Epoch:  13166  Learning Rate:  1.9165190733440102e-07  Varinance:  2.5423561309075736e-07 \n",
      "\n",
      "Epoch:  13167  Learning Rate:  1.9146035122108603e-07  Varinance:  2.538804552580987e-07 \n",
      "\n",
      "Epoch:  13168  Learning Rate:  1.9126898656813858e-07  Varinance:  2.535257935679143e-07 \n",
      "\n",
      "Epoch:  13169  Learning Rate:  1.9107781318419364e-07  Varinance:  2.5317162732711116e-07 \n",
      "\n",
      "Epoch:  13170  Learning Rate:  1.9088683087807747e-07  Varinance:  2.528179558435647e-07 \n",
      "\n",
      "Epoch:  13171  Learning Rate:  1.906960394588084e-07  Varinance:  2.5246477842611813e-07 \n",
      "\n",
      "Epoch:  13172  Learning Rate:  1.905054387355947e-07  Varinance:  2.521120943845773e-07 \n",
      "\n",
      "Epoch:  13173  Learning Rate:  1.9031502851783528e-07  Varinance:  2.517599030297151e-07 \n",
      "\n",
      "Epoch:  13174  Learning Rate:  1.9012480861512058e-07  Varinance:  2.5140820367326635e-07 \n",
      "\n",
      "Epoch:  13175  Learning Rate:  1.8993477883723032e-07  Varinance:  2.5105699562792726e-07 \n",
      "\n",
      "Epoch:  13176  Learning Rate:  1.8974493899413438e-07  Varinance:  2.5070627820735425e-07 \n",
      "\n",
      "Epoch:  13177  Learning Rate:  1.8955528889599361e-07  Varinance:  2.5035605072616245e-07 \n",
      "\n",
      "Epoch:  13178  Learning Rate:  1.893658283531575e-07  Varinance:  2.5000631249992537e-07 \n",
      "\n",
      "Epoch:  13179  Learning Rate:  1.891765571761652e-07  Varinance:  2.496570628451701e-07 \n",
      "\n",
      "Epoch:  13180  Learning Rate:  1.889874751757462e-07  Varinance:  2.4930830107938105e-07 \n",
      "\n",
      "Epoch:  13181  Learning Rate:  1.887985821628181e-07  Varinance:  2.489600265209951e-07 \n",
      "\n",
      "Epoch:  13182  Learning Rate:  1.886098779484879e-07  Varinance:  2.486122384894015e-07 \n",
      "\n",
      "Epoch:  13183  Learning Rate:  1.8842136234405105e-07  Varinance:  2.482649363049401e-07 \n",
      "\n",
      "Epoch:  13184  Learning Rate:  1.8823303516099254e-07  Varinance:  2.4791811928890035e-07 \n",
      "\n",
      "Epoch:  13185  Learning Rate:  1.880448962109849e-07  Varinance:  2.4757178676352043e-07 \n",
      "\n",
      "Epoch:  13186  Learning Rate:  1.8785694530588882e-07  Varinance:  2.472259380519831e-07 \n",
      "\n",
      "Epoch:  13187  Learning Rate:  1.8766918225775401e-07  Varinance:  2.4688057247841893e-07 \n",
      "\n",
      "Epoch:  13188  Learning Rate:  1.8748160687881713e-07  Varinance:  2.4653568936790184e-07 \n",
      "\n",
      "Epoch:  13189  Learning Rate:  1.8729421898150237e-07  Varinance:  2.461912880464487e-07 \n",
      "\n",
      "Epoch:  13190  Learning Rate:  1.8710701837842258e-07  Varinance:  2.4584736784101793e-07 \n",
      "\n",
      "Epoch:  13191  Learning Rate:  1.8692000488237674e-07  Varinance:  2.455039280795079e-07 \n",
      "\n",
      "Epoch:  13192  Learning Rate:  1.8673317830635103e-07  Varinance:  2.45160968090757e-07 \n",
      "\n",
      "Epoch:  13193  Learning Rate:  1.865465384635195e-07  Varinance:  2.4481848720453867e-07 \n",
      "\n",
      "Epoch:  13194  Learning Rate:  1.8636008516724202e-07  Varinance:  2.444764847515649e-07 \n",
      "\n",
      "Epoch:  13195  Learning Rate:  1.8617381823106488e-07  Varinance:  2.441349600634821e-07 \n",
      "\n",
      "Epoch:  13196  Learning Rate:  1.8598773746872184e-07  Varinance:  2.437939124728702e-07 \n",
      "\n",
      "Epoch:  13197  Learning Rate:  1.8580184269413174e-07  Varinance:  2.4345334131324154e-07 \n",
      "\n",
      "Epoch:  13198  Learning Rate:  1.856161337213998e-07  Varinance:  2.4311324591903946e-07 \n",
      "\n",
      "Epoch:  13199  Learning Rate:  1.8543061036481675e-07  Varinance:  2.42773625625638e-07 \n",
      "\n",
      "Epoch:  13200  Learning Rate:  1.8524527243885986e-07  Varinance:  2.4243447976933697e-07 \n",
      "\n",
      "Epoch:  13201  Learning Rate:  1.8506011975819084e-07  Varinance:  2.420958076873661e-07 \n",
      "\n",
      "Epoch:  13202  Learning Rate:  1.8487515213765663e-07  Varinance:  2.417576087178799e-07 \n",
      "\n",
      "Epoch:  13203  Learning Rate:  1.8469036939229032e-07  Varinance:  2.414198821999577e-07 \n",
      "\n",
      "Epoch:  13204  Learning Rate:  1.845057713373088e-07  Varinance:  2.4108262747360184e-07 \n",
      "\n",
      "Epoch:  13205  Learning Rate:  1.8432135778811365e-07  Varinance:  2.407458438797369e-07 \n",
      "\n",
      "Epoch:  13206  Learning Rate:  1.84137128560292e-07  Varinance:  2.404095307602088e-07 \n",
      "\n",
      "Epoch:  13207  Learning Rate:  1.8395308346961425e-07  Varinance:  2.4007368745778064e-07 \n",
      "\n",
      "Epoch:  13208  Learning Rate:  1.8376922233203495e-07  Varinance:  2.3973831331613575e-07 \n",
      "\n",
      "Epoch:  13209  Learning Rate:  1.8358554496369363e-07  Varinance:  2.3940340767987386e-07 \n",
      "\n",
      "Epoch:  13210  Learning Rate:  1.8340205118091257e-07  Varinance:  2.390689698945101e-07 \n",
      "\n",
      "Epoch:  13211  Learning Rate:  1.8321874080019767e-07  Varinance:  2.3873499930647393e-07 \n",
      "\n",
      "Epoch:  13212  Learning Rate:  1.8303561363823914e-07  Varinance:  2.3840149526310777e-07 \n",
      "\n",
      "Epoch:  13213  Learning Rate:  1.8285266951190952e-07  Varinance:  2.3806845711266666e-07 \n",
      "\n",
      "Epoch:  13214  Learning Rate:  1.8266990823826462e-07  Varinance:  2.3773588420431365e-07 \n",
      "\n",
      "Epoch:  13215  Learning Rate:  1.824873296345429e-07  Varinance:  2.3740377588812334e-07 \n",
      "\n",
      "Epoch:  13216  Learning Rate:  1.823049335181663e-07  Varinance:  2.3707213151507755e-07 \n",
      "\n",
      "Epoch:  13217  Learning Rate:  1.8212271970673844e-07  Varinance:  2.367409504370647e-07 \n",
      "\n",
      "Epoch:  13218  Learning Rate:  1.8194068801804514e-07  Varinance:  2.3641023200687863e-07 \n",
      "\n",
      "Epoch:  13219  Learning Rate:  1.8175883827005533e-07  Varinance:  2.3607997557821814e-07 \n",
      "\n",
      "Epoch:  13220  Learning Rate:  1.8157717028091895e-07  Varinance:  2.357501805056823e-07 \n",
      "\n",
      "Epoch:  13221  Learning Rate:  1.8139568386896764e-07  Varinance:  2.3542084614477445e-07 \n",
      "\n",
      "Epoch:  13222  Learning Rate:  1.8121437885271566e-07  Varinance:  2.350919718518972e-07 \n",
      "\n",
      "Epoch:  13223  Learning Rate:  1.8103325505085763e-07  Varinance:  2.3476355698435245e-07 \n",
      "\n",
      "Epoch:  13224  Learning Rate:  1.808523122822694e-07  Varinance:  2.3443560090033985e-07 \n",
      "\n",
      "Epoch:  13225  Learning Rate:  1.8067155036600884e-07  Varinance:  2.341081029589556e-07 \n",
      "\n",
      "Epoch:  13226  Learning Rate:  1.8049096912131372e-07  Varinance:  2.337810625201921e-07 \n",
      "\n",
      "Epoch:  13227  Learning Rate:  1.8031056836760245e-07  Varinance:  2.334544789449333e-07 \n",
      "\n",
      "Epoch:  13228  Learning Rate:  1.801303479244749e-07  Varinance:  2.3312835159495843e-07 \n",
      "\n",
      "Epoch:  13229  Learning Rate:  1.7995030761171025e-07  Varinance:  2.328026798329375e-07 \n",
      "\n",
      "Epoch:  13230  Learning Rate:  1.7977044724926823e-07  Varinance:  2.324774630224308e-07 \n",
      "\n",
      "Epoch:  13231  Learning Rate:  1.795907666572881e-07  Varinance:  2.3215270052788782e-07 \n",
      "\n",
      "Epoch:  13232  Learning Rate:  1.7941126565608994e-07  Varinance:  2.3182839171464577e-07 \n",
      "\n",
      "Epoch:  13233  Learning Rate:  1.7923194406617237e-07  Varinance:  2.3150453594892936e-07 \n",
      "\n",
      "Epoch:  13234  Learning Rate:  1.7905280170821349e-07  Varinance:  2.3118113259784608e-07 \n",
      "\n",
      "Epoch:  13235  Learning Rate:  1.7887383840307155e-07  Varinance:  2.3085818102939017e-07 \n",
      "\n",
      "Epoch:  13236  Learning Rate:  1.7869505397178293e-07  Varinance:  2.305356806124378e-07 \n",
      "\n",
      "Epoch:  13237  Learning Rate:  1.7851644823556285e-07  Varinance:  2.3021363071674693e-07 \n",
      "\n",
      "Epoch:  13238  Learning Rate:  1.7833802101580618e-07  Varinance:  2.2989203071295577e-07 \n",
      "\n",
      "Epoch:  13239  Learning Rate:  1.7815977213408542e-07  Varinance:  2.2957087997258191e-07 \n",
      "\n",
      "Epoch:  13240  Learning Rate:  1.7798170141215132e-07  Varinance:  2.292501778680216e-07 \n",
      "\n",
      "Epoch:  13241  Learning Rate:  1.7780380867193375e-07  Varinance:  2.2892992377254546e-07 \n",
      "\n",
      "Epoch:  13242  Learning Rate:  1.776260937355397e-07  Varinance:  2.2861011706030198e-07 \n",
      "\n",
      "Epoch:  13243  Learning Rate:  1.7744855642525386e-07  Varinance:  2.2829075710631323e-07 \n",
      "\n",
      "Epoch:  13244  Learning Rate:  1.7727119656353957e-07  Varinance:  2.279718432864743e-07 \n",
      "\n",
      "Epoch:  13245  Learning Rate:  1.7709401397303657e-07  Varinance:  2.276533749775521e-07 \n",
      "\n",
      "Epoch:  13246  Learning Rate:  1.7691700847656233e-07  Varinance:  2.2733535155718422e-07 \n",
      "\n",
      "Epoch:  13247  Learning Rate:  1.76740179897111e-07  Varinance:  2.2701777240387849e-07 \n",
      "\n",
      "Epoch:  13248  Learning Rate:  1.765635280578546e-07  Varinance:  2.2670063689700846e-07 \n",
      "\n",
      "Epoch:  13249  Learning Rate:  1.7638705278214097e-07  Varinance:  2.2638394441681716e-07 \n",
      "\n",
      "Epoch:  13250  Learning Rate:  1.7621075389349452e-07  Varinance:  2.2606769434441258e-07 \n",
      "\n",
      "Epoch:  13251  Learning Rate:  1.7603463121561693e-07  Varinance:  2.2575188606176716e-07 \n",
      "\n",
      "Epoch:  13252  Learning Rate:  1.7585868457238527e-07  Varinance:  2.254365189517169e-07 \n",
      "\n",
      "Epoch:  13253  Learning Rate:  1.7568291378785248e-07  Varinance:  2.2512159239795982e-07 \n",
      "\n",
      "Epoch:  13254  Learning Rate:  1.7550731868624846e-07  Varinance:  2.2480710578505572e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13255  Learning Rate:  1.7533189909197775e-07  Varinance:  2.2449305849842174e-07 \n",
      "\n",
      "Epoch:  13256  Learning Rate:  1.751566548296204e-07  Varinance:  2.2417944992433603e-07 \n",
      "\n",
      "Epoch:  13257  Learning Rate:  1.7498158572393283e-07  Varinance:  2.2386627944993324e-07 \n",
      "\n",
      "Epoch:  13258  Learning Rate:  1.7480669159984555e-07  Varinance:  2.235535464632041e-07 \n",
      "\n",
      "Epoch:  13259  Learning Rate:  1.7463197228246445e-07  Varinance:  2.2324125035299443e-07 \n",
      "\n",
      "Epoch:  13260  Learning Rate:  1.7445742759706984e-07  Varinance:  2.2292939050900368e-07 \n",
      "\n",
      "Epoch:  13261  Learning Rate:  1.742830573691177e-07  Varinance:  2.2261796632178485e-07 \n",
      "\n",
      "Epoch:  13262  Learning Rate:  1.7410886142423744e-07  Varinance:  2.2230697718273966e-07 \n",
      "\n",
      "Epoch:  13263  Learning Rate:  1.7393483958823283e-07  Varinance:  2.2199642248412268e-07 \n",
      "\n",
      "Epoch:  13264  Learning Rate:  1.737609916870826e-07  Varinance:  2.2168630161903645e-07 \n",
      "\n",
      "Epoch:  13265  Learning Rate:  1.735873175469385e-07  Varinance:  2.2137661398143154e-07 \n",
      "\n",
      "Epoch:  13266  Learning Rate:  1.7341381699412614e-07  Varinance:  2.2106735896610493e-07 \n",
      "\n",
      "Epoch:  13267  Learning Rate:  1.7324048985514552e-07  Varinance:  2.2075853596869926e-07 \n",
      "\n",
      "Epoch:  13268  Learning Rate:  1.730673359566692e-07  Varinance:  2.2045014438570202e-07 \n",
      "\n",
      "Epoch:  13269  Learning Rate:  1.7289435512554297e-07  Varinance:  2.2014218361444155e-07 \n",
      "\n",
      "Epoch:  13270  Learning Rate:  1.7272154718878657e-07  Varinance:  2.1983465305309048e-07 \n",
      "\n",
      "Epoch:  13271  Learning Rate:  1.7254891197359175e-07  Varinance:  2.1952755210066128e-07 \n",
      "\n",
      "Epoch:  13272  Learning Rate:  1.7237644930732295e-07  Varinance:  2.1922088015700607e-07 \n",
      "\n",
      "Epoch:  13273  Learning Rate:  1.7220415901751814e-07  Varinance:  2.1891463662281525e-07 \n",
      "\n",
      "Epoch:  13274  Learning Rate:  1.7203204093188672e-07  Varinance:  2.1860882089961662e-07 \n",
      "\n",
      "Epoch:  13275  Learning Rate:  1.7186009487831057e-07  Varinance:  2.1830343238977457e-07 \n",
      "\n",
      "Epoch:  13276  Learning Rate:  1.7168832068484328e-07  Varinance:  2.1799847049648622e-07 \n",
      "\n",
      "Epoch:  13277  Learning Rate:  1.7151671817971132e-07  Varinance:  2.176939346237846e-07 \n",
      "\n",
      "Epoch:  13278  Learning Rate:  1.7134528719131183e-07  Varinance:  2.173898241765346e-07 \n",
      "\n",
      "Epoch:  13279  Learning Rate:  1.7117402754821347e-07  Varinance:  2.1708613856043245e-07 \n",
      "\n",
      "Epoch:  13280  Learning Rate:  1.7100293907915724e-07  Varinance:  2.1678287718200458e-07 \n",
      "\n",
      "Epoch:  13281  Learning Rate:  1.7083202161305436e-07  Varinance:  2.1648003944860662e-07 \n",
      "\n",
      "Epoch:  13282  Learning Rate:  1.7066127497898703e-07  Varinance:  2.161776247684226e-07 \n",
      "\n",
      "Epoch:  13283  Learning Rate:  1.7049069900620918e-07  Varinance:  2.1587563255046126e-07 \n",
      "\n",
      "Epoch:  13284  Learning Rate:  1.7032029352414455e-07  Varinance:  2.1557406220455913e-07 \n",
      "\n",
      "Epoch:  13285  Learning Rate:  1.7015005836238735e-07  Varinance:  2.1527291314137634e-07 \n",
      "\n",
      "Epoch:  13286  Learning Rate:  1.69979993350703e-07  Varinance:  2.1497218477239643e-07 \n",
      "\n",
      "Epoch:  13287  Learning Rate:  1.6981009831902611e-07  Varinance:  2.1467187650992496e-07 \n",
      "\n",
      "Epoch:  13288  Learning Rate:  1.6964037309746144e-07  Varinance:  2.1437198776708856e-07 \n",
      "\n",
      "Epoch:  13289  Learning Rate:  1.694708175162843e-07  Varinance:  2.140725179578345e-07 \n",
      "\n",
      "Epoch:  13290  Learning Rate:  1.6930143140593878e-07  Varinance:  2.1377346649692635e-07 \n",
      "\n",
      "Epoch:  13291  Learning Rate:  1.691322145970388e-07  Varinance:  2.1347483279994755e-07 \n",
      "\n",
      "Epoch:  13292  Learning Rate:  1.6896316692036718e-07  Varinance:  2.1317661628329717e-07 \n",
      "\n",
      "Epoch:  13293  Learning Rate:  1.687942882068769e-07  Varinance:  2.1287881636418968e-07 \n",
      "\n",
      "Epoch:  13294  Learning Rate:  1.6862557828768886e-07  Varinance:  2.1258143246065357e-07 \n",
      "\n",
      "Epoch:  13295  Learning Rate:  1.6845703699409286e-07  Varinance:  2.1228446399153027e-07 \n",
      "\n",
      "Epoch:  13296  Learning Rate:  1.682886641575482e-07  Varinance:  2.119879103764739e-07 \n",
      "\n",
      "Epoch:  13297  Learning Rate:  1.6812045960968174e-07  Varinance:  2.1169177103594705e-07 \n",
      "\n",
      "Epoch:  13298  Learning Rate:  1.6795242318228858e-07  Varinance:  2.1139604539122414e-07 \n",
      "\n",
      "Epoch:  13299  Learning Rate:  1.677845547073329e-07  Varinance:  2.111007328643873e-07 \n",
      "\n",
      "Epoch:  13300  Learning Rate:  1.676168540169459e-07  Varinance:  2.1080583287832596e-07 \n",
      "\n",
      "Epoch:  13301  Learning Rate:  1.6744932094342662e-07  Varinance:  2.1051134485673576e-07 \n",
      "\n",
      "Epoch:  13302  Learning Rate:  1.672819553192425e-07  Varinance:  2.1021726822411756e-07 \n",
      "\n",
      "Epoch:  13303  Learning Rate:  1.6711475697702764e-07  Varinance:  2.0992360240577662e-07 \n",
      "\n",
      "Epoch:  13304  Learning Rate:  1.6694772574958342e-07  Varinance:  2.0963034682781908e-07 \n",
      "\n",
      "Epoch:  13305  Learning Rate:  1.6678086146987913e-07  Varinance:  2.0933750091715484e-07 \n",
      "\n",
      "Epoch:  13306  Learning Rate:  1.666141639710502e-07  Varinance:  2.0904506410149376e-07 \n",
      "\n",
      "Epoch:  13307  Learning Rate:  1.6644763308639916e-07  Varinance:  2.087530358093451e-07 \n",
      "\n",
      "Epoch:  13308  Learning Rate:  1.6628126864939477e-07  Varinance:  2.0846141547001644e-07 \n",
      "\n",
      "Epoch:  13309  Learning Rate:  1.6611507049367316e-07  Varinance:  2.0817020251361268e-07 \n",
      "\n",
      "Epoch:  13310  Learning Rate:  1.659490384530359e-07  Varinance:  2.0787939637103557e-07 \n",
      "\n",
      "Epoch:  13311  Learning Rate:  1.6578317236145065e-07  Varinance:  2.0758899647397963e-07 \n",
      "\n",
      "Epoch:  13312  Learning Rate:  1.6561747205305183e-07  Varinance:  2.0729900225493544e-07 \n",
      "\n",
      "Epoch:  13313  Learning Rate:  1.6545193736213888e-07  Varinance:  2.0700941314718573e-07 \n",
      "\n",
      "Epoch:  13314  Learning Rate:  1.652865681231768e-07  Varinance:  2.0672022858480486e-07 \n",
      "\n",
      "Epoch:  13315  Learning Rate:  1.651213641707969e-07  Varinance:  2.0643144800265777e-07 \n",
      "\n",
      "Epoch:  13316  Learning Rate:  1.6495632533979494e-07  Varinance:  2.0614307083639886e-07 \n",
      "\n",
      "Epoch:  13317  Learning Rate:  1.6479145146513174e-07  Varinance:  2.0585509652247175e-07 \n",
      "\n",
      "Epoch:  13318  Learning Rate:  1.6462674238193406e-07  Varinance:  2.0556752449810492e-07 \n",
      "\n",
      "Epoch:  13319  Learning Rate:  1.644621979254925e-07  Varinance:  2.0528035420131536e-07 \n",
      "\n",
      "Epoch:  13320  Learning Rate:  1.6429781793126223e-07  Varinance:  2.0499358507090443e-07 \n",
      "\n",
      "Epoch:  13321  Learning Rate:  1.641336022348639e-07  Varinance:  2.0470721654645732e-07 \n",
      "\n",
      "Epoch:  13322  Learning Rate:  1.6396955067208148e-07  Varinance:  2.0442124806834227e-07 \n",
      "\n",
      "Epoch:  13323  Learning Rate:  1.638056630788634e-07  Varinance:  2.0413567907770915e-07 \n",
      "\n",
      "Epoch:  13324  Learning Rate:  1.6364193929132176e-07  Varinance:  2.0385050901648936e-07 \n",
      "\n",
      "Epoch:  13325  Learning Rate:  1.6347837914573332e-07  Varinance:  2.0356573732739164e-07 \n",
      "\n",
      "Epoch:  13326  Learning Rate:  1.633149824785377e-07  Varinance:  2.0328136345390547e-07 \n",
      "\n",
      "Epoch:  13327  Learning Rate:  1.6315174912633782e-07  Varinance:  2.0299738684029705e-07 \n",
      "\n",
      "Epoch:  13328  Learning Rate:  1.6298867892590098e-07  Varinance:  2.0271380693160888e-07 \n",
      "\n",
      "Epoch:  13329  Learning Rate:  1.6282577171415663e-07  Varinance:  2.0243062317365867e-07 \n",
      "\n",
      "Epoch:  13330  Learning Rate:  1.6266302732819729e-07  Varinance:  2.0214783501303846e-07 \n",
      "\n",
      "Epoch:  13331  Learning Rate:  1.625004456052791e-07  Varinance:  2.0186544189711396e-07 \n",
      "\n",
      "Epoch:  13332  Learning Rate:  1.623380263828201e-07  Varinance:  2.0158344327402075e-07 \n",
      "\n",
      "Epoch:  13333  Learning Rate:  1.621757694984007e-07  Varinance:  2.0130183859266756e-07 \n",
      "\n",
      "Epoch:  13334  Learning Rate:  1.620136747897646e-07  Varinance:  2.0102062730273218e-07 \n",
      "\n",
      "Epoch:  13335  Learning Rate:  1.618517420948168e-07  Varinance:  2.0073980885466122e-07 \n",
      "\n",
      "Epoch:  13336  Learning Rate:  1.6168997125162428e-07  Varinance:  2.0045938269966903e-07 \n",
      "\n",
      "Epoch:  13337  Learning Rate:  1.615283620984168e-07  Varinance:  2.0017934828973654e-07 \n",
      "\n",
      "Epoch:  13338  Learning Rate:  1.6136691447358483e-07  Varinance:  1.9989970507761093e-07 \n",
      "\n",
      "Epoch:  13339  Learning Rate:  1.6120562821568081e-07  Varinance:  1.9962045251680186e-07 \n",
      "\n",
      "Epoch:  13340  Learning Rate:  1.6104450316341817e-07  Varinance:  1.9934159006158446e-07 \n",
      "\n",
      "Epoch:  13341  Learning Rate:  1.6088353915567237e-07  Varinance:  1.9906311716699542e-07 \n",
      "\n",
      "Epoch:  13342  Learning Rate:  1.6072273603147913e-07  Varinance:  1.9878503328883294e-07 \n",
      "\n",
      "Epoch:  13343  Learning Rate:  1.6056209363003504e-07  Varinance:  1.985073378836553e-07 \n",
      "\n",
      "Epoch:  13344  Learning Rate:  1.6040161179069827e-07  Varinance:  1.9823003040877993e-07 \n",
      "\n",
      "Epoch:  13345  Learning Rate:  1.602412903529866e-07  Varinance:  1.9795311032228315e-07 \n",
      "\n",
      "Epoch:  13346  Learning Rate:  1.600811291565784e-07  Varinance:  1.9767657708299618e-07 \n",
      "\n",
      "Epoch:  13347  Learning Rate:  1.5992112804131298e-07  Varinance:  1.974004301505084e-07 \n",
      "\n",
      "Epoch:  13348  Learning Rate:  1.597612868471889e-07  Varinance:  1.971246689851633e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13349  Learning Rate:  1.5960160541436473e-07  Varinance:  1.9684929304805834e-07 \n",
      "\n",
      "Epoch:  13350  Learning Rate:  1.5944208358315954e-07  Varinance:  1.9657430180104383e-07 \n",
      "\n",
      "Epoch:  13351  Learning Rate:  1.5928272119405123e-07  Varinance:  1.9629969470672247e-07 \n",
      "\n",
      "Epoch:  13352  Learning Rate:  1.5912351808767708e-07  Varinance:  1.9602547122844568e-07 \n",
      "\n",
      "Epoch:  13353  Learning Rate:  1.5896447410483459e-07  Varinance:  1.9575163083031654e-07 \n",
      "\n",
      "Epoch:  13354  Learning Rate:  1.5880558908647943e-07  Varinance:  1.9547817297718616e-07 \n",
      "\n",
      "Epoch:  13355  Learning Rate:  1.586468628737266e-07  Varinance:  1.952050971346532e-07 \n",
      "\n",
      "Epoch:  13356  Learning Rate:  1.584882953078496e-07  Varinance:  1.9493240276906287e-07 \n",
      "\n",
      "Epoch:  13357  Learning Rate:  1.5832988623028136e-07  Varinance:  1.9466008934750586e-07 \n",
      "\n",
      "Epoch:  13358  Learning Rate:  1.5817163548261255e-07  Varinance:  1.9438815633781796e-07 \n",
      "\n",
      "Epoch:  13359  Learning Rate:  1.5801354290659216e-07  Varinance:  1.9411660320857636e-07 \n",
      "\n",
      "Epoch:  13360  Learning Rate:  1.5785560834412811e-07  Varinance:  1.9384542942910278e-07 \n",
      "\n",
      "Epoch:  13361  Learning Rate:  1.5769783163728552e-07  Varinance:  1.9357463446945944e-07 \n",
      "\n",
      "Epoch:  13362  Learning Rate:  1.5754021262828748e-07  Varinance:  1.9330421780044888e-07 \n",
      "\n",
      "Epoch:  13363  Learning Rate:  1.5738275115951544e-07  Varinance:  1.93034178893613e-07 \n",
      "\n",
      "Epoch:  13364  Learning Rate:  1.5722544707350768e-07  Varinance:  1.9276451722123192e-07 \n",
      "\n",
      "Epoch:  13365  Learning Rate:  1.5706830021295985e-07  Varinance:  1.9249523225632355e-07 \n",
      "\n",
      "Epoch:  13366  Learning Rate:  1.569113104207256e-07  Varinance:  1.9222632347264001e-07 \n",
      "\n",
      "Epoch:  13367  Learning Rate:  1.567544775398148e-07  Varinance:  1.9195779034467063e-07 \n",
      "\n",
      "Epoch:  13368  Learning Rate:  1.5659780141339433e-07  Varinance:  1.9168963234763813e-07 \n",
      "\n",
      "Epoch:  13369  Learning Rate:  1.5644128188478862e-07  Varinance:  1.914218489574983e-07 \n",
      "\n",
      "Epoch:  13370  Learning Rate:  1.5628491879747783e-07  Varinance:  1.9115443965093918e-07 \n",
      "\n",
      "Epoch:  13371  Learning Rate:  1.5612871199509888e-07  Varinance:  1.9088740390537953e-07 \n",
      "\n",
      "Epoch:  13372  Learning Rate:  1.5597266132144464e-07  Varinance:  1.906207411989691e-07 \n",
      "\n",
      "Epoch:  13373  Learning Rate:  1.5581676662046497e-07  Varinance:  1.9035445101058456e-07 \n",
      "\n",
      "Epoch:  13374  Learning Rate:  1.5566102773626494e-07  Varinance:  1.900885328198325e-07 \n",
      "\n",
      "Epoch:  13375  Learning Rate:  1.5550544451310533e-07  Varinance:  1.8982298610704582e-07 \n",
      "\n",
      "Epoch:  13376  Learning Rate:  1.5535001679540349e-07  Varinance:  1.8955781035328355e-07 \n",
      "\n",
      "Epoch:  13377  Learning Rate:  1.5519474442773137e-07  Varinance:  1.892930050403294e-07 \n",
      "\n",
      "Epoch:  13378  Learning Rate:  1.5503962725481632e-07  Varinance:  1.8902856965069123e-07 \n",
      "\n",
      "Epoch:  13379  Learning Rate:  1.5488466512154177e-07  Varinance:  1.8876450366760033e-07 \n",
      "\n",
      "Epoch:  13380  Learning Rate:  1.547298578729452e-07  Varinance:  1.88500806575008e-07 \n",
      "\n",
      "Epoch:  13381  Learning Rate:  1.5457520535421914e-07  Varinance:  1.8823747785758838e-07 \n",
      "\n",
      "Epoch:  13382  Learning Rate:  1.5442070741071158e-07  Varinance:  1.8797451700073486e-07 \n",
      "\n",
      "Epoch:  13383  Learning Rate:  1.5426636388792432e-07  Varinance:  1.8771192349055973e-07 \n",
      "\n",
      "Epoch:  13384  Learning Rate:  1.5411217463151377e-07  Varinance:  1.8744969681389314e-07 \n",
      "\n",
      "Epoch:  13385  Learning Rate:  1.5395813948729044e-07  Varinance:  1.871878364582821e-07 \n",
      "\n",
      "Epoch:  13386  Learning Rate:  1.5380425830121971e-07  Varinance:  1.8692634191199022e-07 \n",
      "\n",
      "Epoch:  13387  Learning Rate:  1.5365053091942008e-07  Varinance:  1.866652126639939e-07 \n",
      "\n",
      "Epoch:  13388  Learning Rate:  1.534969571881639e-07  Varinance:  1.8640444820398556e-07 \n",
      "\n",
      "Epoch:  13389  Learning Rate:  1.53343536953878e-07  Varinance:  1.8614404802236973e-07 \n",
      "\n",
      "Epoch:  13390  Learning Rate:  1.5319027006314182e-07  Varinance:  1.858840116102628e-07 \n",
      "\n",
      "Epoch:  13391  Learning Rate:  1.530371563626882e-07  Varinance:  1.8562433845949214e-07 \n",
      "\n",
      "Epoch:  13392  Learning Rate:  1.5288419569940397e-07  Varinance:  1.8536502806259498e-07 \n",
      "\n",
      "Epoch:  13393  Learning Rate:  1.5273138792032816e-07  Varinance:  1.851060799128181e-07 \n",
      "\n",
      "Epoch:  13394  Learning Rate:  1.5257873287265274e-07  Varinance:  1.848474935041142e-07 \n",
      "\n",
      "Epoch:  13395  Learning Rate:  1.5242623040372317e-07  Varinance:  1.8458926833114497e-07 \n",
      "\n",
      "Epoch:  13396  Learning Rate:  1.522738803610367e-07  Varinance:  1.8433140388927733e-07 \n",
      "\n",
      "Epoch:  13397  Learning Rate:  1.5212168259224303e-07  Varinance:  1.8407389967458312e-07 \n",
      "\n",
      "Epoch:  13398  Learning Rate:  1.5196963694514489e-07  Varinance:  1.8381675518383823e-07 \n",
      "\n",
      "Epoch:  13399  Learning Rate:  1.5181774326769636e-07  Varinance:  1.8355996991452144e-07 \n",
      "\n",
      "Epoch:  13400  Learning Rate:  1.5166600140800376e-07  Varinance:  1.8330354336481423e-07 \n",
      "\n",
      "Epoch:  13401  Learning Rate:  1.515144112143249e-07  Varinance:  1.830474750335972e-07 \n",
      "\n",
      "Epoch:  13402  Learning Rate:  1.513629725350702e-07  Varinance:  1.8279176442045287e-07 \n",
      "\n",
      "Epoch:  13403  Learning Rate:  1.5121168521880061e-07  Varinance:  1.825364110256622e-07 \n",
      "\n",
      "Epoch:  13404  Learning Rate:  1.510605491142286e-07  Varinance:  1.8228141435020427e-07 \n",
      "\n",
      "Epoch:  13405  Learning Rate:  1.5090956407021855e-07  Varinance:  1.820267738957552e-07 \n",
      "\n",
      "Epoch:  13406  Learning Rate:  1.5075872993578514e-07  Varinance:  1.8177248916468737e-07 \n",
      "\n",
      "Epoch:  13407  Learning Rate:  1.5060804656009395e-07  Varinance:  1.8151855966006883e-07 \n",
      "\n",
      "Epoch:  13408  Learning Rate:  1.5045751379246215e-07  Varinance:  1.8126498488566e-07 \n",
      "\n",
      "Epoch:  13409  Learning Rate:  1.503071314823567e-07  Varinance:  1.810117643459164e-07 \n",
      "\n",
      "Epoch:  13410  Learning Rate:  1.5015689947939496e-07  Varinance:  1.807588975459852e-07 \n",
      "\n",
      "Epoch:  13411  Learning Rate:  1.5000681763334549e-07  Varinance:  1.8050638399170483e-07 \n",
      "\n",
      "Epoch:  13412  Learning Rate:  1.4985688579412615e-07  Varinance:  1.8025422318960413e-07 \n",
      "\n",
      "Epoch:  13413  Learning Rate:  1.4970710381180486e-07  Varinance:  1.8000241464690113e-07 \n",
      "\n",
      "Epoch:  13414  Learning Rate:  1.495574715366001e-07  Varinance:  1.7975095787150308e-07 \n",
      "\n",
      "Epoch:  13415  Learning Rate:  1.4940798881887934e-07  Varinance:  1.7949985237200266e-07 \n",
      "\n",
      "Epoch:  13416  Learning Rate:  1.492586555091598e-07  Varinance:  1.7924909765768096e-07 \n",
      "\n",
      "Epoch:  13417  Learning Rate:  1.4910947145810803e-07  Varinance:  1.7899869323850388e-07 \n",
      "\n",
      "Epoch:  13418  Learning Rate:  1.4896043651654036e-07  Varinance:  1.7874863862512203e-07 \n",
      "\n",
      "Epoch:  13419  Learning Rate:  1.4881155053542163e-07  Varinance:  1.784989333288695e-07 \n",
      "\n",
      "Epoch:  13420  Learning Rate:  1.4866281336586555e-07  Varinance:  1.7824957686176303e-07 \n",
      "\n",
      "Epoch:  13421  Learning Rate:  1.4851422485913553e-07  Varinance:  1.7800056873650177e-07 \n",
      "\n",
      "Epoch:  13422  Learning Rate:  1.4836578486664272e-07  Varinance:  1.7775190846646366e-07 \n",
      "\n",
      "Epoch:  13423  Learning Rate:  1.4821749323994688e-07  Varinance:  1.775035955657083e-07 \n",
      "\n",
      "Epoch:  13424  Learning Rate:  1.480693498307569e-07  Varinance:  1.7725562954897358e-07 \n",
      "\n",
      "Epoch:  13425  Learning Rate:  1.4792135449092905e-07  Varinance:  1.770080099316752e-07 \n",
      "\n",
      "Epoch:  13426  Learning Rate:  1.4777350707246782e-07  Varinance:  1.7676073622990586e-07 \n",
      "\n",
      "Epoch:  13427  Learning Rate:  1.476258074275262e-07  Varinance:  1.7651380796043428e-07 \n",
      "\n",
      "Epoch:  13428  Learning Rate:  1.474782554084043e-07  Varinance:  1.7626722464070477e-07 \n",
      "\n",
      "Epoch:  13429  Learning Rate:  1.4733085086754985e-07  Varinance:  1.7602098578883396e-07 \n",
      "\n",
      "Epoch:  13430  Learning Rate:  1.471835936575588e-07  Varinance:  1.757750909236135e-07 \n",
      "\n",
      "Epoch:  13431  Learning Rate:  1.4703648363117372e-07  Varinance:  1.7552953956450662e-07 \n",
      "\n",
      "Epoch:  13432  Learning Rate:  1.4688952064128448e-07  Varinance:  1.7528433123164793e-07 \n",
      "\n",
      "Epoch:  13433  Learning Rate:  1.4674270454092786e-07  Varinance:  1.750394654458423e-07 \n",
      "\n",
      "Epoch:  13434  Learning Rate:  1.465960351832883e-07  Varinance:  1.7479494172856406e-07 \n",
      "\n",
      "Epoch:  13435  Learning Rate:  1.4644951242169611e-07  Varinance:  1.745507596019566e-07 \n",
      "\n",
      "Epoch:  13436  Learning Rate:  1.463031361096283e-07  Varinance:  1.743069185888291e-07 \n",
      "\n",
      "Epoch:  13437  Learning Rate:  1.4615690610070905e-07  Varinance:  1.740634182126591e-07 \n",
      "\n",
      "Epoch:  13438  Learning Rate:  1.4601082224870808e-07  Varinance:  1.7382025799758928e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13439  Learning Rate:  1.4586488440754127e-07  Varinance:  1.7357743746842705e-07 \n",
      "\n",
      "Epoch:  13440  Learning Rate:  1.4571909243127124e-07  Varinance:  1.7333495615064365e-07 \n",
      "\n",
      "Epoch:  13441  Learning Rate:  1.4557344617410583e-07  Varinance:  1.7309281357037318e-07 \n",
      "\n",
      "Epoch:  13442  Learning Rate:  1.4542794549039847e-07  Varinance:  1.728510092544124e-07 \n",
      "\n",
      "Epoch:  13443  Learning Rate:  1.4528259023464894e-07  Varinance:  1.7260954273021725e-07 \n",
      "\n",
      "Epoch:  13444  Learning Rate:  1.451373802615018e-07  Varinance:  1.7236841352590562e-07 \n",
      "\n",
      "Epoch:  13445  Learning Rate:  1.4499231542574672e-07  Varinance:  1.7212762117025396e-07 \n",
      "\n",
      "Epoch:  13446  Learning Rate:  1.4484739558231943e-07  Varinance:  1.718871651926971e-07 \n",
      "\n",
      "Epoch:  13447  Learning Rate:  1.447026205862998e-07  Varinance:  1.7164704512332713e-07 \n",
      "\n",
      "Epoch:  13448  Learning Rate:  1.445579902929128e-07  Varinance:  1.7140726049289264e-07 \n",
      "\n",
      "Epoch:  13449  Learning Rate:  1.4441350455752787e-07  Varinance:  1.711678108327983e-07 \n",
      "\n",
      "Epoch:  13450  Learning Rate:  1.442691632356598e-07  Varinance:  1.7092869567510168e-07 \n",
      "\n",
      "Epoch:  13451  Learning Rate:  1.44124966182967e-07  Varinance:  1.7068991455251576e-07 \n",
      "\n",
      "Epoch:  13452  Learning Rate:  1.439809132552521e-07  Varinance:  1.7045146699840575e-07 \n",
      "\n",
      "Epoch:  13453  Learning Rate:  1.4383700430846273e-07  Varinance:  1.7021335254678867e-07 \n",
      "\n",
      "Epoch:  13454  Learning Rate:  1.4369323919868966e-07  Varinance:  1.699755707323326e-07 \n",
      "\n",
      "Epoch:  13455  Learning Rate:  1.435496177821675e-07  Varinance:  1.697381210903556e-07 \n",
      "\n",
      "Epoch:  13456  Learning Rate:  1.4340613991527536e-07  Varinance:  1.695010031568255e-07 \n",
      "\n",
      "Epoch:  13457  Learning Rate:  1.4326280545453507e-07  Varinance:  1.6926421646835653e-07 \n",
      "\n",
      "Epoch:  13458  Learning Rate:  1.4311961425661192e-07  Varinance:  1.6902776056221208e-07 \n",
      "\n",
      "Epoch:  13459  Learning Rate:  1.429765661783152e-07  Varinance:  1.6879163497630141e-07 \n",
      "\n",
      "Epoch:  13460  Learning Rate:  1.428336610765966e-07  Varinance:  1.6855583924917924e-07 \n",
      "\n",
      "Epoch:  13461  Learning Rate:  1.426908988085507e-07  Varinance:  1.6832037292004495e-07 \n",
      "\n",
      "Epoch:  13462  Learning Rate:  1.4254827923141575e-07  Varinance:  1.6808523552874157e-07 \n",
      "\n",
      "Epoch:  13463  Learning Rate:  1.424058022025719e-07  Varinance:  1.6785042661575568e-07 \n",
      "\n",
      "Epoch:  13464  Learning Rate:  1.4226346757954217e-07  Varinance:  1.6761594572221385e-07 \n",
      "\n",
      "Epoch:  13465  Learning Rate:  1.4212127521999158e-07  Varinance:  1.6738179238988556e-07 \n",
      "\n",
      "Epoch:  13466  Learning Rate:  1.419792249817283e-07  Varinance:  1.6714796616117983e-07 \n",
      "\n",
      "Epoch:  13467  Learning Rate:  1.418373167227019e-07  Varinance:  1.6691446657914483e-07 \n",
      "\n",
      "Epoch:  13468  Learning Rate:  1.4169555030100373e-07  Varinance:  1.666812931874672e-07 \n",
      "\n",
      "Epoch:  13469  Learning Rate:  1.415539255748679e-07  Varinance:  1.6644844553047094e-07 \n",
      "\n",
      "Epoch:  13470  Learning Rate:  1.414124424026695e-07  Varinance:  1.662159231531172e-07 \n",
      "\n",
      "Epoch:  13471  Learning Rate:  1.41271100642925e-07  Varinance:  1.6598372560100108e-07 \n",
      "\n",
      "Epoch:  13472  Learning Rate:  1.411299001542932e-07  Varinance:  1.6575185242035427e-07 \n",
      "\n",
      "Epoch:  13473  Learning Rate:  1.4098884079557328e-07  Varinance:  1.6552030315804163e-07 \n",
      "\n",
      "Epoch:  13474  Learning Rate:  1.4084792242570566e-07  Varinance:  1.6528907736156118e-07 \n",
      "\n",
      "Epoch:  13475  Learning Rate:  1.4070714490377244e-07  Varinance:  1.65058174579043e-07 \n",
      "\n",
      "Epoch:  13476  Learning Rate:  1.4056650808899588e-07  Varinance:  1.648275943592484e-07 \n",
      "\n",
      "Epoch:  13477  Learning Rate:  1.4042601184073886e-07  Varinance:  1.6459733625156971e-07 \n",
      "\n",
      "Epoch:  13478  Learning Rate:  1.4028565601850564e-07  Varinance:  1.643673998060269e-07 \n",
      "\n",
      "Epoch:  13479  Learning Rate:  1.4014544048194014e-07  Varinance:  1.6413778457327033e-07 \n",
      "\n",
      "Epoch:  13480  Learning Rate:  1.4000536509082678e-07  Varinance:  1.6390849010457754e-07 \n",
      "\n",
      "Epoch:  13481  Learning Rate:  1.3986542970508993e-07  Varinance:  1.636795159518529e-07 \n",
      "\n",
      "Epoch:  13482  Learning Rate:  1.3972563418479468e-07  Varinance:  1.6345086166762673e-07 \n",
      "\n",
      "Epoch:  13483  Learning Rate:  1.3958597839014527e-07  Varinance:  1.6322252680505503e-07 \n",
      "\n",
      "Epoch:  13484  Learning Rate:  1.3944646218148565e-07  Varinance:  1.6299451091791632e-07 \n",
      "\n",
      "Epoch:  13485  Learning Rate:  1.3930708541930006e-07  Varinance:  1.6276681356061418e-07 \n",
      "\n",
      "Epoch:  13486  Learning Rate:  1.3916784796421148e-07  Varinance:  1.625394342881741e-07 \n",
      "\n",
      "Epoch:  13487  Learning Rate:  1.3902874967698225e-07  Varinance:  1.623123726562432e-07 \n",
      "\n",
      "Epoch:  13488  Learning Rate:  1.388897904185145e-07  Varinance:  1.620856282210893e-07 \n",
      "\n",
      "Epoch:  13489  Learning Rate:  1.3875097004984878e-07  Varinance:  1.6185920053960015e-07 \n",
      "\n",
      "Epoch:  13490  Learning Rate:  1.386122884321644e-07  Varinance:  1.6163308916928306e-07 \n",
      "\n",
      "Epoch:  13491  Learning Rate:  1.3847374542678025e-07  Varinance:  1.6140729366826181e-07 \n",
      "\n",
      "Epoch:  13492  Learning Rate:  1.3833534089515306e-07  Varinance:  1.611818135952791e-07 \n",
      "\n",
      "Epoch:  13493  Learning Rate:  1.3819707469887808e-07  Varinance:  1.609566485096935e-07 \n",
      "\n",
      "Epoch:  13494  Learning Rate:  1.3805894669968955e-07  Varinance:  1.607317979714792e-07 \n",
      "\n",
      "Epoch:  13495  Learning Rate:  1.379209567594592e-07  Varinance:  1.6050726154122509e-07 \n",
      "\n",
      "Epoch:  13496  Learning Rate:  1.3778310474019712e-07  Varinance:  1.6028303878013377e-07 \n",
      "\n",
      "Epoch:  13497  Learning Rate:  1.3764539050405101e-07  Varinance:  1.6005912925002156e-07 \n",
      "\n",
      "Epoch:  13498  Learning Rate:  1.3750781391330713e-07  Varinance:  1.598355325133151e-07 \n",
      "\n",
      "Epoch:  13499  Learning Rate:  1.3737037483038862e-07  Varinance:  1.59612248133054e-07 \n",
      "\n",
      "Epoch:  13500  Learning Rate:  1.3723307311785613e-07  Varinance:  1.5938927567288783e-07 \n",
      "\n",
      "Epoch:  13501  Learning Rate:  1.3709590863840845e-07  Varinance:  1.591666146970756e-07 \n",
      "\n",
      "Epoch:  13502  Learning Rate:  1.369588812548808e-07  Varinance:  1.589442647704851e-07 \n",
      "\n",
      "Epoch:  13503  Learning Rate:  1.3682199083024562e-07  Varinance:  1.5872222545859195e-07 \n",
      "\n",
      "Epoch:  13504  Learning Rate:  1.366852372276129e-07  Varinance:  1.5850049632747942e-07 \n",
      "\n",
      "Epoch:  13505  Learning Rate:  1.3654862031022882e-07  Varinance:  1.5827907694383515e-07 \n",
      "\n",
      "Epoch:  13506  Learning Rate:  1.3641213994147613e-07  Varinance:  1.5805796687495383e-07 \n",
      "\n",
      "Epoch:  13507  Learning Rate:  1.3627579598487506e-07  Varinance:  1.578371656887341e-07 \n",
      "\n",
      "Epoch:  13508  Learning Rate:  1.361395883040813e-07  Varinance:  1.5761667295367824e-07 \n",
      "\n",
      "Epoch:  13509  Learning Rate:  1.3600351676288718e-07  Varinance:  1.5739648823889126e-07 \n",
      "\n",
      "Epoch:  13510  Learning Rate:  1.3586758122522092e-07  Varinance:  1.571766111140801e-07 \n",
      "\n",
      "Epoch:  13511  Learning Rate:  1.3573178155514745e-07  Varinance:  1.569570411495534e-07 \n",
      "\n",
      "Epoch:  13512  Learning Rate:  1.3559611761686685e-07  Varinance:  1.567377779162184e-07 \n",
      "\n",
      "Epoch:  13513  Learning Rate:  1.354605892747149e-07  Varinance:  1.5651882098558343e-07 \n",
      "\n",
      "Epoch:  13514  Learning Rate:  1.353251963931638e-07  Varinance:  1.5630016992975484e-07 \n",
      "\n",
      "Epoch:  13515  Learning Rate:  1.3518993883682032e-07  Varinance:  1.5608182432143675e-07 \n",
      "\n",
      "Epoch:  13516  Learning Rate:  1.3505481647042673e-07  Varinance:  1.5586378373393018e-07 \n",
      "\n",
      "Epoch:  13517  Learning Rate:  1.3491982915886112e-07  Varinance:  1.5564604774113223e-07 \n",
      "\n",
      "Epoch:  13518  Learning Rate:  1.347849767671359e-07  Varinance:  1.554286159175358e-07 \n",
      "\n",
      "Epoch:  13519  Learning Rate:  1.346502591603984e-07  Varinance:  1.5521148783822658e-07 \n",
      "\n",
      "Epoch:  13520  Learning Rate:  1.3451567620393157e-07  Varinance:  1.5499466307888546e-07 \n",
      "\n",
      "Epoch:  13521  Learning Rate:  1.3438122776315214e-07  Varinance:  1.5477814121578563e-07 \n",
      "\n",
      "Epoch:  13522  Learning Rate:  1.3424691370361145e-07  Varinance:  1.5456192182579204e-07 \n",
      "\n",
      "Epoch:  13523  Learning Rate:  1.3411273389099588e-07  Varinance:  1.543460044863609e-07 \n",
      "\n",
      "Epoch:  13524  Learning Rate:  1.3397868819112535e-07  Varinance:  1.5413038877553868e-07 \n",
      "\n",
      "Epoch:  13525  Learning Rate:  1.3384477646995418e-07  Varinance:  1.5391507427196175e-07 \n",
      "\n",
      "Epoch:  13526  Learning Rate:  1.3371099859357043e-07  Varinance:  1.5370006055485358e-07 \n",
      "\n",
      "Epoch:  13527  Learning Rate:  1.3357735442819661e-07  Varinance:  1.5348534720402704e-07 \n",
      "\n",
      "Epoch:  13528  Learning Rate:  1.334438438401884e-07  Varinance:  1.5327093379988142e-07 \n",
      "\n",
      "Epoch:  13529  Learning Rate:  1.333104666960349e-07  Varinance:  1.5305681992340223e-07 \n",
      "\n",
      "Epoch:  13530  Learning Rate:  1.331772228623594e-07  Varinance:  1.528430051561603e-07 \n",
      "\n",
      "Epoch:  13531  Learning Rate:  1.3304411220591788e-07  Varinance:  1.5262948908031094e-07 \n",
      "\n",
      "Epoch:  13532  Learning Rate:  1.3291113459359945e-07  Varinance:  1.5241627127859383e-07 \n",
      "\n",
      "Epoch:  13533  Learning Rate:  1.327782898924269e-07  Varinance:  1.522033513343298e-07 \n",
      "\n",
      "Epoch:  13534  Learning Rate:  1.3264557796955532e-07  Varinance:  1.519907288314235e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13535  Learning Rate:  1.3251299869227252e-07  Varinance:  1.517784033543602e-07 \n",
      "\n",
      "Epoch:  13536  Learning Rate:  1.3238055192799967e-07  Varinance:  1.5156637448820576e-07 \n",
      "\n",
      "Epoch:  13537  Learning Rate:  1.3224823754428979e-07  Varinance:  1.5135464181860553e-07 \n",
      "\n",
      "Epoch:  13538  Learning Rate:  1.3211605540882824e-07  Varinance:  1.5114320493178385e-07 \n",
      "\n",
      "Epoch:  13539  Learning Rate:  1.3198400538943337e-07  Varinance:  1.509320634145435e-07 \n",
      "\n",
      "Epoch:  13540  Learning Rate:  1.3185208735405488e-07  Varinance:  1.5072121685426296e-07 \n",
      "\n",
      "Epoch:  13541  Learning Rate:  1.317203011707747e-07  Varinance:  1.505106648388987e-07 \n",
      "\n",
      "Epoch:  13542  Learning Rate:  1.3158864670780647e-07  Varinance:  1.5030040695698228e-07 \n",
      "\n",
      "Epoch:  13543  Learning Rate:  1.314571238334961e-07  Varinance:  1.5009044279762e-07 \n",
      "\n",
      "Epoch:  13544  Learning Rate:  1.3132573241632056e-07  Varinance:  1.498807719504923e-07 \n",
      "\n",
      "Epoch:  13545  Learning Rate:  1.3119447232488814e-07  Varinance:  1.4967139400585267e-07 \n",
      "\n",
      "Epoch:  13546  Learning Rate:  1.3106334342793922e-07  Varinance:  1.4946230855452764e-07 \n",
      "\n",
      "Epoch:  13547  Learning Rate:  1.3093234559434463e-07  Varinance:  1.492535151879137e-07 \n",
      "\n",
      "Epoch:  13548  Learning Rate:  1.3080147869310633e-07  Varinance:  1.4904501349797977e-07 \n",
      "\n",
      "Epoch:  13549  Learning Rate:  1.306707425933578e-07  Varinance:  1.4883680307726415e-07 \n",
      "\n",
      "Epoch:  13550  Learning Rate:  1.3054013716436282e-07  Varinance:  1.4862888351887446e-07 \n",
      "\n",
      "Epoch:  13551  Learning Rate:  1.3040966227551563e-07  Varinance:  1.4842125441648673e-07 \n",
      "\n",
      "Epoch:  13552  Learning Rate:  1.3027931779634182e-07  Varinance:  1.4821391536434454e-07 \n",
      "\n",
      "Epoch:  13553  Learning Rate:  1.3014910359649665e-07  Varinance:  1.4800686595725882e-07 \n",
      "\n",
      "Epoch:  13554  Learning Rate:  1.300190195457657e-07  Varinance:  1.4780010579060504e-07 \n",
      "\n",
      "Epoch:  13555  Learning Rate:  1.2988906551406536e-07  Varinance:  1.4759363446032542e-07 \n",
      "\n",
      "Epoch:  13556  Learning Rate:  1.2975924137144136e-07  Varinance:  1.4738745156292612e-07 \n",
      "\n",
      "Epoch:  13557  Learning Rate:  1.2962954698806954e-07  Varinance:  1.4718155669547703e-07 \n",
      "\n",
      "Epoch:  13558  Learning Rate:  1.2949998223425528e-07  Varinance:  1.469759494556108e-07 \n",
      "\n",
      "Epoch:  13559  Learning Rate:  1.2937054698043428e-07  Varinance:  1.467706294415223e-07 \n",
      "\n",
      "Epoch:  13560  Learning Rate:  1.2924124109717102e-07  Varinance:  1.4656559625196812e-07 \n",
      "\n",
      "Epoch:  13561  Learning Rate:  1.2911206445515945e-07  Varinance:  1.4636084948626388e-07 \n",
      "\n",
      "Epoch:  13562  Learning Rate:  1.2898301692522326e-07  Varinance:  1.461563887442865e-07 \n",
      "\n",
      "Epoch:  13563  Learning Rate:  1.2885409837831478e-07  Varinance:  1.459522136264713e-07 \n",
      "\n",
      "Epoch:  13564  Learning Rate:  1.287253086855152e-07  Varinance:  1.4574832373381174e-07 \n",
      "\n",
      "Epoch:  13565  Learning Rate:  1.2859664771803524e-07  Varinance:  1.4554471866785883e-07 \n",
      "\n",
      "Epoch:  13566  Learning Rate:  1.284681153472137e-07  Varinance:  1.4534139803072005e-07 \n",
      "\n",
      "Epoch:  13567  Learning Rate:  1.2833971144451803e-07  Varinance:  1.4513836142505926e-07 \n",
      "\n",
      "Epoch:  13568  Learning Rate:  1.282114358815447e-07  Varinance:  1.4493560845409394e-07 \n",
      "\n",
      "Epoch:  13569  Learning Rate:  1.2808328853001793e-07  Varinance:  1.4473313872159727e-07 \n",
      "\n",
      "Epoch:  13570  Learning Rate:  1.2795526926179015e-07  Varinance:  1.445309518318955e-07 \n",
      "\n",
      "Epoch:  13571  Learning Rate:  1.278273779488425e-07  Varinance:  1.4432904738986765e-07 \n",
      "\n",
      "Epoch:  13572  Learning Rate:  1.2769961446328347e-07  Varinance:  1.441274250009446e-07 \n",
      "\n",
      "Epoch:  13573  Learning Rate:  1.2757197867734954e-07  Varinance:  1.439260842711086e-07 \n",
      "\n",
      "Epoch:  13574  Learning Rate:  1.2744447046340468e-07  Varinance:  1.4372502480689267e-07 \n",
      "\n",
      "Epoch:  13575  Learning Rate:  1.2731708969394115e-07  Varinance:  1.4352424621537806e-07 \n",
      "\n",
      "Epoch:  13576  Learning Rate:  1.271898362415779e-07  Varinance:  1.4332374810419636e-07 \n",
      "\n",
      "Epoch:  13577  Learning Rate:  1.2706270997906128e-07  Varinance:  1.431235300815269e-07 \n",
      "\n",
      "Epoch:  13578  Learning Rate:  1.2693571077926547e-07  Varinance:  1.4292359175609622e-07 \n",
      "\n",
      "Epoch:  13579  Learning Rate:  1.26808838515191e-07  Varinance:  1.4272393273717756e-07 \n",
      "\n",
      "Epoch:  13580  Learning Rate:  1.266820930599654e-07  Varinance:  1.4252455263459e-07 \n",
      "\n",
      "Epoch:  13581  Learning Rate:  1.2655547428684361e-07  Varinance:  1.4232545105869813e-07 \n",
      "\n",
      "Epoch:  13582  Learning Rate:  1.2642898206920666e-07  Varinance:  1.4212662762040932e-07 \n",
      "\n",
      "Epoch:  13583  Learning Rate:  1.2630261628056211e-07  Varinance:  1.4192808193117606e-07 \n",
      "\n",
      "Epoch:  13584  Learning Rate:  1.2617637679454458e-07  Varinance:  1.4172981360299314e-07 \n",
      "\n",
      "Epoch:  13585  Learning Rate:  1.2605026348491437e-07  Varinance:  1.415318222483973e-07 \n",
      "\n",
      "Epoch:  13586  Learning Rate:  1.2592427622555792e-07  Varinance:  1.4133410748046655e-07 \n",
      "\n",
      "Epoch:  13587  Learning Rate:  1.257984148904884e-07  Varinance:  1.4113666891281952e-07 \n",
      "\n",
      "Epoch:  13588  Learning Rate:  1.256726793538443e-07  Varinance:  1.4093950615961505e-07 \n",
      "\n",
      "Epoch:  13589  Learning Rate:  1.2554706948988997e-07  Varinance:  1.4074261883554943e-07 \n",
      "\n",
      "Epoch:  13590  Learning Rate:  1.254215851730154e-07  Varinance:  1.4054600655585874e-07 \n",
      "\n",
      "Epoch:  13591  Learning Rate:  1.2529622627773664e-07  Varinance:  1.4034966893631612e-07 \n",
      "\n",
      "Epoch:  13592  Learning Rate:  1.2517099267869465e-07  Varinance:  1.4015360559323135e-07 \n",
      "\n",
      "Epoch:  13593  Learning Rate:  1.2504588425065552e-07  Varinance:  1.3995781614345033e-07 \n",
      "\n",
      "Epoch:  13594  Learning Rate:  1.2492090086851129e-07  Varinance:  1.3976230020435414e-07 \n",
      "\n",
      "Epoch:  13595  Learning Rate:  1.2479604240727832e-07  Varinance:  1.3956705739385882e-07 \n",
      "\n",
      "Epoch:  13596  Learning Rate:  1.2467130874209798e-07  Varinance:  1.3937208733041282e-07 \n",
      "\n",
      "Epoch:  13597  Learning Rate:  1.2454669974823693e-07  Varinance:  1.3917738963299896e-07 \n",
      "\n",
      "Epoch:  13598  Learning Rate:  1.2442221530108605e-07  Varinance:  1.3898296392113183e-07 \n",
      "\n",
      "Epoch:  13599  Learning Rate:  1.2429785527616064e-07  Varinance:  1.387888098148577e-07 \n",
      "\n",
      "Epoch:  13600  Learning Rate:  1.2417361954910102e-07  Varinance:  1.3859492693475343e-07 \n",
      "\n",
      "Epoch:  13601  Learning Rate:  1.2404950799567134e-07  Varinance:  1.38401314901926e-07 \n",
      "\n",
      "Epoch:  13602  Learning Rate:  1.2392552049175977e-07  Varinance:  1.3820797333801223e-07 \n",
      "\n",
      "Epoch:  13603  Learning Rate:  1.2380165691337925e-07  Varinance:  1.3801490186517596e-07 \n",
      "\n",
      "Epoch:  13604  Learning Rate:  1.2367791713666593e-07  Varinance:  1.3782210010611033e-07 \n",
      "\n",
      "Epoch:  13605  Learning Rate:  1.2355430103788006e-07  Varinance:  1.3762956768403508e-07 \n",
      "\n",
      "Epoch:  13606  Learning Rate:  1.2343080849340533e-07  Varinance:  1.3743730422269638e-07 \n",
      "\n",
      "Epoch:  13607  Learning Rate:  1.2330743937974955e-07  Varinance:  1.3724530934636585e-07 \n",
      "\n",
      "Epoch:  13608  Learning Rate:  1.2318419357354346e-07  Varinance:  1.370535826798401e-07 \n",
      "\n",
      "Epoch:  13609  Learning Rate:  1.23061070951541e-07  Varinance:  1.368621238484404e-07 \n",
      "\n",
      "Epoch:  13610  Learning Rate:  1.2293807139061997e-07  Varinance:  1.366709324780099e-07 \n",
      "\n",
      "Epoch:  13611  Learning Rate:  1.2281519476778054e-07  Varinance:  1.3648000819491588e-07 \n",
      "\n",
      "Epoch:  13612  Learning Rate:  1.2269244096014592e-07  Varinance:  1.3628935062604712e-07 \n",
      "\n",
      "Epoch:  13613  Learning Rate:  1.2256980984496268e-07  Varinance:  1.3609895939881367e-07 \n",
      "\n",
      "Epoch:  13614  Learning Rate:  1.2244730129959952e-07  Varinance:  1.3590883414114603e-07 \n",
      "\n",
      "Epoch:  13615  Learning Rate:  1.223249152015476e-07  Varinance:  1.3571897448149494e-07 \n",
      "\n",
      "Epoch:  13616  Learning Rate:  1.2220265142842135e-07  Varinance:  1.3552938004882867e-07 \n",
      "\n",
      "Epoch:  13617  Learning Rate:  1.220805098579567e-07  Varinance:  1.3534005047263543e-07 \n",
      "\n",
      "Epoch:  13618  Learning Rate:  1.2195849036801185e-07  Varinance:  1.3515098538292034e-07 \n",
      "\n",
      "Epoch:  13619  Learning Rate:  1.2183659283656776e-07  Varinance:  1.3496218441020558e-07 \n",
      "\n",
      "Epoch:  13620  Learning Rate:  1.2171481714172666e-07  Varinance:  1.3477364718552934e-07 \n",
      "\n",
      "Epoch:  13621  Learning Rate:  1.2159316316171283e-07  Varinance:  1.345853733404452e-07 \n",
      "\n",
      "Epoch:  13622  Learning Rate:  1.214716307748721e-07  Varinance:  1.3439736250702214e-07 \n",
      "\n",
      "Epoch:  13623  Learning Rate:  1.2135021985967246e-07  Varinance:  1.3420961431784148e-07 \n",
      "\n",
      "Epoch:  13624  Learning Rate:  1.2122893029470282e-07  Varinance:  1.3402212840599932e-07 \n",
      "\n",
      "Epoch:  13625  Learning Rate:  1.2110776195867333e-07  Varinance:  1.3383490440510383e-07 \n",
      "\n",
      "Epoch:  13626  Learning Rate:  1.2098671473041614e-07  Varinance:  1.3364794194927504e-07 \n",
      "\n",
      "Epoch:  13627  Learning Rate:  1.2086578848888374e-07  Varinance:  1.3346124067314405e-07 \n",
      "\n",
      "Epoch:  13628  Learning Rate:  1.2074498311314968e-07  Varinance:  1.332748002118524e-07 \n",
      "\n",
      "Epoch:  13629  Learning Rate:  1.20624298482409e-07  Varinance:  1.3308862020105183e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13630  Learning Rate:  1.205037344759769e-07  Varinance:  1.3290270027690156e-07 \n",
      "\n",
      "Epoch:  13631  Learning Rate:  1.2038329097328908e-07  Varinance:  1.3271704007607057e-07 \n",
      "\n",
      "Epoch:  13632  Learning Rate:  1.2026296785390247e-07  Varinance:  1.3253163923573488e-07 \n",
      "\n",
      "Epoch:  13633  Learning Rate:  1.2014276499749375e-07  Varinance:  1.3234649739357742e-07 \n",
      "\n",
      "Epoch:  13634  Learning Rate:  1.2002268228386e-07  Varinance:  1.321616141877872e-07 \n",
      "\n",
      "Epoch:  13635  Learning Rate:  1.1990271959291838e-07  Varinance:  1.319769892570587e-07 \n",
      "\n",
      "Epoch:  13636  Learning Rate:  1.1978287680470653e-07  Varinance:  1.317926222405916e-07 \n",
      "\n",
      "Epoch:  13637  Learning Rate:  1.1966315379938147e-07  Varinance:  1.316085127780882e-07 \n",
      "\n",
      "Epoch:  13638  Learning Rate:  1.1954355045721998e-07  Varinance:  1.314246605097555e-07 \n",
      "\n",
      "Epoch:  13639  Learning Rate:  1.194240666586191e-07  Varinance:  1.3124106507630271e-07 \n",
      "\n",
      "Epoch:  13640  Learning Rate:  1.1930470228409485e-07  Varinance:  1.310577261189409e-07 \n",
      "\n",
      "Epoch:  13641  Learning Rate:  1.191854572142826e-07  Varinance:  1.3087464327938232e-07 \n",
      "\n",
      "Epoch:  13642  Learning Rate:  1.1906633132993771e-07  Varinance:  1.3069181619983983e-07 \n",
      "\n",
      "Epoch:  13643  Learning Rate:  1.1894732451193407e-07  Varinance:  1.3050924452302655e-07 \n",
      "\n",
      "Epoch:  13644  Learning Rate:  1.1882843664126465e-07  Varinance:  1.3032692789215325e-07 \n",
      "\n",
      "Epoch:  13645  Learning Rate:  1.1870966759904197e-07  Varinance:  1.3014486595093064e-07 \n",
      "\n",
      "Epoch:  13646  Learning Rate:  1.1859101726649682e-07  Varinance:  1.2996305834356653e-07 \n",
      "\n",
      "Epoch:  13647  Learning Rate:  1.1847248552497857e-07  Varinance:  1.2978150471476586e-07 \n",
      "\n",
      "Epoch:  13648  Learning Rate:  1.1835407225595594e-07  Varinance:  1.2960020470972993e-07 \n",
      "\n",
      "Epoch:  13649  Learning Rate:  1.1823577734101542e-07  Varinance:  1.294191579741556e-07 \n",
      "\n",
      "Epoch:  13650  Learning Rate:  1.181176006618621e-07  Varinance:  1.292383641542352e-07 \n",
      "\n",
      "Epoch:  13651  Learning Rate:  1.1799954210031909e-07  Varinance:  1.290578228966539e-07 \n",
      "\n",
      "Epoch:  13652  Learning Rate:  1.1788160153832821e-07  Varinance:  1.288775338485918e-07 \n",
      "\n",
      "Epoch:  13653  Learning Rate:  1.1776377885794868e-07  Varinance:  1.2869749665772146e-07 \n",
      "\n",
      "Epoch:  13654  Learning Rate:  1.1764607394135764e-07  Varinance:  1.2851771097220765e-07 \n",
      "\n",
      "Epoch:  13655  Learning Rate:  1.1752848667085054e-07  Varinance:  1.283381764407066e-07 \n",
      "\n",
      "Epoch:  13656  Learning Rate:  1.1741101692883992e-07  Varinance:  1.2815889271236536e-07 \n",
      "\n",
      "Epoch:  13657  Learning Rate:  1.1729366459785578e-07  Varinance:  1.2797985943682149e-07 \n",
      "\n",
      "Epoch:  13658  Learning Rate:  1.1717642956054623e-07  Varinance:  1.278010762642008e-07 \n",
      "\n",
      "Epoch:  13659  Learning Rate:  1.17059311699676e-07  Varinance:  1.2762254284511906e-07 \n",
      "\n",
      "Epoch:  13660  Learning Rate:  1.1694231089812704e-07  Varinance:  1.2744425883067978e-07 \n",
      "\n",
      "Epoch:  13661  Learning Rate:  1.1682542703889889e-07  Varinance:  1.272662238724738e-07 \n",
      "\n",
      "Epoch:  13662  Learning Rate:  1.1670866000510754e-07  Varinance:  1.2708843762257867e-07 \n",
      "\n",
      "Epoch:  13663  Learning Rate:  1.1659200967998571e-07  Varinance:  1.2691089973355803e-07 \n",
      "\n",
      "Epoch:  13664  Learning Rate:  1.1647547594688349e-07  Varinance:  1.2673360985846126e-07 \n",
      "\n",
      "Epoch:  13665  Learning Rate:  1.1635905868926691e-07  Varinance:  1.2655656765082113e-07 \n",
      "\n",
      "Epoch:  13666  Learning Rate:  1.1624275779071873e-07  Varinance:  1.263797727646557e-07 \n",
      "\n",
      "Epoch:  13667  Learning Rate:  1.1612657313493782e-07  Varinance:  1.2620322485446595e-07 \n",
      "\n",
      "Epoch:  13668  Learning Rate:  1.1601050460573992e-07  Varinance:  1.2602692357523549e-07 \n",
      "\n",
      "Epoch:  13669  Learning Rate:  1.158945520870563e-07  Varinance:  1.2585086858242994e-07 \n",
      "\n",
      "Epoch:  13670  Learning Rate:  1.157787154629342e-07  Varinance:  1.2567505953199614e-07 \n",
      "\n",
      "Epoch:  13671  Learning Rate:  1.1566299461753744e-07  Varinance:  1.254994960803621e-07 \n",
      "\n",
      "Epoch:  13672  Learning Rate:  1.1554738943514492e-07  Varinance:  1.2532417788443437e-07 \n",
      "\n",
      "Epoch:  13673  Learning Rate:  1.1543189980015129e-07  Varinance:  1.2514910460160019e-07 \n",
      "\n",
      "Epoch:  13674  Learning Rate:  1.1531652559706723e-07  Varinance:  1.2497427588972498e-07 \n",
      "\n",
      "Epoch:  13675  Learning Rate:  1.1520126671051842e-07  Varinance:  1.2479969140715198e-07 \n",
      "\n",
      "Epoch:  13676  Learning Rate:  1.150861230252457e-07  Varinance:  1.246253508127019e-07 \n",
      "\n",
      "Epoch:  13677  Learning Rate:  1.1497109442610583e-07  Varinance:  1.244512537656719e-07 \n",
      "\n",
      "Epoch:  13678  Learning Rate:  1.1485618079806991e-07  Varinance:  1.2427739992583566e-07 \n",
      "\n",
      "Epoch:  13679  Learning Rate:  1.1474138202622421e-07  Varinance:  1.2410378895344073e-07 \n",
      "\n",
      "Epoch:  13680  Learning Rate:  1.1462669799577026e-07  Varinance:  1.2393042050921065e-07 \n",
      "\n",
      "Epoch:  13681  Learning Rate:  1.145121285920239e-07  Varinance:  1.2375729425434247e-07 \n",
      "\n",
      "Epoch:  13682  Learning Rate:  1.1439767370041564e-07  Varinance:  1.235844098505065e-07 \n",
      "\n",
      "Epoch:  13683  Learning Rate:  1.1428333320649043e-07  Varinance:  1.2341176695984574e-07 \n",
      "\n",
      "Epoch:  13684  Learning Rate:  1.1416910699590814e-07  Varinance:  1.2323936524497512e-07 \n",
      "\n",
      "Epoch:  13685  Learning Rate:  1.1405499495444237e-07  Varinance:  1.2306720436898137e-07 \n",
      "\n",
      "Epoch:  13686  Learning Rate:  1.1394099696798085e-07  Varinance:  1.228952839954205e-07 \n",
      "\n",
      "Epoch:  13687  Learning Rate:  1.1382711292252599e-07  Varinance:  1.2272360378831988e-07 \n",
      "\n",
      "Epoch:  13688  Learning Rate:  1.1371334270419355e-07  Varinance:  1.2255216341217574e-07 \n",
      "\n",
      "Epoch:  13689  Learning Rate:  1.1359968619921308e-07  Varinance:  1.2238096253195303e-07 \n",
      "\n",
      "Epoch:  13690  Learning Rate:  1.1348614329392848e-07  Varinance:  1.222100008130848e-07 \n",
      "\n",
      "Epoch:  13691  Learning Rate:  1.1337271387479662e-07  Varinance:  1.2203927792147129e-07 \n",
      "\n",
      "Epoch:  13692  Learning Rate:  1.132593978283879e-07  Varinance:  1.218687935234801e-07 \n",
      "\n",
      "Epoch:  13693  Learning Rate:  1.1314619504138664e-07  Varinance:  1.2169854728594348e-07 \n",
      "\n",
      "Epoch:  13694  Learning Rate:  1.1303310540058986e-07  Varinance:  1.2152853887616042e-07 \n",
      "\n",
      "Epoch:  13695  Learning Rate:  1.1292012879290768e-07  Varinance:  1.2135876796189432e-07 \n",
      "\n",
      "Epoch:  13696  Learning Rate:  1.128072651053639e-07  Varinance:  1.211892342113726e-07 \n",
      "\n",
      "Epoch:  13697  Learning Rate:  1.1269451422509465e-07  Varinance:  1.2101993729328623e-07 \n",
      "\n",
      "Epoch:  13698  Learning Rate:  1.1258187603934902e-07  Varinance:  1.2085087687678893e-07 \n",
      "\n",
      "Epoch:  13699  Learning Rate:  1.1246935043548858e-07  Varinance:  1.2068205263149715e-07 \n",
      "\n",
      "Epoch:  13700  Learning Rate:  1.1235693730098817e-07  Varinance:  1.2051346422748746e-07 \n",
      "\n",
      "Epoch:  13701  Learning Rate:  1.1224463652343443e-07  Varinance:  1.203451113352987e-07 \n",
      "\n",
      "Epoch:  13702  Learning Rate:  1.1213244799052635e-07  Varinance:  1.2017699362592945e-07 \n",
      "\n",
      "Epoch:  13703  Learning Rate:  1.120203715900758e-07  Varinance:  1.2000911077083797e-07 \n",
      "\n",
      "Epoch:  13704  Learning Rate:  1.119084072100062e-07  Varinance:  1.198414624419414e-07 \n",
      "\n",
      "Epoch:  13705  Learning Rate:  1.1179655473835292e-07  Varinance:  1.1967404831161527e-07 \n",
      "\n",
      "Epoch:  13706  Learning Rate:  1.116848140632639e-07  Varinance:  1.195068680526932e-07 \n",
      "\n",
      "Epoch:  13707  Learning Rate:  1.1157318507299825e-07  Varinance:  1.1933992133846453e-07 \n",
      "\n",
      "Epoch:  13708  Learning Rate:  1.1146166765592676e-07  Varinance:  1.1917320784267633e-07 \n",
      "\n",
      "Epoch:  13709  Learning Rate:  1.1135026170053241e-07  Varinance:  1.1900672723953096e-07 \n",
      "\n",
      "Epoch:  13710  Learning Rate:  1.1123896709540907e-07  Varinance:  1.1884047920368596e-07 \n",
      "\n",
      "Epoch:  13711  Learning Rate:  1.1112778372926188e-07  Varinance:  1.1867446341025335e-07 \n",
      "\n",
      "Epoch:  13712  Learning Rate:  1.1101671149090788e-07  Varinance:  1.1850867953479897e-07 \n",
      "\n",
      "Epoch:  13713  Learning Rate:  1.109057502692746e-07  Varinance:  1.183431272533423e-07 \n",
      "\n",
      "Epoch:  13714  Learning Rate:  1.1079489995340085e-07  Varinance:  1.1817780624235423e-07 \n",
      "\n",
      "Epoch:  13715  Learning Rate:  1.106841604324361e-07  Varinance:  1.1801271617875876e-07 \n",
      "\n",
      "Epoch:  13716  Learning Rate:  1.1057353159564118e-07  Varinance:  1.1784785673993089e-07 \n",
      "\n",
      "Epoch:  13717  Learning Rate:  1.1046301333238708e-07  Varinance:  1.1768322760369626e-07 \n",
      "\n",
      "Epoch:  13718  Learning Rate:  1.1035260553215534e-07  Varinance:  1.1751882844833056e-07 \n",
      "\n",
      "Epoch:  13719  Learning Rate:  1.102423080845385e-07  Varinance:  1.1735465895255898e-07 \n",
      "\n",
      "Epoch:  13720  Learning Rate:  1.1013212087923895e-07  Varinance:  1.171907187955559e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13721  Learning Rate:  1.1002204380606924e-07  Varinance:  1.1702700765694263e-07 \n",
      "\n",
      "Epoch:  13722  Learning Rate:  1.099120767549527e-07  Varinance:  1.1686352521678927e-07 \n",
      "\n",
      "Epoch:  13723  Learning Rate:  1.098022196159221e-07  Varinance:  1.1670027115561247e-07 \n",
      "\n",
      "Epoch:  13724  Learning Rate:  1.0969247227912006e-07  Varinance:  1.1653724515437516e-07 \n",
      "\n",
      "Epoch:  13725  Learning Rate:  1.0958283463479962e-07  Varinance:  1.16374446894486e-07 \n",
      "\n",
      "Epoch:  13726  Learning Rate:  1.0947330657332297e-07  Varinance:  1.1621187605779863e-07 \n",
      "\n",
      "Epoch:  13727  Learning Rate:  1.0936388798516181e-07  Varinance:  1.160495323266116e-07 \n",
      "\n",
      "Epoch:  13728  Learning Rate:  1.0925457876089794e-07  Varinance:  1.1588741538366604e-07 \n",
      "\n",
      "Epoch:  13729  Learning Rate:  1.0914537879122194e-07  Varinance:  1.157255249121475e-07 \n",
      "\n",
      "Epoch:  13730  Learning Rate:  1.090362879669338e-07  Varinance:  1.1556386059568367e-07 \n",
      "\n",
      "Epoch:  13731  Learning Rate:  1.0892730617894256e-07  Varinance:  1.1540242211834427e-07 \n",
      "\n",
      "Epoch:  13732  Learning Rate:  1.0881843331826676e-07  Varinance:  1.1524120916464027e-07 \n",
      "\n",
      "Epoch:  13733  Learning Rate:  1.0870966927603335e-07  Varinance:  1.1508022141952346e-07 \n",
      "\n",
      "Epoch:  13734  Learning Rate:  1.0860101394347807e-07  Varinance:  1.1491945856838609e-07 \n",
      "\n",
      "Epoch:  13735  Learning Rate:  1.0849246721194598e-07  Varinance:  1.147589202970587e-07 \n",
      "\n",
      "Epoch:  13736  Learning Rate:  1.0838402897289014e-07  Varinance:  1.1459860629181194e-07 \n",
      "\n",
      "Epoch:  13737  Learning Rate:  1.082756991178721e-07  Varinance:  1.1443851623935432e-07 \n",
      "\n",
      "Epoch:  13738  Learning Rate:  1.0816747753856241e-07  Varinance:  1.1427864982683198e-07 \n",
      "\n",
      "Epoch:  13739  Learning Rate:  1.0805936412673926e-07  Varinance:  1.1411900674182813e-07 \n",
      "\n",
      "Epoch:  13740  Learning Rate:  1.0795135877428908e-07  Varinance:  1.1395958667236245e-07 \n",
      "\n",
      "Epoch:  13741  Learning Rate:  1.0784346137320685e-07  Varinance:  1.1380038930689079e-07 \n",
      "\n",
      "Epoch:  13742  Learning Rate:  1.0773567181559495e-07  Varinance:  1.13641414334303e-07 \n",
      "\n",
      "Epoch:  13743  Learning Rate:  1.0762798999366368e-07  Varinance:  1.134826614439248e-07 \n",
      "\n",
      "Epoch:  13744  Learning Rate:  1.0752041579973155e-07  Varinance:  1.133241303255155e-07 \n",
      "\n",
      "Epoch:  13745  Learning Rate:  1.074129491262242e-07  Varinance:  1.1316582066926776e-07 \n",
      "\n",
      "Epoch:  13746  Learning Rate:  1.073055898656749e-07  Varinance:  1.1300773216580707e-07 \n",
      "\n",
      "Epoch:  13747  Learning Rate:  1.0719833791072425e-07  Varinance:  1.1284986450619108e-07 \n",
      "\n",
      "Epoch:  13748  Learning Rate:  1.070911931541206e-07  Varinance:  1.1269221738190945e-07 \n",
      "\n",
      "Epoch:  13749  Learning Rate:  1.0698415548871905e-07  Varinance:  1.1253479048488162e-07 \n",
      "\n",
      "Epoch:  13750  Learning Rate:  1.0687722480748171e-07  Varinance:  1.1237758350745852e-07 \n",
      "\n",
      "Epoch:  13751  Learning Rate:  1.0677040100347827e-07  Varinance:  1.1222059614242061e-07 \n",
      "\n",
      "Epoch:  13752  Learning Rate:  1.0666368396988474e-07  Varinance:  1.1206382808297736e-07 \n",
      "\n",
      "Epoch:  13753  Learning Rate:  1.0655707359998387e-07  Varinance:  1.1190727902276694e-07 \n",
      "\n",
      "Epoch:  13754  Learning Rate:  1.0645056978716568e-07  Varinance:  1.1175094865585585e-07 \n",
      "\n",
      "Epoch:  13755  Learning Rate:  1.0634417242492613e-07  Varinance:  1.1159483667673675e-07 \n",
      "\n",
      "Epoch:  13756  Learning Rate:  1.0623788140686769e-07  Varinance:  1.1143894278033028e-07 \n",
      "\n",
      "Epoch:  13757  Learning Rate:  1.0613169662669969e-07  Varinance:  1.1128326666198295e-07 \n",
      "\n",
      "Epoch:  13758  Learning Rate:  1.0602561797823717e-07  Varinance:  1.1112780801746673e-07 \n",
      "\n",
      "Epoch:  13759  Learning Rate:  1.0591964535540145e-07  Varinance:  1.1097256654297866e-07 \n",
      "\n",
      "Epoch:  13760  Learning Rate:  1.0581377865221975e-07  Varinance:  1.108175419351402e-07 \n",
      "\n",
      "Epoch:  13761  Learning Rate:  1.0570801776282569e-07  Varinance:  1.1066273389099694e-07 \n",
      "\n",
      "Epoch:  13762  Learning Rate:  1.056023625814582e-07  Varinance:  1.1050814210801659e-07 \n",
      "\n",
      "Epoch:  13763  Learning Rate:  1.0549681300246192e-07  Varinance:  1.1035376628409061e-07 \n",
      "\n",
      "Epoch:  13764  Learning Rate:  1.053913689202876e-07  Varinance:  1.1019960611753213e-07 \n",
      "\n",
      "Epoch:  13765  Learning Rate:  1.0528603022949099e-07  Varinance:  1.1004566130707568e-07 \n",
      "\n",
      "Epoch:  13766  Learning Rate:  1.0518079682473321e-07  Varinance:  1.0989193155187671e-07 \n",
      "\n",
      "Epoch:  13767  Learning Rate:  1.050756686007812e-07  Varinance:  1.0973841655151088e-07 \n",
      "\n",
      "Epoch:  13768  Learning Rate:  1.0497064545250654e-07  Varinance:  1.0958511600597397e-07 \n",
      "\n",
      "Epoch:  13769  Learning Rate:  1.0486572727488589e-07  Varinance:  1.0943202961567967e-07 \n",
      "\n",
      "Epoch:  13770  Learning Rate:  1.0476091396300147e-07  Varinance:  1.0927915708146134e-07 \n",
      "\n",
      "Epoch:  13771  Learning Rate:  1.0465620541203969e-07  Varinance:  1.0912649810456987e-07 \n",
      "\n",
      "Epoch:  13772  Learning Rate:  1.045516015172919e-07  Varinance:  1.089740523866735e-07 \n",
      "\n",
      "Epoch:  13773  Learning Rate:  1.0444710217415451e-07  Varinance:  1.0882181962985723e-07 \n",
      "\n",
      "Epoch:  13774  Learning Rate:  1.0434270727812801e-07  Varinance:  1.0866979953662227e-07 \n",
      "\n",
      "Epoch:  13775  Learning Rate:  1.0423841672481749e-07  Varinance:  1.0851799180988575e-07 \n",
      "\n",
      "Epoch:  13776  Learning Rate:  1.0413423040993217e-07  Varinance:  1.0836639615297868e-07 \n",
      "\n",
      "Epoch:  13777  Learning Rate:  1.0403014822928612e-07  Varinance:  1.0821501226964768e-07 \n",
      "\n",
      "Epoch:  13778  Learning Rate:  1.03926170078797e-07  Varinance:  1.0806383986405282e-07 \n",
      "\n",
      "Epoch:  13779  Learning Rate:  1.0382229585448641e-07  Varinance:  1.0791287864076747e-07 \n",
      "\n",
      "Epoch:  13780  Learning Rate:  1.037185254524805e-07  Varinance:  1.0776212830477767e-07 \n",
      "\n",
      "Epoch:  13781  Learning Rate:  1.0361485876900873e-07  Varinance:  1.0761158856148161e-07 \n",
      "\n",
      "Epoch:  13782  Learning Rate:  1.0351129570040414e-07  Varinance:  1.0746125911668936e-07 \n",
      "\n",
      "Epoch:  13783  Learning Rate:  1.0340783614310407e-07  Varinance:  1.0731113967662088e-07 \n",
      "\n",
      "Epoch:  13784  Learning Rate:  1.0330447999364877e-07  Varinance:  1.0716122994790768e-07 \n",
      "\n",
      "Epoch:  13785  Learning Rate:  1.0320122714868187e-07  Varinance:  1.0701152963759062e-07 \n",
      "\n",
      "Epoch:  13786  Learning Rate:  1.0309807750495091e-07  Varinance:  1.0686203845311994e-07 \n",
      "\n",
      "Epoch:  13787  Learning Rate:  1.0299503095930607e-07  Varinance:  1.0671275610235447e-07 \n",
      "\n",
      "Epoch:  13788  Learning Rate:  1.0289208740870055e-07  Varinance:  1.0656368229356116e-07 \n",
      "\n",
      "Epoch:  13789  Learning Rate:  1.0278924675019122e-07  Varinance:  1.0641481673541493e-07 \n",
      "\n",
      "Epoch:  13790  Learning Rate:  1.026865088809372e-07  Varinance:  1.0626615913699649e-07 \n",
      "\n",
      "Epoch:  13791  Learning Rate:  1.0258387369820063e-07  Varinance:  1.0611770920779408e-07 \n",
      "\n",
      "Epoch:  13792  Learning Rate:  1.0248134109934611e-07  Varinance:  1.0596946665770145e-07 \n",
      "\n",
      "Epoch:  13793  Learning Rate:  1.0237891098184141e-07  Varinance:  1.0582143119701758e-07 \n",
      "\n",
      "Epoch:  13794  Learning Rate:  1.0227658324325623e-07  Varinance:  1.0567360253644616e-07 \n",
      "\n",
      "Epoch:  13795  Learning Rate:  1.0217435778126264e-07  Varinance:  1.0552598038709498e-07 \n",
      "\n",
      "Epoch:  13796  Learning Rate:  1.0207223449363552e-07  Varinance:  1.0537856446047583e-07 \n",
      "\n",
      "Epoch:  13797  Learning Rate:  1.0197021327825139e-07  Varinance:  1.0523135446850238e-07 \n",
      "\n",
      "Epoch:  13798  Learning Rate:  1.0186829403308888e-07  Varinance:  1.0508435012349181e-07 \n",
      "\n",
      "Epoch:  13799  Learning Rate:  1.0176647665622905e-07  Varinance:  1.049375511381629e-07 \n",
      "\n",
      "Epoch:  13800  Learning Rate:  1.0166476104585438e-07  Varinance:  1.0479095722563569e-07 \n",
      "\n",
      "Epoch:  13801  Learning Rate:  1.0156314710024903e-07  Varinance:  1.0464456809943096e-07 \n",
      "\n",
      "Epoch:  13802  Learning Rate:  1.0146163471779942e-07  Varinance:  1.0449838347346977e-07 \n",
      "\n",
      "Epoch:  13803  Learning Rate:  1.01360223796993e-07  Varinance:  1.0435240306207309e-07 \n",
      "\n",
      "Epoch:  13804  Learning Rate:  1.0125891423641862e-07  Varinance:  1.0420662657995999e-07 \n",
      "\n",
      "Epoch:  13805  Learning Rate:  1.0115770593476711e-07  Varinance:  1.0406105374224904e-07 \n",
      "\n",
      "Epoch:  13806  Learning Rate:  1.0105659879082996e-07  Varinance:  1.039156842644565e-07 \n",
      "\n",
      "Epoch:  13807  Learning Rate:  1.0095559270350002e-07  Varinance:  1.0377051786249603e-07 \n",
      "\n",
      "Epoch:  13808  Learning Rate:  1.0085468757177104e-07  Varinance:  1.0362555425267812e-07 \n",
      "\n",
      "Epoch:  13809  Learning Rate:  1.0075388329473819e-07  Varinance:  1.034807931517096e-07 \n",
      "\n",
      "Epoch:  13810  Learning Rate:  1.0065317977159704e-07  Varinance:  1.0333623427669335e-07 \n",
      "\n",
      "Epoch:  13811  Learning Rate:  1.0055257690164387e-07  Varinance:  1.0319187734512645e-07 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13812  Learning Rate:  1.0045207458427615e-07  Varinance:  1.0304772207490164e-07 \n",
      "\n",
      "Epoch:  13813  Learning Rate:  1.003516727189914e-07  Varinance:  1.0290376818430543e-07 \n",
      "\n",
      "Epoch:  13814  Learning Rate:  1.0025137120538755e-07  Varinance:  1.0276001539201784e-07 \n",
      "\n",
      "Epoch:  13815  Learning Rate:  1.0015116994316343e-07  Varinance:  1.0261646341711193e-07 \n",
      "\n",
      "Epoch:  13816  Learning Rate:  1.000510688321176e-07  Varinance:  1.0247311197905317e-07 \n",
      "\n",
      "Epoch:  13817  Learning Rate:  9.995106777214877e-08  Varinance:  1.0232996079769928e-07 \n",
      "\n",
      "Epoch:  13818  Learning Rate:  9.985116666325621e-08  Varinance:  1.0218700959329828e-07 \n",
      "\n",
      "Epoch:  13819  Learning Rate:  9.975136540553864e-08  Varinance:  1.0204425808649004e-07 \n",
      "\n",
      "Epoch:  13820  Learning Rate:  9.965166389919463e-08  Varinance:  1.0190170599830436e-07 \n",
      "\n",
      "Epoch:  13821  Learning Rate:  9.955206204452297e-08  Varinance:  1.0175935305016075e-07 \n",
      "\n",
      "Epoch:  13822  Learning Rate:  9.945255974192165e-08  Varinance:  1.0161719896386782e-07 \n",
      "\n",
      "Epoch:  13823  Learning Rate:  9.935315689188838e-08  Varinance:  1.0147524346162289e-07 \n",
      "\n",
      "Epoch:  13824  Learning Rate:  9.92538533950201e-08  Varinance:  1.0133348626601169e-07 \n",
      "\n",
      "Epoch:  13825  Learning Rate:  9.915464915201364e-08  Varinance:  1.0119192710000637e-07 \n",
      "\n",
      "Epoch:  13826  Learning Rate:  9.905554406366463e-08  Varinance:  1.010505656869672e-07 \n",
      "\n",
      "Epoch:  13827  Learning Rate:  9.895653803086776e-08  Varinance:  1.0090940175064058e-07 \n",
      "\n",
      "Epoch:  13828  Learning Rate:  9.885763095461731e-08  Varinance:  1.007684350151587e-07 \n",
      "\n",
      "Epoch:  13829  Learning Rate:  9.875882273600607e-08  Varinance:  1.0062766520503928e-07 \n",
      "\n",
      "Epoch:  13830  Learning Rate:  9.866011327622563e-08  Varinance:  1.0048709204518477e-07 \n",
      "\n",
      "Epoch:  13831  Learning Rate:  9.856150247656685e-08  Varinance:  1.003467152608823e-07 \n",
      "\n",
      "Epoch:  13832  Learning Rate:  9.846299023841876e-08  Varinance:  1.0020653457780175e-07 \n",
      "\n",
      "Epoch:  13833  Learning Rate:  9.836457646326894e-08  Varinance:  1.0006654972199724e-07 \n",
      "\n",
      "Epoch:  13834  Learning Rate:  9.826626105270396e-08  Varinance:  9.992676041990524e-08 \n",
      "\n",
      "Epoch:  13835  Learning Rate:  9.816804390840822e-08  Varinance:  9.978716639834437e-08 \n",
      "\n",
      "Epoch:  13836  Learning Rate:  9.80699249321644e-08  Varinance:  9.964776738451492e-08 \n",
      "\n",
      "Epoch:  13837  Learning Rate:  9.797190402585386e-08  Varinance:  9.95085631059982e-08 \n",
      "\n",
      "Epoch:  13838  Learning Rate:  9.787398109145549e-08  Varinance:  9.93695532907565e-08 \n",
      "\n",
      "Epoch:  13839  Learning Rate:  9.77761560310464e-08  Varinance:  9.923073766713102e-08 \n",
      "\n",
      "Epoch:  13840  Learning Rate:  9.767842874680129e-08  Varinance:  9.909211596384356e-08 \n",
      "\n",
      "Epoch:  13841  Learning Rate:  9.758079914099323e-08  Varinance:  9.89536879099945e-08 \n",
      "\n",
      "Epoch:  13842  Learning Rate:  9.748326711599247e-08  Varinance:  9.88154532350627e-08 \n",
      "\n",
      "Epoch:  13843  Learning Rate:  9.738583257426677e-08  Varinance:  9.867741166890487e-08 \n",
      "\n",
      "Epoch:  13844  Learning Rate:  9.728849541838192e-08  Varinance:  9.853956294175517e-08 \n",
      "\n",
      "Epoch:  13845  Learning Rate:  9.719125555100061e-08  Varinance:  9.840190678422486e-08 \n",
      "\n",
      "Epoch:  13846  Learning Rate:  9.709411287488277e-08  Varinance:  9.82644429273006e-08 \n",
      "\n",
      "Epoch:  13847  Learning Rate:  9.699706729288608e-08  Varinance:  9.812717110234581e-08 \n",
      "\n",
      "Epoch:  13848  Learning Rate:  9.690011870796475e-08  Varinance:  9.79900910410989e-08 \n",
      "\n",
      "Epoch:  13849  Learning Rate:  9.680326702317004e-08  Varinance:  9.785320247567297e-08 \n",
      "\n",
      "Epoch:  13850  Learning Rate:  9.670651214165059e-08  Varinance:  9.771650513855539e-08 \n",
      "\n",
      "Epoch:  13851  Learning Rate:  9.660985396665134e-08  Varinance:  9.757999876260721e-08 \n",
      "\n",
      "Epoch:  13852  Learning Rate:  9.651329240151393e-08  Varinance:  9.744368308106304e-08 \n",
      "\n",
      "Epoch:  13853  Learning Rate:  9.641682734967716e-08  Varinance:  9.730755782752907e-08 \n",
      "\n",
      "Epoch:  13854  Learning Rate:  9.632045871467576e-08  Varinance:  9.717162273598471e-08 \n",
      "\n",
      "Epoch:  13855  Learning Rate:  9.622418640014109e-08  Varinance:  9.703587754078061e-08 \n",
      "\n",
      "Epoch:  13856  Learning Rate:  9.612801030980068e-08  Varinance:  9.690032197663861e-08 \n",
      "\n",
      "Epoch:  13857  Learning Rate:  9.603193034747874e-08  Varinance:  9.676495577865101e-08 \n",
      "\n",
      "Epoch:  13858  Learning Rate:  9.593594641709518e-08  Varinance:  9.662977868228024e-08 \n",
      "\n",
      "Epoch:  13859  Learning Rate:  9.584005842266584e-08  Varinance:  9.64947904233586e-08 \n",
      "\n",
      "Epoch:  13860  Learning Rate:  9.57442662683031e-08  Varinance:  9.635999073808646e-08 \n",
      "\n",
      "Epoch:  13861  Learning Rate:  9.56485698582146e-08  Varinance:  9.622537936303365e-08 \n",
      "\n",
      "Epoch:  13862  Learning Rate:  9.555296909670374e-08  Varinance:  9.609095603513769e-08 \n",
      "\n",
      "Epoch:  13863  Learning Rate:  9.545746388817013e-08  Varinance:  9.595672049170361e-08 \n",
      "\n",
      "Epoch:  13864  Learning Rate:  9.536205413710835e-08  Varinance:  9.58226724704034e-08 \n",
      "\n",
      "Epoch:  13865  Learning Rate:  9.52667397481085e-08  Varinance:  9.568881170927554e-08 \n",
      "\n",
      "Epoch:  13866  Learning Rate:  9.517152062585649e-08  Varinance:  9.555513794672471e-08 \n",
      "\n",
      "Epoch:  13867  Learning Rate:  9.507639667513304e-08  Varinance:  9.54216509215201e-08 \n",
      "\n",
      "Epoch:  13868  Learning Rate:  9.498136780081402e-08  Varinance:  9.528835037279684e-08 \n",
      "\n",
      "Epoch:  13869  Learning Rate:  9.488643390787089e-08  Varinance:  9.51552360400541e-08 \n",
      "\n",
      "Epoch:  13870  Learning Rate:  9.479159490136957e-08  Varinance:  9.502230766315499e-08 \n",
      "\n",
      "Epoch:  13871  Learning Rate:  9.469685068647105e-08  Varinance:  9.488956498232602e-08 \n",
      "\n",
      "Epoch:  13872  Learning Rate:  9.460220116843096e-08  Varinance:  9.475700773815658e-08 \n",
      "\n",
      "Epoch:  13873  Learning Rate:  9.450764625260006e-08  Varinance:  9.462463567159878e-08 \n",
      "\n",
      "Epoch:  13874  Learning Rate:  9.441318584442331e-08  Varinance:  9.44924485239656e-08 \n",
      "\n",
      "Epoch:  13875  Learning Rate:  9.431881984944009e-08  Varinance:  9.436044603693247e-08 \n",
      "\n",
      "Epoch:  13876  Learning Rate:  9.422454817328476e-08  Varinance:  9.422862795253524e-08 \n",
      "\n",
      "Epoch:  13877  Learning Rate:  9.413037072168544e-08  Varinance:  9.409699401317023e-08 \n",
      "\n",
      "Epoch:  13878  Learning Rate:  9.403628740046453e-08  Varinance:  9.396554396159355e-08 \n",
      "\n",
      "Epoch:  13879  Learning Rate:  9.394229811553903e-08  Varinance:  9.383427754092072e-08 \n",
      "\n",
      "Epoch:  13880  Learning Rate:  9.384840277291945e-08  Varinance:  9.370319449462642e-08 \n",
      "\n",
      "Epoch:  13881  Learning Rate:  9.375460127871032e-08  Varinance:  9.35722945665427e-08 \n",
      "\n",
      "Epoch:  13882  Learning Rate:  9.366089353911044e-08  Varinance:  9.344157750086048e-08 \n",
      "\n",
      "Epoch:  13883  Learning Rate:  9.356727946041189e-08  Varinance:  9.331104304212766e-08 \n",
      "\n",
      "Epoch:  13884  Learning Rate:  9.347375894900062e-08  Varinance:  9.318069093524907e-08 \n",
      "\n",
      "Epoch:  13885  Learning Rate:  9.338033191135592e-08  Varinance:  9.30505209254858e-08 \n",
      "\n",
      "Epoch:  13886  Learning Rate:  9.328699825405107e-08  Varinance:  9.29205327584552e-08 \n",
      "\n",
      "Epoch:  13887  Learning Rate:  9.319375788375225e-08  Varinance:  9.279072618012899e-08 \n",
      "\n",
      "Epoch:  13888  Learning Rate:  9.310061070721892e-08  Varinance:  9.266110093683475e-08 \n",
      "\n",
      "Epoch:  13889  Learning Rate:  9.300755663130422e-08  Varinance:  9.253165677525405e-08 \n",
      "\n",
      "Epoch:  13890  Learning Rate:  9.291459556295388e-08  Varinance:  9.24023934424224e-08 \n",
      "\n",
      "Epoch:  13891  Learning Rate:  9.282172740920671e-08  Varinance:  9.227331068572866e-08 \n",
      "\n",
      "Epoch:  13892  Learning Rate:  9.272895207719484e-08  Varinance:  9.214440825291457e-08 \n",
      "\n",
      "Epoch:  13893  Learning Rate:  9.263626947414277e-08  Varinance:  9.201568589207463e-08 \n",
      "\n",
      "Epoch:  13894  Learning Rate:  9.254367950736773e-08  Varinance:  9.188714335165423e-08 \n",
      "\n",
      "Epoch:  13895  Learning Rate:  9.245118208428008e-08  Varinance:  9.175878038045117e-08 \n",
      "\n",
      "Epoch:  13896  Learning Rate:  9.235877711238221e-08  Varinance:  9.163059672761381e-08 \n",
      "\n",
      "Epoch:  13897  Learning Rate:  9.226646449926899e-08  Varinance:  9.150259214264098e-08 \n",
      "\n",
      "Epoch:  13898  Learning Rate:  9.217424415262812e-08  Varinance:  9.137476637538145e-08 \n",
      "\n",
      "Epoch:  13899  Learning Rate:  9.208211598023909e-08  Varinance:  9.124711917603341e-08 \n",
      "\n",
      "Epoch:  13900  Learning Rate:  9.19900798899737e-08  Varinance:  9.111965029514437e-08 \n",
      "\n",
      "Epoch:  13901  Learning Rate:  9.189813578979572e-08  Varinance:  9.099235948360931e-08 \n",
      "\n",
      "Epoch:  13902  Learning Rate:  9.180628358776134e-08  Varinance:  9.086524649267222e-08 \n",
      "\n",
      "Epoch:  13903  Learning Rate:  9.17145231920182e-08  Varinance:  9.073831107392422e-08 \n",
      "\n",
      "Epoch:  13904  Learning Rate:  9.162285451080573e-08  Varinance:  9.06115529793035e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13905  Learning Rate:  9.153127745245557e-08  Varinance:  9.048497196109471e-08 \n",
      "\n",
      "Epoch:  13906  Learning Rate:  9.14397919253905e-08  Varinance:  9.035856777192865e-08 \n",
      "\n",
      "Epoch:  13907  Learning Rate:  9.13483978381248e-08  Varinance:  9.023234016478191e-08 \n",
      "\n",
      "Epoch:  13908  Learning Rate:  9.125709509926473e-08  Varinance:  9.010628889297523e-08 \n",
      "\n",
      "Epoch:  13909  Learning Rate:  9.116588361750733e-08  Varinance:  8.998041371017495e-08 \n",
      "\n",
      "Epoch:  13910  Learning Rate:  9.1074763301641e-08  Varinance:  8.985471437039119e-08 \n",
      "\n",
      "Epoch:  13911  Learning Rate:  9.098373406054573e-08  Varinance:  8.972919062797768e-08 \n",
      "\n",
      "Epoch:  13912  Learning Rate:  9.089279580319211e-08  Varinance:  8.960384223763132e-08 \n",
      "\n",
      "Epoch:  13913  Learning Rate:  9.08019484386417e-08  Varinance:  8.947866895439173e-08 \n",
      "\n",
      "Epoch:  13914  Learning Rate:  9.071119187604745e-08  Varinance:  8.9353670533641e-08 \n",
      "\n",
      "Epoch:  13915  Learning Rate:  9.062052602465263e-08  Varinance:  8.922884673110202e-08 \n",
      "\n",
      "Epoch:  13916  Learning Rate:  9.05299507937914e-08  Varinance:  8.910419730283984e-08 \n",
      "\n",
      "Epoch:  13917  Learning Rate:  9.043946609288834e-08  Varinance:  8.897972200526003e-08 \n",
      "\n",
      "Epoch:  13918  Learning Rate:  9.034907183145907e-08  Varinance:  8.885542059510835e-08 \n",
      "\n",
      "Epoch:  13919  Learning Rate:  9.025876791910915e-08  Varinance:  8.873129282947049e-08 \n",
      "\n",
      "Epoch:  13920  Learning Rate:  9.016855426553453e-08  Varinance:  8.860733846577142e-08 \n",
      "\n",
      "Epoch:  13921  Learning Rate:  9.007843078052185e-08  Varinance:  8.848355726177528e-08 \n",
      "\n",
      "Epoch:  13922  Learning Rate:  8.998839737394745e-08  Varinance:  8.835994897558369e-08 \n",
      "\n",
      "Epoch:  13923  Learning Rate:  8.989845395577777e-08  Varinance:  8.823651336563713e-08 \n",
      "\n",
      "Epoch:  13924  Learning Rate:  8.980860043606967e-08  Varinance:  8.811325019071324e-08 \n",
      "\n",
      "Epoch:  13925  Learning Rate:  8.971883672496952e-08  Varinance:  8.799015920992657e-08 \n",
      "\n",
      "Epoch:  13926  Learning Rate:  8.962916273271339e-08  Varinance:  8.786724018272826e-08 \n",
      "\n",
      "Epoch:  13927  Learning Rate:  8.953957836962764e-08  Varinance:  8.774449286890542e-08 \n",
      "\n",
      "Epoch:  13928  Learning Rate:  8.945008354612772e-08  Varinance:  8.76219170285811e-08 \n",
      "\n",
      "Epoch:  13929  Learning Rate:  8.936067817271862e-08  Varinance:  8.749951242221247e-08 \n",
      "\n",
      "Epoch:  13930  Learning Rate:  8.927136215999532e-08  Varinance:  8.737727881059227e-08 \n",
      "\n",
      "Epoch:  13931  Learning Rate:  8.918213541864161e-08  Varinance:  8.725521595484712e-08 \n",
      "\n",
      "Epoch:  13932  Learning Rate:  8.909299785943076e-08  Varinance:  8.71333236164373e-08 \n",
      "\n",
      "Epoch:  13933  Learning Rate:  8.900394939322503e-08  Varinance:  8.701160155715639e-08 \n",
      "\n",
      "Epoch:  13934  Learning Rate:  8.891498993097625e-08  Varinance:  8.689004953913065e-08 \n",
      "\n",
      "Epoch:  13935  Learning Rate:  8.882611938372484e-08  Varinance:  8.676866732481899e-08 \n",
      "\n",
      "Epoch:  13936  Learning Rate:  8.873733766260005e-08  Varinance:  8.664745467701123e-08 \n",
      "\n",
      "Epoch:  13937  Learning Rate:  8.864864467882047e-08  Varinance:  8.65264113588295e-08 \n",
      "\n",
      "Epoch:  13938  Learning Rate:  8.856004034369296e-08  Varinance:  8.640553713372652e-08 \n",
      "\n",
      "Epoch:  13939  Learning Rate:  8.847152456861301e-08  Varinance:  8.628483176548542e-08 \n",
      "\n",
      "Epoch:  13940  Learning Rate:  8.838309726506516e-08  Varinance:  8.616429501821942e-08 \n",
      "\n",
      "Epoch:  13941  Learning Rate:  8.829475834462194e-08  Varinance:  8.604392665637113e-08 \n",
      "\n",
      "Epoch:  13942  Learning Rate:  8.820650771894427e-08  Varinance:  8.592372644471263e-08 \n",
      "\n",
      "Epoch:  13943  Learning Rate:  8.811834529978182e-08  Varinance:  8.580369414834368e-08 \n",
      "\n",
      "Epoch:  13944  Learning Rate:  8.803027099897202e-08  Varinance:  8.568382953269304e-08 \n",
      "\n",
      "Epoch:  13945  Learning Rate:  8.79422847284404e-08  Varinance:  8.556413236351691e-08 \n",
      "\n",
      "Epoch:  13946  Learning Rate:  8.785438640020099e-08  Varinance:  8.544460240689871e-08 \n",
      "\n",
      "Epoch:  13947  Learning Rate:  8.77665759263553e-08  Varinance:  8.532523942924861e-08 \n",
      "\n",
      "Epoch:  13948  Learning Rate:  8.767885321909284e-08  Varinance:  8.520604319730313e-08 \n",
      "\n",
      "Epoch:  13949  Learning Rate:  8.759121819069076e-08  Varinance:  8.508701347812492e-08 \n",
      "\n",
      "Epoch:  13950  Learning Rate:  8.750367075351434e-08  Varinance:  8.496815003910114e-08 \n",
      "\n",
      "Epoch:  13951  Learning Rate:  8.741621082001595e-08  Varinance:  8.484945264794483e-08 \n",
      "\n",
      "Epoch:  13952  Learning Rate:  8.73288383027355e-08  Varinance:  8.47309210726932e-08 \n",
      "\n",
      "Epoch:  13953  Learning Rate:  8.72415531143008e-08  Varinance:  8.461255508170749e-08 \n",
      "\n",
      "Epoch:  13954  Learning Rate:  8.715435516742648e-08  Varinance:  8.449435444367258e-08 \n",
      "\n",
      "Epoch:  13955  Learning Rate:  8.706724437491443e-08  Varinance:  8.437631892759642e-08 \n",
      "\n",
      "Epoch:  13956  Learning Rate:  8.698022064965417e-08  Varinance:  8.425844830281004e-08 \n",
      "\n",
      "Epoch:  13957  Learning Rate:  8.689328390462182e-08  Varinance:  8.41407423389657e-08 \n",
      "\n",
      "Epoch:  13958  Learning Rate:  8.680643405288044e-08  Varinance:  8.402320080603843e-08 \n",
      "\n",
      "Epoch:  13959  Learning Rate:  8.671967100758052e-08  Varinance:  8.390582347432424e-08 \n",
      "\n",
      "Epoch:  13960  Learning Rate:  8.663299468195881e-08  Varinance:  8.378861011444008e-08 \n",
      "\n",
      "Epoch:  13961  Learning Rate:  8.654640498933887e-08  Varinance:  8.367156049732331e-08 \n",
      "\n",
      "Epoch:  13962  Learning Rate:  8.645990184313128e-08  Varinance:  8.355467439423128e-08 \n",
      "\n",
      "Epoch:  13963  Learning Rate:  8.637348515683274e-08  Varinance:  8.343795157674118e-08 \n",
      "\n",
      "Epoch:  13964  Learning Rate:  8.628715484402655e-08  Varinance:  8.332139181674843e-08 \n",
      "\n",
      "Epoch:  13965  Learning Rate:  8.620091081838224e-08  Varinance:  8.320499488646797e-08 \n",
      "\n",
      "Epoch:  13966  Learning Rate:  8.611475299365609e-08  Varinance:  8.308876055843266e-08 \n",
      "\n",
      "Epoch:  13967  Learning Rate:  8.60286812836901e-08  Varinance:  8.297268860549313e-08 \n",
      "\n",
      "Epoch:  13968  Learning Rate:  8.594269560241242e-08  Varinance:  8.285677880081732e-08 \n",
      "\n",
      "Epoch:  13969  Learning Rate:  8.585679586383765e-08  Varinance:  8.274103091789005e-08 \n",
      "\n",
      "Epoch:  13970  Learning Rate:  8.57709819820659e-08  Varinance:  8.262544473051287e-08 \n",
      "\n",
      "Epoch:  13971  Learning Rate:  8.568525387128314e-08  Varinance:  8.251002001280242e-08 \n",
      "\n",
      "Epoch:  13972  Learning Rate:  8.559961144576153e-08  Varinance:  8.239475653919181e-08 \n",
      "\n",
      "Epoch:  13973  Learning Rate:  8.55140546198585e-08  Varinance:  8.227965408442893e-08 \n",
      "\n",
      "Epoch:  13974  Learning Rate:  8.542858330801707e-08  Varinance:  8.216471242357634e-08 \n",
      "\n",
      "Epoch:  13975  Learning Rate:  8.534319742476622e-08  Varinance:  8.204993133201087e-08 \n",
      "\n",
      "Epoch:  13976  Learning Rate:  8.525789688471989e-08  Varinance:  8.193531058542309e-08 \n",
      "\n",
      "Epoch:  13977  Learning Rate:  8.517268160257742e-08  Varinance:  8.182084995981723e-08 \n",
      "\n",
      "Epoch:  13978  Learning Rate:  8.508755149312379e-08  Varinance:  8.17065492315096e-08 \n",
      "\n",
      "Epoch:  13979  Learning Rate:  8.500250647122875e-08  Varinance:  8.159240817712979e-08 \n",
      "\n",
      "Epoch:  13980  Learning Rate:  8.491754645184726e-08  Varinance:  8.147842657361918e-08 \n",
      "\n",
      "Epoch:  13981  Learning Rate:  8.483267135001915e-08  Varinance:  8.136460419823077e-08 \n",
      "\n",
      "Epoch:  13982  Learning Rate:  8.47478810808696e-08  Varinance:  8.125094082852869e-08 \n",
      "\n",
      "Epoch:  13983  Learning Rate:  8.46631755596082e-08  Varinance:  8.113743624238784e-08 \n",
      "\n",
      "Epoch:  13984  Learning Rate:  8.457855470152927e-08  Varinance:  8.102409021799368e-08 \n",
      "\n",
      "Epoch:  13985  Learning Rate:  8.449401842201224e-08  Varinance:  8.091090253384068e-08 \n",
      "\n",
      "Epoch:  13986  Learning Rate:  8.440956663652067e-08  Varinance:  8.079787296873364e-08 \n",
      "\n",
      "Epoch:  13987  Learning Rate:  8.432519926060262e-08  Varinance:  8.068500130178605e-08 \n",
      "\n",
      "Epoch:  13988  Learning Rate:  8.424091620989101e-08  Varinance:  8.057228731241992e-08 \n",
      "\n",
      "Epoch:  13989  Learning Rate:  8.415671740010261e-08  Varinance:  8.04597307803655e-08 \n",
      "\n",
      "Epoch:  13990  Learning Rate:  8.40726027470385e-08  Varinance:  8.034733148566068e-08 \n",
      "\n",
      "Epoch:  13991  Learning Rate:  8.398857216658428e-08  Varinance:  8.023508920865092e-08 \n",
      "\n",
      "Epoch:  13992  Learning Rate:  8.390462557470923e-08  Varinance:  8.01230037299877e-08 \n",
      "\n",
      "Epoch:  13993  Learning Rate:  8.38207628874666e-08  Varinance:  8.001107483062976e-08 \n",
      "\n",
      "Epoch:  13994  Learning Rate:  8.373698402099399e-08  Varinance:  7.989930229184155e-08 \n",
      "\n",
      "Epoch:  13995  Learning Rate:  8.365328889151238e-08  Varinance:  7.978768589519308e-08 \n",
      "\n",
      "Epoch:  13996  Learning Rate:  8.356967741532662e-08  Varinance:  7.96762254225595e-08 \n",
      "\n",
      "Epoch:  13997  Learning Rate:  8.34861495088251e-08  Varinance:  7.956492065612069e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13998  Learning Rate:  8.34027050884802e-08  Varinance:  7.945377137836109e-08 \n",
      "\n",
      "Epoch:  13999  Learning Rate:  8.331934407084734e-08  Varinance:  7.934277737206811e-08 \n",
      "\n",
      "Epoch:  14000  Learning Rate:  8.323606637256534e-08  Varinance:  7.923193842033354e-08 \n",
      "\n",
      "Epoch:  14001  Learning Rate:  8.31528719103568e-08  Varinance:  7.91212543065518e-08 \n",
      "\n",
      "Epoch:  14002  Learning Rate:  8.30697606010271e-08  Varinance:  7.901072481441998e-08 \n",
      "\n",
      "Epoch:  14003  Learning Rate:  8.298673236146477e-08  Varinance:  7.890034972793728e-08 \n",
      "\n",
      "Epoch:  14004  Learning Rate:  8.290378710864186e-08  Varinance:  7.879012883140467e-08 \n",
      "\n",
      "Epoch:  14005  Learning Rate:  8.282092475961297e-08  Varinance:  7.868006190942473e-08 \n",
      "\n",
      "Epoch:  14006  Learning Rate:  8.27381452315156e-08  Varinance:  7.857014874690009e-08 \n",
      "\n",
      "Epoch:  14007  Learning Rate:  8.26554484415705e-08  Varinance:  7.84603891290347e-08 \n",
      "\n",
      "Epoch:  14008  Learning Rate:  8.257283430708073e-08  Varinance:  7.835078284133231e-08 \n",
      "\n",
      "Epoch:  14009  Learning Rate:  8.249030274543216e-08  Varinance:  7.824132966959631e-08 \n",
      "\n",
      "Epoch:  14010  Learning Rate:  8.240785367409305e-08  Varinance:  7.813202939992928e-08 \n",
      "\n",
      "Epoch:  14011  Learning Rate:  8.232548701061463e-08  Varinance:  7.802288181873265e-08 \n",
      "\n",
      "Epoch:  14012  Learning Rate:  8.224320267263008e-08  Varinance:  7.791388671270651e-08 \n",
      "\n",
      "Epoch:  14013  Learning Rate:  8.21610005778549e-08  Varinance:  7.780504386884805e-08 \n",
      "\n",
      "Epoch:  14014  Learning Rate:  8.207888064408731e-08  Varinance:  7.769635307445292e-08 \n",
      "\n",
      "Epoch:  14015  Learning Rate:  8.19968427892072e-08  Varinance:  7.758781411711357e-08 \n",
      "\n",
      "Epoch:  14016  Learning Rate:  8.191488693117656e-08  Varinance:  7.747942678471921e-08 \n",
      "\n",
      "Epoch:  14017  Learning Rate:  8.183301298803983e-08  Varinance:  7.737119086545534e-08 \n",
      "\n",
      "Epoch:  14018  Learning Rate:  8.17512208779229e-08  Varinance:  7.726310614780365e-08 \n",
      "\n",
      "Epoch:  14019  Learning Rate:  8.166951051903352e-08  Varinance:  7.715517242054047e-08 \n",
      "\n",
      "Epoch:  14020  Learning Rate:  8.15878818296616e-08  Varinance:  7.704738947273803e-08 \n",
      "\n",
      "Epoch:  14021  Learning Rate:  8.150633472817833e-08  Varinance:  7.693975709376297e-08 \n",
      "\n",
      "Epoch:  14022  Learning Rate:  8.142486913303641e-08  Varinance:  7.683227507327615e-08 \n",
      "\n",
      "Epoch:  14023  Learning Rate:  8.134348496277057e-08  Varinance:  7.672494320123226e-08 \n",
      "\n",
      "Epoch:  14024  Learning Rate:  8.126218213599648e-08  Varinance:  7.661776126787946e-08 \n",
      "\n",
      "Epoch:  14025  Learning Rate:  8.118096057141126e-08  Varinance:  7.651072906375915e-08 \n",
      "\n",
      "Epoch:  14026  Learning Rate:  8.109982018779328e-08  Varinance:  7.64038463797045e-08 \n",
      "\n",
      "Epoch:  14027  Learning Rate:  8.101876090400235e-08  Varinance:  7.62971130068418e-08 \n",
      "\n",
      "Epoch:  14028  Learning Rate:  8.09377826389791e-08  Varinance:  7.619052873658872e-08 \n",
      "\n",
      "Epoch:  14029  Learning Rate:  8.085688531174509e-08  Varinance:  7.608409336065443e-08 \n",
      "\n",
      "Epoch:  14030  Learning Rate:  8.077606884140325e-08  Varinance:  7.597780667103901e-08 \n",
      "\n",
      "Epoch:  14031  Learning Rate:  8.069533314713701e-08  Varinance:  7.587166846003312e-08 \n",
      "\n",
      "Epoch:  14032  Learning Rate:  8.061467814821048e-08  Varinance:  7.576567852021783e-08 \n",
      "\n",
      "Epoch:  14033  Learning Rate:  8.053410376396898e-08  Varinance:  7.565983664446322e-08 \n",
      "\n",
      "Epoch:  14034  Learning Rate:  8.045360991383793e-08  Varinance:  7.555414262592948e-08 \n",
      "\n",
      "Epoch:  14035  Learning Rate:  8.037319651732337e-08  Varinance:  7.544859625806548e-08 \n",
      "\n",
      "Epoch:  14036  Learning Rate:  8.029286349401216e-08  Varinance:  7.534319733460868e-08 \n",
      "\n",
      "Epoch:  14037  Learning Rate:  8.021261076357114e-08  Varinance:  7.52379456495846e-08 \n",
      "\n",
      "Epoch:  14038  Learning Rate:  8.013243824574744e-08  Varinance:  7.513284099730659e-08 \n",
      "\n",
      "Epoch:  14039  Learning Rate:  8.005234586036878e-08  Varinance:  7.502788317237553e-08 \n",
      "\n",
      "Epoch:  14040  Learning Rate:  7.997233352734268e-08  Varinance:  7.492307196967848e-08 \n",
      "\n",
      "Epoch:  14041  Learning Rate:  7.989240116665673e-08  Varinance:  7.481840718438982e-08 \n",
      "\n",
      "Epoch:  14042  Learning Rate:  7.981254869837851e-08  Varinance:  7.471388861196978e-08 \n",
      "\n",
      "Epoch:  14043  Learning Rate:  7.973277604265574e-08  Varinance:  7.460951604816438e-08 \n",
      "\n",
      "Epoch:  14044  Learning Rate:  7.965308311971568e-08  Varinance:  7.45052892890049e-08 \n",
      "\n",
      "Epoch:  14045  Learning Rate:  7.957346984986523e-08  Varinance:  7.44012081308076e-08 \n",
      "\n",
      "Epoch:  14046  Learning Rate:  7.949393615349141e-08  Varinance:  7.429727237017354e-08 \n",
      "\n",
      "Epoch:  14047  Learning Rate:  7.941448195106036e-08  Varinance:  7.419348180398715e-08 \n",
      "\n",
      "Epoch:  14048  Learning Rate:  7.933510716311775e-08  Varinance:  7.408983622941734e-08 \n",
      "\n",
      "Epoch:  14049  Learning Rate:  7.925581171028904e-08  Varinance:  7.398633544391615e-08 \n",
      "\n",
      "Epoch:  14050  Learning Rate:  7.917659551327865e-08  Varinance:  7.388297924521854e-08 \n",
      "\n",
      "Epoch:  14051  Learning Rate:  7.909745849287022e-08  Varinance:  7.377976743134208e-08 \n",
      "\n",
      "Epoch:  14052  Learning Rate:  7.901840056992703e-08  Varinance:  7.367669980058644e-08 \n",
      "\n",
      "Epoch:  14053  Learning Rate:  7.893942166539099e-08  Varinance:  7.357377615153336e-08 \n",
      "\n",
      "Epoch:  14054  Learning Rate:  7.886052170028305e-08  Varinance:  7.347099628304515e-08 \n",
      "\n",
      "Epoch:  14055  Learning Rate:  7.878170059570354e-08  Varinance:  7.336835999426589e-08 \n",
      "\n",
      "Epoch:  14056  Learning Rate:  7.870295827283117e-08  Varinance:  7.326586708461999e-08 \n",
      "\n",
      "Epoch:  14057  Learning Rate:  7.862429465292363e-08  Varinance:  7.316351735381207e-08 \n",
      "\n",
      "Epoch:  14058  Learning Rate:  7.854570965731717e-08  Varinance:  7.306131060182653e-08 \n",
      "\n",
      "Epoch:  14059  Learning Rate:  7.846720320742705e-08  Varinance:  7.295924662892718e-08 \n",
      "\n",
      "Epoch:  14060  Learning Rate:  7.838877522474666e-08  Varinance:  7.285732523565716e-08 \n",
      "\n",
      "Epoch:  14061  Learning Rate:  7.83104256308479e-08  Varinance:  7.275554622283742e-08 \n",
      "\n",
      "Epoch:  14062  Learning Rate:  7.823215434738144e-08  Varinance:  7.265390939156794e-08 \n",
      "\n",
      "Epoch:  14063  Learning Rate:  7.815396129607583e-08  Varinance:  7.255241454322632e-08 \n",
      "\n",
      "Epoch:  14064  Learning Rate:  7.807584639873792e-08  Varinance:  7.245106147946762e-08 \n",
      "\n",
      "Epoch:  14065  Learning Rate:  7.799780957725304e-08  Varinance:  7.234985000222396e-08 \n",
      "\n",
      "Epoch:  14066  Learning Rate:  7.791985075358422e-08  Varinance:  7.224877991370417e-08 \n",
      "\n",
      "Epoch:  14067  Learning Rate:  7.784196984977253e-08  Varinance:  7.214785101639366e-08 \n",
      "\n",
      "Epoch:  14068  Learning Rate:  7.77641667879373e-08  Varinance:  7.204706311305293e-08 \n",
      "\n",
      "Epoch:  14069  Learning Rate:  7.768644149027535e-08  Varinance:  7.194641600671888e-08 \n",
      "\n",
      "Epoch:  14070  Learning Rate:  7.760879387906122e-08  Varinance:  7.184590950070322e-08 \n",
      "\n",
      "Epoch:  14071  Learning Rate:  7.753122387664757e-08  Varinance:  7.174554339859247e-08 \n",
      "\n",
      "Epoch:  14072  Learning Rate:  7.745373140546427e-08  Varinance:  7.164531750424752e-08 \n",
      "\n",
      "Epoch:  14073  Learning Rate:  7.737631638801882e-08  Varinance:  7.154523162180326e-08 \n",
      "\n",
      "Epoch:  14074  Learning Rate:  7.729897874689606e-08  Varinance:  7.144528555566845e-08 \n",
      "\n",
      "Epoch:  14075  Learning Rate:  7.722171840475865e-08  Varinance:  7.134547911052432e-08 \n",
      "\n",
      "Epoch:  14076  Learning Rate:  7.714453528434606e-08  Varinance:  7.124581209132572e-08 \n",
      "\n",
      "Epoch:  14077  Learning Rate:  7.706742930847506e-08  Varinance:  7.114628430329971e-08 \n",
      "\n",
      "Epoch:  14078  Learning Rate:  7.699040040003992e-08  Varinance:  7.104689555194547e-08 \n",
      "\n",
      "Epoch:  14079  Learning Rate:  7.69134484820116e-08  Varinance:  7.094764564303383e-08 \n",
      "\n",
      "Epoch:  14080  Learning Rate:  7.683657347743802e-08  Varinance:  7.084853438260702e-08 \n",
      "\n",
      "Epoch:  14081  Learning Rate:  7.675977530944447e-08  Varinance:  7.074956157697841e-08 \n",
      "\n",
      "Epoch:  14082  Learning Rate:  7.668305390123263e-08  Varinance:  7.065072703273122e-08 \n",
      "\n",
      "Epoch:  14083  Learning Rate:  7.660640917608094e-08  Varinance:  7.055203055671966e-08 \n",
      "\n",
      "Epoch:  14084  Learning Rate:  7.652984105734494e-08  Varinance:  7.045347195606743e-08 \n",
      "\n",
      "Epoch:  14085  Learning Rate:  7.645334946845639e-08  Varinance:  7.03550510381677e-08 \n",
      "\n",
      "Epoch:  14086  Learning Rate:  7.637693433292353e-08  Varinance:  7.025676761068273e-08 \n",
      "\n",
      "Epoch:  14087  Learning Rate:  7.63005955743315e-08  Varinance:  7.015862148154346e-08 \n",
      "\n",
      "Epoch:  14088  Learning Rate:  7.622433311634142e-08  Varinance:  7.006061245894936e-08 \n",
      "\n",
      "Epoch:  14089  Learning Rate:  7.61481468826908e-08  Varinance:  6.99627403513671e-08 \n",
      "\n",
      "Epoch:  14090  Learning Rate:  7.607203679719327e-08  Varinance:  6.986500496753173e-08 \n",
      "\n",
      "Epoch:  14091  Learning Rate:  7.599600278373901e-08  Varinance:  6.976740611644515e-08 \n",
      "\n",
      "Epoch:  14092  Learning Rate:  7.592004476629388e-08  Varinance:  6.966994360737611e-08 \n",
      "\n",
      "Epoch:  14093  Learning Rate:  7.584416266889969e-08  Varinance:  6.957261724985983e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14094  Learning Rate:  7.576835641567463e-08  Varinance:  6.947542685369756e-08 \n",
      "\n",
      "Epoch:  14095  Learning Rate:  7.56926259308123e-08  Varinance:  6.937837222895654e-08 \n",
      "\n",
      "Epoch:  14096  Learning Rate:  7.561697113858208e-08  Varinance:  6.928145318596856e-08 \n",
      "\n",
      "Epoch:  14097  Learning Rate:  7.554139196332942e-08  Varinance:  6.918466953533113e-08 \n",
      "\n",
      "Epoch:  14098  Learning Rate:  7.546588832947505e-08  Varinance:  6.908802108790611e-08 \n",
      "\n",
      "Epoch:  14099  Learning Rate:  7.539046016151513e-08  Varinance:  6.899150765481956e-08 \n",
      "\n",
      "Epoch:  14100  Learning Rate:  7.531510738402182e-08  Varinance:  6.88951290474614e-08 \n",
      "\n",
      "Epoch:  14101  Learning Rate:  7.523982992164214e-08  Varinance:  6.879888507748504e-08 \n",
      "\n",
      "Epoch:  14102  Learning Rate:  7.516462769909853e-08  Varinance:  6.87027755568072e-08 \n",
      "\n",
      "Epoch:  14103  Learning Rate:  7.508950064118902e-08  Varinance:  6.86068002976067e-08 \n",
      "\n",
      "Epoch:  14104  Learning Rate:  7.50144486727864e-08  Varinance:  6.851095911232539e-08 \n",
      "\n",
      "Epoch:  14105  Learning Rate:  7.49394717188387e-08  Varinance:  6.841525181366691e-08 \n",
      "\n",
      "Epoch:  14106  Learning Rate:  7.486456970436884e-08  Varinance:  6.831967821459658e-08 \n",
      "\n",
      "Epoch:  14107  Learning Rate:  7.478974255447506e-08  Varinance:  6.822423812834098e-08 \n",
      "\n",
      "Epoch:  14108  Learning Rate:  7.471499019433007e-08  Varinance:  6.812893136838757e-08 \n",
      "\n",
      "Epoch:  14109  Learning Rate:  7.464031254918135e-08  Varinance:  6.803375774848468e-08 \n",
      "\n",
      "Epoch:  14110  Learning Rate:  7.456570954435154e-08  Varinance:  6.793871708264e-08 \n",
      "\n",
      "Epoch:  14111  Learning Rate:  7.44911811052375e-08  Varinance:  6.784380918512187e-08 \n",
      "\n",
      "Epoch:  14112  Learning Rate:  7.441672715731063e-08  Varinance:  6.77490338704578e-08 \n",
      "\n",
      "Epoch:  14113  Learning Rate:  7.434234762611725e-08  Varinance:  6.765439095343438e-08 \n",
      "\n",
      "Epoch:  14114  Learning Rate:  7.426804243727768e-08  Varinance:  6.755988024909696e-08 \n",
      "\n",
      "Epoch:  14115  Learning Rate:  7.419381151648662e-08  Varinance:  6.74655015727493e-08 \n",
      "\n",
      "Epoch:  14116  Learning Rate:  7.41196547895134e-08  Varinance:  6.737125473995334e-08 \n",
      "\n",
      "Epoch:  14117  Learning Rate:  7.404557218220112e-08  Varinance:  6.7277139566528e-08 \n",
      "\n",
      "Epoch:  14118  Learning Rate:  7.397156362046708e-08  Varinance:  6.718315586855022e-08 \n",
      "\n",
      "Epoch:  14119  Learning Rate:  7.389762903030296e-08  Varinance:  6.708930346235361e-08 \n",
      "\n",
      "Epoch:  14120  Learning Rate:  7.382376833777402e-08  Varinance:  6.699558216452836e-08 \n",
      "\n",
      "Epoch:  14121  Learning Rate:  7.374998146901957e-08  Varinance:  6.690199179192087e-08 \n",
      "\n",
      "Epoch:  14122  Learning Rate:  7.36762683502526e-08  Varinance:  6.680853216163343e-08 \n",
      "\n",
      "Epoch:  14123  Learning Rate:  7.360262890776026e-08  Varinance:  6.671520309102405e-08 \n",
      "\n",
      "Epoch:  14124  Learning Rate:  7.352906306790295e-08  Varinance:  6.662200439770514e-08 \n",
      "\n",
      "Epoch:  14125  Learning Rate:  7.345557075711472e-08  Varinance:  6.652893589954469e-08 \n",
      "\n",
      "Epoch:  14126  Learning Rate:  7.338215190190348e-08  Varinance:  6.64359974146648e-08 \n",
      "\n",
      "Epoch:  14127  Learning Rate:  7.330880642885026e-08  Varinance:  6.634318876144168e-08 \n",
      "\n",
      "Epoch:  14128  Learning Rate:  7.323553426460946e-08  Varinance:  6.625050975850528e-08 \n",
      "\n",
      "Epoch:  14129  Learning Rate:  7.316233533590915e-08  Varinance:  6.615796022473891e-08 \n",
      "\n",
      "Epoch:  14130  Learning Rate:  7.308920956955028e-08  Varinance:  6.60655399792791e-08 \n",
      "\n",
      "Epoch:  14131  Learning Rate:  7.301615689240693e-08  Varinance:  6.597324884151436e-08 \n",
      "\n",
      "Epoch:  14132  Learning Rate:  7.29431772314267e-08  Varinance:  6.588108663108623e-08 \n",
      "\n",
      "Epoch:  14133  Learning Rate:  7.287027051362977e-08  Varinance:  6.57890531678879e-08 \n",
      "\n",
      "Epoch:  14134  Learning Rate:  7.279743666610943e-08  Varinance:  6.569714827206425e-08 \n",
      "\n",
      "Epoch:  14135  Learning Rate:  7.272467561603168e-08  Varinance:  6.560537176401134e-08 \n",
      "\n",
      "Epoch:  14136  Learning Rate:  7.265198729063576e-08  Varinance:  6.551372346437619e-08 \n",
      "\n",
      "Epoch:  14137  Learning Rate:  7.257937161723316e-08  Varinance:  6.542220319405656e-08 \n",
      "\n",
      "Epoch:  14138  Learning Rate:  7.250682852320811e-08  Varinance:  6.533081077419971e-08 \n",
      "\n",
      "Epoch:  14139  Learning Rate:  7.243435793601775e-08  Varinance:  6.523954602620348e-08 \n",
      "\n",
      "Epoch:  14140  Learning Rate:  7.236195978319137e-08  Varinance:  6.514840877171497e-08 \n",
      "\n",
      "Epoch:  14141  Learning Rate:  7.228963399233066e-08  Varinance:  6.50573988326304e-08 \n",
      "\n",
      "Epoch:  14142  Learning Rate:  7.221738049111012e-08  Varinance:  6.496651603109482e-08 \n",
      "\n",
      "Epoch:  14143  Learning Rate:  7.214519920727607e-08  Varinance:  6.487576018950174e-08 \n",
      "\n",
      "Epoch:  14144  Learning Rate:  7.207309006864712e-08  Varinance:  6.4785131130493e-08 \n",
      "\n",
      "Epoch:  14145  Learning Rate:  7.200105300311436e-08  Varinance:  6.469462867695753e-08 \n",
      "\n",
      "Epoch:  14146  Learning Rate:  7.192908793864061e-08  Varinance:  6.460425265203233e-08 \n",
      "\n",
      "Epoch:  14147  Learning Rate:  7.185719480326066e-08  Varinance:  6.45140028791013e-08 \n",
      "\n",
      "Epoch:  14148  Learning Rate:  7.178537352508163e-08  Varinance:  6.4423879181795e-08 \n",
      "\n",
      "Epoch:  14149  Learning Rate:  7.171362403228213e-08  Varinance:  6.433388138399041e-08 \n",
      "\n",
      "Epoch:  14150  Learning Rate:  7.164194625311261e-08  Varinance:  6.424400930981078e-08 \n",
      "\n",
      "Epoch:  14151  Learning Rate:  7.15703401158952e-08  Varinance:  6.415426278362436e-08 \n",
      "\n",
      "Epoch:  14152  Learning Rate:  7.1498805549024e-08  Varinance:  6.406464163004541e-08 \n",
      "\n",
      "Epoch:  14153  Learning Rate:  7.142734248096429e-08  Varinance:  6.397514567393303e-08 \n",
      "\n",
      "Epoch:  14154  Learning Rate:  7.135595084025291e-08  Varinance:  6.388577474039091e-08 \n",
      "\n",
      "Epoch:  14155  Learning Rate:  7.128463055549842e-08  Varinance:  6.379652865476713e-08 \n",
      "\n",
      "Epoch:  14156  Learning Rate:  7.121338155538044e-08  Varinance:  6.370740724265371e-08 \n",
      "\n",
      "Epoch:  14157  Learning Rate:  7.114220376864981e-08  Varinance:  6.361841032988655e-08 \n",
      "\n",
      "Epoch:  14158  Learning Rate:  7.107109712412902e-08  Varinance:  6.352953774254418e-08 \n",
      "\n",
      "Epoch:  14159  Learning Rate:  7.100006155071127e-08  Varinance:  6.344078930694877e-08 \n",
      "\n",
      "Epoch:  14160  Learning Rate:  7.092909697736087e-08  Varinance:  6.335216484966487e-08 \n",
      "\n",
      "Epoch:  14161  Learning Rate:  7.085820333311346e-08  Varinance:  6.326366419749933e-08 \n",
      "\n",
      "Epoch:  14162  Learning Rate:  7.078738054707531e-08  Varinance:  6.317528717750095e-08 \n",
      "\n",
      "Epoch:  14163  Learning Rate:  7.071662854842347e-08  Varinance:  6.308703361696009e-08 \n",
      "\n",
      "Epoch:  14164  Learning Rate:  7.064594726640621e-08  Varinance:  6.299890334340866e-08 \n",
      "\n",
      "Epoch:  14165  Learning Rate:  7.057533663034209e-08  Varinance:  6.29108961846188e-08 \n",
      "\n",
      "Epoch:  14166  Learning Rate:  7.05047965696205e-08  Varinance:  6.282301196860394e-08 \n",
      "\n",
      "Epoch:  14167  Learning Rate:  7.043432701370121e-08  Varinance:  6.273525052361752e-08 \n",
      "\n",
      "Epoch:  14168  Learning Rate:  7.036392789211493e-08  Varinance:  6.26476116781529e-08 \n",
      "\n",
      "Epoch:  14169  Learning Rate:  7.029359913446241e-08  Varinance:  6.256009526094306e-08 \n",
      "\n",
      "Epoch:  14170  Learning Rate:  7.022334067041475e-08  Varinance:  6.24727011009602e-08 \n",
      "\n",
      "Epoch:  14171  Learning Rate:  7.015315242971375e-08  Varinance:  6.23854290274157e-08 \n",
      "\n",
      "Epoch:  14172  Learning Rate:  7.008303434217101e-08  Varinance:  6.229827886975881e-08 \n",
      "\n",
      "Epoch:  14173  Learning Rate:  7.001298633766836e-08  Varinance:  6.221125045767774e-08 \n",
      "\n",
      "Epoch:  14174  Learning Rate:  6.994300834615797e-08  Varinance:  6.212434362109839e-08 \n",
      "\n",
      "Epoch:  14175  Learning Rate:  6.987310029766178e-08  Varinance:  6.203755819018423e-08 \n",
      "\n",
      "Epoch:  14176  Learning Rate:  6.980326212227157e-08  Varinance:  6.1950893995336e-08 \n",
      "\n",
      "Epoch:  14177  Learning Rate:  6.973349375014943e-08  Varinance:  6.186435086719134e-08 \n",
      "\n",
      "Epoch:  14178  Learning Rate:  6.966379511152685e-08  Varinance:  6.177792863662475e-08 \n",
      "\n",
      "Epoch:  14179  Learning Rate:  6.959416613670507e-08  Varinance:  6.169162713474631e-08 \n",
      "\n",
      "Epoch:  14180  Learning Rate:  6.952460675605533e-08  Varinance:  6.160544619290265e-08 \n",
      "\n",
      "Epoch:  14181  Learning Rate:  6.945511690001816e-08  Varinance:  6.151938564267586e-08 \n",
      "\n",
      "Epoch:  14182  Learning Rate:  6.938569649910367e-08  Varinance:  6.143344531588323e-08 \n",
      "\n",
      "Epoch:  14183  Learning Rate:  6.931634548389135e-08  Varinance:  6.134762504457706e-08 \n",
      "\n",
      "Epoch:  14184  Learning Rate:  6.924706378503039e-08  Varinance:  6.126192466104422e-08 \n",
      "\n",
      "Epoch:  14185  Learning Rate:  6.9177851333239e-08  Varinance:  6.117634399780611e-08 \n",
      "\n",
      "Epoch:  14186  Learning Rate:  6.910870805930459e-08  Varinance:  6.109088288761742e-08 \n",
      "\n",
      "Epoch:  14187  Learning Rate:  6.903963389408411e-08  Varinance:  6.100554116346714e-08 \n",
      "\n",
      "Epoch:  14188  Learning Rate:  6.897062876850328e-08  Varinance:  6.092031865857736e-08 \n",
      "\n",
      "Epoch:  14189  Learning Rate:  6.890169261355685e-08  Varinance:  6.083521520640315e-08 \n",
      "\n",
      "Epoch:  14190  Learning Rate:  6.88328253603089e-08  Varinance:  6.07502306406322e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14191  Learning Rate:  6.876402693989203e-08  Varinance:  6.066536479518461e-08 \n",
      "\n",
      "Epoch:  14192  Learning Rate:  6.869529728350771e-08  Varinance:  6.05806175042126e-08 \n",
      "\n",
      "Epoch:  14193  Learning Rate:  6.862663632242653e-08  Varinance:  6.049598860209952e-08 \n",
      "\n",
      "Epoch:  14194  Learning Rate:  6.855804398798739e-08  Varinance:  6.041147792346068e-08 \n",
      "\n",
      "Epoch:  14195  Learning Rate:  6.848952021159782e-08  Varinance:  6.032708530314222e-08 \n",
      "\n",
      "Epoch:  14196  Learning Rate:  6.84210649247343e-08  Varinance:  6.024281057622101e-08 \n",
      "\n",
      "Epoch:  14197  Learning Rate:  6.835267805894141e-08  Varinance:  6.015865357800425e-08 \n",
      "\n",
      "Epoch:  14198  Learning Rate:  6.828435954583226e-08  Varinance:  6.007461414402929e-08 \n",
      "\n",
      "Epoch:  14199  Learning Rate:  6.821610931708824e-08  Varinance:  5.99906921100634e-08 \n",
      "\n",
      "Epoch:  14200  Learning Rate:  6.814792730445934e-08  Varinance:  5.990688731210264e-08 \n",
      "\n",
      "Epoch:  14201  Learning Rate:  6.807981343976342e-08  Varinance:  5.982319958637284e-08 \n",
      "\n",
      "Epoch:  14202  Learning Rate:  6.80117676548865e-08  Varinance:  5.973962876932836e-08 \n",
      "\n",
      "Epoch:  14203  Learning Rate:  6.794378988178302e-08  Varinance:  5.965617469765205e-08 \n",
      "\n",
      "Epoch:  14204  Learning Rate:  6.787588005247509e-08  Varinance:  5.9572837208254926e-08 \n",
      "\n",
      "Epoch:  14205  Learning Rate:  6.780803809905273e-08  Varinance:  5.9489616138275815e-08 \n",
      "\n",
      "Epoch:  14206  Learning Rate:  6.774026395367425e-08  Varinance:  5.940651132508127e-08 \n",
      "\n",
      "Epoch:  14207  Learning Rate:  6.767255754856537e-08  Varinance:  5.932352260626439e-08 \n",
      "\n",
      "Epoch:  14208  Learning Rate:  6.760491881601956e-08  Varinance:  5.9240649819645816e-08 \n",
      "\n",
      "Epoch:  14209  Learning Rate:  6.753734768839831e-08  Varinance:  5.91578928032725e-08 \n",
      "\n",
      "Epoch:  14210  Learning Rate:  6.746984409813038e-08  Varinance:  5.907525139541765e-08 \n",
      "\n",
      "Epoch:  14211  Learning Rate:  6.740240797771206e-08  Varinance:  5.8992725434580404e-08 \n",
      "\n",
      "Epoch:  14212  Learning Rate:  6.733503925970744e-08  Varinance:  5.8910314759485525e-08 \n",
      "\n",
      "Epoch:  14213  Learning Rate:  6.72677378767477e-08  Varinance:  5.8828019209083266e-08 \n",
      "\n",
      "Epoch:  14214  Learning Rate:  6.720050376153145e-08  Varinance:  5.8745838622548216e-08 \n",
      "\n",
      "Epoch:  14215  Learning Rate:  6.713333684682443e-08  Varinance:  5.8663772839280275e-08 \n",
      "\n",
      "Epoch:  14216  Learning Rate:  6.706623706545996e-08  Varinance:  5.85818216989035e-08 \n",
      "\n",
      "Epoch:  14217  Learning Rate:  6.699920435033816e-08  Varinance:  5.8499985041265976e-08 \n",
      "\n",
      "Epoch:  14218  Learning Rate:  6.693223863442618e-08  Varinance:  5.8418262706439505e-08 \n",
      "\n",
      "Epoch:  14219  Learning Rate:  6.686533985075851e-08  Varinance:  5.833665453471931e-08 \n",
      "\n",
      "Epoch:  14220  Learning Rate:  6.679850793243627e-08  Varinance:  5.825516036662391e-08 \n",
      "\n",
      "Epoch:  14221  Learning Rate:  6.673174281262743e-08  Varinance:  5.8173780042894044e-08 \n",
      "\n",
      "Epoch:  14222  Learning Rate:  6.666504442456707e-08  Varinance:  5.809251340449347e-08 \n",
      "\n",
      "Epoch:  14223  Learning Rate:  6.659841270155669e-08  Varinance:  5.8011360292607986e-08 \n",
      "\n",
      "Epoch:  14224  Learning Rate:  6.653184757696445e-08  Varinance:  5.793032054864518e-08 \n",
      "\n",
      "Epoch:  14225  Learning Rate:  6.646534898422543e-08  Varinance:  5.784939401423424e-08 \n",
      "\n",
      "Epoch:  14226  Learning Rate:  6.639891685684094e-08  Varinance:  5.776858053122557e-08 \n",
      "\n",
      "Epoch:  14227  Learning Rate:  6.633255112837874e-08  Varinance:  5.7687879941690703e-08 \n",
      "\n",
      "Epoch:  14228  Learning Rate:  6.626625173247329e-08  Varinance:  5.760729208792119e-08 \n",
      "\n",
      "Epoch:  14229  Learning Rate:  6.62000186028251e-08  Varinance:  5.75268168124295e-08 \n",
      "\n",
      "Epoch:  14230  Learning Rate:  6.613385167320104e-08  Varinance:  5.7446453957947914e-08 \n",
      "\n",
      "Epoch:  14231  Learning Rate:  6.606775087743404e-08  Varinance:  5.736620336742839e-08 \n",
      "\n",
      "Epoch:  14232  Learning Rate:  6.600171614942355e-08  Varinance:  5.728606488404228e-08 \n",
      "\n",
      "Epoch:  14233  Learning Rate:  6.59357474231347e-08  Varinance:  5.720603835118005e-08 \n",
      "\n",
      "Epoch:  14234  Learning Rate:  6.586984463259865e-08  Varinance:  5.712612361245112e-08 \n",
      "\n",
      "Epoch:  14235  Learning Rate:  6.580400771191285e-08  Varinance:  5.704632051168276e-08 \n",
      "\n",
      "Epoch:  14236  Learning Rate:  6.573823659524023e-08  Varinance:  5.6966628892921053e-08 \n",
      "\n",
      "Epoch:  14237  Learning Rate:  6.567253121680956e-08  Varinance:  5.6887048600429714e-08 \n",
      "\n",
      "Epoch:  14238  Learning Rate:  6.560689151091572e-08  Varinance:  5.680757947869003e-08 \n",
      "\n",
      "Epoch:  14239  Learning Rate:  6.554131741191884e-08  Varinance:  5.6728221372400534e-08 \n",
      "\n",
      "Epoch:  14240  Learning Rate:  6.547580885424473e-08  Varinance:  5.6648974126476695e-08 \n",
      "\n",
      "Epoch:  14241  Learning Rate:  6.541036577238503e-08  Varinance:  5.656983758605087e-08 \n",
      "\n",
      "Epoch:  14242  Learning Rate:  6.534498810089657e-08  Varinance:  5.649081159647112e-08 \n",
      "\n",
      "Epoch:  14243  Learning Rate:  6.527967577440154e-08  Varinance:  5.641189600330217e-08 \n",
      "\n",
      "Epoch:  14244  Learning Rate:  6.521442872758784e-08  Varinance:  5.633309065232428e-08 \n",
      "\n",
      "Epoch:  14245  Learning Rate:  6.514924689520829e-08  Varinance:  5.625439538953315e-08 \n",
      "\n",
      "Epoch:  14246  Learning Rate:  6.508413021208107e-08  Varinance:  5.617581006113963e-08 \n",
      "\n",
      "Epoch:  14247  Learning Rate:  6.501907861308938e-08  Varinance:  5.609733451356938e-08 \n",
      "\n",
      "Epoch:  14248  Learning Rate:  6.495409203318183e-08  Varinance:  5.601896859346281e-08 \n",
      "\n",
      "Epoch:  14249  Learning Rate:  6.488917040737172e-08  Varinance:  5.594071214767399e-08 \n",
      "\n",
      "Epoch:  14250  Learning Rate:  6.482431367073732e-08  Varinance:  5.58625650232715e-08 \n",
      "\n",
      "Epoch:  14251  Learning Rate:  6.475952175842209e-08  Varinance:  5.5784527067537376e-08 \n",
      "\n",
      "Epoch:  14252  Learning Rate:  6.469479460563403e-08  Varinance:  5.570659812796698e-08 \n",
      "\n",
      "Epoch:  14253  Learning Rate:  6.463013214764584e-08  Varinance:  5.562877805226873e-08 \n",
      "\n",
      "Epoch:  14254  Learning Rate:  6.456553431979532e-08  Varinance:  5.55510666883638e-08 \n",
      "\n",
      "Epoch:  14255  Learning Rate:  6.450100105748448e-08  Varinance:  5.547346388438598e-08 \n",
      "\n",
      "Epoch:  14256  Learning Rate:  6.443653229617997e-08  Varinance:  5.5395969488680646e-08 \n",
      "\n",
      "Epoch:  14257  Learning Rate:  6.437212797141324e-08  Varinance:  5.53185833498056e-08 \n",
      "\n",
      "Epoch:  14258  Learning Rate:  6.430778801877984e-08  Varinance:  5.524130531653004e-08 \n",
      "\n",
      "Epoch:  14259  Learning Rate:  6.424351237393981e-08  Varinance:  5.516413523783438e-08 \n",
      "\n",
      "Epoch:  14260  Learning Rate:  6.417930097261741e-08  Varinance:  5.508707296291005e-08 \n",
      "\n",
      "Epoch:  14261  Learning Rate:  6.411515375060144e-08  Varinance:  5.5010118341159135e-08 \n",
      "\n",
      "Epoch:  14262  Learning Rate:  6.405107064374456e-08  Varinance:  5.493327122219427e-08 \n",
      "\n",
      "Epoch:  14263  Learning Rate:  6.398705158796354e-08  Varinance:  5.485653145583762e-08 \n",
      "\n",
      "Epoch:  14264  Learning Rate:  6.392309651923957e-08  Varinance:  5.477989889212175e-08 \n",
      "\n",
      "Epoch:  14265  Learning Rate:  6.385920537361745e-08  Varinance:  5.470337338128846e-08 \n",
      "\n",
      "Epoch:  14266  Learning Rate:  6.379537808720588e-08  Varinance:  5.462695477378882e-08 \n",
      "\n",
      "Epoch:  14267  Learning Rate:  6.373161459617786e-08  Varinance:  5.455064292028279e-08 \n",
      "\n",
      "Epoch:  14268  Learning Rate:  6.366791483676974e-08  Varinance:  5.447443767163894e-08 \n",
      "\n",
      "Epoch:  14269  Learning Rate:  6.360427874528164e-08  Varinance:  5.4398338878934395e-08 \n",
      "\n",
      "Epoch:  14270  Learning Rate:  6.354070625807771e-08  Varinance:  5.4322346393453725e-08 \n",
      "\n",
      "Epoch:  14271  Learning Rate:  6.347719731158532e-08  Varinance:  5.4246460066689805e-08 \n",
      "\n",
      "Epoch:  14272  Learning Rate:  6.341375184229542e-08  Varinance:  5.4170679750342816e-08 \n",
      "\n",
      "Epoch:  14273  Learning Rate:  6.335036978676276e-08  Varinance:  5.4095005296320104e-08 \n",
      "\n",
      "Epoch:  14274  Learning Rate:  6.328705108160518e-08  Varinance:  5.401943655673585e-08 \n",
      "\n",
      "Epoch:  14275  Learning Rate:  6.322379566350393e-08  Varinance:  5.394397338391089e-08 \n",
      "\n",
      "Epoch:  14276  Learning Rate:  6.316060346920352e-08  Varinance:  5.386861563037249e-08 \n",
      "\n",
      "Epoch:  14277  Learning Rate:  6.309747443551195e-08  Varinance:  5.3793363148853425e-08 \n",
      "\n",
      "Epoch:  14278  Learning Rate:  6.303440849930008e-08  Varinance:  5.3718215792292706e-08 \n",
      "\n",
      "Epoch:  14279  Learning Rate:  6.297140559750185e-08  Varinance:  5.364317341383463e-08 \n",
      "\n",
      "Epoch:  14280  Learning Rate:  6.290846566711456e-08  Varinance:  5.356823586682863e-08 \n",
      "\n",
      "Epoch:  14281  Learning Rate:  6.28455886451982e-08  Varinance:  5.3493403004829015e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14282  Learning Rate:  6.278277446887559e-08  Varinance:  5.341867468159486e-08 \n",
      "\n",
      "Epoch:  14283  Learning Rate:  6.272002307533281e-08  Varinance:  5.334405075108895e-08 \n",
      "\n",
      "Epoch:  14284  Learning Rate:  6.265733440181832e-08  Varinance:  5.326953106747868e-08 \n",
      "\n",
      "Epoch:  14285  Learning Rate:  6.259470838564335e-08  Varinance:  5.3195115485134945e-08 \n",
      "\n",
      "Epoch:  14286  Learning Rate:  6.253214496418209e-08  Varinance:  5.312080385863209e-08 \n",
      "\n",
      "Epoch:  14287  Learning Rate:  6.246964407487101e-08  Varinance:  5.304659604274761e-08 \n",
      "\n",
      "Epoch:  14288  Learning Rate:  6.24072056552091e-08  Varinance:  5.29724918924619e-08 \n",
      "\n",
      "Epoch:  14289  Learning Rate:  6.234482964275814e-08  Varinance:  5.289849126295808e-08 \n",
      "\n",
      "Epoch:  14290  Learning Rate:  6.228251597514203e-08  Varinance:  5.282459400962105e-08 \n",
      "\n",
      "Epoch:  14291  Learning Rate:  6.22202645900471e-08  Varinance:  5.275079998803828e-08 \n",
      "\n",
      "Epoch:  14292  Learning Rate:  6.215807542522182e-08  Varinance:  5.26771090539988e-08 \n",
      "\n",
      "Epoch:  14293  Learning Rate:  6.209594841847725e-08  Varinance:  5.2603521063493075e-08 \n",
      "\n",
      "Epoch:  14294  Learning Rate:  6.203388350768627e-08  Varinance:  5.253003587271277e-08 \n",
      "\n",
      "Epoch:  14295  Learning Rate:  6.197188063078387e-08  Varinance:  5.245665333805044e-08 \n",
      "\n",
      "Epoch:  14296  Learning Rate:  6.190993972576736e-08  Varinance:  5.238337331609943e-08 \n",
      "\n",
      "Epoch:  14297  Learning Rate:  6.184806073069576e-08  Varinance:  5.231019566365288e-08 \n",
      "\n",
      "Epoch:  14298  Learning Rate:  6.178624358368992e-08  Varinance:  5.2237120237704515e-08 \n",
      "\n",
      "Epoch:  14299  Learning Rate:  6.172448822293292e-08  Varinance:  5.2164146895447666e-08 \n",
      "\n",
      "Epoch:  14300  Learning Rate:  6.16627945866693e-08  Varinance:  5.2091275494275175e-08 \n",
      "\n",
      "Epoch:  14301  Learning Rate:  6.160116261320527e-08  Varinance:  5.2018505891779066e-08 \n",
      "\n",
      "Epoch:  14302  Learning Rate:  6.153959224090911e-08  Varinance:  5.194583794575032e-08 \n",
      "\n",
      "Epoch:  14303  Learning Rate:  6.147808340821032e-08  Varinance:  5.187327151417878e-08 \n",
      "\n",
      "Epoch:  14304  Learning Rate:  6.141663605359996e-08  Varinance:  5.180080645525207e-08 \n",
      "\n",
      "Epoch:  14305  Learning Rate:  6.135525011563088e-08  Varinance:  5.172844262735654e-08 \n",
      "\n",
      "Epoch:  14306  Learning Rate:  6.129392553291701e-08  Varinance:  5.165617988907613e-08 \n",
      "\n",
      "Epoch:  14307  Learning Rate:  6.123266224413379e-08  Varinance:  5.1584018099192355e-08 \n",
      "\n",
      "Epoch:  14308  Learning Rate:  6.117146018801781e-08  Varinance:  5.151195711668401e-08 \n",
      "\n",
      "Epoch:  14309  Learning Rate:  6.111031930336723e-08  Varinance:  5.143999680072688e-08 \n",
      "\n",
      "Epoch:  14310  Learning Rate:  6.104923952904104e-08  Varinance:  5.136813701069368e-08 \n",
      "\n",
      "Epoch:  14311  Learning Rate:  6.098822080395937e-08  Varinance:  5.1296377606153e-08 \n",
      "\n",
      "Epoch:  14312  Learning Rate:  6.092726306710367e-08  Varinance:  5.1224718446870164e-08 \n",
      "\n",
      "Epoch:  14313  Learning Rate:  6.086636625751614e-08  Varinance:  5.115315939280624e-08 \n",
      "\n",
      "Epoch:  14314  Learning Rate:  6.080553031429982e-08  Varinance:  5.108170030411789e-08 \n",
      "\n",
      "Epoch:  14315  Learning Rate:  6.074475517661899e-08  Varinance:  5.101034104115715e-08 \n",
      "\n",
      "Epoch:  14316  Learning Rate:  6.068404078369839e-08  Varinance:  5.093908146447115e-08 \n",
      "\n",
      "Epoch:  14317  Learning Rate:  6.062338707482353e-08  Varinance:  5.086792143480199e-08 \n",
      "\n",
      "Epoch:  14318  Learning Rate:  6.05627939893409e-08  Varinance:  5.079686081308577e-08 \n",
      "\n",
      "Epoch:  14319  Learning Rate:  6.050226146665732e-08  Varinance:  5.072589946045342e-08 \n",
      "\n",
      "Epoch:  14320  Learning Rate:  6.044178944624013e-08  Varinance:  5.0655037238229645e-08 \n",
      "\n",
      "Epoch:  14321  Learning Rate:  6.038137786761752e-08  Varinance:  5.058427400793292e-08 \n",
      "\n",
      "Epoch:  14322  Learning Rate:  6.032102667037782e-08  Varinance:  5.051360963127514e-08 \n",
      "\n",
      "Epoch:  14323  Learning Rate:  6.026073579416982e-08  Varinance:  5.04430439701614e-08 \n",
      "\n",
      "Epoch:  14324  Learning Rate:  6.020050517870254e-08  Varinance:  5.037257688668988e-08 \n",
      "\n",
      "Epoch:  14325  Learning Rate:  6.014033476374555e-08  Varinance:  5.0302208243150875e-08 \n",
      "\n",
      "Epoch:  14326  Learning Rate:  6.008022448912833e-08  Varinance:  5.023193790202757e-08 \n",
      "\n",
      "Epoch:  14327  Learning Rate:  6.00201742947405e-08  Varinance:  5.0161765725995106e-08 \n",
      "\n",
      "Epoch:  14328  Learning Rate:  5.996018412053208e-08  Varinance:  5.009169157792044e-08 \n",
      "\n",
      "Epoch:  14329  Learning Rate:  5.990025390651278e-08  Varinance:  5.0021715320862123e-08 \n",
      "\n",
      "Epoch:  14330  Learning Rate:  5.984038359275226e-08  Varinance:  4.995183681806999e-08 \n",
      "\n",
      "Epoch:  14331  Learning Rate:  5.978057311938044e-08  Varinance:  4.988205593298509e-08 \n",
      "\n",
      "Epoch:  14332  Learning Rate:  5.972082242658672e-08  Varinance:  4.9812372529238695e-08 \n",
      "\n",
      "Epoch:  14333  Learning Rate:  5.966113145462028e-08  Varinance:  4.9742786470653145e-08 \n",
      "\n",
      "Epoch:  14334  Learning Rate:  5.960150014379039e-08  Varinance:  4.967329762124081e-08 \n",
      "\n",
      "Epoch:  14335  Learning Rate:  5.954192843446561e-08  Varinance:  4.960390584520403e-08 \n",
      "\n",
      "Epoch:  14336  Learning Rate:  5.948241626707412e-08  Varinance:  4.9534611006934875e-08 \n",
      "\n",
      "Epoch:  14337  Learning Rate:  5.9422963582103945e-08  Varinance:  4.946541297101482e-08 \n",
      "\n",
      "Epoch:  14338  Learning Rate:  5.936357032010231e-08  Varinance:  4.939631160221472e-08 \n",
      "\n",
      "Epoch:  14339  Learning Rate:  5.9304236421675946e-08  Varinance:  4.9327306765493804e-08 \n",
      "\n",
      "Epoch:  14340  Learning Rate:  5.924496182749084e-08  Varinance:  4.9258398326000454e-08 \n",
      "\n",
      "Epoch:  14341  Learning Rate:  5.918574647827261e-08  Varinance:  4.9189586149071287e-08 \n",
      "\n",
      "Epoch:  14342  Learning Rate:  5.912659031480578e-08  Varinance:  4.912087010023103e-08 \n",
      "\n",
      "Epoch:  14343  Learning Rate:  5.9067493277934086e-08  Varinance:  4.9052250045192253e-08 \n",
      "\n",
      "Epoch:  14344  Learning Rate:  5.9008455308560706e-08  Varinance:  4.8983725849855154e-08 \n",
      "\n",
      "Epoch:  14345  Learning Rate:  5.8949476347647554e-08  Varinance:  4.89152973803074e-08 \n",
      "\n",
      "Epoch:  14346  Learning Rate:  5.8890556336215546e-08  Varinance:  4.8846964502823244e-08 \n",
      "\n",
      "Epoch:  14347  Learning Rate:  5.88316952153449e-08  Varinance:  4.877872708386423e-08 \n",
      "\n",
      "Epoch:  14348  Learning Rate:  5.877289292617436e-08  Varinance:  4.871058499007832e-08 \n",
      "\n",
      "Epoch:  14349  Learning Rate:  5.8714149409901534e-08  Varinance:  4.8642538088299716e-08 \n",
      "\n",
      "Epoch:  14350  Learning Rate:  5.8655464607783125e-08  Varinance:  4.857458624554869e-08 \n",
      "\n",
      "Epoch:  14351  Learning Rate:  5.859683846113421e-08  Varinance:  4.850672932903126e-08 \n",
      "\n",
      "Epoch:  14352  Learning Rate:  5.8538270911328536e-08  Varinance:  4.8438967206139127e-08 \n",
      "\n",
      "Epoch:  14353  Learning Rate:  5.8479761899798764e-08  Varinance:  4.8371299744448724e-08 \n",
      "\n",
      "Epoch:  14354  Learning Rate:  5.842131136803575e-08  Varinance:  4.8303726811722e-08 \n",
      "\n",
      "Epoch:  14355  Learning Rate:  5.836291925758898e-08  Varinance:  4.823624827590547e-08 \n",
      "\n",
      "Epoch:  14356  Learning Rate:  5.830458551006622e-08  Varinance:  4.816886400513009e-08 \n",
      "\n",
      "Epoch:  14357  Learning Rate:  5.824631006713395e-08  Varinance:  4.8101573867711055e-08 \n",
      "\n",
      "Epoch:  14358  Learning Rate:  5.818809287051659e-08  Varinance:  4.803437773214752e-08 \n",
      "\n",
      "Epoch:  14359  Learning Rate:  5.812993386199684e-08  Varinance:  4.796727546712251e-08 \n",
      "\n",
      "Epoch:  14360  Learning Rate:  5.807183298341591e-08  Varinance:  4.790026694150199e-08 \n",
      "\n",
      "Epoch:  14361  Learning Rate:  5.80137901766728e-08  Varinance:  4.78333520243356e-08 \n",
      "\n",
      "Epoch:  14362  Learning Rate:  5.795580538372459e-08  Varinance:  4.776653058485579e-08 \n",
      "\n",
      "Epoch:  14363  Learning Rate:  5.789787854658671e-08  Varinance:  4.769980249247763e-08 \n",
      "\n",
      "Epoch:  14364  Learning Rate:  5.784000960733219e-08  Varinance:  4.763316761679864e-08 \n",
      "\n",
      "Epoch:  14365  Learning Rate:  5.7782198508091997e-08  Varinance:  4.756662582759854e-08 \n",
      "\n",
      "Epoch:  14366  Learning Rate:  5.772444519105524e-08  Varinance:  4.750017699483906e-08 \n",
      "\n",
      "Epoch:  14367  Learning Rate:  5.766674959846847e-08  Varinance:  4.743382098866315e-08 \n",
      "\n",
      "Epoch:  14368  Learning Rate:  5.7609111672636006e-08  Varinance:  4.736755767939563e-08 \n",
      "\n",
      "Epoch:  14369  Learning Rate:  5.7551531355920125e-08  Varinance:  4.730138693754232e-08 \n",
      "\n",
      "Epoch:  14370  Learning Rate:  5.749400859074039e-08  Varinance:  4.723530863378994e-08 \n",
      "\n",
      "Epoch:  14371  Learning Rate:  5.743654331957404e-08  Varinance:  4.7169322639005845e-08 \n",
      "\n",
      "Epoch:  14372  Learning Rate:  5.737913548495568e-08  Varinance:  4.710342882423781e-08 \n",
      "\n",
      "Epoch:  14373  Learning Rate:  5.73217850294777e-08  Varinance:  4.703762706071386e-08 \n",
      "\n",
      "Epoch:  14374  Learning Rate:  5.726449189578953e-08  Varinance:  4.697191721984147e-08 \n",
      "\n",
      "Epoch:  14375  Learning Rate:  5.7207256026597926e-08  Varinance:  4.690629917320824e-08 \n",
      "\n",
      "Epoch:  14376  Learning Rate:  5.715007736466721e-08  Varinance:  4.684077279258096e-08 \n",
      "\n",
      "Epoch:  14377  Learning Rate:  5.709295585281862e-08  Varinance:  4.67753379499056e-08 \n",
      "\n",
      "Epoch:  14378  Learning Rate:  5.703589143393054e-08  Varinance:  4.6709994517307e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14379  Learning Rate:  5.697888405093876e-08  Varinance:  4.664474236708861e-08 \n",
      "\n",
      "Epoch:  14380  Learning Rate:  5.692193364683577e-08  Varinance:  4.6579581371732466e-08 \n",
      "\n",
      "Epoch:  14381  Learning Rate:  5.686504016467108e-08  Varinance:  4.651451140389824e-08 \n",
      "\n",
      "Epoch:  14382  Learning Rate:  5.680820354755138e-08  Varinance:  4.644953233642399e-08 \n",
      "\n",
      "Epoch:  14383  Learning Rate:  5.675142373863997e-08  Varinance:  4.638464404232524e-08 \n",
      "\n",
      "Epoch:  14384  Learning Rate:  5.669470068115702e-08  Varinance:  4.631984639479491e-08 \n",
      "\n",
      "Epoch:  14385  Learning Rate:  5.6638034318379374e-08  Varinance:  4.6255139267203056e-08 \n",
      "\n",
      "Epoch:  14386  Learning Rate:  5.6581424593640876e-08  Varinance:  4.619052253309666e-08 \n",
      "\n",
      "Epoch:  14387  Learning Rate:  5.6524871450331684e-08  Varinance:  4.6125996066199494e-08 \n",
      "\n",
      "Epoch:  14388  Learning Rate:  5.646837483189855e-08  Varinance:  4.6061559740411246e-08 \n",
      "\n",
      "Epoch:  14389  Learning Rate:  5.641193468184506e-08  Varinance:  4.599721342980826e-08 \n",
      "\n",
      "Epoch:  14390  Learning Rate:  5.6355550943730944e-08  Varinance:  4.593295700864261e-08 \n",
      "\n",
      "Epoch:  14391  Learning Rate:  5.6299223561172366e-08  Varinance:  4.586879035134206e-08 \n",
      "\n",
      "Epoch:  14392  Learning Rate:  5.6242952477842157e-08  Varinance:  4.580471333250977e-08 \n",
      "\n",
      "Epoch:  14393  Learning Rate:  5.61867376374691e-08  Varinance:  4.5740725826924095e-08 \n",
      "\n",
      "Epoch:  14394  Learning Rate:  5.6130578983838264e-08  Varinance:  4.5676827709538463e-08 \n",
      "\n",
      "Epoch:  14395  Learning Rate:  5.6074476460791193e-08  Varinance:  4.5613018855480525e-08 \n",
      "\n",
      "Epoch:  14396  Learning Rate:  5.6018430012225244e-08  Varinance:  4.554929914005283e-08 \n",
      "\n",
      "Epoch:  14397  Learning Rate:  5.596243958209389e-08  Varinance:  4.548566843873198e-08 \n",
      "\n",
      "Epoch:  14398  Learning Rate:  5.590650511440687e-08  Varinance:  4.542212662716855e-08 \n",
      "\n",
      "Epoch:  14399  Learning Rate:  5.585062655322964e-08  Varinance:  4.535867358118678e-08 \n",
      "\n",
      "Epoch:  14400  Learning Rate:  5.5794803842683596e-08  Varinance:  4.5295309176784424e-08 \n",
      "\n",
      "Epoch:  14401  Learning Rate:  5.573903692694596e-08  Varinance:  4.5232033290132605e-08 \n",
      "\n",
      "Epoch:  14402  Learning Rate:  5.5683325750249994e-08  Varinance:  4.516884579757494e-08 \n",
      "\n",
      "Epoch:  14403  Learning Rate:  5.5627670256884414e-08  Varinance:  4.5105746575628284e-08 \n",
      "\n",
      "Epoch:  14404  Learning Rate:  5.557207039119363e-08  Varinance:  4.504273550098183e-08 \n",
      "\n",
      "Epoch:  14405  Learning Rate:  5.5516526097577965e-08  Varinance:  4.497981245049703e-08 \n",
      "\n",
      "Epoch:  14406  Learning Rate:  5.546103732049302e-08  Varinance:  4.491697730120736e-08 \n",
      "\n",
      "Epoch:  14407  Learning Rate:  5.5405604004449933e-08  Varinance:  4.4854229930318067e-08 \n",
      "\n",
      "Epoch:  14408  Learning Rate:  5.535022609401556e-08  Varinance:  4.47915702152061e-08 \n",
      "\n",
      "Epoch:  14409  Learning Rate:  5.529490353381189e-08  Varinance:  4.4728998033419244e-08 \n",
      "\n",
      "Epoch:  14410  Learning Rate:  5.523963626851626e-08  Varinance:  4.46665132626768e-08 \n",
      "\n",
      "Epoch:  14411  Learning Rate:  5.51844242428616e-08  Varinance:  4.460411578086875e-08 \n",
      "\n",
      "Epoch:  14412  Learning Rate:  5.5129267401635785e-08  Varinance:  4.454180546605565e-08 \n",
      "\n",
      "Epoch:  14413  Learning Rate:  5.507416568968187e-08  Varinance:  4.44795821964684e-08 \n",
      "\n",
      "Epoch:  14414  Learning Rate:  5.501911905189833e-08  Varinance:  4.441744585050817e-08 \n",
      "\n",
      "Epoch:  14415  Learning Rate:  5.4964127433238426e-08  Varinance:  4.435539630674553e-08 \n",
      "\n",
      "Epoch:  14416  Learning Rate:  5.4909190778710535e-08  Varinance:  4.4293433443921134e-08 \n",
      "\n",
      "Epoch:  14417  Learning Rate:  5.48543090333779e-08  Varinance:  4.423155714094491e-08 \n",
      "\n",
      "Epoch:  14418  Learning Rate:  5.479948214235897e-08  Varinance:  4.4169767276895904e-08 \n",
      "\n",
      "Epoch:  14419  Learning Rate:  5.474471005082675e-08  Varinance:  4.4108063731022114e-08 \n",
      "\n",
      "Epoch:  14420  Learning Rate:  5.4689992704009045e-08  Varinance:  4.40464463827402e-08 \n",
      "\n",
      "Epoch:  14421  Learning Rate:  5.46353300471887e-08  Varinance:  4.398491511163546e-08 \n",
      "\n",
      "Epoch:  14422  Learning Rate:  5.458072202570295e-08  Varinance:  4.392346979746091e-08 \n",
      "\n",
      "Epoch:  14423  Learning Rate:  5.452616858494368e-08  Varinance:  4.386211032013803e-08 \n",
      "\n",
      "Epoch:  14424  Learning Rate:  5.4471669670357634e-08  Varinance:  4.380083655975587e-08 \n",
      "\n",
      "Epoch:  14425  Learning Rate:  5.44172252274458e-08  Varinance:  4.3739648396571026e-08 \n",
      "\n",
      "Epoch:  14426  Learning Rate:  5.436283520176363e-08  Varinance:  4.367854571100734e-08 \n",
      "\n",
      "Epoch:  14427  Learning Rate:  5.4308499538921294e-08  Varinance:  4.361752838365571e-08 \n",
      "\n",
      "Epoch:  14428  Learning Rate:  5.425421818458302e-08  Varinance:  4.3556596295274e-08 \n",
      "\n",
      "Epoch:  14429  Learning Rate:  5.419999108446735e-08  Varinance:  4.3495749326786175e-08 \n",
      "\n",
      "Epoch:  14430  Learning Rate:  5.414581818434738e-08  Varinance:  4.3434987359283024e-08 \n",
      "\n",
      "Epoch:  14431  Learning Rate:  5.409169943005011e-08  Varinance:  4.3374310274021294e-08 \n",
      "\n",
      "Epoch:  14432  Learning Rate:  5.4037634767456776e-08  Varinance:  4.331371795242359e-08 \n",
      "\n",
      "Epoch:  14433  Learning Rate:  5.398362414250262e-08  Varinance:  4.32532102760782e-08 \n",
      "\n",
      "Epoch:  14434  Learning Rate:  5.3929667501177196e-08  Varinance:  4.319278712673879e-08 \n",
      "\n",
      "Epoch:  14435  Learning Rate:  5.387576478952377e-08  Varinance:  4.313244838632439e-08 \n",
      "\n",
      "Epoch:  14436  Learning Rate:  5.382191595363952e-08  Varinance:  4.307219393691853e-08 \n",
      "\n",
      "Epoch:  14437  Learning Rate:  5.376812093967581e-08  Varinance:  4.30120236607699e-08 \n",
      "\n",
      "Epoch:  14438  Learning Rate:  5.3714379693837515e-08  Varinance:  4.295193744029155e-08 \n",
      "\n",
      "Epoch:  14439  Learning Rate:  5.36606921623833e-08  Varinance:  4.289193515806079e-08 \n",
      "\n",
      "Epoch:  14440  Learning Rate:  5.360705829162582e-08  Varinance:  4.283201669681896e-08 \n",
      "\n",
      "Epoch:  14441  Learning Rate:  5.355347802793109e-08  Varinance:  4.2772181939471223e-08 \n",
      "\n",
      "Epoch:  14442  Learning Rate:  5.3499951317718753e-08  Varinance:  4.2712430769086445e-08 \n",
      "\n",
      "Epoch:  14443  Learning Rate:  5.34464781074623e-08  Varinance:  4.265276306889641e-08 \n",
      "\n",
      "Epoch:  14444  Learning Rate:  5.33930583436884e-08  Varinance:  4.259317872229646e-08 \n",
      "\n",
      "Epoch:  14445  Learning Rate:  5.333969197297721e-08  Varinance:  4.253367761284469e-08 \n",
      "\n",
      "Epoch:  14446  Learning Rate:  5.328637894196251e-08  Varinance:  4.2474259624261846e-08 \n",
      "\n",
      "Epoch:  14447  Learning Rate:  5.323311919733121e-08  Varinance:  4.241492464043113e-08 \n",
      "\n",
      "Epoch:  14448  Learning Rate:  5.3179912685823535e-08  Varinance:  4.235567254539795e-08 \n",
      "\n",
      "Epoch:  14449  Learning Rate:  5.312675935423288e-08  Varinance:  4.2296503223369836e-08 \n",
      "\n",
      "Epoch:  14450  Learning Rate:  5.307365914940611e-08  Varinance:  4.223741655871563e-08 \n",
      "\n",
      "Epoch:  14451  Learning Rate:  5.302061201824291e-08  Varinance:  4.2178412435966165e-08 \n",
      "\n",
      "Epoch:  14452  Learning Rate:  5.296761790769606e-08  Varinance:  4.2119490739813425e-08 \n",
      "\n",
      "Epoch:  14453  Learning Rate:  5.291467676477161e-08  Varinance:  4.2060651355110476e-08 \n",
      "\n",
      "Epoch:  14454  Learning Rate:  5.2861788536528336e-08  Varinance:  4.200189416687125e-08 \n",
      "\n",
      "Epoch:  14455  Learning Rate:  5.280895317007792e-08  Varinance:  4.194321906027028e-08 \n",
      "\n",
      "Epoch:  14456  Learning Rate:  5.275617061258517e-08  Varinance:  4.188462592064269e-08 \n",
      "\n",
      "Epoch:  14457  Learning Rate:  5.2703440811267425e-08  Varinance:  4.182611463348332e-08 \n",
      "\n",
      "Epoch:  14458  Learning Rate:  5.2650763713394786e-08  Varinance:  4.176768508444742e-08 \n",
      "\n",
      "Epoch:  14459  Learning Rate:  5.2598139266290344e-08  Varinance:  4.170933715934983e-08 \n",
      "\n",
      "Epoch:  14460  Learning Rate:  5.254556741732955e-08  Varinance:  4.1651070744164895e-08 \n",
      "\n",
      "Epoch:  14461  Learning Rate:  5.249304811394046e-08  Varinance:  4.159288572502626e-08 \n",
      "\n",
      "Epoch:  14462  Learning Rate:  5.244058130360395e-08  Varinance:  4.153478198822664e-08 \n",
      "\n",
      "Epoch:  14463  Learning Rate:  5.238816693385311e-08  Varinance:  4.147675942021772e-08 \n",
      "\n",
      "Epoch:  14464  Learning Rate:  5.233580495227357e-08  Varinance:  4.1418817907609375e-08 \n",
      "\n",
      "Epoch:  14465  Learning Rate:  5.228349530650326e-08  Varinance:  4.1360957337170344e-08 \n",
      "\n",
      "Epoch:  14466  Learning Rate:  5.22312379442327e-08  Varinance:  4.1303177595827376e-08 \n",
      "\n",
      "Epoch:  14467  Learning Rate:  5.217903281320444e-08  Varinance:  4.124547857066519e-08 \n",
      "\n",
      "Epoch:  14468  Learning Rate:  5.2126879861213244e-08  Varinance:  4.118786014892624e-08 \n",
      "\n",
      "Epoch:  14469  Learning Rate:  5.207477903610635e-08  Varinance:  4.113032221801049e-08 \n",
      "\n",
      "Epoch:  14470  Learning Rate:  5.2022730285782824e-08  Varinance:  4.107286466547537e-08 \n",
      "\n",
      "Epoch:  14471  Learning Rate:  5.197073355819383e-08  Varinance:  4.101548737903492e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14472  Learning Rate:  5.191878880134283e-08  Varinance:  4.095819024656051e-08 \n",
      "\n",
      "Epoch:  14473  Learning Rate:  5.1866895963284944e-08  Varinance:  4.090097315607998e-08 \n",
      "\n",
      "Epoch:  14474  Learning Rate:  5.181505499212726e-08  Varinance:  4.08438359957776e-08 \n",
      "\n",
      "Epoch:  14475  Learning Rate:  5.176326583602898e-08  Varinance:  4.0786778653993836e-08 \n",
      "\n",
      "Epoch:  14476  Learning Rate:  5.171152844320084e-08  Varinance:  4.072980101922515e-08 \n",
      "\n",
      "Epoch:  14477  Learning Rate:  5.165984276190536e-08  Varinance:  4.06729029801239e-08 \n",
      "\n",
      "Epoch:  14478  Learning Rate:  5.1608208740457046e-08  Varinance:  4.061608442549757e-08 \n",
      "\n",
      "Epoch:  14479  Learning Rate:  5.155662632722177e-08  Varinance:  4.0559345244309415e-08 \n",
      "\n",
      "Epoch:  14480  Learning Rate:  5.150509547061711e-08  Varinance:  4.050268532567764e-08 \n",
      "\n",
      "Epoch:  14481  Learning Rate:  5.1453616119112124e-08  Varinance:  4.0446104558875366e-08 \n",
      "\n",
      "Epoch:  14482  Learning Rate:  5.140218822122765e-08  Varinance:  4.038960283333039e-08 \n",
      "\n",
      "Epoch:  14483  Learning Rate:  5.135081172553567e-08  Varinance:  4.0333180038624966e-08 \n",
      "\n",
      "Epoch:  14484  Learning Rate:  5.1299486580659605e-08  Varinance:  4.0276836064495753e-08 \n",
      "\n",
      "Epoch:  14485  Learning Rate:  5.124821273527448e-08  Varinance:  4.0220570800833e-08 \n",
      "\n",
      "Epoch:  14486  Learning Rate:  5.1196990138106375e-08  Varinance:  4.016438413768122e-08 \n",
      "\n",
      "Epoch:  14487  Learning Rate:  5.114581873793258e-08  Varinance:  4.010827596523839e-08 \n",
      "\n",
      "Epoch:  14488  Learning Rate:  5.109469848358187e-08  Varinance:  4.005224617385585e-08 \n",
      "\n",
      "Epoch:  14489  Learning Rate:  5.10436293239339e-08  Varinance:  3.9996294654038135e-08 \n",
      "\n",
      "Epoch:  14490  Learning Rate:  5.0992611207919414e-08  Varinance:  3.994042129644274e-08 \n",
      "\n",
      "Epoch:  14491  Learning Rate:  5.094164408452048e-08  Varinance:  3.9884625991880054e-08 \n",
      "\n",
      "Epoch:  14492  Learning Rate:  5.089072790276989e-08  Varinance:  3.9828908631312564e-08 \n",
      "\n",
      "Epoch:  14493  Learning Rate:  5.083986261175134e-08  Varinance:  3.9773269105855505e-08 \n",
      "\n",
      "Epoch:  14494  Learning Rate:  5.078904816059973e-08  Varinance:  3.971770730677608e-08 \n",
      "\n",
      "Epoch:  14495  Learning Rate:  5.0738284498500514e-08  Varinance:  3.966222312549339e-08 \n",
      "\n",
      "Epoch:  14496  Learning Rate:  5.068757157469002e-08  Varinance:  3.9606816453578215e-08 \n",
      "\n",
      "Epoch:  14497  Learning Rate:  5.063690933845524e-08  Varinance:  3.955148718275282e-08 \n",
      "\n",
      "Epoch:  14498  Learning Rate:  5.0586297739134104e-08  Varinance:  3.949623520489084e-08 \n",
      "\n",
      "Epoch:  14499  Learning Rate:  5.053573672611493e-08  Varinance:  3.944106041201658e-08 \n",
      "\n",
      "Epoch:  14500  Learning Rate:  5.048522624883659e-08  Varinance:  3.938596269630557e-08 \n",
      "\n",
      "Epoch:  14501  Learning Rate:  5.0434766256788804e-08  Varinance:  3.933094195008384e-08 \n",
      "\n",
      "Epoch:  14502  Learning Rate:  5.038435669951149e-08  Varinance:  3.9275998065827834e-08 \n",
      "\n",
      "Epoch:  14503  Learning Rate:  5.033399752659497e-08  Varinance:  3.9221130936164205e-08 \n",
      "\n",
      "Epoch:  14504  Learning Rate:  5.0283688687680264e-08  Varinance:  3.91663404538696e-08 \n",
      "\n",
      "Epoch:  14505  Learning Rate:  5.0233430132458425e-08  Varinance:  3.911162651187059e-08 \n",
      "\n",
      "Epoch:  14506  Learning Rate:  5.0183221810670836e-08  Varinance:  3.9056989003242915e-08 \n",
      "\n",
      "Epoch:  14507  Learning Rate:  5.013306367210932e-08  Varinance:  3.900242782121209e-08 \n",
      "\n",
      "Epoch:  14508  Learning Rate:  5.008295566661565e-08  Varinance:  3.894794285915266e-08 \n",
      "\n",
      "Epoch:  14509  Learning Rate:  5.0032897744081816e-08  Varinance:  3.889353401058812e-08 \n",
      "\n",
      "Epoch:  14510  Learning Rate:  4.998288985444982e-08  Varinance:  3.883920116919071e-08 \n",
      "\n",
      "Epoch:  14511  Learning Rate:  4.993293194771193e-08  Varinance:  3.878494422878119e-08 \n",
      "\n",
      "Epoch:  14512  Learning Rate:  4.988302397391014e-08  Varinance:  3.8730763083328823e-08 \n",
      "\n",
      "Epoch:  14513  Learning Rate:  4.983316588313639e-08  Varinance:  3.8676657626950537e-08 \n",
      "\n",
      "Epoch:  14514  Learning Rate:  4.978335762553278e-08  Varinance:  3.862262775391161e-08 \n",
      "\n",
      "Epoch:  14515  Learning Rate:  4.9733599151290925e-08  Varinance:  3.8568673358624894e-08 \n",
      "\n",
      "Epoch:  14516  Learning Rate:  4.9683890410652294e-08  Varinance:  3.8514794335650733e-08 \n",
      "\n",
      "Epoch:  14517  Learning Rate:  4.963423135390829e-08  Varinance:  3.846099057969677e-08 \n",
      "\n",
      "Epoch:  14518  Learning Rate:  4.958462193139978e-08  Varinance:  3.840726198561773e-08 \n",
      "\n",
      "Epoch:  14519  Learning Rate:  4.9535062093517255e-08  Varinance:  3.835360844841538e-08 \n",
      "\n",
      "Epoch:  14520  Learning Rate:  4.948555179070103e-08  Varinance:  3.8300029863237725e-08 \n",
      "\n",
      "Epoch:  14521  Learning Rate:  4.943609097344072e-08  Varinance:  3.824652612537968e-08 \n",
      "\n",
      "Epoch:  14522  Learning Rate:  4.938667959227542e-08  Varinance:  3.8193097130282274e-08 \n",
      "\n",
      "Epoch:  14523  Learning Rate:  4.9337317597793905e-08  Varinance:  3.813974277353263e-08 \n",
      "\n",
      "Epoch:  14524  Learning Rate:  4.928800494063411e-08  Varinance:  3.8086462950863685e-08 \n",
      "\n",
      "Epoch:  14525  Learning Rate:  4.923874157148336e-08  Varinance:  3.803325755815408e-08 \n",
      "\n",
      "Epoch:  14526  Learning Rate:  4.9189527441078203e-08  Varinance:  3.7980126491428016e-08 \n",
      "\n",
      "Epoch:  14527  Learning Rate:  4.9140362500204664e-08  Varinance:  3.7927069646854556e-08 \n",
      "\n",
      "Epoch:  14528  Learning Rate:  4.9091246699697725e-08  Varinance:  3.78740869207482e-08 \n",
      "\n",
      "Epoch:  14529  Learning Rate:  4.9042179990441486e-08  Varinance:  3.7821178209568176e-08 \n",
      "\n",
      "Epoch:  14530  Learning Rate:  4.899316232336941e-08  Varinance:  3.776834340991833e-08 \n",
      "\n",
      "Epoch:  14531  Learning Rate:  4.894419364946374e-08  Varinance:  3.7715582418546975e-08 \n",
      "\n",
      "Epoch:  14532  Learning Rate:  4.889527391975572e-08  Varinance:  3.766289513234665e-08 \n",
      "\n",
      "Epoch:  14533  Learning Rate:  4.884640308532578e-08  Varinance:  3.7610281448354065e-08 \n",
      "\n",
      "Epoch:  14534  Learning Rate:  4.879758109730299e-08  Varinance:  3.755774126374936e-08 \n",
      "\n",
      "Epoch:  14535  Learning Rate:  4.8748807906865273e-08  Varinance:  3.750527447585673e-08 \n",
      "\n",
      "Epoch:  14536  Learning Rate:  4.8700083465239625e-08  Varinance:  3.745288098214365e-08 \n",
      "\n",
      "Epoch:  14537  Learning Rate:  4.865140772370149e-08  Varinance:  3.7400560680220843e-08 \n",
      "\n",
      "Epoch:  14538  Learning Rate:  4.860278063357505e-08  Varinance:  3.7348313467842054e-08 \n",
      "\n",
      "Epoch:  14539  Learning Rate:  4.855420214623338e-08  Varinance:  3.729613924290387e-08 \n",
      "\n",
      "Epoch:  14540  Learning Rate:  4.85056722130979e-08  Varinance:  3.724403790344566e-08 \n",
      "\n",
      "Epoch:  14541  Learning Rate:  4.845719078563868e-08  Varinance:  3.719200934764879e-08 \n",
      "\n",
      "Epoch:  14542  Learning Rate:  4.840875781537419e-08  Varinance:  3.7140053473837305e-08 \n",
      "\n",
      "Epoch:  14543  Learning Rate:  4.836037325387165e-08  Varinance:  3.708817018047713e-08 \n",
      "\n",
      "Epoch:  14544  Learning Rate:  4.831203705274638e-08  Varinance:  3.703635936617603e-08 \n",
      "\n",
      "Epoch:  14545  Learning Rate:  4.826374916366211e-08  Varinance:  3.698462092968341e-08 \n",
      "\n",
      "Epoch:  14546  Learning Rate:  4.821550953833111e-08  Varinance:  3.693295476989028e-08 \n",
      "\n",
      "Epoch:  14547  Learning Rate:  4.8167318128513664e-08  Varinance:  3.688136078582845e-08 \n",
      "\n",
      "Epoch:  14548  Learning Rate:  4.811917488601827e-08  Varinance:  3.682983887667122e-08 \n",
      "\n",
      "Epoch:  14549  Learning Rate:  4.807107976270186e-08  Varinance:  3.677838894173258e-08 \n",
      "\n",
      "Epoch:  14550  Learning Rate:  4.802303271046923e-08  Varinance:  3.6727010880467196e-08 \n",
      "\n",
      "Epoch:  14551  Learning Rate:  4.797503368127322e-08  Varinance:  3.667570459247017e-08 \n",
      "\n",
      "Epoch:  14552  Learning Rate:  4.7927082627114973e-08  Varinance:  3.662446997747687e-08 \n",
      "\n",
      "Epoch:  14553  Learning Rate:  4.787917950004335e-08  Varinance:  3.6573306935362884e-08 \n",
      "\n",
      "Epoch:  14554  Learning Rate:  4.783132425215513e-08  Varinance:  3.652221536614325e-08 \n",
      "\n",
      "Epoch:  14555  Learning Rate:  4.778351683559523e-08  Varinance:  3.64711951699731e-08 \n",
      "\n",
      "Epoch:  14556  Learning Rate:  4.773575720255615e-08  Varinance:  3.642024624714688e-08 \n",
      "\n",
      "Epoch:  14557  Learning Rate:  4.7688045305278246e-08  Varinance:  3.636936849809836e-08 \n",
      "\n",
      "Epoch:  14558  Learning Rate:  4.764038109604954e-08  Varinance:  3.631856182340036e-08 \n",
      "\n",
      "Epoch:  14559  Learning Rate:  4.759276452720599e-08  Varinance:  3.626782612376464e-08 \n",
      "\n",
      "Epoch:  14560  Learning Rate:  4.7545195551130927e-08  Varinance:  3.6217161300041756e-08 \n",
      "\n",
      "Epoch:  14561  Learning Rate:  4.7497674120255294e-08  Varinance:  3.61665672532204e-08 \n",
      "\n",
      "Epoch:  14562  Learning Rate:  4.7450200187057825e-08  Varinance:  3.6116043884427964e-08 \n",
      "\n",
      "Epoch:  14563  Learning Rate:  4.7402773704064494e-08  Varinance:  3.6065591094929833e-08 \n",
      "\n",
      "Epoch:  14564  Learning Rate:  4.735539462384874e-08  Varinance:  3.6015208786129315e-08 \n",
      "\n",
      "Epoch:  14565  Learning Rate:  4.730806289903164e-08  Varinance:  3.5964896859567465e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14566  Learning Rate:  4.7260778482281374e-08  Varinance:  3.5914655216922875e-08 \n",
      "\n",
      "Epoch:  14567  Learning Rate:  4.721354132631345e-08  Varinance:  3.5864483760011616e-08 \n",
      "\n",
      "Epoch:  14568  Learning Rate:  4.7166351383890866e-08  Varinance:  3.581438239078653e-08 \n",
      "\n",
      "Epoch:  14569  Learning Rate:  4.7119208607823604e-08  Varinance:  3.5764351011337825e-08 \n",
      "\n",
      "Epoch:  14570  Learning Rate:  4.707211295096878e-08  Varinance:  3.571438952389234e-08 \n",
      "\n",
      "Epoch:  14571  Learning Rate:  4.702506436623093e-08  Varinance:  3.5664497830813514e-08 \n",
      "\n",
      "Epoch:  14572  Learning Rate:  4.6978062806561354e-08  Varinance:  3.561467583460118e-08 \n",
      "\n",
      "Epoch:  14573  Learning Rate:  4.6931108224958504e-08  Varinance:  3.556492343789136e-08 \n",
      "\n",
      "Epoch:  14574  Learning Rate:  4.68842005744677e-08  Varinance:  3.551524054345623e-08 \n",
      "\n",
      "Epoch:  14575  Learning Rate:  4.683733980818147e-08  Varinance:  3.5465627054203403e-08 \n",
      "\n",
      "Epoch:  14576  Learning Rate:  4.679052587923894e-08  Varinance:  3.5416082873176524e-08 \n",
      "\n",
      "Epoch:  14577  Learning Rate:  4.6743758740826114e-08  Varinance:  3.536660790355452e-08 \n",
      "\n",
      "Epoch:  14578  Learning Rate:  4.6697038346176e-08  Varinance:  3.5317202048651616e-08 \n",
      "\n",
      "Epoch:  14579  Learning Rate:  4.665036464856813e-08  Varinance:  3.526786521191708e-08 \n",
      "\n",
      "Epoch:  14580  Learning Rate:  4.660373760132872e-08  Varinance:  3.521859729693504e-08 \n",
      "\n",
      "Epoch:  14581  Learning Rate:  4.655715715783087e-08  Varinance:  3.516939820742449e-08 \n",
      "\n",
      "Epoch:  14582  Learning Rate:  4.651062327149405e-08  Varinance:  3.512026784723849e-08 \n",
      "\n",
      "Epoch:  14583  Learning Rate:  4.6464135895784304e-08  Varinance:  3.5071206120364834e-08 \n",
      "\n",
      "Epoch:  14584  Learning Rate:  4.641769498421441e-08  Varinance:  3.5022212930925296e-08 \n",
      "\n",
      "Epoch:  14585  Learning Rate:  4.6371300490343365e-08  Varinance:  3.49732881831756e-08 \n",
      "\n",
      "Epoch:  14586  Learning Rate:  4.632495236777659e-08  Varinance:  3.4924431781505205e-08 \n",
      "\n",
      "Epoch:  14587  Learning Rate:  4.627865057016613e-08  Varinance:  3.487564363043714e-08 \n",
      "\n",
      "Epoch:  14588  Learning Rate:  4.6232395051210093e-08  Varinance:  3.482692363462796e-08 \n",
      "\n",
      "Epoch:  14589  Learning Rate:  4.6186185764652955e-08  Varinance:  3.4778271698866985e-08 \n",
      "\n",
      "Epoch:  14590  Learning Rate:  4.614002266428536e-08  Varinance:  3.4729687728076974e-08 \n",
      "\n",
      "Epoch:  14591  Learning Rate:  4.609390570394435e-08  Varinance:  3.468117162731335e-08 \n",
      "\n",
      "Epoch:  14592  Learning Rate:  4.604783483751288e-08  Varinance:  3.463272330176416e-08 \n",
      "\n",
      "Epoch:  14593  Learning Rate:  4.600181001892001e-08  Varinance:  3.458434265674993e-08 \n",
      "\n",
      "Epoch:  14594  Learning Rate:  4.595583120214108e-08  Varinance:  3.453602959772342e-08 \n",
      "\n",
      "Epoch:  14595  Learning Rate:  4.590989834119717e-08  Varinance:  3.448778403026962e-08 \n",
      "\n",
      "Epoch:  14596  Learning Rate:  4.5864011390155346e-08  Varinance:  3.4439605860105014e-08 \n",
      "\n",
      "Epoch:  14597  Learning Rate:  4.581817030312882e-08  Varinance:  3.4391494993078195e-08 \n",
      "\n",
      "Epoch:  14598  Learning Rate:  4.577237503427642e-08  Varinance:  3.434345133516914e-08 \n",
      "\n",
      "Epoch:  14599  Learning Rate:  4.572662553780278e-08  Varinance:  3.429547479248918e-08 \n",
      "\n",
      "Epoch:  14600  Learning Rate:  4.568092176795857e-08  Varinance:  3.424756527128079e-08 \n",
      "\n",
      "Epoch:  14601  Learning Rate:  4.563526367903994e-08  Varinance:  3.4199722677917445e-08 \n",
      "\n",
      "Epoch:  14602  Learning Rate:  4.558965122538871e-08  Varinance:  3.415194691890351e-08 \n",
      "\n",
      "Epoch:  14603  Learning Rate:  4.554408436139258e-08  Varinance:  3.4104237900873615e-08 \n",
      "\n",
      "Epoch:  14604  Learning Rate:  4.549856304148461e-08  Varinance:  3.405659553059316e-08 \n",
      "\n",
      "Epoch:  14605  Learning Rate:  4.5453087220143474e-08  Varinance:  3.4009019714957694e-08 \n",
      "\n",
      "Epoch:  14606  Learning Rate:  4.5407656851893264e-08  Varinance:  3.396151036099281e-08 \n",
      "\n",
      "Epoch:  14607  Learning Rate:  4.536227189130377e-08  Varinance:  3.391406737585399e-08 \n",
      "\n",
      "Epoch:  14608  Learning Rate:  4.531693229298995e-08  Varinance:  3.386669066682642e-08 \n",
      "\n",
      "Epoch:  14609  Learning Rate:  4.5271638011612114e-08  Varinance:  3.381938014132493e-08 \n",
      "\n",
      "Epoch:  14610  Learning Rate:  4.5226389001876144e-08  Varinance:  3.3772135706893305e-08 \n",
      "\n",
      "Epoch:  14611  Learning Rate:  4.5181185218532954e-08  Varinance:  3.3724957271204864e-08 \n",
      "\n",
      "Epoch:  14612  Learning Rate:  4.513602661637865e-08  Varinance:  3.367784474206179e-08 \n",
      "\n",
      "Epoch:  14613  Learning Rate:  4.509091315025482e-08  Varinance:  3.363079802739505e-08 \n",
      "\n",
      "Epoch:  14614  Learning Rate:  4.504584477504789e-08  Varinance:  3.358381703526424e-08 \n",
      "\n",
      "Epoch:  14615  Learning Rate:  4.5000821445689414e-08  Varinance:  3.353690167385738e-08 \n",
      "\n",
      "Epoch:  14616  Learning Rate:  4.4955843117156206e-08  Varinance:  3.349005185149088e-08 \n",
      "\n",
      "Epoch:  14617  Learning Rate:  4.491090974446987e-08  Varinance:  3.344326747660885e-08 \n",
      "\n",
      "Epoch:  14618  Learning Rate:  4.486602128269693e-08  Varinance:  3.339654845778368e-08 \n",
      "\n",
      "Epoch:  14619  Learning Rate:  4.48211776869491e-08  Varinance:  3.3349894703715366e-08 \n",
      "\n",
      "Epoch:  14620  Learning Rate:  4.4776378912382695e-08  Varinance:  3.330330612323142e-08 \n",
      "\n",
      "Epoch:  14621  Learning Rate:  4.473162491419893e-08  Varinance:  3.325678262528674e-08 \n",
      "\n",
      "Epoch:  14622  Learning Rate:  4.4686915647643725e-08  Varinance:  3.32103241189634e-08 \n",
      "\n",
      "Epoch:  14623  Learning Rate:  4.464225106800797e-08  Varinance:  3.31639305134706e-08 \n",
      "\n",
      "Epoch:  14624  Learning Rate:  4.4597631130627004e-08  Varinance:  3.311760171814404e-08 \n",
      "\n",
      "Epoch:  14625  Learning Rate:  4.455305579088081e-08  Varinance:  3.3071337642446396e-08 \n",
      "\n",
      "Epoch:  14626  Learning Rate:  4.450852500419419e-08  Varinance:  3.302513819596673e-08 \n",
      "\n",
      "Epoch:  14627  Learning Rate:  4.4464038726036295e-08  Varinance:  3.297900328842038e-08 \n",
      "\n",
      "Epoch:  14628  Learning Rate:  4.441959691192074e-08  Varinance:  3.293293282964884e-08 \n",
      "\n",
      "Epoch:  14629  Learning Rate:  4.437519951740589e-08  Varinance:  3.2886926729619515e-08 \n",
      "\n",
      "Epoch:  14630  Learning Rate:  4.433084649809425e-08  Varinance:  3.284098489842573e-08 \n",
      "\n",
      "Epoch:  14631  Learning Rate:  4.428653780963272e-08  Varinance:  3.279510724628604e-08 \n",
      "\n",
      "Epoch:  14632  Learning Rate:  4.4242273407712775e-08  Varinance:  3.274929368354477e-08 \n",
      "\n",
      "Epoch:  14633  Learning Rate:  4.419805324806992e-08  Varinance:  3.270354412067139e-08 \n",
      "\n",
      "Epoch:  14634  Learning Rate:  4.4153877286483996e-08  Varinance:  3.2657858468260435e-08 \n",
      "\n",
      "Epoch:  14635  Learning Rate:  4.410974547877896e-08  Varinance:  3.261223663703131e-08 \n",
      "\n",
      "Epoch:  14636  Learning Rate:  4.406565778082316e-08  Varinance:  3.2566678537828175e-08 \n",
      "\n",
      "Epoch:  14637  Learning Rate:  4.4021614148528814e-08  Varinance:  3.2521184081619846e-08 \n",
      "\n",
      "Epoch:  14638  Learning Rate:  4.3977614537852205e-08  Varinance:  3.2475753179499156e-08 \n",
      "\n",
      "Epoch:  14639  Learning Rate:  4.3933658904793876e-08  Varinance:  3.243038574268349e-08 \n",
      "\n",
      "Epoch:  14640  Learning Rate:  4.388974720539811e-08  Varinance:  3.2385081682514146e-08 \n",
      "\n",
      "Epoch:  14641  Learning Rate:  4.3845879395753135e-08  Varinance:  3.233984091045627e-08 \n",
      "\n",
      "Epoch:  14642  Learning Rate:  4.3802055431991275e-08  Varinance:  3.229466333809869e-08 \n",
      "\n",
      "Epoch:  14643  Learning Rate:  4.375827527028851e-08  Varinance:  3.224954887715374e-08 \n",
      "\n",
      "Epoch:  14644  Learning Rate:  4.371453886686458e-08  Varinance:  3.22044974394572e-08 \n",
      "\n",
      "Epoch:  14645  Learning Rate:  4.367084617798324e-08  Varinance:  3.215950893696768e-08 \n",
      "\n",
      "Epoch:  14646  Learning Rate:  4.3627197159951714e-08  Varinance:  3.211458328176712e-08 \n",
      "\n",
      "Epoch:  14647  Learning Rate:  4.3583591769120905e-08  Varinance:  3.206972038606016e-08 \n",
      "\n",
      "Epoch:  14648  Learning Rate:  4.3540029961885576e-08  Varinance:  3.202492016217408e-08 \n",
      "\n",
      "Epoch:  14649  Learning Rate:  4.3496511694683836e-08  Varinance:  3.198018252255865e-08 \n",
      "\n",
      "Epoch:  14650  Learning Rate:  4.3453036923997415e-08  Varinance:  3.1935507379785937e-08 \n",
      "\n",
      "Epoch:  14651  Learning Rate:  4.3409605606351466e-08  Varinance:  3.189089464655025e-08 \n",
      "\n",
      "Epoch:  14652  Learning Rate:  4.336621769831482e-08  Varinance:  3.184634423566753e-08 \n",
      "\n",
      "Epoch:  14653  Learning Rate:  4.332287315649948e-08  Varinance:  3.1801856060075854e-08 \n",
      "\n",
      "Epoch:  14654  Learning Rate:  4.327957193756083e-08  Varinance:  3.175743003283479e-08 \n",
      "\n",
      "Epoch:  14655  Learning Rate:  4.323631399819781e-08  Varinance:  3.1713066067125384e-08 \n",
      "\n",
      "Epoch:  14656  Learning Rate:  4.319309929515238e-08  Varinance:  3.166876407624994e-08 \n",
      "\n",
      "Epoch:  14657  Learning Rate:  4.314992778520977e-08  Varinance:  3.162452397363189e-08 \n",
      "\n",
      "Epoch:  14658  Learning Rate:  4.3106799425198624e-08  Varinance:  3.158034567281572e-08 \n",
      "\n",
      "Epoch:  14659  Learning Rate:  4.3063714171990486e-08  Varinance:  3.153622908746636e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14660  Learning Rate:  4.3020671982500035e-08  Varinance:  3.1492174131369666e-08 \n",
      "\n",
      "Epoch:  14661  Learning Rate:  4.297767281368524e-08  Varinance:  3.144818071843183e-08 \n",
      "\n",
      "Epoch:  14662  Learning Rate:  4.2934716622546825e-08  Varinance:  3.1404248762679326e-08 \n",
      "\n",
      "Epoch:  14663  Learning Rate:  4.289180336612854e-08  Varinance:  3.1360378178258705e-08 \n",
      "\n",
      "Epoch:  14664  Learning Rate:  4.284893300151727e-08  Varinance:  3.131656887943648e-08 \n",
      "\n",
      "Epoch:  14665  Learning Rate:  4.280610548584257e-08  Varinance:  3.127282078059902e-08 \n",
      "\n",
      "Epoch:  14666  Learning Rate:  4.276332077627693e-08  Varinance:  3.122913379625197e-08 \n",
      "\n",
      "Epoch:  14667  Learning Rate:  4.272057883003555e-08  Varinance:  3.118550784102074e-08 \n",
      "\n",
      "Epoch:  14668  Learning Rate:  4.2677879604376634e-08  Varinance:  3.114194282964989e-08 \n",
      "\n",
      "Epoch:  14669  Learning Rate:  4.2635223056600885e-08  Varinance:  3.109843867700307e-08 \n",
      "\n",
      "Epoch:  14670  Learning Rate:  4.2592609144051665e-08  Varinance:  3.105499529806289e-08 \n",
      "\n",
      "Epoch:  14671  Learning Rate:  4.2550037824115216e-08  Varinance:  3.10116126079307e-08 \n",
      "\n",
      "Epoch:  14672  Learning Rate:  4.250750905422013e-08  Varinance:  3.096829052182658e-08 \n",
      "\n",
      "Epoch:  14673  Learning Rate:  4.246502279183758e-08  Varinance:  3.0925028955088685e-08 \n",
      "\n",
      "Epoch:  14674  Learning Rate:  4.2422578994481425e-08  Varinance:  3.088182782317381e-08 \n",
      "\n",
      "Epoch:  14675  Learning Rate:  4.23801776197078e-08  Varinance:  3.083868704165671e-08 \n",
      "\n",
      "Epoch:  14676  Learning Rate:  4.233781862511526e-08  Varinance:  3.079560652623009e-08 \n",
      "\n",
      "Epoch:  14677  Learning Rate:  4.229550196834493e-08  Varinance:  3.075258619270444e-08 \n",
      "\n",
      "Epoch:  14678  Learning Rate:  4.225322760708011e-08  Varinance:  3.070962595700795e-08 \n",
      "\n",
      "Epoch:  14679  Learning Rate:  4.221099549904634e-08  Varinance:  3.066672573518594e-08 \n",
      "\n",
      "Epoch:  14680  Learning Rate:  4.216880560201165e-08  Varinance:  3.062388544340134e-08 \n",
      "\n",
      "Epoch:  14681  Learning Rate:  4.212665787378609e-08  Varinance:  3.0581104997934085e-08 \n",
      "\n",
      "Epoch:  14682  Learning Rate:  4.208455227222191e-08  Varinance:  3.053838431518106e-08 \n",
      "\n",
      "Epoch:  14683  Learning Rate:  4.2042488755213435e-08  Varinance:  3.0495723311655944e-08 \n",
      "\n",
      "Epoch:  14684  Learning Rate:  4.2000467280697294e-08  Varinance:  3.045312190398904e-08 \n",
      "\n",
      "Epoch:  14685  Learning Rate:  4.1958487806651926e-08  Varinance:  3.0410580008927236e-08 \n",
      "\n",
      "Epoch:  14686  Learning Rate:  4.19165502910978e-08  Varinance:  3.036809754333337e-08 \n",
      "\n",
      "Epoch:  14687  Learning Rate:  4.1874654692097524e-08  Varinance:  3.0325674424186786e-08 \n",
      "\n",
      "Epoch:  14688  Learning Rate:  4.183280096775543e-08  Varinance:  3.028331056858265e-08 \n",
      "\n",
      "Epoch:  14689  Learning Rate:  4.179098907621771e-08  Varinance:  3.024100589373198e-08 \n",
      "\n",
      "Epoch:  14690  Learning Rate:  4.174921897567264e-08  Varinance:  3.019876031696143e-08 \n",
      "\n",
      "Epoch:  14691  Learning Rate:  4.170749062435001e-08  Varinance:  3.0156573755713144e-08 \n",
      "\n",
      "Epoch:  14692  Learning Rate:  4.166580398052141e-08  Varinance:  3.011444612754471e-08 \n",
      "\n",
      "Epoch:  14693  Learning Rate:  4.162415900250033e-08  Varinance:  3.0072377350128564e-08 \n",
      "\n",
      "Epoch:  14694  Learning Rate:  4.1582555648641734e-08  Varinance:  3.003036734125246e-08 \n",
      "\n",
      "Epoch:  14695  Learning Rate:  4.154099387734217e-08  Varinance:  2.998841601881891e-08 \n",
      "\n",
      "Epoch:  14696  Learning Rate:  4.149947364704002e-08  Varinance:  2.9946523300845106e-08 \n",
      "\n",
      "Epoch:  14697  Learning Rate:  4.1457994916214976e-08  Varinance:  2.990468910546276e-08 \n",
      "\n",
      "Epoch:  14698  Learning Rate:  4.141655764338831e-08  Varinance:  2.986291335091797e-08 \n",
      "\n",
      "Epoch:  14699  Learning Rate:  4.137516178712266e-08  Varinance:  2.982119595557111e-08 \n",
      "\n",
      "Epoch:  14700  Learning Rate:  4.1333807306022316e-08  Varinance:  2.9779536837896334e-08 \n",
      "\n",
      "Epoch:  14701  Learning Rate:  4.129249415873272e-08  Varinance:  2.9737935916481957e-08 \n",
      "\n",
      "Epoch:  14702  Learning Rate:  4.1251222303940655e-08  Varinance:  2.9696393110029943e-08 \n",
      "\n",
      "Epoch:  14703  Learning Rate:  4.1209991700374404e-08  Varinance:  2.9654908337355823e-08 \n",
      "\n",
      "Epoch:  14704  Learning Rate:  4.116880230680329e-08  Varinance:  2.961348151738853e-08 \n",
      "\n",
      "Epoch:  14705  Learning Rate:  4.112765408203783e-08  Varinance:  2.957211256917026e-08 \n",
      "\n",
      "Epoch:  14706  Learning Rate:  4.108654698492996e-08  Varinance:  2.9530801411856408e-08 \n",
      "\n",
      "Epoch:  14707  Learning Rate:  4.10454809743725e-08  Varinance:  2.9489547964714983e-08 \n",
      "\n",
      "Epoch:  14708  Learning Rate:  4.1004456009299364e-08  Varinance:  2.94483521471271e-08 \n",
      "\n",
      "Epoch:  14709  Learning Rate:  4.0963472048685726e-08  Varinance:  2.9407213878586385e-08 \n",
      "\n",
      "Epoch:  14710  Learning Rate:  4.092252905154755e-08  Varinance:  2.936613307869893e-08 \n",
      "\n",
      "Epoch:  14711  Learning Rate:  4.088162697694176e-08  Varinance:  2.9325109667183124e-08 \n",
      "\n",
      "Epoch:  14712  Learning Rate:  4.084076578396643e-08  Varinance:  2.9284143563869524e-08 \n",
      "\n",
      "Epoch:  14713  Learning Rate:  4.079994543176028e-08  Varinance:  2.924323468870077e-08 \n",
      "\n",
      "Epoch:  14714  Learning Rate:  4.075916587950297e-08  Varinance:  2.9202382961731037e-08 \n",
      "\n",
      "Epoch:  14715  Learning Rate:  4.071842708641486e-08  Varinance:  2.916158830312649e-08 \n",
      "\n",
      "Epoch:  14716  Learning Rate:  4.0677729011757305e-08  Varinance:  2.9120850633164717e-08 \n",
      "\n",
      "Epoch:  14717  Learning Rate:  4.063707161483215e-08  Varinance:  2.9080169872234663e-08 \n",
      "\n",
      "Epoch:  14718  Learning Rate:  4.059645485498192e-08  Varinance:  2.903954594083651e-08 \n",
      "\n",
      "Epoch:  14719  Learning Rate:  4.0555878691590004e-08  Varinance:  2.8998978759581474e-08 \n",
      "\n",
      "Epoch:  14720  Learning Rate:  4.051534308408016e-08  Varinance:  2.8958468249191796e-08 \n",
      "\n",
      "Epoch:  14721  Learning Rate:  4.04748479919167e-08  Varinance:  2.8918014330500145e-08 \n",
      "\n",
      "Epoch:  14722  Learning Rate:  4.043439337460468e-08  Varinance:  2.887761692445009e-08 \n",
      "\n",
      "Epoch:  14723  Learning Rate:  4.0393979191689407e-08  Varinance:  2.8837275952095556e-08 \n",
      "\n",
      "Epoch:  14724  Learning Rate:  4.035360540275661e-08  Varinance:  2.8796991334600725e-08 \n",
      "\n",
      "Epoch:  14725  Learning Rate:  4.031327196743266e-08  Varinance:  2.8756762993239934e-08 \n",
      "\n",
      "Epoch:  14726  Learning Rate:  4.027297884538404e-08  Varinance:  2.871659084939748e-08 \n",
      "\n",
      "Epoch:  14727  Learning Rate:  4.023272599631754e-08  Varinance:  2.8676474824567592e-08 \n",
      "\n",
      "Epoch:  14728  Learning Rate:  4.0192513379980464e-08  Varinance:  2.863641484035387e-08 \n",
      "\n",
      "Epoch:  14729  Learning Rate:  4.0152340956160116e-08  Varinance:  2.859641081846972e-08 \n",
      "\n",
      "Epoch:  14730  Learning Rate:  4.011220868468407e-08  Varinance:  2.8556462680737824e-08 \n",
      "\n",
      "Epoch:  14731  Learning Rate:  4.0072116525419984e-08  Varinance:  2.8516570349090063e-08 \n",
      "\n",
      "Epoch:  14732  Learning Rate:  4.0032064438275835e-08  Varinance:  2.8476733745567395e-08 \n",
      "\n",
      "Epoch:  14733  Learning Rate:  3.999205238319946e-08  Varinance:  2.8436952792319664e-08 \n",
      "\n",
      "Epoch:  14734  Learning Rate:  3.9952080320178724e-08  Varinance:  2.8397227411605578e-08 \n",
      "\n",
      "Epoch:  14735  Learning Rate:  3.991214820924171e-08  Varinance:  2.8357557525792138e-08 \n",
      "\n",
      "Epoch:  14736  Learning Rate:  3.9872256010456235e-08  Varinance:  2.831794305735511e-08 \n",
      "\n",
      "Epoch:  14737  Learning Rate:  3.983240368393002e-08  Varinance:  2.827838392887845e-08 \n",
      "\n",
      "Epoch:  14738  Learning Rate:  3.979259118981088e-08  Varinance:  2.8238880063054258e-08 \n",
      "\n",
      "Epoch:  14739  Learning Rate:  3.9752818488286244e-08  Varinance:  2.8199431382682635e-08 \n",
      "\n",
      "Epoch:  14740  Learning Rate:  3.971308553958334e-08  Varinance:  2.8160037810671527e-08 \n",
      "\n",
      "Epoch:  14741  Learning Rate:  3.9673392303969353e-08  Varinance:  2.8120699270036675e-08 \n",
      "\n",
      "Epoch:  14742  Learning Rate:  3.9633738741750986e-08  Varinance:  2.8081415683901067e-08 \n",
      "\n",
      "Epoch:  14743  Learning Rate:  3.9594124813274585e-08  Varinance:  2.8042186975495374e-08 \n",
      "\n",
      "Epoch:  14744  Learning Rate:  3.9554550478926366e-08  Varinance:  2.8003013068157426e-08 \n",
      "\n",
      "Epoch:  14745  Learning Rate:  3.9515015699131924e-08  Varinance:  2.7963893885332135e-08 \n",
      "\n",
      "Epoch:  14746  Learning Rate:  3.947552043435647e-08  Varinance:  2.792482935057136e-08 \n",
      "\n",
      "Epoch:  14747  Learning Rate:  3.943606464510468e-08  Varinance:  2.788581938753377e-08 \n",
      "\n",
      "Epoch:  14748  Learning Rate:  3.939664829192088e-08  Varinance:  2.784686391998476e-08 \n",
      "\n",
      "Epoch:  14749  Learning Rate:  3.935727133538866e-08  Varinance:  2.7807962871795923e-08 \n",
      "\n",
      "Epoch:  14750  Learning Rate:  3.931793373613098e-08  Varinance:  2.776911616694552e-08 \n",
      "\n",
      "Epoch:  14751  Learning Rate:  3.9278635454810394e-08  Varinance:  2.7730323729517894e-08 \n",
      "\n",
      "Epoch:  14752  Learning Rate:  3.923937645212853e-08  Varinance:  2.7691585483703443e-08 \n",
      "\n",
      "Epoch:  14753  Learning Rate:  3.920015668882632e-08  Varinance:  2.7652901353798473e-08 \n",
      "\n",
      "Epoch:  14754  Learning Rate:  3.916097612568413e-08  Varinance:  2.761427126420504e-08 \n",
      "\n",
      "Epoch:  14755  Learning Rate:  3.9121834723521334e-08  Varinance:  2.7575695139430908e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14756  Learning Rate:  3.908273244319645e-08  Varinance:  2.753717290408901e-08 \n",
      "\n",
      "Epoch:  14757  Learning Rate:  3.9043669245607333e-08  Varinance:  2.7498704482897872e-08 \n",
      "\n",
      "Epoch:  14758  Learning Rate:  3.900464509169072e-08  Varinance:  2.7460289800681106e-08 \n",
      "\n",
      "Epoch:  14759  Learning Rate:  3.896565994242245e-08  Varinance:  2.7421928782367338e-08 \n",
      "\n",
      "Epoch:  14760  Learning Rate:  3.89267137588173e-08  Varinance:  2.7383621352990056e-08 \n",
      "\n",
      "Epoch:  14761  Learning Rate:  3.8887806501929214e-08  Varinance:  2.7345367437687483e-08 \n",
      "\n",
      "Epoch:  14762  Learning Rate:  3.884893813285088e-08  Varinance:  2.730716696170251e-08 \n",
      "\n",
      "Epoch:  14763  Learning Rate:  3.881010861271384e-08  Varinance:  2.7269019850382185e-08 \n",
      "\n",
      "Epoch:  14764  Learning Rate:  3.877131790268872e-08  Varinance:  2.7230926029178115e-08 \n",
      "\n",
      "Epoch:  14765  Learning Rate:  3.8732565963984736e-08  Varinance:  2.7192885423645964e-08 \n",
      "\n",
      "Epoch:  14766  Learning Rate:  3.869385275784987e-08  Varinance:  2.7154897959445394e-08 \n",
      "\n",
      "Epoch:  14767  Learning Rate:  3.865517824557106e-08  Varinance:  2.71169635623399e-08 \n",
      "\n",
      "Epoch:  14768  Learning Rate:  3.861654238847371e-08  Varinance:  2.7079082158196714e-08 \n",
      "\n",
      "Epoch:  14769  Learning Rate:  3.85779451479219e-08  Varinance:  2.7041253672986687e-08 \n",
      "\n",
      "Epoch:  14770  Learning Rate:  3.853938648531853e-08  Varinance:  2.700347803278383e-08 \n",
      "\n",
      "Epoch:  14771  Learning Rate:  3.8500866362104843e-08  Varinance:  2.6965755163765697e-08 \n",
      "\n",
      "Epoch:  14772  Learning Rate:  3.8462384739760665e-08  Varinance:  2.6928084992212874e-08 \n",
      "\n",
      "Epoch:  14773  Learning Rate:  3.8423941579804506e-08  Varinance:  2.6890467444508936e-08 \n",
      "\n",
      "Epoch:  14774  Learning Rate:  3.838553684379312e-08  Varinance:  2.68529024471403e-08 \n",
      "\n",
      "Epoch:  14775  Learning Rate:  3.834717049332178e-08  Varinance:  2.6815389926696066e-08 \n",
      "\n",
      "Epoch:  14776  Learning Rate:  3.8308842490024056e-08  Varinance:  2.6777929809867988e-08 \n",
      "\n",
      "Epoch:  14777  Learning Rate:  3.827055279557209e-08  Varinance:  2.6740522023449956e-08 \n",
      "\n",
      "Epoch:  14778  Learning Rate:  3.8232301371676106e-08  Varinance:  2.670316649433839e-08 \n",
      "\n",
      "Epoch:  14779  Learning Rate:  3.819408818008461e-08  Varinance:  2.6665863149531753e-08 \n",
      "\n",
      "Epoch:  14780  Learning Rate:  3.8155913182584546e-08  Varinance:  2.662861191613049e-08 \n",
      "\n",
      "Epoch:  14781  Learning Rate:  3.8117776341000846e-08  Varinance:  2.6591412721336856e-08 \n",
      "\n",
      "Epoch:  14782  Learning Rate:  3.807967761719659e-08  Varinance:  2.655426549245485e-08 \n",
      "\n",
      "Epoch:  14783  Learning Rate:  3.80416169730732e-08  Varinance:  2.6517170156890076e-08 \n",
      "\n",
      "Epoch:  14784  Learning Rate:  3.800359437056995e-08  Varinance:  2.648012664214929e-08 \n",
      "\n",
      "Epoch:  14785  Learning Rate:  3.796560977166417e-08  Varinance:  2.644313487584079e-08 \n",
      "\n",
      "Epoch:  14786  Learning Rate:  3.79276631383714e-08  Varinance:  2.6406194785673918e-08 \n",
      "\n",
      "Epoch:  14787  Learning Rate:  3.7889754432744916e-08  Varinance:  2.6369306299458994e-08 \n",
      "\n",
      "Epoch:  14788  Learning Rate:  3.7851883616875957e-08  Varinance:  2.6332469345107194e-08 \n",
      "\n",
      "Epoch:  14789  Learning Rate:  3.781405065289384e-08  Varinance:  2.6295683850630386e-08 \n",
      "\n",
      "Epoch:  14790  Learning Rate:  3.777625550296553e-08  Varinance:  2.6258949744141113e-08 \n",
      "\n",
      "Epoch:  14791  Learning Rate:  3.773849812929587e-08  Varinance:  2.6222266953852044e-08 \n",
      "\n",
      "Epoch:  14792  Learning Rate:  3.770077849412741e-08  Varinance:  2.6185635408076426e-08 \n",
      "\n",
      "Epoch:  14793  Learning Rate:  3.766309655974066e-08  Varinance:  2.614905503522756e-08 \n",
      "\n",
      "Epoch:  14794  Learning Rate:  3.762545228845361e-08  Varinance:  2.611252576381873e-08 \n",
      "\n",
      "Epoch:  14795  Learning Rate:  3.7587845642621906e-08  Varinance:  2.6076047522463107e-08 \n",
      "\n",
      "Epoch:  14796  Learning Rate:  3.755027658463906e-08  Varinance:  2.603962023987357e-08 \n",
      "\n",
      "Epoch:  14797  Learning Rate:  3.751274507693592e-08  Varinance:  2.600324384486269e-08 \n",
      "\n",
      "Epoch:  14798  Learning Rate:  3.747525108198091e-08  Varinance:  2.5966918266342195e-08 \n",
      "\n",
      "Epoch:  14799  Learning Rate:  3.743779456228018e-08  Varinance:  2.5930643433323406e-08 \n",
      "\n",
      "Epoch:  14800  Learning Rate:  3.740037548037712e-08  Varinance:  2.589441927491671e-08 \n",
      "\n",
      "Epoch:  14801  Learning Rate:  3.7362993798852604e-08  Varinance:  2.5858245720331543e-08 \n",
      "\n",
      "Epoch:  14802  Learning Rate:  3.7325649480325064e-08  Varinance:  2.582212269887621e-08 \n",
      "\n",
      "Epoch:  14803  Learning Rate:  3.728834248745011e-08  Varinance:  2.5786050139957788e-08 \n",
      "\n",
      "Epoch:  14804  Learning Rate:  3.725107278292069e-08  Varinance:  2.5750027973082052e-08 \n",
      "\n",
      "Epoch:  14805  Learning Rate:  3.721384032946722e-08  Varinance:  2.5714056127852982e-08 \n",
      "\n",
      "Epoch:  14806  Learning Rate:  3.7176645089857184e-08  Varinance:  2.5678134533973176e-08 \n",
      "\n",
      "Epoch:  14807  Learning Rate:  3.713948702689533e-08  Varinance:  2.5642263121243337e-08 \n",
      "\n",
      "Epoch:  14808  Learning Rate:  3.710236610342354e-08  Varinance:  2.5606441819562242e-08 \n",
      "\n",
      "Epoch:  14809  Learning Rate:  3.7065282282321e-08  Varinance:  2.5570670558926587e-08 \n",
      "\n",
      "Epoch:  14810  Learning Rate:  3.702823552650384e-08  Varinance:  2.5534949269430967e-08 \n",
      "\n",
      "Epoch:  14811  Learning Rate:  3.699122579892523e-08  Varinance:  2.5499277881267337e-08 \n",
      "\n",
      "Epoch:  14812  Learning Rate:  3.695425306257556e-08  Varinance:  2.546365632472546e-08 \n",
      "\n",
      "Epoch:  14813  Learning Rate:  3.6917317280482034e-08  Varinance:  2.5428084530192393e-08 \n",
      "\n",
      "Epoch:  14814  Learning Rate:  3.688041841570879e-08  Varinance:  2.5392562428152428e-08 \n",
      "\n",
      "Epoch:  14815  Learning Rate:  3.684355643135711e-08  Varinance:  2.5357089949186972e-08 \n",
      "\n",
      "Epoch:  14816  Learning Rate:  3.6806731290564935e-08  Varinance:  2.5321667023974417e-08 \n",
      "\n",
      "Epoch:  14817  Learning Rate:  3.676994295650705e-08  Varinance:  2.5286293583290065e-08 \n",
      "\n",
      "Epoch:  14818  Learning Rate:  3.673319139239525e-08  Varinance:  2.5250969558005677e-08 \n",
      "\n",
      "Epoch:  14819  Learning Rate:  3.66964765614779e-08  Varinance:  2.521569487908983e-08 \n",
      "\n",
      "Epoch:  14820  Learning Rate:  3.665979842704011e-08  Varinance:  2.5180469477607463e-08 \n",
      "\n",
      "Epoch:  14821  Learning Rate:  3.6623156952403866e-08  Varinance:  2.5145293284719804e-08 \n",
      "\n",
      "Epoch:  14822  Learning Rate:  3.6586552100927627e-08  Varinance:  2.5110166231684248e-08 \n",
      "\n",
      "Epoch:  14823  Learning Rate:  3.654998383600653e-08  Varinance:  2.5075088249854225e-08 \n",
      "\n",
      "Epoch:  14824  Learning Rate:  3.651345212107226e-08  Varinance:  2.5040059270679146e-08 \n",
      "\n",
      "Epoch:  14825  Learning Rate:  3.647695691959321e-08  Varinance:  2.5005079225703926e-08 \n",
      "\n",
      "Epoch:  14826  Learning Rate:  3.6440498195074124e-08  Varinance:  2.4970148046569364e-08 \n",
      "\n",
      "Epoch:  14827  Learning Rate:  3.6404075911056204e-08  Varinance:  2.493526566501168e-08 \n",
      "\n",
      "Epoch:  14828  Learning Rate:  3.6367690031117296e-08  Varinance:  2.490043201286244e-08 \n",
      "\n",
      "Epoch:  14829  Learning Rate:  3.6331340518871445e-08  Varinance:  2.4865647022048452e-08 \n",
      "\n",
      "Epoch:  14830  Learning Rate:  3.629502733796908e-08  Varinance:  2.4830910624591607e-08 \n",
      "\n",
      "Epoch:  14831  Learning Rate:  3.625875045209714e-08  Varinance:  2.4796222752608858e-08 \n",
      "\n",
      "Epoch:  14832  Learning Rate:  3.6222509824978676e-08  Varinance:  2.4761583338311725e-08 \n",
      "\n",
      "Epoch:  14833  Learning Rate:  3.618630542037299e-08  Varinance:  2.4726992314006682e-08 \n",
      "\n",
      "Epoch:  14834  Learning Rate:  3.6150137202075807e-08  Varinance:  2.469244961209468e-08 \n",
      "\n",
      "Epoch:  14835  Learning Rate:  3.6114005133918834e-08  Varinance:  2.4657955165071115e-08 \n",
      "\n",
      "Epoch:  14836  Learning Rate:  3.607790917976994e-08  Varinance:  2.4623508905525666e-08 \n",
      "\n",
      "Epoch:  14837  Learning Rate:  3.60418493035333e-08  Varinance:  2.4589110766142205e-08 \n",
      "\n",
      "Epoch:  14838  Learning Rate:  3.600582546914896e-08  Varinance:  2.4554760679698703e-08 \n",
      "\n",
      "Epoch:  14839  Learning Rate:  3.59698376405931e-08  Varinance:  2.4520458579066803e-08 \n",
      "\n",
      "Epoch:  14840  Learning Rate:  3.5933885781877806e-08  Varinance:  2.4486204397212167e-08 \n",
      "\n",
      "Epoch:  14841  Learning Rate:  3.589796985705135e-08  Varinance:  2.445199806719401e-08 \n",
      "\n",
      "Epoch:  14842  Learning Rate:  3.5862089830197756e-08  Varinance:  2.4417839522165086e-08 \n",
      "\n",
      "Epoch:  14843  Learning Rate:  3.58262456654369e-08  Varinance:  2.438372869537151e-08 \n",
      "\n",
      "Epoch:  14844  Learning Rate:  3.5790437326924776e-08  Varinance:  2.4349665520152655e-08 \n",
      "\n",
      "Epoch:  14845  Learning Rate:  3.5754664778852946e-08  Varinance:  2.4315649929941103e-08 \n",
      "\n",
      "Epoch:  14846  Learning Rate:  3.571892798544882e-08  Varinance:  2.428168185826218e-08 \n",
      "\n",
      "Epoch:  14847  Learning Rate:  3.568322691097571e-08  Varinance:  2.4247761238734307e-08 \n",
      "\n",
      "Epoch:  14848  Learning Rate:  3.56475615197325e-08  Varinance:  2.4213888005068584e-08 \n",
      "\n",
      "Epoch:  14849  Learning Rate:  3.5611931776053707e-08  Varinance:  2.4180062091068686e-08 \n",
      "\n",
      "Epoch:  14850  Learning Rate:  3.557633764430972e-08  Varinance:  2.4146283430630787e-08 \n",
      "\n",
      "Epoch:  14851  Learning Rate:  3.554077908890635e-08  Varinance:  2.411255195774338e-08 \n",
      "\n",
      "Epoch:  14852  Learning Rate:  3.5505256074284956e-08  Varinance:  2.407886760648728e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14853  Learning Rate:  3.5469768564922667e-08  Varinance:  2.404523031103513e-08 \n",
      "\n",
      "Epoch:  14854  Learning Rate:  3.54343165253319e-08  Varinance:  2.4011640005651772e-08 \n",
      "\n",
      "Epoch:  14855  Learning Rate:  3.53988999200606e-08  Varinance:  2.39780966246938e-08 \n",
      "\n",
      "Epoch:  14856  Learning Rate:  3.536351871369212e-08  Varinance:  2.3944600102609523e-08 \n",
      "\n",
      "Epoch:  14857  Learning Rate:  3.532817287084535e-08  Varinance:  2.39111503739388e-08 \n",
      "\n",
      "Epoch:  14858  Learning Rate:  3.529286235617441e-08  Varinance:  2.3877747373312956e-08 \n",
      "\n",
      "Epoch:  14859  Learning Rate:  3.525758713436869e-08  Varinance:  2.3844391035454715e-08 \n",
      "\n",
      "Epoch:  14860  Learning Rate:  3.522234717015312e-08  Varinance:  2.381108129517771e-08 \n",
      "\n",
      "Epoch:  14861  Learning Rate:  3.518714242828764e-08  Varinance:  2.3777818087386938e-08 \n",
      "\n",
      "Epoch:  14862  Learning Rate:  3.5151972873567466e-08  Varinance:  2.374460134707821e-08 \n",
      "\n",
      "Epoch:  14863  Learning Rate:  3.511683847082316e-08  Varinance:  2.371143100933815e-08 \n",
      "\n",
      "Epoch:  14864  Learning Rate:  3.508173918492025e-08  Varinance:  2.3678307009344078e-08 \n",
      "\n",
      "Epoch:  14865  Learning Rate:  3.5046674980759384e-08  Varinance:  2.3645229282363864e-08 \n",
      "\n",
      "Epoch:  14866  Learning Rate:  3.501164582327648e-08  Varinance:  2.361219776375588e-08 \n",
      "\n",
      "Epoch:  14867  Learning Rate:  3.497665167744232e-08  Varinance:  2.3579212388968568e-08 \n",
      "\n",
      "Epoch:  14868  Learning Rate:  3.4941692508262686e-08  Varinance:  2.3546273093540772e-08 \n",
      "\n",
      "Epoch:  14869  Learning Rate:  3.490676828077854e-08  Varinance:  2.3513379813101322e-08 \n",
      "\n",
      "Epoch:  14870  Learning Rate:  3.487187896006558e-08  Varinance:  2.3480532483368966e-08 \n",
      "\n",
      "Epoch:  14871  Learning Rate:  3.483702451123449e-08  Varinance:  2.3447731040152246e-08 \n",
      "\n",
      "Epoch:  14872  Learning Rate:  3.4802204899430745e-08  Varinance:  2.3414975419349396e-08 \n",
      "\n",
      "Epoch:  14873  Learning Rate:  3.476742008983487e-08  Varinance:  2.338226555694825e-08 \n",
      "\n",
      "Epoch:  14874  Learning Rate:  3.473267004766197e-08  Varinance:  2.3349601389025837e-08 \n",
      "\n",
      "Epoch:  14875  Learning Rate:  3.4697954738161966e-08  Varinance:  2.331698285174873e-08 \n",
      "\n",
      "Epoch:  14876  Learning Rate:  3.4663274126619645e-08  Varinance:  2.3284409881372593e-08 \n",
      "\n",
      "Epoch:  14877  Learning Rate:  3.4628628178354334e-08  Varinance:  2.325188241424213e-08 \n",
      "\n",
      "Epoch:  14878  Learning Rate:  3.4594016858720034e-08  Varinance:  2.3219400386790974e-08 \n",
      "\n",
      "Epoch:  14879  Learning Rate:  3.455944013310553e-08  Varinance:  2.3186963735541563e-08 \n",
      "\n",
      "Epoch:  14880  Learning Rate:  3.452489796693405e-08  Varinance:  2.3154572397105087e-08 \n",
      "\n",
      "Epoch:  14881  Learning Rate:  3.449039032566334e-08  Varinance:  2.3122226308181042e-08 \n",
      "\n",
      "Epoch:  14882  Learning Rate:  3.4455917174785895e-08  Varinance:  2.30899254055576e-08 \n",
      "\n",
      "Epoch:  14883  Learning Rate:  3.44214784798285e-08  Varinance:  2.3057669626111158e-08 \n",
      "\n",
      "Epoch:  14884  Learning Rate:  3.438707420635245e-08  Varinance:  2.302545890680629e-08 \n",
      "\n",
      "Epoch:  14885  Learning Rate:  3.4352704319953416e-08  Varinance:  2.2993293184695627e-08 \n",
      "\n",
      "Epoch:  14886  Learning Rate:  3.4318368786261624e-08  Varinance:  2.296117239691974e-08 \n",
      "\n",
      "Epoch:  14887  Learning Rate:  3.4284067570941475e-08  Varinance:  2.2929096480707097e-08 \n",
      "\n",
      "Epoch:  14888  Learning Rate:  3.424980063969169e-08  Varinance:  2.2897065373373596e-08 \n",
      "\n",
      "Epoch:  14889  Learning Rate:  3.4215567958245466e-08  Varinance:  2.286507901232296e-08 \n",
      "\n",
      "Epoch:  14890  Learning Rate:  3.418136949237005e-08  Varinance:  2.2833137335046282e-08 \n",
      "\n",
      "Epoch:  14891  Learning Rate:  3.414720520786691e-08  Varinance:  2.2801240279121954e-08 \n",
      "\n",
      "Epoch:  14892  Learning Rate:  3.411307507057189e-08  Varinance:  2.2769387782215592e-08 \n",
      "\n",
      "Epoch:  14893  Learning Rate:  3.407897904635478e-08  Varinance:  2.2737579782079873e-08 \n",
      "\n",
      "Epoch:  14894  Learning Rate:  3.40449171011195e-08  Varinance:  2.2705816216554524e-08 \n",
      "\n",
      "Epoch:  14895  Learning Rate:  3.4010889200804215e-08  Varinance:  2.2674097023565866e-08 \n",
      "\n",
      "Epoch:  14896  Learning Rate:  3.397689531138096e-08  Varinance:  2.2642422141127164e-08 \n",
      "\n",
      "Epoch:  14897  Learning Rate:  3.3942935398855795e-08  Varinance:  2.2610791507338208e-08 \n",
      "\n",
      "Epoch:  14898  Learning Rate:  3.3909009429268915e-08  Varinance:  2.2579205060385262e-08 \n",
      "\n",
      "Epoch:  14899  Learning Rate:  3.3875117368694296e-08  Varinance:  2.2547662738540923e-08 \n",
      "\n",
      "Epoch:  14900  Learning Rate:  3.384125918323986e-08  Varinance:  2.2516164480164043e-08 \n",
      "\n",
      "Epoch:  14901  Learning Rate:  3.380743483904737e-08  Varinance:  2.2484710223699647e-08 \n",
      "\n",
      "Epoch:  14902  Learning Rate:  3.37736443022926e-08  Varinance:  2.245329990767852e-08 \n",
      "\n",
      "Epoch:  14903  Learning Rate:  3.373988753918494e-08  Varinance:  2.242193347071755e-08 \n",
      "\n",
      "Epoch:  14904  Learning Rate:  3.3706164515967576e-08  Varinance:  2.23906108515193e-08 \n",
      "\n",
      "Epoch:  14905  Learning Rate:  3.3672475198917596e-08  Varinance:  2.235933198887196e-08 \n",
      "\n",
      "Epoch:  14906  Learning Rate:  3.3638819554345615e-08  Varinance:  2.2328096821649233e-08 \n",
      "\n",
      "Epoch:  14907  Learning Rate:  3.360519754859594e-08  Varinance:  2.2296905288810214e-08 \n",
      "\n",
      "Epoch:  14908  Learning Rate:  3.357160914804667e-08  Varinance:  2.2265757329399348e-08 \n",
      "\n",
      "Epoch:  14909  Learning Rate:  3.353805431910935e-08  Varinance:  2.2234652882545993e-08 \n",
      "\n",
      "Epoch:  14910  Learning Rate:  3.3504533028229075e-08  Varinance:  2.2203591887464778e-08 \n",
      "\n",
      "Epoch:  14911  Learning Rate:  3.347104524188469e-08  Varinance:  2.217257428345517e-08 \n",
      "\n",
      "Epoch:  14912  Learning Rate:  3.3437590926588326e-08  Varinance:  2.214160000990144e-08 \n",
      "\n",
      "Epoch:  14913  Learning Rate:  3.340417004888563e-08  Varinance:  2.2110669006272523e-08 \n",
      "\n",
      "Epoch:  14914  Learning Rate:  3.3370782575355816e-08  Varinance:  2.207978121212192e-08 \n",
      "\n",
      "Epoch:  14915  Learning Rate:  3.333742847261136e-08  Varinance:  2.2048936567087652e-08 \n",
      "\n",
      "Epoch:  14916  Learning Rate:  3.330410770729815e-08  Varinance:  2.2018135010891834e-08 \n",
      "\n",
      "Epoch:  14917  Learning Rate:  3.327082024609537e-08  Varinance:  2.1987376483341006e-08 \n",
      "\n",
      "Epoch:  14918  Learning Rate:  3.323756605571566e-08  Varinance:  2.195666092432574e-08 \n",
      "\n",
      "Epoch:  14919  Learning Rate:  3.320434510290478e-08  Varinance:  2.1925988273820556e-08 \n",
      "\n",
      "Epoch:  14920  Learning Rate:  3.317115735444171e-08  Varinance:  2.1895358471883845e-08 \n",
      "\n",
      "Epoch:  14921  Learning Rate:  3.313800277713882e-08  Varinance:  2.1864771458657727e-08 \n",
      "\n",
      "Epoch:  14922  Learning Rate:  3.310488133784147e-08  Varinance:  2.1834227174368017e-08 \n",
      "\n",
      "Epoch:  14923  Learning Rate:  3.307179300342816e-08  Varinance:  2.18037255593238e-08 \n",
      "\n",
      "Epoch:  14924  Learning Rate:  3.303873774081066e-08  Varinance:  2.177326655391779e-08 \n",
      "\n",
      "Epoch:  14925  Learning Rate:  3.300571551693366e-08  Varinance:  2.1742850098625876e-08 \n",
      "\n",
      "Epoch:  14926  Learning Rate:  3.297272629877487e-08  Varinance:  2.171247613400711e-08 \n",
      "\n",
      "Epoch:  14927  Learning Rate:  3.2939770053345184e-08  Varinance:  2.1682144600703578e-08 \n",
      "\n",
      "Epoch:  14928  Learning Rate:  3.290684674768829e-08  Varinance:  2.165185543944029e-08 \n",
      "\n",
      "Epoch:  14929  Learning Rate:  3.2873956348880833e-08  Varinance:  2.162160859102514e-08 \n",
      "\n",
      "Epoch:  14930  Learning Rate:  3.284109882403252e-08  Varinance:  2.159140399634847e-08 \n",
      "\n",
      "Epoch:  14931  Learning Rate:  3.2808274140285775e-08  Varinance:  2.1561241596383435e-08 \n",
      "\n",
      "Epoch:  14932  Learning Rate:  3.277548226481589e-08  Varinance:  2.1531121332185568e-08 \n",
      "\n",
      "Epoch:  14933  Learning Rate:  3.2742723164830956e-08  Varinance:  2.1501043144892747e-08 \n",
      "\n",
      "Epoch:  14934  Learning Rate:  3.270999680757197e-08  Varinance:  2.147100697572508e-08 \n",
      "\n",
      "Epoch:  14935  Learning Rate:  3.2677303160312516e-08  Varinance:  2.144101276598479e-08 \n",
      "\n",
      "Epoch:  14936  Learning Rate:  3.2644642190358886e-08  Varinance:  2.141106045705616e-08 \n",
      "\n",
      "Epoch:  14937  Learning Rate:  3.261201386505023e-08  Varinance:  2.1381149990405148e-08 \n",
      "\n",
      "Epoch:  14938  Learning Rate:  3.2579418151758146e-08  Varinance:  2.1351281307579703e-08 \n",
      "\n",
      "Epoch:  14939  Learning Rate:  3.254685501788688e-08  Varinance:  2.1321454350209347e-08 \n",
      "\n",
      "Epoch:  14940  Learning Rate:  3.2514324430873405e-08  Varinance:  2.129166906000515e-08 \n",
      "\n",
      "Epoch:  14941  Learning Rate:  3.248182635818706e-08  Varinance:  2.126192537875961e-08 \n",
      "\n",
      "Epoch:  14942  Learning Rate:  3.244936076732973e-08  Varinance:  2.123222324834654e-08 \n",
      "\n",
      "Epoch:  14943  Learning Rate:  3.241692762583593e-08  Varinance:  2.1202562610721016e-08 \n",
      "\n",
      "Epoch:  14944  Learning Rate:  3.2384526901272453e-08  Varinance:  2.1172943407918993e-08 \n",
      "\n",
      "Epoch:  14945  Learning Rate:  3.235215856123852e-08  Varinance:  2.1143365582057612e-08 \n",
      "\n",
      "Epoch:  14946  Learning Rate:  3.2319822573365897e-08  Varinance:  2.1113829075334807e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14947  Learning Rate:  3.228751890531855e-08  Varinance:  2.108433383002925e-08 \n",
      "\n",
      "Epoch:  14948  Learning Rate:  3.225524752479279e-08  Varinance:  2.1054879788500258e-08 \n",
      "\n",
      "Epoch:  14949  Learning Rate:  3.222300839951719e-08  Varinance:  2.102546689318774e-08 \n",
      "\n",
      "Epoch:  14950  Learning Rate:  3.2190801497252736e-08  Varinance:  2.099609508661179e-08 \n",
      "\n",
      "Epoch:  14951  Learning Rate:  3.2158626785792455e-08  Varinance:  2.0966764311373026e-08 \n",
      "\n",
      "Epoch:  14952  Learning Rate:  3.2126484232961584e-08  Varinance:  2.0937474510152168e-08 \n",
      "\n",
      "Epoch:  14953  Learning Rate:  3.209437380661768e-08  Varinance:  2.090822562571002e-08 \n",
      "\n",
      "Epoch:  14954  Learning Rate:  3.2062295474650255e-08  Varinance:  2.0879017600887337e-08 \n",
      "\n",
      "Epoch:  14955  Learning Rate:  3.203024920498093e-08  Varinance:  2.0849850378604735e-08 \n",
      "\n",
      "Epoch:  14956  Learning Rate:  3.1998234965563523e-08  Varinance:  2.082072390186263e-08 \n",
      "\n",
      "Epoch:  14957  Learning Rate:  3.196625272438376e-08  Varinance:  2.079163811374085e-08 \n",
      "\n",
      "Epoch:  14958  Learning Rate:  3.193430244945932e-08  Varinance:  2.0762592957398956e-08 \n",
      "\n",
      "Epoch:  14959  Learning Rate:  3.190238410884005e-08  Varinance:  2.0733588376075842e-08 \n",
      "\n",
      "Epoch:  14960  Learning Rate:  3.187049767060755e-08  Varinance:  2.0704624313089693e-08 \n",
      "\n",
      "Epoch:  14961  Learning Rate:  3.1838643102875315e-08  Varinance:  2.067570071183788e-08 \n",
      "\n",
      "Epoch:  14962  Learning Rate:  3.1806820373788894e-08  Varinance:  2.064681751579684e-08 \n",
      "\n",
      "Epoch:  14963  Learning Rate:  3.1775029451525495e-08  Varinance:  2.061797466852205e-08 \n",
      "\n",
      "Epoch:  14964  Learning Rate:  3.17432703042942e-08  Varinance:  2.0589172113647613e-08 \n",
      "\n",
      "Epoch:  14965  Learning Rate:  3.171154290033579e-08  Varinance:  2.05604097948866e-08 \n",
      "\n",
      "Epoch:  14966  Learning Rate:  3.167984720792299e-08  Varinance:  2.053168765603063e-08 \n",
      "\n",
      "Epoch:  14967  Learning Rate:  3.164818319536004e-08  Varinance:  2.0503005640949854e-08 \n",
      "\n",
      "Epoch:  14968  Learning Rate:  3.161655083098285e-08  Varinance:  2.0474363693592828e-08 \n",
      "\n",
      "Epoch:  14969  Learning Rate:  3.15849500831592e-08  Varinance:  2.0445761757986408e-08 \n",
      "\n",
      "Epoch:  14970  Learning Rate:  3.1553380920288254e-08  Varinance:  2.041719977823572e-08 \n",
      "\n",
      "Epoch:  14971  Learning Rate:  3.152184331080081e-08  Varinance:  2.038867769852375e-08 \n",
      "\n",
      "Epoch:  14972  Learning Rate:  3.149033722315935e-08  Varinance:  2.036019546311168e-08 \n",
      "\n",
      "Epoch:  14973  Learning Rate:  3.145886262585774e-08  Varinance:  2.033175301633849e-08 \n",
      "\n",
      "Epoch:  14974  Learning Rate:  3.142741948742132e-08  Varinance:  2.0303350302620896e-08 \n",
      "\n",
      "Epoch:  14975  Learning Rate:  3.1396007776407066e-08  Varinance:  2.027498726645329e-08 \n",
      "\n",
      "Epoch:  14976  Learning Rate:  3.1364627461403205e-08  Varinance:  2.0246663852407577e-08 \n",
      "\n",
      "Epoch:  14977  Learning Rate:  3.1333278511029366e-08  Varinance:  2.0218380005133187e-08 \n",
      "\n",
      "Epoch:  14978  Learning Rate:  3.13019608939367e-08  Varinance:  2.019013566935664e-08 \n",
      "\n",
      "Epoch:  14979  Learning Rate:  3.127067457880754e-08  Varinance:  2.01619307898819e-08 \n",
      "\n",
      "Epoch:  14980  Learning Rate:  3.123941953435556e-08  Varinance:  2.0133765311589955e-08 \n",
      "\n",
      "Epoch:  14981  Learning Rate:  3.1208195729325665e-08  Varinance:  2.0105639179438797e-08 \n",
      "\n",
      "Epoch:  14982  Learning Rate:  3.117700313249416e-08  Varinance:  2.007755233846331e-08 \n",
      "\n",
      "Epoch:  14983  Learning Rate:  3.114584171266838e-08  Varinance:  2.004950473377516e-08 \n",
      "\n",
      "Epoch:  14984  Learning Rate:  3.1114711438686855e-08  Varinance:  2.002149631056277e-08 \n",
      "\n",
      "Epoch:  14985  Learning Rate:  3.108361227941942e-08  Varinance:  1.9993527014090908e-08 \n",
      "\n",
      "Epoch:  14986  Learning Rate:  3.1052544203766845e-08  Varinance:  1.9965596789701015e-08 \n",
      "\n",
      "Epoch:  14987  Learning Rate:  3.1021507180661013e-08  Varinance:  1.9937705582810838e-08 \n",
      "\n",
      "Epoch:  14988  Learning Rate:  3.0990501179065e-08  Varinance:  1.990985333891435e-08 \n",
      "\n",
      "Epoch:  14989  Learning Rate:  3.095952616797275e-08  Varinance:  1.9882040003581684e-08 \n",
      "\n",
      "Epoch:  14990  Learning Rate:  3.092858211640919e-08  Varinance:  1.9854265522458996e-08 \n",
      "\n",
      "Epoch:  14991  Learning Rate:  3.0897668993430386e-08  Varinance:  1.9826529841268456e-08 \n",
      "\n",
      "Epoch:  14992  Learning Rate:  3.086678676812314e-08  Varinance:  1.9798832905807836e-08 \n",
      "\n",
      "Epoch:  14993  Learning Rate:  3.083593540960518e-08  Varinance:  1.977117466195084e-08 \n",
      "\n",
      "Epoch:  14994  Learning Rate:  3.0805114887025264e-08  Varinance:  1.974355505564672e-08 \n",
      "\n",
      "Epoch:  14995  Learning Rate:  3.07743251695628e-08  Varinance:  1.971597403292023e-08 \n",
      "\n",
      "Epoch:  14996  Learning Rate:  3.074356622642806e-08  Varinance:  1.9688431539871526e-08 \n",
      "\n",
      "Epoch:  14997  Learning Rate:  3.071283802686207e-08  Varinance:  1.966092752267606e-08 \n",
      "\n",
      "Epoch:  14998  Learning Rate:  3.0682140540136706e-08  Varinance:  1.9633461927584546e-08 \n",
      "\n",
      "Epoch:  14999  Learning Rate:  3.065147373555445e-08  Varinance:  1.960603470092257e-08 \n",
      "\n",
      "Epoch:  15000  Learning Rate:  3.062083758244842e-08  Varinance:  1.9578645789090916e-08 \n",
      "\n",
      "Epoch:  15001  Learning Rate:  3.059023205018258e-08  Varinance:  1.9551295138565172e-08 \n",
      "\n",
      "Epoch:  15002  Learning Rate:  3.055965710815134e-08  Varinance:  1.952398269589569e-08 \n",
      "\n",
      "Epoch:  15003  Learning Rate:  3.05291127257797e-08  Varinance:  1.94967084077075e-08 \n",
      "\n",
      "Epoch:  15004  Learning Rate:  3.049859887252339e-08  Varinance:  1.946947222070019e-08 \n",
      "\n",
      "Epoch:  15005  Learning Rate:  3.046811551786849e-08  Varinance:  1.9442274081647876e-08 \n",
      "\n",
      "Epoch:  15006  Learning Rate:  3.0437662631331596e-08  Varinance:  1.9415113937398818e-08 \n",
      "\n",
      "Epoch:  15007  Learning Rate:  3.040724018245992e-08  Varinance:  1.9387991734875738e-08 \n",
      "\n",
      "Epoch:  15008  Learning Rate:  3.037684814083096e-08  Varinance:  1.9360907421075438e-08 \n",
      "\n",
      "Epoch:  15009  Learning Rate:  3.0346486476052675e-08  Varinance:  1.9333860943068763e-08 \n",
      "\n",
      "Epoch:  15010  Learning Rate:  3.031615515776334e-08  Varinance:  1.930685224800049e-08 \n",
      "\n",
      "Epoch:  15011  Learning Rate:  3.0285854155631746e-08  Varinance:  1.9279881283089247e-08 \n",
      "\n",
      "Epoch:  15012  Learning Rate:  3.025558343935683e-08  Varinance:  1.9252947995627448e-08 \n",
      "\n",
      "Epoch:  15013  Learning Rate:  3.0225342978667814e-08  Varinance:  1.9226052332980943e-08 \n",
      "\n",
      "Epoch:  15014  Learning Rate:  3.019513274332435e-08  Varinance:  1.9199194242589317e-08 \n",
      "\n",
      "Epoch:  15015  Learning Rate:  3.016495270311616e-08  Varinance:  1.9172373671965502e-08 \n",
      "\n",
      "Epoch:  15016  Learning Rate:  3.013480282786312e-08  Varinance:  1.914559056869576e-08 \n",
      "\n",
      "Epoch:  15017  Learning Rate:  3.010468308741547e-08  Varinance:  1.911884488043957e-08 \n",
      "\n",
      "Epoch:  15018  Learning Rate:  3.007459345165342e-08  Varinance:  1.9092136554929526e-08 \n",
      "\n",
      "Epoch:  15019  Learning Rate:  3.004453389048728e-08  Varinance:  1.9065465539971316e-08 \n",
      "\n",
      "Epoch:  15020  Learning Rate:  3.001450437385759e-08  Varinance:  1.9038831783443324e-08 \n",
      "\n",
      "Epoch:  15021  Learning Rate:  2.9984504871734765e-08  Varinance:  1.9012235233296962e-08 \n",
      "\n",
      "Epoch:  15022  Learning Rate:  2.9954535354119256e-08  Varinance:  1.8985675837556274e-08 \n",
      "\n",
      "Epoch:  15023  Learning Rate:  2.992459579104166e-08  Varinance:  1.8959153544317924e-08 \n",
      "\n",
      "Epoch:  15024  Learning Rate:  2.989468615256234e-08  Varinance:  1.893266830175107e-08 \n",
      "\n",
      "Epoch:  15025  Learning Rate:  2.986480640877167e-08  Varinance:  1.8906220058097283e-08 \n",
      "\n",
      "Epoch:  15026  Learning Rate:  2.983495652978985e-08  Varinance:  1.8879808761670505e-08 \n",
      "\n",
      "Epoch:  15027  Learning Rate:  2.980513648576709e-08  Varinance:  1.885343436085668e-08 \n",
      "\n",
      "Epoch:  15028  Learning Rate:  2.9775346246883295e-08  Varinance:  1.8827096804114057e-08 \n",
      "\n",
      "Epoch:  15029  Learning Rate:  2.9745585783348183e-08  Varinance:  1.8800796039972823e-08 \n",
      "\n",
      "Epoch:  15030  Learning Rate:  2.971585506540139e-08  Varinance:  1.8774532017035054e-08 \n",
      "\n",
      "Epoch:  15031  Learning Rate:  2.968615406331213e-08  Varinance:  1.874830468397464e-08 \n",
      "\n",
      "Epoch:  15032  Learning Rate:  2.9656482747379355e-08  Varinance:  1.8722113989537168e-08 \n",
      "\n",
      "Epoch:  15033  Learning Rate:  2.9626841087931857e-08  Varinance:  1.8695959882539888e-08 \n",
      "\n",
      "Epoch:  15034  Learning Rate:  2.9597229055327908e-08  Varinance:  1.8669842311871357e-08 \n",
      "\n",
      "Epoch:  15035  Learning Rate:  2.9567646619955435e-08  Varinance:  1.8643761226491728e-08 \n",
      "\n",
      "Epoch:  15036  Learning Rate:  2.9538093752232096e-08  Varinance:  1.8617716575432392e-08 \n",
      "\n",
      "Epoch:  15037  Learning Rate:  2.9508570422604972e-08  Varinance:  1.8591708307795936e-08 \n",
      "\n",
      "Epoch:  15038  Learning Rate:  2.947907660155068e-08  Varinance:  1.8565736372756053e-08 \n",
      "\n",
      "Epoch:  15039  Learning Rate:  2.9449612259575494e-08  Varinance:  1.853980071955744e-08 \n",
      "\n",
      "Epoch:  15040  Learning Rate:  2.942017736721502e-08  Varinance:  1.8513901297515754e-08 \n",
      "\n",
      "Epoch:  15041  Learning Rate:  2.9390771895034365e-08  Varinance:  1.8488038056017272e-08 \n",
      "\n",
      "Epoch:  15042  Learning Rate:  2.936139581362801e-08  Varinance:  1.8462210944519165e-08 \n",
      "\n",
      "Epoch:  15043  Learning Rate:  2.933204909361996e-08  Varinance:  1.8436419912549144e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15044  Learning Rate:  2.930273170566345e-08  Varinance:  1.8410664909705435e-08 \n",
      "\n",
      "Epoch:  15045  Learning Rate:  2.9273443620441035e-08  Varinance:  1.8384945885656665e-08 \n",
      "\n",
      "Epoch:  15046  Learning Rate:  2.9244184808664733e-08  Varinance:  1.8359262790141777e-08 \n",
      "\n",
      "Epoch:  15047  Learning Rate:  2.9214955241075678e-08  Varinance:  1.8333615572969995e-08 \n",
      "\n",
      "Epoch:  15048  Learning Rate:  2.9185754888444244e-08  Varinance:  1.8308004184020454e-08 \n",
      "\n",
      "Epoch:  15049  Learning Rate:  2.9156583721570183e-08  Varinance:  1.828242857324251e-08 \n",
      "\n",
      "Epoch:  15050  Learning Rate:  2.9127441711282277e-08  Varinance:  1.8256888690655366e-08 \n",
      "\n",
      "Epoch:  15051  Learning Rate:  2.9098328828438455e-08  Varinance:  1.823138448634805e-08 \n",
      "\n",
      "Epoch:  15052  Learning Rate:  2.9069245043925933e-08  Varinance:  1.8205915910479312e-08 \n",
      "\n",
      "Epoch:  15053  Learning Rate:  2.9040190328660882e-08  Varinance:  1.8180482913277527e-08 \n",
      "\n",
      "Epoch:  15054  Learning Rate:  2.901116465358853e-08  Varinance:  1.8155085445040673e-08 \n",
      "\n",
      "Epoch:  15055  Learning Rate:  2.89821679896833e-08  Varinance:  1.8129723456135953e-08 \n",
      "\n",
      "Epoch:  15056  Learning Rate:  2.8953200307948472e-08  Varinance:  1.8104396897000107e-08 \n",
      "\n",
      "Epoch:  15057  Learning Rate:  2.8924261579416367e-08  Varinance:  1.8079105718139047e-08 \n",
      "\n",
      "Epoch:  15058  Learning Rate:  2.8895351775148197e-08  Varinance:  1.8053849870127826e-08 \n",
      "\n",
      "Epoch:  15059  Learning Rate:  2.8866470866234264e-08  Varinance:  1.802862930361054e-08 \n",
      "\n",
      "Epoch:  15060  Learning Rate:  2.8837618823793606e-08  Varinance:  1.8003443969300232e-08 \n",
      "\n",
      "Epoch:  15061  Learning Rate:  2.8808795618974118e-08  Varinance:  1.7978293817978875e-08 \n",
      "\n",
      "Epoch:  15062  Learning Rate:  2.8780001222952704e-08  Varinance:  1.7953178800496986e-08 \n",
      "\n",
      "Epoch:  15063  Learning Rate:  2.8751235606934913e-08  Varinance:  1.7928098867773947e-08 \n",
      "\n",
      "Epoch:  15064  Learning Rate:  2.8722498742155072e-08  Varinance:  1.7903053970797635e-08 \n",
      "\n",
      "Epoch:  15065  Learning Rate:  2.8693790599876417e-08  Varinance:  1.7878044060624392e-08 \n",
      "\n",
      "Epoch:  15066  Learning Rate:  2.866511115139075e-08  Varinance:  1.7853069088378945e-08 \n",
      "\n",
      "Epoch:  15067  Learning Rate:  2.8636460368018577e-08  Varinance:  1.7828129005254282e-08 \n",
      "\n",
      "Epoch:  15068  Learning Rate:  2.860783822110921e-08  Varinance:  1.7803223762511648e-08 \n",
      "\n",
      "Epoch:  15069  Learning Rate:  2.8579244682040447e-08  Varinance:  1.7778353311480176e-08 \n",
      "\n",
      "Epoch:  15070  Learning Rate:  2.855067972221869e-08  Varinance:  1.7753517603557184e-08 \n",
      "\n",
      "Epoch:  15071  Learning Rate:  2.8522143313079092e-08  Varinance:  1.7728716590207828e-08 \n",
      "\n",
      "Epoch:  15072  Learning Rate:  2.8493635426085187e-08  Varinance:  1.7703950222965057e-08 \n",
      "\n",
      "Epoch:  15073  Learning Rate:  2.846515603272908e-08  Varinance:  1.7679218453429527e-08 \n",
      "\n",
      "Epoch:  15074  Learning Rate:  2.843670510453132e-08  Varinance:  1.7654521233269516e-08 \n",
      "\n",
      "Epoch:  15075  Learning Rate:  2.840828261304109e-08  Varinance:  1.7629858514220872e-08 \n",
      "\n",
      "Epoch:  15076  Learning Rate:  2.8379888529835845e-08  Varinance:  1.7605230248086678e-08 \n",
      "\n",
      "Epoch:  15077  Learning Rate:  2.8351522826521437e-08  Varinance:  1.7580636386737546e-08 \n",
      "\n",
      "Epoch:  15078  Learning Rate:  2.8323185474732272e-08  Varinance:  1.7556076882111245e-08 \n",
      "\n",
      "Epoch:  15079  Learning Rate:  2.8294876446130946e-08  Varinance:  1.7531551686212698e-08 \n",
      "\n",
      "Epoch:  15080  Learning Rate:  2.8266595712408364e-08  Varinance:  1.7507060751113866e-08 \n",
      "\n",
      "Epoch:  15081  Learning Rate:  2.823834324528391e-08  Varinance:  1.748260402895373e-08 \n",
      "\n",
      "Epoch:  15082  Learning Rate:  2.8210119016505046e-08  Varinance:  1.7458181471937944e-08 \n",
      "\n",
      "Epoch:  15083  Learning Rate:  2.818192299784751e-08  Varinance:  1.743379303233912e-08 \n",
      "\n",
      "Epoch:  15084  Learning Rate:  2.8153755161115358e-08  Varinance:  1.7409438662496475e-08 \n",
      "\n",
      "Epoch:  15085  Learning Rate:  2.812561547814072e-08  Varinance:  1.7385118314815803e-08 \n",
      "\n",
      "Epoch:  15086  Learning Rate:  2.8097503920783853e-08  Varinance:  1.73608319417694e-08 \n",
      "\n",
      "Epoch:  15087  Learning Rate:  2.8069420460933297e-08  Varinance:  1.7336579495895945e-08 \n",
      "\n",
      "Epoch:  15088  Learning Rate:  2.8041365070505544e-08  Varinance:  1.7312360929800473e-08 \n",
      "\n",
      "Epoch:  15089  Learning Rate:  2.8013337721445194e-08  Varinance:  1.7288176196154066e-08 \n",
      "\n",
      "Epoch:  15090  Learning Rate:  2.7985338385724855e-08  Varinance:  1.7264025247694086e-08 \n",
      "\n",
      "Epoch:  15091  Learning Rate:  2.795736703534528e-08  Varinance:  1.7239908037223867e-08 \n",
      "\n",
      "Epoch:  15092  Learning Rate:  2.7929423642335075e-08  Varinance:  1.721582451761268e-08 \n",
      "\n",
      "Epoch:  15093  Learning Rate:  2.7901508178750785e-08  Varinance:  1.719177464179562e-08 \n",
      "\n",
      "Epoch:  15094  Learning Rate:  2.787362061667705e-08  Varinance:  1.7167758362773548e-08 \n",
      "\n",
      "Epoch:  15095  Learning Rate:  2.7845760928226256e-08  Varinance:  1.714377563361303e-08 \n",
      "\n",
      "Epoch:  15096  Learning Rate:  2.781792908553866e-08  Varinance:  1.7119826407446017e-08 \n",
      "\n",
      "Epoch:  15097  Learning Rate:  2.7790125060782515e-08  Varinance:  1.7095910637470118e-08 \n",
      "\n",
      "Epoch:  15098  Learning Rate:  2.776234882615375e-08  Varinance:  1.7072028276948263e-08 \n",
      "\n",
      "Epoch:  15099  Learning Rate:  2.773460035387607e-08  Varinance:  1.704817927920866e-08 \n",
      "\n",
      "Epoch:  15100  Learning Rate:  2.7706879616201112e-08  Varinance:  1.7024363597644736e-08 \n",
      "\n",
      "Epoch:  15101  Learning Rate:  2.7679186585408073e-08  Varinance:  1.7000581185715008e-08 \n",
      "\n",
      "Epoch:  15102  Learning Rate:  2.765152123380388e-08  Varinance:  1.6976831996943074e-08 \n",
      "\n",
      "Epoch:  15103  Learning Rate:  2.7623883533723274e-08  Varinance:  1.695311598491729e-08 \n",
      "\n",
      "Epoch:  15104  Learning Rate:  2.75962734575285e-08  Varinance:  1.692943310329101e-08 \n",
      "\n",
      "Epoch:  15105  Learning Rate:  2.7568690977609486e-08  Varinance:  1.6905783305782282e-08 \n",
      "\n",
      "Epoch:  15106  Learning Rate:  2.75411360663837e-08  Varinance:  1.6882166546173808e-08 \n",
      "\n",
      "Epoch:  15107  Learning Rate:  2.7513608696296325e-08  Varinance:  1.6858582778312848e-08 \n",
      "\n",
      "Epoch:  15108  Learning Rate:  2.7486108839819936e-08  Varinance:  1.6835031956111145e-08 \n",
      "\n",
      "Epoch:  15109  Learning Rate:  2.7458636469454628e-08  Varinance:  1.681151403354488e-08 \n",
      "\n",
      "Epoch:  15110  Learning Rate:  2.7431191557728127e-08  Varinance:  1.6788028964654348e-08 \n",
      "\n",
      "Epoch:  15111  Learning Rate:  2.7403774077195472e-08  Varinance:  1.6764576703544225e-08 \n",
      "\n",
      "Epoch:  15112  Learning Rate:  2.737638400043913e-08  Varinance:  1.6741157204383245e-08 \n",
      "\n",
      "Epoch:  15113  Learning Rate:  2.734902130006911e-08  Varinance:  1.671777042140417e-08 \n",
      "\n",
      "Epoch:  15114  Learning Rate:  2.732168594872268e-08  Varinance:  1.669441630890369e-08 \n",
      "\n",
      "Epoch:  15115  Learning Rate:  2.729437791906442e-08  Varinance:  1.6671094821242344e-08 \n",
      "\n",
      "Epoch:  15116  Learning Rate:  2.7267097183786403e-08  Varinance:  1.6647805912844488e-08 \n",
      "\n",
      "Epoch:  15117  Learning Rate:  2.7239843715607845e-08  Varinance:  1.6624549538197965e-08 \n",
      "\n",
      "Epoch:  15118  Learning Rate:  2.7212617487275225e-08  Varinance:  1.6601325651854376e-08 \n",
      "\n",
      "Epoch:  15119  Learning Rate:  2.7185418471562402e-08  Varinance:  1.6578134208428756e-08 \n",
      "\n",
      "Epoch:  15120  Learning Rate:  2.715824664127032e-08  Varinance:  1.6554975162599535e-08 \n",
      "\n",
      "Epoch:  15121  Learning Rate:  2.713110196922714e-08  Varinance:  1.653184846910846e-08 \n",
      "\n",
      "Epoch:  15122  Learning Rate:  2.7103984428288148e-08  Varinance:  1.6508754082760496e-08 \n",
      "\n",
      "Epoch:  15123  Learning Rate:  2.7076893991335886e-08  Varinance:  1.6485691958423812e-08 \n",
      "\n",
      "Epoch:  15124  Learning Rate:  2.7049830631279874e-08  Varinance:  1.6462662051029442e-08 \n",
      "\n",
      "Epoch:  15125  Learning Rate:  2.7022794321056702e-08  Varinance:  1.6439664315571557e-08 \n",
      "\n",
      "Epoch:  15126  Learning Rate:  2.6995785033630143e-08  Varinance:  1.6416698707107138e-08 \n",
      "\n",
      "Epoch:  15127  Learning Rate:  2.6968802741990874e-08  Varinance:  1.6393765180755956e-08 \n",
      "\n",
      "Epoch:  15128  Learning Rate:  2.6941847419156542e-08  Varinance:  1.6370863691700476e-08 \n",
      "\n",
      "Epoch:  15129  Learning Rate:  2.6914919038171924e-08  Varinance:  1.6347994195185766e-08 \n",
      "\n",
      "Epoch:  15130  Learning Rate:  2.688801757210859e-08  Varinance:  1.6325156646519485e-08 \n",
      "\n",
      "Epoch:  15131  Learning Rate:  2.686114299406502e-08  Varinance:  1.6302351001071532e-08 \n",
      "\n",
      "Epoch:  15132  Learning Rate:  2.6834295277166725e-08  Varinance:  1.627957721427435e-08 \n",
      "\n",
      "Epoch:  15133  Learning Rate:  2.680747439456595e-08  Varinance:  1.6256835241622562e-08 \n",
      "\n",
      "Epoch:  15134  Learning Rate:  2.67806803194418e-08  Varinance:  1.6234125038672982e-08 \n",
      "\n",
      "Epoch:  15135  Learning Rate:  2.6753913025000152e-08  Varinance:  1.621144656104449e-08 \n",
      "\n",
      "Epoch:  15136  Learning Rate:  2.672717248447381e-08  Varinance:  1.6188799764417985e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15137  Learning Rate:  2.670045867112218e-08  Varinance:  1.616618460453632e-08 \n",
      "\n",
      "Epoch:  15138  Learning Rate:  2.6673771558231394e-08  Varinance:  1.614360103720401e-08 \n",
      "\n",
      "Epoch:  15139  Learning Rate:  2.664711111911444e-08  Varinance:  1.6121049018287483e-08 \n",
      "\n",
      "Epoch:  15140  Learning Rate:  2.6620477327110826e-08  Varinance:  1.6098528503714747e-08 \n",
      "\n",
      "Epoch:  15141  Learning Rate:  2.659387015558671e-08  Varinance:  1.60760394494754e-08 \n",
      "\n",
      "Epoch:  15142  Learning Rate:  2.656728957793501e-08  Varinance:  1.6053581811620503e-08 \n",
      "\n",
      "Epoch:  15143  Learning Rate:  2.6540735567575103e-08  Varinance:  1.603115554626252e-08 \n",
      "\n",
      "Epoch:  15144  Learning Rate:  2.6514208097952927e-08  Varinance:  1.600876060957528e-08 \n",
      "\n",
      "Epoch:  15145  Learning Rate:  2.6487707142541108e-08  Varinance:  1.5986396957793664e-08 \n",
      "\n",
      "Epoch:  15146  Learning Rate:  2.646123267483864e-08  Varinance:  1.5964064547213864e-08 \n",
      "\n",
      "Epoch:  15147  Learning Rate:  2.6434784668371006e-08  Varinance:  1.5941763334193063e-08 \n",
      "\n",
      "Epoch:  15148  Learning Rate:  2.6408363096690284e-08  Varinance:  1.591949327514942e-08 \n",
      "\n",
      "Epoch:  15149  Learning Rate:  2.638196793337486e-08  Varinance:  1.5897254326561964e-08 \n",
      "\n",
      "Epoch:  15150  Learning Rate:  2.6355599152029575e-08  Varinance:  1.5875046444970534e-08 \n",
      "\n",
      "Epoch:  15151  Learning Rate:  2.6329256726285584e-08  Varinance:  1.5852869586975728e-08 \n",
      "\n",
      "Epoch:  15152  Learning Rate:  2.6302940629800563e-08  Varinance:  1.5830723709238606e-08 \n",
      "\n",
      "Epoch:  15153  Learning Rate:  2.627665083625837e-08  Varinance:  1.5808608768480942e-08 \n",
      "\n",
      "Epoch:  15154  Learning Rate:  2.6250387319369145e-08  Varinance:  1.578652472148491e-08 \n",
      "\n",
      "Epoch:  15155  Learning Rate:  2.6224150052869482e-08  Varinance:  1.5764471525093058e-08 \n",
      "\n",
      "Epoch:  15156  Learning Rate:  2.6197939010522053e-08  Varinance:  1.5742449136208216e-08 \n",
      "\n",
      "Epoch:  15157  Learning Rate:  2.6171754166115773e-08  Varinance:  1.572045751179343e-08 \n",
      "\n",
      "Epoch:  15158  Learning Rate:  2.614559549346589e-08  Varinance:  1.5698496608871924e-08 \n",
      "\n",
      "Epoch:  15159  Learning Rate:  2.611946296641367e-08  Varinance:  1.5676566384526782e-08 \n",
      "\n",
      "Epoch:  15160  Learning Rate:  2.609335655882655e-08  Varinance:  1.565466679590121e-08 \n",
      "\n",
      "Epoch:  15161  Learning Rate:  2.6067276244598217e-08  Varinance:  1.5632797800198236e-08 \n",
      "\n",
      "Epoch:  15162  Learning Rate:  2.6041221997648292e-08  Varinance:  1.5610959354680667e-08 \n",
      "\n",
      "Epoch:  15163  Learning Rate:  2.6015193791922493e-08  Varinance:  1.558915141667101e-08 \n",
      "\n",
      "Epoch:  15164  Learning Rate:  2.5989191601392698e-08  Varinance:  1.5567373943551398e-08 \n",
      "\n",
      "Epoch:  15165  Learning Rate:  2.596321540005667e-08  Varinance:  1.5545626892763554e-08 \n",
      "\n",
      "Epoch:  15166  Learning Rate:  2.593726516193821e-08  Varinance:  1.552391022180848e-08 \n",
      "\n",
      "Epoch:  15167  Learning Rate:  2.5911340861087026e-08  Varinance:  1.5502223888246722e-08 \n",
      "\n",
      "Epoch:  15168  Learning Rate:  2.5885442471578905e-08  Varinance:  1.5480567849698053e-08 \n",
      "\n",
      "Epoch:  15169  Learning Rate:  2.585956996751541e-08  Varinance:  1.545894206384145e-08 \n",
      "\n",
      "Epoch:  15170  Learning Rate:  2.5833723323023998e-08  Varinance:  1.543734648841501e-08 \n",
      "\n",
      "Epoch:  15171  Learning Rate:  2.5807902512258108e-08  Varinance:  1.5415781081215865e-08 \n",
      "\n",
      "Epoch:  15172  Learning Rate:  2.578210750939688e-08  Varinance:  1.5394245800100173e-08 \n",
      "\n",
      "Epoch:  15173  Learning Rate:  2.5756338288645258e-08  Varinance:  1.537274060298278e-08 \n",
      "\n",
      "Epoch:  15174  Learning Rate:  2.573059482423412e-08  Varinance:  1.5351265447837504e-08 \n",
      "\n",
      "Epoch:  15175  Learning Rate:  2.5704877090419955e-08  Varinance:  1.532982029269681e-08 \n",
      "\n",
      "Epoch:  15176  Learning Rate:  2.5679185061484974e-08  Varinance:  1.5308405095651792e-08 \n",
      "\n",
      "Epoch:  15177  Learning Rate:  2.5653518711737236e-08  Varinance:  1.5287019814852084e-08 \n",
      "\n",
      "Epoch:  15178  Learning Rate:  2.5627878015510355e-08  Varinance:  1.526566440850579e-08 \n",
      "\n",
      "Epoch:  15179  Learning Rate:  2.5602262947163575e-08  Varinance:  1.524433883487945e-08 \n",
      "\n",
      "Epoch:  15180  Learning Rate:  2.557667348108192e-08  Varinance:  1.5223043052297737e-08 \n",
      "\n",
      "Epoch:  15181  Learning Rate:  2.555110959167588e-08  Varinance:  1.520177701914371e-08 \n",
      "\n",
      "Epoch:  15182  Learning Rate:  2.552557125338156e-08  Varinance:  1.5180540693858504e-08 \n",
      "\n",
      "Epoch:  15183  Learning Rate:  2.5500058440660578e-08  Varinance:  1.515933403494132e-08 \n",
      "\n",
      "Epoch:  15184  Learning Rate:  2.5474571128000205e-08  Varinance:  1.5138157000949332e-08 \n",
      "\n",
      "Epoch:  15185  Learning Rate:  2.5449109289913084e-08  Varinance:  1.5117009550497597e-08 \n",
      "\n",
      "Epoch:  15186  Learning Rate:  2.5423672900937325e-08  Varinance:  1.5095891642259053e-08 \n",
      "\n",
      "Epoch:  15187  Learning Rate:  2.5398261935636628e-08  Varinance:  1.5074803234964204e-08 \n",
      "\n",
      "Epoch:  15188  Learning Rate:  2.537287636859999e-08  Varinance:  1.5053744287401364e-08 \n",
      "\n",
      "Epoch:  15189  Learning Rate:  2.5347516174441788e-08  Varinance:  1.5032714758416368e-08 \n",
      "\n",
      "Epoch:  15190  Learning Rate:  2.5322181327801914e-08  Varinance:  1.501171460691254e-08 \n",
      "\n",
      "Epoch:  15191  Learning Rate:  2.5296871803345483e-08  Varinance:  1.4990743791850616e-08 \n",
      "\n",
      "Epoch:  15192  Learning Rate:  2.5271587575762917e-08  Varinance:  1.496980227224867e-08 \n",
      "\n",
      "Epoch:  15193  Learning Rate:  2.5246328619770075e-08  Varinance:  1.4948890007182057e-08 \n",
      "\n",
      "Epoch:  15194  Learning Rate:  2.5221094910107965e-08  Varinance:  1.4928006955783165e-08 \n",
      "\n",
      "Epoch:  15195  Learning Rate:  2.5195886421542814e-08  Varinance:  1.4907153077241628e-08 \n",
      "\n",
      "Epoch:  15196  Learning Rate:  2.517070312886623e-08  Varinance:  1.4886328330804026e-08 \n",
      "\n",
      "Epoch:  15197  Learning Rate:  2.514554500689487e-08  Varinance:  1.486553267577388e-08 \n",
      "\n",
      "Epoch:  15198  Learning Rate:  2.512041203047062e-08  Varinance:  1.4844766071511562e-08 \n",
      "\n",
      "Epoch:  15199  Learning Rate:  2.5095304174460446e-08  Varinance:  1.4824028477434214e-08 \n",
      "\n",
      "Epoch:  15200  Learning Rate:  2.5070221413756583e-08  Varinance:  1.4803319853015722e-08 \n",
      "\n",
      "Epoch:  15201  Learning Rate:  2.5045163723276222e-08  Varinance:  1.4782640157786429e-08 \n",
      "\n",
      "Epoch:  15202  Learning Rate:  2.502013107796162e-08  Varinance:  1.4761989351333371e-08 \n",
      "\n",
      "Epoch:  15203  Learning Rate:  2.4995123452780234e-08  Varinance:  1.474136739329999e-08 \n",
      "\n",
      "Epoch:  15204  Learning Rate:  2.4970140822724382e-08  Varinance:  1.4720774243386094e-08 \n",
      "\n",
      "Epoch:  15205  Learning Rate:  2.4945183162811387e-08  Varinance:  1.4700209861347802e-08 \n",
      "\n",
      "Epoch:  15206  Learning Rate:  2.492025044808368e-08  Varinance:  1.4679674206997445e-08 \n",
      "\n",
      "Epoch:  15207  Learning Rate:  2.48953426536085e-08  Varinance:  1.4659167240203548e-08 \n",
      "\n",
      "Epoch:  15208  Learning Rate:  2.4870459754478e-08  Varinance:  1.463868892089054e-08 \n",
      "\n",
      "Epoch:  15209  Learning Rate:  2.484560172580937e-08  Varinance:  1.4618239209038995e-08 \n",
      "\n",
      "Epoch:  15210  Learning Rate:  2.482076854274454e-08  Varinance:  1.4597818064685336e-08 \n",
      "\n",
      "Epoch:  15211  Learning Rate:  2.4795960180450277e-08  Varinance:  1.4577425447921816e-08 \n",
      "\n",
      "Epoch:  15212  Learning Rate:  2.4771176614118303e-08  Varinance:  1.4557061318896437e-08 \n",
      "\n",
      "Epoch:  15213  Learning Rate:  2.4746417818965008e-08  Varinance:  1.4536725637812929e-08 \n",
      "\n",
      "Epoch:  15214  Learning Rate:  2.4721683770231593e-08  Varinance:  1.4516418364930447e-08 \n",
      "\n",
      "Epoch:  15215  Learning Rate:  2.4696974443183968e-08  Varinance:  1.4496139460563836e-08 \n",
      "\n",
      "Epoch:  15216  Learning Rate:  2.467228981311288e-08  Varinance:  1.4475888885083317e-08 \n",
      "\n",
      "Epoch:  15217  Learning Rate:  2.4647629855333673e-08  Varinance:  1.4455666598914471e-08 \n",
      "\n",
      "Epoch:  15218  Learning Rate:  2.4622994545186324e-08  Varinance:  1.4435472562538171e-08 \n",
      "\n",
      "Epoch:  15219  Learning Rate:  2.4598383858035617e-08  Varinance:  1.441530673649049e-08 \n",
      "\n",
      "Epoch:  15220  Learning Rate:  2.4573797769270817e-08  Varinance:  1.4395169081362683e-08 \n",
      "\n",
      "Epoch:  15221  Learning Rate:  2.4549236254305793e-08  Varinance:  1.4375059557800908e-08 \n",
      "\n",
      "Epoch:  15222  Learning Rate:  2.4524699288579112e-08  Varinance:  1.4354978126506448e-08 \n",
      "\n",
      "Epoch:  15223  Learning Rate:  2.4500186847553764e-08  Varinance:  1.4334924748235436e-08 \n",
      "\n",
      "Epoch:  15224  Learning Rate:  2.447569890671726e-08  Varinance:  1.4314899383798824e-08 \n",
      "\n",
      "Epoch:  15225  Learning Rate:  2.4451235441581745e-08  Varinance:  1.4294901994062317e-08 \n",
      "\n",
      "Epoch:  15226  Learning Rate:  2.442679642768371e-08  Varinance:  1.427493253994628e-08 \n",
      "\n",
      "Epoch:  15227  Learning Rate:  2.4402381840584093e-08  Varinance:  1.4254990982425729e-08 \n",
      "\n",
      "Epoch:  15228  Learning Rate:  2.43779916558684e-08  Varinance:  1.4235077282530036e-08 \n",
      "\n",
      "Epoch:  15229  Learning Rate:  2.4353625849146388e-08  Varinance:  1.421519140134318e-08 \n",
      "\n",
      "Epoch:  15230  Learning Rate:  2.4329284396052254e-08  Varinance:  1.4195333300003434e-08 \n",
      "\n",
      "Epoch:  15231  Learning Rate:  2.4304967272244503e-08  Varinance:  1.4175502939703375e-08 \n",
      "\n",
      "Epoch:  15232  Learning Rate:  2.4280674453406095e-08  Varinance:  1.415570028168979e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15233  Learning Rate:  2.4256405915244162e-08  Varinance:  1.4135925287263595e-08 \n",
      "\n",
      "Epoch:  15234  Learning Rate:  2.423216163349012e-08  Varinance:  1.4116177917779826e-08 \n",
      "\n",
      "Epoch:  15235  Learning Rate:  2.4207941583899775e-08  Varinance:  1.409645813464735e-08 \n",
      "\n",
      "Epoch:  15236  Learning Rate:  2.418374574225303e-08  Varinance:  1.4076765899329094e-08 \n",
      "\n",
      "Epoch:  15237  Learning Rate:  2.4159574084354005e-08  Varinance:  1.4057101173341773e-08 \n",
      "\n",
      "Epoch:  15238  Learning Rate:  2.413542658603112e-08  Varinance:  1.403746391825586e-08 \n",
      "\n",
      "Epoch:  15239  Learning Rate:  2.4111303223136828e-08  Varinance:  1.4017854095695511e-08 \n",
      "\n",
      "Epoch:  15240  Learning Rate:  2.4087203971547725e-08  Varinance:  1.3998271667338495e-08 \n",
      "\n",
      "Epoch:  15241  Learning Rate:  2.4063128807164648e-08  Varinance:  1.3978716594916162e-08 \n",
      "\n",
      "Epoch:  15242  Learning Rate:  2.4039077705912384e-08  Varinance:  1.395918884021318e-08 \n",
      "\n",
      "Epoch:  15243  Learning Rate:  2.401505064373978e-08  Varinance:  1.3939688365067737e-08 \n",
      "\n",
      "Epoch:  15244  Learning Rate:  2.3991047596619864e-08  Varinance:  1.3920215131371299e-08 \n",
      "\n",
      "Epoch:  15245  Learning Rate:  2.396706854054955e-08  Varinance:  1.3900769101068556e-08 \n",
      "\n",
      "Epoch:  15246  Learning Rate:  2.394311345154977e-08  Varinance:  1.3881350236157364e-08 \n",
      "\n",
      "Epoch:  15247  Learning Rate:  2.3919182305665397e-08  Varinance:  1.3861958498688671e-08 \n",
      "\n",
      "Epoch:  15248  Learning Rate:  2.3895275078965366e-08  Varinance:  1.3842593850766476e-08 \n",
      "\n",
      "Epoch:  15249  Learning Rate:  2.38713917475424e-08  Varinance:  1.3823256254547585e-08 \n",
      "\n",
      "Epoch:  15250  Learning Rate:  2.384753228751313e-08  Varinance:  1.3803945672241802e-08 \n",
      "\n",
      "Epoch:  15251  Learning Rate:  2.3823696675018182e-08  Varinance:  1.3784662066111688e-08 \n",
      "\n",
      "Epoch:  15252  Learning Rate:  2.379988488622189e-08  Varinance:  1.3765405398472508e-08 \n",
      "\n",
      "Epoch:  15253  Learning Rate:  2.3776096897312427e-08  Varinance:  1.3746175631692184e-08 \n",
      "\n",
      "Epoch:  15254  Learning Rate:  2.3752332684501883e-08  Varinance:  1.3726972728191196e-08 \n",
      "\n",
      "Epoch:  15255  Learning Rate:  2.3728592224026003e-08  Varinance:  1.3707796650442578e-08 \n",
      "\n",
      "Epoch:  15256  Learning Rate:  2.3704875492144283e-08  Varinance:  1.3688647360971641e-08 \n",
      "\n",
      "Epoch:  15257  Learning Rate:  2.3681182465140074e-08  Varinance:  1.366952482235619e-08 \n",
      "\n",
      "Epoch:  15258  Learning Rate:  2.3657513119320303e-08  Varinance:  1.3650428997226262e-08 \n",
      "\n",
      "Epoch:  15259  Learning Rate:  2.363386743101562e-08  Varinance:  1.3631359848264098e-08 \n",
      "\n",
      "Epoch:  15260  Learning Rate:  2.36102453765803e-08  Varinance:  1.3612317338204066e-08 \n",
      "\n",
      "Epoch:  15261  Learning Rate:  2.3586646932392363e-08  Varinance:  1.3593301429832598e-08 \n",
      "\n",
      "Epoch:  15262  Learning Rate:  2.3563072074853324e-08  Varinance:  1.3574312085988161e-08 \n",
      "\n",
      "Epoch:  15263  Learning Rate:  2.3539520780388283e-08  Varinance:  1.3555349269560983e-08 \n",
      "\n",
      "Epoch:  15264  Learning Rate:  2.3515993025446023e-08  Varinance:  1.3536412943493284e-08 \n",
      "\n",
      "Epoch:  15265  Learning Rate:  2.349248878649875e-08  Varinance:  1.3517503070779006e-08 \n",
      "\n",
      "Epoch:  15266  Learning Rate:  2.346900804004218e-08  Varinance:  1.3498619614463778e-08 \n",
      "\n",
      "Epoch:  15267  Learning Rate:  2.344555076259565e-08  Varinance:  1.3479762537644863e-08 \n",
      "\n",
      "Epoch:  15268  Learning Rate:  2.3422116930701833e-08  Varinance:  1.3460931803471074e-08 \n",
      "\n",
      "Epoch:  15269  Learning Rate:  2.3398706520926856e-08  Varinance:  1.3442127375142746e-08 \n",
      "\n",
      "Epoch:  15270  Learning Rate:  2.3375319509860393e-08  Varinance:  1.342334921591148e-08 \n",
      "\n",
      "Epoch:  15271  Learning Rate:  2.3351955874115388e-08  Varinance:  1.3404597289080361e-08 \n",
      "\n",
      "Epoch:  15272  Learning Rate:  2.332861559032816e-08  Varinance:  1.3385871558003687e-08 \n",
      "\n",
      "Epoch:  15273  Learning Rate:  2.3305298635158513e-08  Varinance:  1.3367171986086946e-08 \n",
      "\n",
      "Epoch:  15274  Learning Rate:  2.3282004985289434e-08  Varinance:  1.3348498536786753e-08 \n",
      "\n",
      "Epoch:  15275  Learning Rate:  2.3258734617427287e-08  Varinance:  1.3329851173610771e-08 \n",
      "\n",
      "Epoch:  15276  Learning Rate:  2.3235487508301655e-08  Varinance:  1.3311229860117687e-08 \n",
      "\n",
      "Epoch:  15277  Learning Rate:  2.3212263634665506e-08  Varinance:  1.3292634559916955e-08 \n",
      "\n",
      "Epoch:  15278  Learning Rate:  2.3189062973294928e-08  Varinance:  1.3274065236669005e-08 \n",
      "\n",
      "Epoch:  15279  Learning Rate:  2.3165885500989214e-08  Varinance:  1.3255521854084987e-08 \n",
      "\n",
      "Epoch:  15280  Learning Rate:  2.3142731194570973e-08  Varinance:  1.3237004375926744e-08 \n",
      "\n",
      "Epoch:  15281  Learning Rate:  2.3119600030885855e-08  Varinance:  1.3218512766006742e-08 \n",
      "\n",
      "Epoch:  15282  Learning Rate:  2.3096491986802653e-08  Varinance:  1.3200046988188004e-08 \n",
      "\n",
      "Epoch:  15283  Learning Rate:  2.3073407039213402e-08  Varinance:  1.3181607006384073e-08 \n",
      "\n",
      "Epoch:  15284  Learning Rate:  2.3050345165033112e-08  Varinance:  1.316319278455877e-08 \n",
      "\n",
      "Epoch:  15285  Learning Rate:  2.3027306341199873e-08  Varinance:  1.31448042867264e-08 \n",
      "\n",
      "Epoch:  15286  Learning Rate:  2.300429054467493e-08  Varinance:  1.312644147695148e-08 \n",
      "\n",
      "Epoch:  15287  Learning Rate:  2.298129775244245e-08  Varinance:  1.3108104319348741e-08 \n",
      "\n",
      "Epoch:  15288  Learning Rate:  2.2958327941509594e-08  Varinance:  1.3089792778083038e-08 \n",
      "\n",
      "Epoch:  15289  Learning Rate:  2.293538108890664e-08  Varinance:  1.3071506817369285e-08 \n",
      "\n",
      "Epoch:  15290  Learning Rate:  2.291245717168668e-08  Varinance:  1.3053246401472438e-08 \n",
      "\n",
      "Epoch:  15291  Learning Rate:  2.2889556166925805e-08  Varinance:  1.303501149470723e-08 \n",
      "\n",
      "Epoch:  15292  Learning Rate:  2.286667805172296e-08  Varinance:  1.3016802061438382e-08 \n",
      "\n",
      "Epoch:  15293  Learning Rate:  2.2843822803200116e-08  Varinance:  1.2998618066080359e-08 \n",
      "\n",
      "Epoch:  15294  Learning Rate:  2.282099039850198e-08  Varinance:  1.298045947309733e-08 \n",
      "\n",
      "Epoch:  15295  Learning Rate:  2.27981808147961e-08  Varinance:  1.2962326247003106e-08 \n",
      "\n",
      "Epoch:  15296  Learning Rate:  2.2775394029272976e-08  Varinance:  1.2944218352361077e-08 \n",
      "\n",
      "Epoch:  15297  Learning Rate:  2.275263001914578e-08  Varinance:  1.2926135753784175e-08 \n",
      "\n",
      "Epoch:  15298  Learning Rate:  2.272988876165046e-08  Varinance:  1.2908078415934634e-08 \n",
      "\n",
      "Epoch:  15299  Learning Rate:  2.2707170234045835e-08  Varinance:  1.2890046303524191e-08 \n",
      "\n",
      "Epoch:  15300  Learning Rate:  2.2684474413613337e-08  Varinance:  1.287203938131383e-08 \n",
      "\n",
      "Epoch:  15301  Learning Rate:  2.26618012776571e-08  Varinance:  1.2854057614113763e-08 \n",
      "\n",
      "Epoch:  15302  Learning Rate:  2.2639150803504073e-08  Varinance:  1.2836100966783369e-08 \n",
      "\n",
      "Epoch:  15303  Learning Rate:  2.2616522968503735e-08  Varinance:  1.2818169404231107e-08 \n",
      "\n",
      "Epoch:  15304  Learning Rate:  2.2593917750028207e-08  Varinance:  1.2800262891414507e-08 \n",
      "\n",
      "Epoch:  15305  Learning Rate:  2.257133512547236e-08  Varinance:  1.2782381393339913e-08 \n",
      "\n",
      "Epoch:  15306  Learning Rate:  2.2548775072253512e-08  Varinance:  1.2764524875062698e-08 \n",
      "\n",
      "Epoch:  15307  Learning Rate:  2.2526237567811615e-08  Varinance:  1.2746693301686997e-08 \n",
      "\n",
      "Epoch:  15308  Learning Rate:  2.2503722589609124e-08  Varinance:  1.2728886638365699e-08 \n",
      "\n",
      "Epoch:  15309  Learning Rate:  2.2481230115131142e-08  Varinance:  1.2711104850300369e-08 \n",
      "\n",
      "Epoch:  15310  Learning Rate:  2.2458760121885145e-08  Varinance:  1.2693347902741187e-08 \n",
      "\n",
      "Epoch:  15311  Learning Rate:  2.2436312587401103e-08  Varinance:  1.267561576098692e-08 \n",
      "\n",
      "Epoch:  15312  Learning Rate:  2.2413887489231558e-08  Varinance:  1.2657908390384683e-08 \n",
      "\n",
      "Epoch:  15313  Learning Rate:  2.2391484804951368e-08  Varinance:  1.2640225756330123e-08 \n",
      "\n",
      "Epoch:  15314  Learning Rate:  2.236910451215781e-08  Varinance:  1.2622567824267196e-08 \n",
      "\n",
      "Epoch:  15315  Learning Rate:  2.234674658847067e-08  Varinance:  1.2604934559688119e-08 \n",
      "\n",
      "Epoch:  15316  Learning Rate:  2.2324411011531977e-08  Varinance:  1.2587325928133326e-08 \n",
      "\n",
      "Epoch:  15317  Learning Rate:  2.2302097759006118e-08  Varinance:  1.256974189519138e-08 \n",
      "\n",
      "Epoch:  15318  Learning Rate:  2.2279806808579917e-08  Varinance:  1.255218242649897e-08 \n",
      "\n",
      "Epoch:  15319  Learning Rate:  2.2257538137962382e-08  Varinance:  1.2534647487740649e-08 \n",
      "\n",
      "Epoch:  15320  Learning Rate:  2.2235291724884798e-08  Varinance:  1.251713704464904e-08 \n",
      "\n",
      "Epoch:  15321  Learning Rate:  2.2213067547100834e-08  Varinance:  1.24996510630046e-08 \n",
      "\n",
      "Epoch:  15322  Learning Rate:  2.2190865582386268e-08  Varinance:  1.248218950863558e-08 \n",
      "\n",
      "Epoch:  15323  Learning Rate:  2.216868580853913e-08  Varinance:  1.2464752347417971e-08 \n",
      "\n",
      "Epoch:  15324  Learning Rate:  2.214652820337961e-08  Varinance:  1.2447339545275438e-08 \n",
      "\n",
      "Epoch:  15325  Learning Rate:  2.2124392744750178e-08  Varinance:  1.2429951068179285e-08 \n",
      "\n",
      "Epoch:  15326  Learning Rate:  2.2102279410515336e-08  Varinance:  1.241258688214823e-08 \n",
      "\n",
      "Epoch:  15327  Learning Rate:  2.208018817856171e-08  Varinance:  1.2395246953248591e-08 \n",
      "\n",
      "Epoch:  15328  Learning Rate:  2.2058119026798134e-08  Varinance:  1.237793124759404e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15329  Learning Rate:  2.2036071933155428e-08  Varinance:  1.236063973134559e-08 \n",
      "\n",
      "Epoch:  15330  Learning Rate:  2.201404687558645e-08  Varinance:  1.2343372370711534e-08 \n",
      "\n",
      "Epoch:  15331  Learning Rate:  2.1992043832066226e-08  Varinance:  1.2326129131947362e-08 \n",
      "\n",
      "Epoch:  15332  Learning Rate:  2.197006278059166e-08  Varinance:  1.2308909981355745e-08 \n",
      "\n",
      "Epoch:  15333  Learning Rate:  2.1948103699181674e-08  Varinance:  1.2291714885286308e-08 \n",
      "\n",
      "Epoch:  15334  Learning Rate:  2.192616656587725e-08  Varinance:  1.2274543810135804e-08 \n",
      "\n",
      "Epoch:  15335  Learning Rate:  2.190425135874122e-08  Varinance:  1.2257396722347892e-08 \n",
      "\n",
      "Epoch:  15336  Learning Rate:  2.1882358055858334e-08  Varinance:  1.2240273588413104e-08 \n",
      "\n",
      "Epoch:  15337  Learning Rate:  2.1860486635335367e-08  Varinance:  1.2223174374868788e-08 \n",
      "\n",
      "Epoch:  15338  Learning Rate:  2.183863707530086e-08  Varinance:  1.2206099048299033e-08 \n",
      "\n",
      "Epoch:  15339  Learning Rate:  2.1816809353905243e-08  Varinance:  1.2189047575334653e-08 \n",
      "\n",
      "Epoch:  15340  Learning Rate:  2.1795003449320765e-08  Varinance:  1.2172019922652953e-08 \n",
      "\n",
      "Epoch:  15341  Learning Rate:  2.1773219339741586e-08  Varinance:  1.2155016056977915e-08 \n",
      "\n",
      "Epoch:  15342  Learning Rate:  2.1751457003383563e-08  Varinance:  1.2138035945079956e-08 \n",
      "\n",
      "Epoch:  15343  Learning Rate:  2.172971641848432e-08  Varinance:  1.2121079553775925e-08 \n",
      "\n",
      "Epoch:  15344  Learning Rate:  2.170799756330334e-08  Varinance:  1.2104146849929023e-08 \n",
      "\n",
      "Epoch:  15345  Learning Rate:  2.1686300416121736e-08  Varinance:  1.208723780044878e-08 \n",
      "\n",
      "Epoch:  15346  Learning Rate:  2.1664624955242318e-08  Varinance:  1.207035237229083e-08 \n",
      "\n",
      "Epoch:  15347  Learning Rate:  2.1642971158989697e-08  Varinance:  1.2053490532457091e-08 \n",
      "\n",
      "Epoch:  15348  Learning Rate:  2.162133900571004e-08  Varinance:  1.2036652247995544e-08 \n",
      "\n",
      "Epoch:  15349  Learning Rate:  2.1599728473771148e-08  Varinance:  1.201983748600019e-08 \n",
      "\n",
      "Epoch:  15350  Learning Rate:  2.157813954156257e-08  Varinance:  1.2003046213611014e-08 \n",
      "\n",
      "Epoch:  15351  Learning Rate:  2.1556572187495334e-08  Varinance:  1.1986278398013891e-08 \n",
      "\n",
      "Epoch:  15352  Learning Rate:  2.1535026390002042e-08  Varinance:  1.1969534006440586e-08 \n",
      "\n",
      "Epoch:  15353  Learning Rate:  2.1513502127536973e-08  Varinance:  1.1952813006168515e-08 \n",
      "\n",
      "Epoch:  15354  Learning Rate:  2.149199937857582e-08  Varinance:  1.1936115364520929e-08 \n",
      "\n",
      "Epoch:  15355  Learning Rate:  2.1470518121615845e-08  Varinance:  1.191944104886668e-08 \n",
      "\n",
      "Epoch:  15356  Learning Rate:  2.1449058335175736e-08  Varinance:  1.1902790026620216e-08 \n",
      "\n",
      "Epoch:  15357  Learning Rate:  2.142761999779579e-08  Varinance:  1.1886162265241499e-08 \n",
      "\n",
      "Epoch:  15358  Learning Rate:  2.140620308803763e-08  Varinance:  1.186955773223595e-08 \n",
      "\n",
      "Epoch:  15359  Learning Rate:  2.1384807584484303e-08  Varinance:  1.1852976395154422e-08 \n",
      "\n",
      "Epoch:  15360  Learning Rate:  2.1363433465740378e-08  Varinance:  1.183641822159298e-08 \n",
      "\n",
      "Epoch:  15361  Learning Rate:  2.1342080710431698e-08  Varinance:  1.1819883179193074e-08 \n",
      "\n",
      "Epoch:  15362  Learning Rate:  2.1320749297205473e-08  Varinance:  1.180337123564132e-08 \n",
      "\n",
      "Epoch:  15363  Learning Rate:  2.1299439204730357e-08  Varinance:  1.1786882358669473e-08 \n",
      "\n",
      "Epoch:  15364  Learning Rate:  2.1278150411696223e-08  Varinance:  1.1770416516054369e-08 \n",
      "\n",
      "Epoch:  15365  Learning Rate:  2.1256882896814233e-08  Varinance:  1.1753973675617854e-08 \n",
      "\n",
      "Epoch:  15366  Learning Rate:  2.123563663881695e-08  Varinance:  1.173755380522677e-08 \n",
      "\n",
      "Epoch:  15367  Learning Rate:  2.1214411616458075e-08  Varinance:  1.1721156872792721e-08 \n",
      "\n",
      "Epoch:  15368  Learning Rate:  2.119320780851255e-08  Varinance:  1.1704782846272265e-08 \n",
      "\n",
      "Epoch:  15369  Learning Rate:  2.1172025193776635e-08  Varinance:  1.1688431693666677e-08 \n",
      "\n",
      "Epoch:  15370  Learning Rate:  2.115086375106768e-08  Varinance:  1.1672103383021943e-08 \n",
      "\n",
      "Epoch:  15371  Learning Rate:  2.1129723459224235e-08  Varinance:  1.1655797882428674e-08 \n",
      "\n",
      "Epoch:  15372  Learning Rate:  2.1108604297105975e-08  Varinance:  1.163951516002207e-08 \n",
      "\n",
      "Epoch:  15373  Learning Rate:  2.1087506243593806e-08  Varinance:  1.1623255183981874e-08 \n",
      "\n",
      "Epoch:  15374  Learning Rate:  2.106642927758964e-08  Varinance:  1.1607017922532167e-08 \n",
      "\n",
      "Epoch:  15375  Learning Rate:  2.104537337801647e-08  Varinance:  1.1590803343941541e-08 \n",
      "\n",
      "Epoch:  15376  Learning Rate:  2.1024338523818468e-08  Varinance:  1.1574611416522873e-08 \n",
      "\n",
      "Epoch:  15377  Learning Rate:  2.1003324693960743e-08  Varinance:  1.1558442108633304e-08 \n",
      "\n",
      "Epoch:  15378  Learning Rate:  2.0982331867429426e-08  Varinance:  1.1542295388674182e-08 \n",
      "\n",
      "Epoch:  15379  Learning Rate:  2.096136002323176e-08  Varinance:  1.1526171225090995e-08 \n",
      "\n",
      "Epoch:  15380  Learning Rate:  2.0940409140395862e-08  Varinance:  1.1510069586373355e-08 \n",
      "\n",
      "Epoch:  15381  Learning Rate:  2.0919479197970818e-08  Varinance:  1.149399044105477e-08 \n",
      "\n",
      "Epoch:  15382  Learning Rate:  2.0898570175026748e-08  Varinance:  1.1477933757712824e-08 \n",
      "\n",
      "Epoch:  15383  Learning Rate:  2.0877682050654596e-08  Varinance:  1.146189950496896e-08 \n",
      "\n",
      "Epoch:  15384  Learning Rate:  2.0856814803966235e-08  Varinance:  1.1445887651488455e-08 \n",
      "\n",
      "Epoch:  15385  Learning Rate:  2.0835968414094375e-08  Varinance:  1.142989816598036e-08 \n",
      "\n",
      "Epoch:  15386  Learning Rate:  2.0815142860192706e-08  Varinance:  1.1413931017197434e-08 \n",
      "\n",
      "Epoch:  15387  Learning Rate:  2.0794338121435636e-08  Varinance:  1.1397986173936136e-08 \n",
      "\n",
      "Epoch:  15388  Learning Rate:  2.077355417701838e-08  Varinance:  1.1382063605036386e-08 \n",
      "\n",
      "Epoch:  15389  Learning Rate:  2.0752791006157066e-08  Varinance:  1.1366163279381757e-08 \n",
      "\n",
      "Epoch:  15390  Learning Rate:  2.073204858808849e-08  Varinance:  1.1350285165899253e-08 \n",
      "\n",
      "Epoch:  15391  Learning Rate:  2.071132690207019e-08  Varinance:  1.1334429233559282e-08 \n",
      "\n",
      "Epoch:  15392  Learning Rate:  2.069062592738056e-08  Varinance:  1.1318595451375603e-08 \n",
      "\n",
      "Epoch:  15393  Learning Rate:  2.0669945643318577e-08  Varinance:  1.1302783788405258e-08 \n",
      "\n",
      "Epoch:  15394  Learning Rate:  2.0649286029203926e-08  Varinance:  1.1286994213748558e-08 \n",
      "\n",
      "Epoch:  15395  Learning Rate:  2.062864706437706e-08  Varinance:  1.1271226696548855e-08 \n",
      "\n",
      "Epoch:  15396  Learning Rate:  2.060802872819898e-08  Varinance:  1.1255481205992735e-08 \n",
      "\n",
      "Epoch:  15397  Learning Rate:  2.0587431000051304e-08  Varinance:  1.123975771130978e-08 \n",
      "\n",
      "Epoch:  15398  Learning Rate:  2.0566853859336386e-08  Varinance:  1.1224056181772565e-08 \n",
      "\n",
      "Epoch:  15399  Learning Rate:  2.0546297285477036e-08  Varinance:  1.1208376586696584e-08 \n",
      "\n",
      "Epoch:  15400  Learning Rate:  2.0525761257916686e-08  Varinance:  1.11927188954402e-08 \n",
      "\n",
      "Epoch:  15401  Learning Rate:  2.050524575611927e-08  Varinance:  1.117708307740462e-08 \n",
      "\n",
      "Epoch:  15402  Learning Rate:  2.0484750759569352e-08  Varinance:  1.1161469102033674e-08 \n",
      "\n",
      "Epoch:  15403  Learning Rate:  2.0464276247771903e-08  Varinance:  1.1145876938814002e-08 \n",
      "\n",
      "Epoch:  15404  Learning Rate:  2.044382220025237e-08  Varinance:  1.1130306557274829e-08 \n",
      "\n",
      "Epoch:  15405  Learning Rate:  2.0423388596556776e-08  Varinance:  1.1114757926987945e-08 \n",
      "\n",
      "Epoch:  15406  Learning Rate:  2.040297541625148e-08  Varinance:  1.1099231017567647e-08 \n",
      "\n",
      "Epoch:  15407  Learning Rate:  2.038258263892327e-08  Varinance:  1.1083725798670682e-08 \n",
      "\n",
      "Epoch:  15408  Learning Rate:  2.0362210244179427e-08  Varinance:  1.106824223999622e-08 \n",
      "\n",
      "Epoch:  15409  Learning Rate:  2.0341858211647526e-08  Varinance:  1.105278031128565e-08 \n",
      "\n",
      "Epoch:  15410  Learning Rate:  2.0321526520975498e-08  Varinance:  1.1037339982322742e-08 \n",
      "\n",
      "Epoch:  15411  Learning Rate:  2.0301215151831718e-08  Varinance:  1.1021921222933441e-08 \n",
      "\n",
      "Epoch:  15412  Learning Rate:  2.0280924083904785e-08  Varinance:  1.1006524002985842e-08 \n",
      "\n",
      "Epoch:  15413  Learning Rate:  2.0260653296903587e-08  Varinance:  1.0991148292390137e-08 \n",
      "\n",
      "Epoch:  15414  Learning Rate:  2.0240402770557413e-08  Varinance:  1.0975794061098547e-08 \n",
      "\n",
      "Epoch:  15415  Learning Rate:  2.0220172484615693e-08  Varinance:  1.0960461279105312e-08 \n",
      "\n",
      "Epoch:  15416  Learning Rate:  2.0199962418848148e-08  Varinance:  1.094514991644647e-08 \n",
      "\n",
      "Epoch:  15417  Learning Rate:  2.0179772553044665e-08  Varinance:  1.0929859943200035e-08 \n",
      "\n",
      "Epoch:  15418  Learning Rate:  2.0159602867015452e-08  Varinance:  1.0914591329485782e-08 \n",
      "\n",
      "Epoch:  15419  Learning Rate:  2.0139453340590788e-08  Varinance:  1.0899344045465227e-08 \n",
      "\n",
      "Epoch:  15420  Learning Rate:  2.0119323953621105e-08  Varinance:  1.088411806134157e-08 \n",
      "\n",
      "Epoch:  15421  Learning Rate:  2.0099214685977092e-08  Varinance:  1.0868913347359637e-08 \n",
      "\n",
      "Epoch:  15422  Learning Rate:  2.0079125517549433e-08  Varinance:  1.0853729873805858e-08 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15423  Learning Rate:  2.005905642824894e-08  Varinance:  1.0838567611008053e-08 \n",
      "\n",
      "Epoch:  15424  Learning Rate:  2.003900739800657e-08  Varinance:  1.0823426529335615e-08 \n",
      "\n",
      "Epoch:  15425  Learning Rate:  2.0018978406773278e-08  Varinance:  1.0808306599199288e-08 \n",
      "\n",
      "Epoch:  15426  Learning Rate:  1.999896943452002e-08  Varinance:  1.0793207791051151e-08 \n",
      "\n",
      "Epoch:  15427  Learning Rate:  1.9978980461237898e-08  Varinance:  1.0778130075384562e-08 \n",
      "\n",
      "Epoch:  15428  Learning Rate:  1.9959011466937904e-08  Varinance:  1.0763073422734093e-08 \n",
      "\n",
      "Epoch:  15429  Learning Rate:  1.9939062431651005e-08  Varinance:  1.0748037803675526e-08 \n",
      "\n",
      "Epoch:  15430  Learning Rate:  1.9919133335428237e-08  Varinance:  1.0733023188825625e-08 \n",
      "\n",
      "Epoch:  15431  Learning Rate:  1.989922415834046e-08  Varinance:  1.071802954884232e-08 \n",
      "\n",
      "Epoch:  15432  Learning Rate:  1.9879334880478503e-08  Varinance:  1.0703056854424489e-08 \n",
      "\n",
      "Epoch:  15433  Learning Rate:  1.9859465481953044e-08  Varinance:  1.0688105076311946e-08 \n",
      "\n",
      "Epoch:  15434  Learning Rate:  1.983961594289476e-08  Varinance:  1.067317418528538e-08 \n",
      "\n",
      "Epoch:  15435  Learning Rate:  1.981978624345407e-08  Varinance:  1.0658264152166296e-08 \n",
      "\n",
      "Epoch:  15436  Learning Rate:  1.9799976363801244e-08  Varinance:  1.0643374947817e-08 \n",
      "\n",
      "Epoch:  15437  Learning Rate:  1.9780186284126464e-08  Varinance:  1.062850654314039e-08 \n",
      "\n",
      "Epoch:  15438  Learning Rate:  1.9760415984639613e-08  Varinance:  1.061365890908012e-08 \n",
      "\n",
      "Epoch:  15439  Learning Rate:  1.9740665445570363e-08  Varinance:  1.0598832016620405e-08 \n",
      "\n",
      "Epoch:  15440  Learning Rate:  1.9720934647168238e-08  Varinance:  1.0584025836785986e-08 \n",
      "\n",
      "Epoch:  15441  Learning Rate:  1.9701223569702406e-08  Varinance:  1.0569240340642086e-08 \n",
      "\n",
      "Epoch:  15442  Learning Rate:  1.9681532193461746e-08  Varinance:  1.0554475499294346e-08 \n",
      "\n",
      "Epoch:  15443  Learning Rate:  1.966186049875496e-08  Varinance:  1.0539731283888812e-08 \n",
      "\n",
      "Epoch:  15444  Learning Rate:  1.9642208465910305e-08  Varinance:  1.0525007665611724e-08 \n",
      "\n",
      "Epoch:  15445  Learning Rate:  1.9622576075275722e-08  Varinance:  1.0510304615689683e-08 \n",
      "\n",
      "Epoch:  15446  Learning Rate:  1.960296330721888e-08  Varinance:  1.0495622105389454e-08 \n",
      "\n",
      "Epoch:  15447  Learning Rate:  1.9583370142126983e-08  Varinance:  1.0480960106017938e-08 \n",
      "\n",
      "Epoch:  15448  Learning Rate:  1.9563796560406858e-08  Varinance:  1.0466318588922116e-08 \n",
      "\n",
      "Epoch:  15449  Learning Rate:  1.9544242542484892e-08  Varinance:  1.0451697525489006e-08 \n",
      "\n",
      "Epoch:  15450  Learning Rate:  1.9524708068807127e-08  Varinance:  1.0437096887145622e-08 \n",
      "\n",
      "Epoch:  15451  Learning Rate:  1.9505193119839062e-08  Varinance:  1.042251664535879e-08 \n",
      "\n",
      "Epoch:  15452  Learning Rate:  1.9485697676065705e-08  Varinance:  1.0407956771635306e-08 \n",
      "\n",
      "Epoch:  15453  Learning Rate:  1.9466221717991684e-08  Varinance:  1.0393417237521733e-08 \n",
      "\n",
      "Epoch:  15454  Learning Rate:  1.9446765226141002e-08  Varinance:  1.0378898014604379e-08 \n",
      "\n",
      "Epoch:  15455  Learning Rate:  1.9427328181057135e-08  Varinance:  1.0364399074509247e-08 \n",
      "\n",
      "Epoch:  15456  Learning Rate:  1.94079105633031e-08  Varinance:  1.0349920388901982e-08 \n",
      "\n",
      "Epoch:  15457  Learning Rate:  1.9388512353461243e-08  Varinance:  1.033546192948784e-08 \n",
      "\n",
      "Epoch:  15458  Learning Rate:  1.9369133532133327e-08  Varinance:  1.03210236680115e-08 \n",
      "\n",
      "Epoch:  15459  Learning Rate:  1.9349774079940586e-08  Varinance:  1.0306605576257214e-08 \n",
      "\n",
      "Epoch:  15460  Learning Rate:  1.9330433977523542e-08  Varinance:  1.0292207626048628e-08 \n",
      "\n",
      "Epoch:  15461  Learning Rate:  1.931111320554205e-08  Varinance:  1.0277829789248733e-08 \n",
      "\n",
      "Epoch:  15462  Learning Rate:  1.9291811744675406e-08  Varinance:  1.026347203775984e-08 \n",
      "\n",
      "Epoch:  15463  Learning Rate:  1.9272529575622116e-08  Varinance:  1.0249134343523502e-08 \n",
      "\n",
      "Epoch:  15464  Learning Rate:  1.9253266679100006e-08  Varinance:  1.0234816678520511e-08 \n",
      "\n",
      "Epoch:  15465  Learning Rate:  1.9234023035846146e-08  Varinance:  1.022051901477069e-08 \n",
      "\n",
      "Epoch:  15466  Learning Rate:  1.921479862661696e-08  Varinance:  1.0206241324333054e-08 \n",
      "\n",
      "Epoch:  15467  Learning Rate:  1.9195593432188e-08  Varinance:  1.0191983579305623e-08 \n",
      "\n",
      "Epoch:  15468  Learning Rate:  1.917640743335404e-08  Varinance:  1.0177745751825387e-08 \n",
      "\n",
      "Epoch:  15469  Learning Rate:  1.9157240610929145e-08  Varinance:  1.0163527814068262e-08 \n",
      "\n",
      "Epoch:  15470  Learning Rate:  1.9138092945746457e-08  Varinance:  1.0149329738249036e-08 \n",
      "\n",
      "Epoch:  15471  Learning Rate:  1.9118964418658276e-08  Varinance:  1.0135151496621346e-08 \n",
      "\n",
      "Epoch:  15472  Learning Rate:  1.9099855010536136e-08  Varinance:  1.0120993061477479e-08 \n",
      "\n",
      "Epoch:  15473  Learning Rate:  1.9080764702270604e-08  Varinance:  1.010685440514854e-08 \n",
      "\n",
      "Epoch:  15474  Learning Rate:  1.9061693474771328e-08  Varinance:  1.0092735500004252e-08 \n",
      "\n",
      "Epoch:  15475  Learning Rate:  1.904264130896715e-08  Varinance:  1.007863631845293e-08 \n",
      "\n",
      "Epoch:  15476  Learning Rate:  1.9023608185805868e-08  Varinance:  1.006455683294144e-08 \n",
      "\n",
      "Epoch:  15477  Learning Rate:  1.900459408625432e-08  Varinance:  1.0050497015955171e-08 \n",
      "\n",
      "Epoch:  15478  Learning Rate:  1.898559899129848e-08  Varinance:  1.0036456840017845e-08 \n",
      "\n",
      "Epoch:  15479  Learning Rate:  1.8966622881943214e-08  Varinance:  1.002243627769167e-08 \n",
      "\n",
      "Epoch:  15480  Learning Rate:  1.894766573921241e-08  Varinance:  1.0008435301577151e-08 \n",
      "\n",
      "Epoch:  15481  Learning Rate:  1.8928727544148887e-08  Varinance:  9.994453884313066e-09 \n",
      "\n",
      "Epoch:  15482  Learning Rate:  1.8909808277814522e-08  Varinance:  9.980491998576421e-09 \n",
      "\n",
      "Epoch:  15483  Learning Rate:  1.889090792129001e-08  Varinance:  9.966549617082388e-09 \n",
      "\n",
      "Epoch:  15484  Learning Rate:  1.887202645567496e-08  Varinance:  9.952626712584287e-09 \n",
      "\n",
      "Epoch:  15485  Learning Rate:  1.885316386208797e-08  Varinance:  9.938723257873402e-09 \n",
      "\n",
      "Epoch:  15486  Learning Rate:  1.8834320121666414e-08  Varinance:  9.924839225779126e-09 \n",
      "\n",
      "Epoch:  15487  Learning Rate:  1.881549521556652e-08  Varinance:  9.910974589168775e-09 \n",
      "\n",
      "Epoch:  15488  Learning Rate:  1.879668912496344e-08  Varinance:  9.897129320947572e-09 \n",
      "\n",
      "Epoch:  15489  Learning Rate:  1.877790183105105e-08  Varinance:  9.883303394058586e-09 \n",
      "\n",
      "Epoch:  15490  Learning Rate:  1.8759133315042022e-08  Varinance:  9.869496781482682e-09 \n",
      "\n",
      "Epoch:  15491  Learning Rate:  1.8740383558167906e-08  Varinance:  9.855709456238512e-09 \n",
      "\n",
      "Epoch:  15492  Learning Rate:  1.8721652541678915e-08  Varinance:  9.841941391382306e-09 \n",
      "\n",
      "Epoch:  15493  Learning Rate:  1.8702940246843985e-08  Varinance:  9.828192560008046e-09 \n",
      "\n",
      "Epoch:  15494  Learning Rate:  1.8684246654950898e-08  Varinance:  9.814462935247262e-09 \n",
      "\n",
      "Epoch:  15495  Learning Rate:  1.8665571747306023e-08  Varinance:  9.80075249026902e-09 \n",
      "\n",
      "Epoch:  15496  Learning Rate:  1.8646915505234448e-08  Varinance:  9.787061198279865e-09 \n",
      "\n",
      "Epoch:  15497  Learning Rate:  1.86282779100799e-08  Varinance:  9.773389032523775e-09 \n",
      "\n",
      "Epoch:  15498  Learning Rate:  1.8609658943204852e-08  Varinance:  9.759735966282134e-09 \n",
      "\n",
      "Epoch:  15499  Learning Rate:  1.8591058585990293e-08  Varinance:  9.746101972873558e-09 \n",
      "\n",
      "Epoch:  15500  Learning Rate:  1.857247681983584e-08  Varinance:  9.732487025654028e-09 \n",
      "\n",
      "Epoch:  15501  Learning Rate:  1.8553913626159786e-08  Varinance:  9.718891098016716e-09 \n",
      "\n",
      "Epoch:  15502  Learning Rate:  1.8535368986398903e-08  Varinance:  9.705314163391966e-09 \n",
      "\n",
      "Epoch:  15503  Learning Rate:  1.8516842882008515e-08  Varinance:  9.691756195247234e-09 \n",
      "\n",
      "Epoch:  15504  Learning Rate:  1.8498335294462592e-08  Varinance:  9.678217167087041e-09 \n",
      "\n",
      "Epoch:  15505  Learning Rate:  1.84798462052535e-08  Varinance:  9.664697052452957e-09 \n",
      "\n",
      "Epoch:  15506  Learning Rate:  1.846137559589212e-08  Varinance:  9.651195824923414e-09 \n",
      "\n",
      "Epoch:  15507  Learning Rate:  1.8442923447907913e-08  Varinance:  9.637713458113853e-09 \n",
      "\n",
      "Epoch:  15508  Learning Rate:  1.8424489742848686e-08  Varinance:  9.624249925676536e-09 \n",
      "\n",
      "Epoch:  15509  Learning Rate:  1.8406074462280738e-08  Varinance:  9.61080520130054e-09 \n",
      "\n",
      "Epoch:  15510  Learning Rate:  1.8387677587788755e-08  Varinance:  9.597379258711687e-09 \n",
      "\n",
      "Epoch:  15511  Learning Rate:  1.836929910097592e-08  Varinance:  9.583972071672514e-09 \n",
      "\n",
      "Epoch:  15512  Learning Rate:  1.8350938983463724e-08  Varinance:  9.570583613982237e-09 \n",
      "\n",
      "Epoch:  15513  Learning Rate:  1.8332597216892e-08  Varinance:  9.557213859476575e-09 \n",
      "\n",
      "Epoch:  15514  Learning Rate:  1.831427378291906e-08  Varinance:  9.543862782027898e-09 \n",
      "\n",
      "Epoch:  15515  Learning Rate:  1.8295968663221425e-08  Varinance:  9.530530355545043e-09 \n",
      "\n",
      "Epoch:  15516  Learning Rate:  1.8277681839493948e-08  Varinance:  9.517216553973295e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15517  Learning Rate:  1.8259413293449864e-08  Varinance:  9.503921351294332e-09 \n",
      "\n",
      "Epoch:  15518  Learning Rate:  1.8241163006820597e-08  Varinance:  9.490644721526186e-09 \n",
      "\n",
      "Epoch:  15519  Learning Rate:  1.8222930961355824e-08  Varinance:  9.477386638723213e-09 \n",
      "\n",
      "Epoch:  15520  Learning Rate:  1.8204717138823564e-08  Varinance:  9.464147076975919e-09 \n",
      "\n",
      "Epoch:  15521  Learning Rate:  1.8186521521009958e-08  Varinance:  9.450926010411098e-09 \n",
      "\n",
      "Epoch:  15522  Learning Rate:  1.8168344089719356e-08  Varinance:  9.437723413191659e-09 \n",
      "\n",
      "Epoch:  15523  Learning Rate:  1.815018482677439e-08  Varinance:  9.424539259516603e-09 \n",
      "\n",
      "Epoch:  15524  Learning Rate:  1.8132043714015767e-08  Varinance:  9.411373523620975e-09 \n",
      "\n",
      "Epoch:  15525  Learning Rate:  1.8113920733302363e-08  Varinance:  9.398226179775813e-09 \n",
      "\n",
      "Epoch:  15526  Learning Rate:  1.8095815866511174e-08  Varinance:  9.38509720228813e-09 \n",
      "\n",
      "Epoch:  15527  Learning Rate:  1.8077729095537393e-08  Varinance:  9.371986565500729e-09 \n",
      "\n",
      "Epoch:  15528  Learning Rate:  1.805966040229421e-08  Varinance:  9.358894243792358e-09 \n",
      "\n",
      "Epoch:  15529  Learning Rate:  1.8041609768712904e-08  Varinance:  9.345820211577525e-09 \n",
      "\n",
      "Epoch:  15530  Learning Rate:  1.8023577176742905e-08  Varinance:  9.332764443306478e-09 \n",
      "\n",
      "Epoch:  15531  Learning Rate:  1.8005562608351578e-08  Varinance:  9.319726913465153e-09 \n",
      "\n",
      "Epoch:  15532  Learning Rate:  1.798756604552433e-08  Varinance:  9.306707596575136e-09 \n",
      "\n",
      "Epoch:  15533  Learning Rate:  1.7969587470264664e-08  Varinance:  9.293706467193633e-09 \n",
      "\n",
      "Epoch:  15534  Learning Rate:  1.795162686459396e-08  Varinance:  9.280723499913297e-09 \n",
      "\n",
      "Epoch:  15535  Learning Rate:  1.7933684210551584e-08  Varinance:  9.267758669362368e-09 \n",
      "\n",
      "Epoch:  15536  Learning Rate:  1.791575949019495e-08  Varinance:  9.254811950204503e-09 \n",
      "\n",
      "Epoch:  15537  Learning Rate:  1.7897852685599295e-08  Varinance:  9.241883317138747e-09 \n",
      "\n",
      "Epoch:  15538  Learning Rate:  1.7879963778857787e-08  Varinance:  9.228972744899495e-09 \n",
      "\n",
      "Epoch:  15539  Learning Rate:  1.786209275208158e-08  Varinance:  9.216080208256434e-09 \n",
      "\n",
      "Epoch:  15540  Learning Rate:  1.7844239587399615e-08  Varinance:  9.203205682014528e-09 \n",
      "\n",
      "Epoch:  15541  Learning Rate:  1.7826404266958722e-08  Varinance:  9.190349141013844e-09 \n",
      "\n",
      "Epoch:  15542  Learning Rate:  1.7808586772923548e-08  Varinance:  9.177510560129688e-09 \n",
      "\n",
      "Epoch:  15543  Learning Rate:  1.7790787087476664e-08  Varinance:  9.164689914272439e-09 \n",
      "\n",
      "Epoch:  15544  Learning Rate:  1.7773005192818353e-08  Varinance:  9.151887178387523e-09 \n",
      "\n",
      "Epoch:  15545  Learning Rate:  1.7755241071166682e-08  Varinance:  9.13910232745536e-09 \n",
      "\n",
      "Epoch:  15546  Learning Rate:  1.7737494704757595e-08  Varinance:  9.126335336491329e-09 \n",
      "\n",
      "Epoch:  15547  Learning Rate:  1.7719766075844687e-08  Varinance:  9.113586180545745e-09 \n",
      "\n",
      "Epoch:  15548  Learning Rate:  1.7702055166699303e-08  Varinance:  9.100854834703671e-09 \n",
      "\n",
      "Epoch:  15549  Learning Rate:  1.768436195961059e-08  Varinance:  9.08814127408508e-09 \n",
      "\n",
      "Epoch:  15550  Learning Rate:  1.7666686436885315e-08  Varinance:  9.075445473844666e-09 \n",
      "\n",
      "Epoch:  15551  Learning Rate:  1.7649028580847914e-08  Varinance:  9.062767409171833e-09 \n",
      "\n",
      "Epoch:  15552  Learning Rate:  1.7631388373840594e-08  Varinance:  9.050107055290642e-09 \n",
      "\n",
      "Epoch:  15553  Learning Rate:  1.7613765798223123e-08  Varinance:  9.037464387459767e-09 \n",
      "\n",
      "Epoch:  15554  Learning Rate:  1.759616083637288e-08  Varinance:  9.024839380972475e-09 \n",
      "\n",
      "Epoch:  15555  Learning Rate:  1.7578573470684977e-08  Varinance:  9.012232011156452e-09 \n",
      "\n",
      "Epoch:  15556  Learning Rate:  1.7561003683572008e-08  Varinance:  8.99964225337395e-09 \n",
      "\n",
      "Epoch:  15557  Learning Rate:  1.7543451457464183e-08  Varinance:  8.9870700830216e-09 \n",
      "\n",
      "Epoch:  15558  Learning Rate:  1.752591677480925e-08  Varinance:  8.974515475530408e-09 \n",
      "\n",
      "Epoch:  15559  Learning Rate:  1.750839961807258e-08  Varinance:  8.961978406365702e-09 \n",
      "\n",
      "Epoch:  15560  Learning Rate:  1.749089996973699e-08  Varinance:  8.94945885102708e-09 \n",
      "\n",
      "Epoch:  15561  Learning Rate:  1.7473417812302797e-08  Varinance:  8.936956785048403e-09 \n",
      "\n",
      "Epoch:  15562  Learning Rate:  1.74559531282879e-08  Varinance:  8.924472183997611e-09 \n",
      "\n",
      "Epoch:  15563  Learning Rate:  1.7438505900227588e-08  Varinance:  8.912005023476874e-09 \n",
      "\n",
      "Epoch:  15564  Learning Rate:  1.7421076110674597e-08  Varinance:  8.899555279122409e-09 \n",
      "\n",
      "Epoch:  15565  Learning Rate:  1.7403663742199203e-08  Varinance:  8.88712292660447e-09 \n",
      "\n",
      "Epoch:  15566  Learning Rate:  1.7386268777388995e-08  Varinance:  8.8747079416273e-09 \n",
      "\n",
      "Epoch:  15567  Learning Rate:  1.736889119884899e-08  Varinance:  8.862310299929079e-09 \n",
      "\n",
      "Epoch:  15568  Learning Rate:  1.7351530989201658e-08  Varinance:  8.849929977281917e-09 \n",
      "\n",
      "Epoch:  15569  Learning Rate:  1.733418813108676e-08  Varinance:  8.837566949491669e-09 \n",
      "\n",
      "Epoch:  15570  Learning Rate:  1.731686260716141e-08  Varinance:  8.825221192398085e-09 \n",
      "\n",
      "Epoch:  15571  Learning Rate:  1.7299554400100138e-08  Varinance:  8.812892681874636e-09 \n",
      "\n",
      "Epoch:  15572  Learning Rate:  1.728226349259471e-08  Varinance:  8.800581393828494e-09 \n",
      "\n",
      "Epoch:  15573  Learning Rate:  1.7264989867354213e-08  Varinance:  8.788287304200494e-09 \n",
      "\n",
      "Epoch:  15574  Learning Rate:  1.7247733507104994e-08  Varinance:  8.77601038896507e-09 \n",
      "\n",
      "Epoch:  15575  Learning Rate:  1.7230494394590747e-08  Varinance:  8.763750624130261e-09 \n",
      "\n",
      "Epoch:  15576  Learning Rate:  1.721327251257233e-08  Varinance:  8.75150798573752e-09 \n",
      "\n",
      "Epoch:  15577  Learning Rate:  1.7196067843827833e-08  Varinance:  8.73928244986187e-09 \n",
      "\n",
      "Epoch:  15578  Learning Rate:  1.717888037115264e-08  Varinance:  8.72707399261172e-09 \n",
      "\n",
      "Epoch:  15579  Learning Rate:  1.7161710077359253e-08  Varinance:  8.714882590128856e-09 \n",
      "\n",
      "Epoch:  15580  Learning Rate:  1.714455694527734e-08  Varinance:  8.702708218588393e-09 \n",
      "\n",
      "Epoch:  15581  Learning Rate:  1.7127420957753835e-08  Varinance:  8.690550854198727e-09 \n",
      "\n",
      "Epoch:  15582  Learning Rate:  1.7110302097652714e-08  Varinance:  8.678410473201527e-09 \n",
      "\n",
      "Epoch:  15583  Learning Rate:  1.7093200347855084e-08  Varinance:  8.66628705187155e-09 \n",
      "\n",
      "Epoch:  15584  Learning Rate:  1.7076115691259258e-08  Varinance:  8.654180566516798e-09 \n",
      "\n",
      "Epoch:  15585  Learning Rate:  1.7059048110780547e-08  Varinance:  8.642090993478329e-09 \n",
      "\n",
      "Epoch:  15586  Learning Rate:  1.7041997589351337e-08  Varinance:  8.630018309130262e-09 \n",
      "\n",
      "Epoch:  15587  Learning Rate:  1.7024964109921167e-08  Varinance:  8.617962489879711e-09 \n",
      "\n",
      "Epoch:  15588  Learning Rate:  1.7007947655456523e-08  Varinance:  8.605923512166758e-09 \n",
      "\n",
      "Epoch:  15589  Learning Rate:  1.6990948208940955e-08  Varinance:  8.593901352464419e-09 \n",
      "\n",
      "Epoch:  15590  Learning Rate:  1.697396575337498e-08  Varinance:  8.58189598727849e-09 \n",
      "\n",
      "Epoch:  15591  Learning Rate:  1.6957000271776206e-08  Varinance:  8.569907393147677e-09 \n",
      "\n",
      "Epoch:  15592  Learning Rate:  1.6940051747179113e-08  Varinance:  8.557935546643433e-09 \n",
      "\n",
      "Epoch:  15593  Learning Rate:  1.692312016263515e-08  Varinance:  8.545980424369933e-09 \n",
      "\n",
      "Epoch:  15594  Learning Rate:  1.690620550121279e-08  Varinance:  8.534042002964044e-09 \n",
      "\n",
      "Epoch:  15595  Learning Rate:  1.688930774599734e-08  Varinance:  8.522120259095264e-09 \n",
      "\n",
      "Epoch:  15596  Learning Rate:  1.6872426880091016e-08  Varinance:  8.510215169465715e-09 \n",
      "\n",
      "Epoch:  15597  Learning Rate:  1.685556288661301e-08  Varinance:  8.498326710809976e-09 \n",
      "\n",
      "Epoch:  15598  Learning Rate:  1.6838715748699286e-08  Varinance:  8.486454859895217e-09 \n",
      "\n",
      "Epoch:  15599  Learning Rate:  1.682188544950269e-08  Varinance:  8.474599593521031e-09 \n",
      "\n",
      "Epoch:  15600  Learning Rate:  1.6805071972192974e-08  Varinance:  8.462760888519424e-09 \n",
      "\n",
      "Epoch:  15601  Learning Rate:  1.6788275299956634e-08  Varinance:  8.450938721754762e-09 \n",
      "\n",
      "Epoch:  15602  Learning Rate:  1.6771495415996956e-08  Varinance:  8.43913307012374e-09 \n",
      "\n",
      "Epoch:  15603  Learning Rate:  1.675473230353413e-08  Varinance:  8.427343910555346e-09 \n",
      "\n",
      "Epoch:  15604  Learning Rate:  1.6737985945805e-08  Varinance:  8.415571220010718e-09 \n",
      "\n",
      "Epoch:  15605  Learning Rate:  1.672125632606321e-08  Varinance:  8.403814975483261e-09 \n",
      "\n",
      "Epoch:  15606  Learning Rate:  1.6704543427579108e-08  Varinance:  8.39207515399849e-09 \n",
      "\n",
      "Epoch:  15607  Learning Rate:  1.668784723363986e-08  Varinance:  8.380351732614019e-09 \n",
      "\n",
      "Epoch:  15608  Learning Rate:  1.667116772754923e-08  Varinance:  8.368644688419509e-09 \n",
      "\n",
      "Epoch:  15609  Learning Rate:  1.6654504892627692e-08  Varinance:  8.356953998536653e-09 \n",
      "\n",
      "Epoch:  15610  Learning Rate:  1.6637858712212464e-08  Varinance:  8.345279640119022e-09 \n",
      "\n",
      "Epoch:  15611  Learning Rate:  1.6621229169657332e-08  Varinance:  8.333621590352183e-09 \n",
      "\n",
      "Epoch:  15612  Learning Rate:  1.6604616248332725e-08  Varinance:  8.32197982645355e-09 \n",
      "\n",
      "Epoch:  15613  Learning Rate:  1.6588019931625782e-08  Varinance:  8.310354325672366e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15614  Learning Rate:  1.6571440202940152e-08  Varinance:  8.298745065289647e-09 \n",
      "\n",
      "Epoch:  15615  Learning Rate:  1.655487704569608e-08  Varinance:  8.287152022618153e-09 \n",
      "\n",
      "Epoch:  15616  Learning Rate:  1.6538330443330455e-08  Varinance:  8.275575175002365e-09 \n",
      "\n",
      "Epoch:  15617  Learning Rate:  1.652180037929666e-08  Varinance:  8.264014499818326e-09 \n",
      "\n",
      "Epoch:  15618  Learning Rate:  1.6505286837064587e-08  Varinance:  8.252469974473771e-09 \n",
      "\n",
      "Epoch:  15619  Learning Rate:  1.6488789800120754e-08  Varinance:  8.240941576407967e-09 \n",
      "\n",
      "Epoch:  15620  Learning Rate:  1.6472309251968098e-08  Varinance:  8.229429283091692e-09 \n",
      "\n",
      "Epoch:  15621  Learning Rate:  1.6455845176126068e-08  Varinance:  8.217933072027208e-09 \n",
      "\n",
      "Epoch:  15622  Learning Rate:  1.6439397556130556e-08  Varinance:  8.206452920748194e-09 \n",
      "\n",
      "Epoch:  15623  Learning Rate:  1.6422966375533996e-08  Varinance:  8.19498880681975e-09 \n",
      "\n",
      "Epoch:  15624  Learning Rate:  1.640655161790518e-08  Varinance:  8.183540707838224e-09 \n",
      "\n",
      "Epoch:  15625  Learning Rate:  1.6390153266829324e-08  Varinance:  8.172108601431355e-09 \n",
      "\n",
      "Epoch:  15626  Learning Rate:  1.6373771305908127e-08  Varinance:  8.160692465258099e-09 \n",
      "\n",
      "Epoch:  15627  Learning Rate:  1.6357405718759603e-08  Varinance:  8.149292277008629e-09 \n",
      "\n",
      "Epoch:  15628  Learning Rate:  1.634105648901813e-08  Varinance:  8.137908014404275e-09 \n",
      "\n",
      "Epoch:  15629  Learning Rate:  1.6324723600334535e-08  Varinance:  8.126539655197499e-09 \n",
      "\n",
      "Epoch:  15630  Learning Rate:  1.6308407036375903e-08  Varinance:  8.115187177171862e-09 \n",
      "\n",
      "Epoch:  15631  Learning Rate:  1.6292106780825635e-08  Varinance:  8.103850558141883e-09 \n",
      "\n",
      "Epoch:  15632  Learning Rate:  1.6275822817383537e-08  Varinance:  8.09252977595315e-09 \n",
      "\n",
      "Epoch:  15633  Learning Rate:  1.6259555129765615e-08  Varinance:  8.081224808482183e-09 \n",
      "\n",
      "Epoch:  15634  Learning Rate:  1.6243303701704175e-08  Varinance:  8.069935633636396e-09 \n",
      "\n",
      "Epoch:  15635  Learning Rate:  1.622706851694776e-08  Varinance:  8.05866222935407e-09 \n",
      "\n",
      "Epoch:  15636  Learning Rate:  1.6210849559261243e-08  Varinance:  8.04740457360431e-09 \n",
      "\n",
      "Epoch:  15637  Learning Rate:  1.6194646812425636e-08  Varinance:  8.036162644387018e-09 \n",
      "\n",
      "Epoch:  15638  Learning Rate:  1.6178460260238166e-08  Varinance:  8.024936419732751e-09 \n",
      "\n",
      "Epoch:  15639  Learning Rate:  1.6162289886512328e-08  Varinance:  8.013725877702834e-09 \n",
      "\n",
      "Epoch:  15640  Learning Rate:  1.614613567507773e-08  Varinance:  8.00253099638922e-09 \n",
      "\n",
      "Epoch:  15641  Learning Rate:  1.612999760978012e-08  Varinance:  7.99135175391446e-09 \n",
      "\n",
      "Epoch:  15642  Learning Rate:  1.611387567448149e-08  Varinance:  7.98018812843167e-09 \n",
      "\n",
      "Epoch:  15643  Learning Rate:  1.609776985305988e-08  Varinance:  7.969040098124483e-09 \n",
      "\n",
      "Epoch:  15644  Learning Rate:  1.6081680129409437e-08  Varinance:  7.95790764120704e-09 \n",
      "\n",
      "Epoch:  15645  Learning Rate:  1.606560648744049e-08  Varinance:  7.946790735923831e-09 \n",
      "\n",
      "Epoch:  15646  Learning Rate:  1.604954891107937e-08  Varinance:  7.935689360549822e-09 \n",
      "\n",
      "Epoch:  15647  Learning Rate:  1.6033507384268472e-08  Varinance:  7.924603493390297e-09 \n",
      "\n",
      "Epoch:  15648  Learning Rate:  1.601748189096632e-08  Varinance:  7.913533112780846e-09 \n",
      "\n",
      "Epoch:  15649  Learning Rate:  1.6001472415147395e-08  Varinance:  7.902478197087332e-09 \n",
      "\n",
      "Epoch:  15650  Learning Rate:  1.598547894080222e-08  Varinance:  7.89143872470583e-09 \n",
      "\n",
      "Epoch:  15651  Learning Rate:  1.5969501451937288e-08  Varinance:  7.880414674062632e-09 \n",
      "\n",
      "Epoch:  15652  Learning Rate:  1.5953539932575166e-08  Varinance:  7.869406023614072e-09 \n",
      "\n",
      "Epoch:  15653  Learning Rate:  1.5937594366754307e-08  Varinance:  7.858412751846676e-09 \n",
      "\n",
      "Epoch:  15654  Learning Rate:  1.5921664738529116e-08  Varinance:  7.847434837276987e-09 \n",
      "\n",
      "Epoch:  15655  Learning Rate:  1.5905751031970018e-08  Varinance:  7.836472258451565e-09 \n",
      "\n",
      "Epoch:  15656  Learning Rate:  1.5889853231163277e-08  Varinance:  7.825524993946936e-09 \n",
      "\n",
      "Epoch:  15657  Learning Rate:  1.5873971320211063e-08  Varinance:  7.814593022369556e-09 \n",
      "\n",
      "Epoch:  15658  Learning Rate:  1.5858105283231518e-08  Varinance:  7.803676322355794e-09 \n",
      "\n",
      "Epoch:  15659  Learning Rate:  1.584225510435858e-08  Varinance:  7.792774872571778e-09 \n",
      "\n",
      "Epoch:  15660  Learning Rate:  1.582642076774204e-08  Varinance:  7.781888651713528e-09 \n",
      "\n",
      "Epoch:  15661  Learning Rate:  1.581060225754761e-08  Varinance:  7.771017638506787e-09 \n",
      "\n",
      "Epoch:  15662  Learning Rate:  1.579479955795676e-08  Varinance:  7.760161811707028e-09 \n",
      "\n",
      "Epoch:  15663  Learning Rate:  1.5779012653166755e-08  Varinance:  7.749321150099395e-09 \n",
      "\n",
      "Epoch:  15664  Learning Rate:  1.5763241527390745e-08  Varinance:  7.738495632498671e-09 \n",
      "\n",
      "Epoch:  15665  Learning Rate:  1.5747486164857577e-08  Varinance:  7.727685237749261e-09 \n",
      "\n",
      "Epoch:  15666  Learning Rate:  1.5731746549811885e-08  Varinance:  7.71688994472504e-09 \n",
      "\n",
      "Epoch:  15667  Learning Rate:  1.5716022666514027e-08  Varinance:  7.706109732329482e-09 \n",
      "\n",
      "Epoch:  15668  Learning Rate:  1.5700314499240173e-08  Varinance:  7.695344579495499e-09 \n",
      "\n",
      "Epoch:  15669  Learning Rate:  1.5684622032282126e-08  Varinance:  7.684594465185436e-09 \n",
      "\n",
      "Epoch:  15670  Learning Rate:  1.566894524994739e-08  Varinance:  7.673859368391025e-09 \n",
      "\n",
      "Epoch:  15671  Learning Rate:  1.565328413655924e-08  Varinance:  7.663139268133349e-09 \n",
      "\n",
      "Epoch:  15672  Learning Rate:  1.563763867645653e-08  Varinance:  7.652434143462821e-09 \n",
      "\n",
      "Epoch:  15673  Learning Rate:  1.562200885399377e-08  Varinance:  7.641743973459044e-09 \n",
      "\n",
      "Epoch:  15674  Learning Rate:  1.5606394653541195e-08  Varinance:  7.631068737230919e-09 \n",
      "\n",
      "Epoch:  15675  Learning Rate:  1.559079605948457e-08  Varinance:  7.620408413916514e-09 \n",
      "\n",
      "Epoch:  15676  Learning Rate:  1.5575213056225283e-08  Varinance:  7.60976298268303e-09 \n",
      "\n",
      "Epoch:  15677  Learning Rate:  1.5559645628180374e-08  Varinance:  7.59913242272678e-09 \n",
      "\n",
      "Epoch:  15678  Learning Rate:  1.554409375978239e-08  Varinance:  7.588516713273133e-09 \n",
      "\n",
      "Epoch:  15679  Learning Rate:  1.5528557435479435e-08  Varinance:  7.577915833576508e-09 \n",
      "\n",
      "Epoch:  15680  Learning Rate:  1.5513036639735235e-08  Varinance:  7.567329762920225e-09 \n",
      "\n",
      "Epoch:  15681  Learning Rate:  1.5497531357028967e-08  Varinance:  7.556758480616622e-09 \n",
      "\n",
      "Epoch:  15682  Learning Rate:  1.5482041571855348e-08  Varinance:  7.546201966006915e-09 \n",
      "\n",
      "Epoch:  15683  Learning Rate:  1.5466567268724567e-08  Varinance:  7.535660198461177e-09 \n",
      "\n",
      "Epoch:  15684  Learning Rate:  1.5451108432162366e-08  Varinance:  7.525133157378299e-09 \n",
      "\n",
      "Epoch:  15685  Learning Rate:  1.543566504670989e-08  Varinance:  7.514620822185952e-09 \n",
      "\n",
      "Epoch:  15686  Learning Rate:  1.5420237096923715e-08  Varinance:  7.504123172340573e-09 \n",
      "\n",
      "Epoch:  15687  Learning Rate:  1.5404824567375952e-08  Varinance:  7.493640187327216e-09 \n",
      "\n",
      "Epoch:  15688  Learning Rate:  1.538942744265404e-08  Varinance:  7.483171846659677e-09 \n",
      "\n",
      "Epoch:  15689  Learning Rate:  1.5374045707360824e-08  Varinance:  7.472718129880342e-09 \n",
      "\n",
      "Epoch:  15690  Learning Rate:  1.5358679346114625e-08  Varinance:  7.462279016560173e-09 \n",
      "\n",
      "Epoch:  15691  Learning Rate:  1.534332834354905e-08  Varinance:  7.451854486298674e-09 \n",
      "\n",
      "Epoch:  15692  Learning Rate:  1.532799268431307e-08  Varinance:  7.4414445187238474e-09 \n",
      "\n",
      "Epoch:  15693  Learning Rate:  1.5312672353071084e-08  Varinance:  7.431049093492179e-09 \n",
      "\n",
      "Epoch:  15694  Learning Rate:  1.5297367334502723e-08  Varinance:  7.420668190288494e-09 \n",
      "\n",
      "Epoch:  15695  Learning Rate:  1.5282077613302943e-08  Varinance:  7.410301788826081e-09 \n",
      "\n",
      "Epoch:  15696  Learning Rate:  1.526680317418208e-08  Varinance:  7.399949868846538e-09 \n",
      "\n",
      "Epoch:  15697  Learning Rate:  1.5251544001865663e-08  Varinance:  7.389612410119763e-09 \n",
      "\n",
      "Epoch:  15698  Learning Rate:  1.523630008109452e-08  Varinance:  7.379289392443918e-09 \n",
      "\n",
      "Epoch:  15699  Learning Rate:  1.5221071396624697e-08  Varinance:  7.368980795645382e-09 \n",
      "\n",
      "Epoch:  15700  Learning Rate:  1.5205857933227567e-08  Varinance:  7.358686599578747e-09 \n",
      "\n",
      "Epoch:  15701  Learning Rate:  1.519065967568964e-08  Varinance:  7.3484067841266634e-09 \n",
      "\n",
      "Epoch:  15702  Learning Rate:  1.5175476608812625e-08  Varinance:  7.3381413291999674e-09 \n",
      "\n",
      "Epoch:  15703  Learning Rate:  1.516030871741351e-08  Varinance:  7.32789021473753e-09 \n",
      "\n",
      "Epoch:  15704  Learning Rate:  1.5145155986324376e-08  Varinance:  7.317653420706249e-09 \n",
      "\n",
      "Epoch:  15705  Learning Rate:  1.5130018400392467e-08  Varinance:  7.307430927101009e-09 \n",
      "\n",
      "Epoch:  15706  Learning Rate:  1.5114895944480243e-08  Varinance:  7.2972227139446345e-09 \n",
      "\n",
      "Epoch:  15707  Learning Rate:  1.5099788603465227e-08  Varinance:  7.287028761287892e-09 \n",
      "\n",
      "Epoch:  15708  Learning Rate:  1.508469636224004e-08  Varinance:  7.276849049209333e-09 \n",
      "\n",
      "Epoch:  15709  Learning Rate:  1.5069619205712502e-08  Varinance:  7.266683557815417e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15710  Learning Rate:  1.5054557118805426e-08  Varinance:  7.2565322672403695e-09 \n",
      "\n",
      "Epoch:  15711  Learning Rate:  1.5039510086456693e-08  Varinance:  7.246395157646167e-09 \n",
      "\n",
      "Epoch:  15712  Learning Rate:  1.5024478093619332e-08  Varinance:  7.236272209222498e-09 \n",
      "\n",
      "Epoch:  15713  Learning Rate:  1.5009461125261314e-08  Varinance:  7.2261634021867285e-09 \n",
      "\n",
      "Epoch:  15714  Learning Rate:  1.499445916636567e-08  Varinance:  7.216068716783881e-09 \n",
      "\n",
      "Epoch:  15715  Learning Rate:  1.4979472201930418e-08  Varinance:  7.2059881332865035e-09 \n",
      "\n",
      "Epoch:  15716  Learning Rate:  1.4964500216968642e-08  Varinance:  7.195921631994774e-09 \n",
      "\n",
      "Epoch:  15717  Learning Rate:  1.494954319650833e-08  Varinance:  7.185869193236369e-09 \n",
      "\n",
      "Epoch:  15718  Learning Rate:  1.4934601125592433e-08  Varinance:  7.1758307973664446e-09 \n",
      "\n",
      "Epoch:  15719  Learning Rate:  1.4919673989278934e-08  Varinance:  7.165806424767599e-09 \n",
      "\n",
      "Epoch:  15720  Learning Rate:  1.4904761772640667e-08  Varinance:  7.155796055849839e-09 \n",
      "\n",
      "Epoch:  15721  Learning Rate:  1.4889864460765389e-08  Varinance:  7.1457996710505585e-09 \n",
      "\n",
      "Epoch:  15722  Learning Rate:  1.4874982038755839e-08  Varinance:  7.135817250834406e-09 \n",
      "\n",
      "Epoch:  15723  Learning Rate:  1.4860114491729567e-08  Varinance:  7.125848775693397e-09 \n",
      "\n",
      "Epoch:  15724  Learning Rate:  1.4845261804819e-08  Varinance:  7.115894226146773e-09 \n",
      "\n",
      "Epoch:  15725  Learning Rate:  1.4830423963171498e-08  Varinance:  7.105953582740989e-09 \n",
      "\n",
      "Epoch:  15726  Learning Rate:  1.4815600951949198e-08  Varinance:  7.096026826049676e-09 \n",
      "\n",
      "Epoch:  15727  Learning Rate:  1.4800792756329056e-08  Varinance:  7.086113936673601e-09 \n",
      "\n",
      "Epoch:  15728  Learning Rate:  1.478599936150293e-08  Varinance:  7.07621489524066e-09 \n",
      "\n",
      "Epoch:  15729  Learning Rate:  1.47712207526774e-08  Varinance:  7.066329682405734e-09 \n",
      "\n",
      "Epoch:  15730  Learning Rate:  1.4756456915073853e-08  Varinance:  7.056458278850802e-09 \n",
      "\n",
      "Epoch:  15731  Learning Rate:  1.4741707833928424e-08  Varinance:  7.046600665284807e-09 \n",
      "\n",
      "Epoch:  15732  Learning Rate:  1.4726973494492082e-08  Varinance:  7.03675682244364e-09 \n",
      "\n",
      "Epoch:  15733  Learning Rate:  1.4712253882030465e-08  Varinance:  7.0269267310901016e-09 \n",
      "\n",
      "Epoch:  15734  Learning Rate:  1.4697548981823926e-08  Varinance:  7.01711037201387e-09 \n",
      "\n",
      "Epoch:  15735  Learning Rate:  1.4682858779167623e-08  Varinance:  7.0073077260314805e-09 \n",
      "\n",
      "Epoch:  15736  Learning Rate:  1.466818325937132e-08  Varinance:  6.997518773986195e-09 \n",
      "\n",
      "Epoch:  15737  Learning Rate:  1.4653522407759477e-08  Varinance:  6.987743496748109e-09 \n",
      "\n",
      "Epoch:  15738  Learning Rate:  1.4638876209671286e-08  Varinance:  6.97798187521402e-09 \n",
      "\n",
      "Epoch:  15739  Learning Rate:  1.4624244650460525e-08  Varinance:  6.968233890307408e-09 \n",
      "\n",
      "Epoch:  15740  Learning Rate:  1.4609627715495606e-08  Varinance:  6.958499522978404e-09 \n",
      "\n",
      "Epoch:  15741  Learning Rate:  1.4595025390159645e-08  Varinance:  6.948778754203777e-09 \n",
      "\n",
      "Epoch:  15742  Learning Rate:  1.4580437659850294e-08  Varinance:  6.939071564986796e-09 \n",
      "\n",
      "Epoch:  15743  Learning Rate:  1.4565864509979791e-08  Varinance:  6.929377936357342e-09 \n",
      "\n",
      "Epoch:  15744  Learning Rate:  1.4551305925975039e-08  Varinance:  6.9196978493717685e-09 \n",
      "\n",
      "Epoch:  15745  Learning Rate:  1.453676189327742e-08  Varinance:  6.910031285112897e-09 \n",
      "\n",
      "Epoch:  15746  Learning Rate:  1.452223239734291e-08  Varinance:  6.900378224689974e-09 \n",
      "\n",
      "Epoch:  15747  Learning Rate:  1.450771742364198e-08  Varinance:  6.890738649238635e-09 \n",
      "\n",
      "Epoch:  15748  Learning Rate:  1.449321695765971e-08  Varinance:  6.881112539920896e-09 \n",
      "\n",
      "Epoch:  15749  Learning Rate:  1.4478730984895606e-08  Varinance:  6.87149987792501e-09 \n",
      "\n",
      "Epoch:  15750  Learning Rate:  1.4464259490863667e-08  Varinance:  6.861900644465587e-09 \n",
      "\n",
      "Epoch:  15751  Learning Rate:  1.4449802461092448e-08  Varinance:  6.852314820783454e-09 \n",
      "\n",
      "Epoch:  15752  Learning Rate:  1.4435359881124895e-08  Varinance:  6.842742388145643e-09 \n",
      "\n",
      "Epoch:  15753  Learning Rate:  1.4420931736518403e-08  Varinance:  6.833183327845356e-09 \n",
      "\n",
      "Epoch:  15754  Learning Rate:  1.4406518012844872e-08  Varinance:  6.823637621201928e-09 \n",
      "\n",
      "Epoch:  15755  Learning Rate:  1.4392118695690557e-08  Varinance:  6.814105249560813e-09 \n",
      "\n",
      "Epoch:  15756  Learning Rate:  1.4377733770656108e-08  Varinance:  6.804586194293455e-09 \n",
      "\n",
      "Epoch:  15757  Learning Rate:  1.4363363223356655e-08  Varinance:  6.79508043679739e-09 \n",
      "\n",
      "Epoch:  15758  Learning Rate:  1.4349007039421622e-08  Varinance:  6.785587958496122e-09 \n",
      "\n",
      "Epoch:  15759  Learning Rate:  1.4334665204494826e-08  Varinance:  6.776108740839099e-09 \n",
      "\n",
      "Epoch:  15760  Learning Rate:  1.43203377042344e-08  Varinance:  6.766642765301689e-09 \n",
      "\n",
      "Epoch:  15761  Learning Rate:  1.4306024524312902e-08  Varinance:  6.757190013385137e-09 \n",
      "\n",
      "Epoch:  15762  Learning Rate:  1.4291725650417115e-08  Varinance:  6.7477504666165515e-09 \n",
      "\n",
      "Epoch:  15763  Learning Rate:  1.4277441068248149e-08  Varinance:  6.738324106548778e-09 \n",
      "\n",
      "Epoch:  15764  Learning Rate:  1.4263170763521462e-08  Varinance:  6.728910914760502e-09 \n",
      "\n",
      "Epoch:  15765  Learning Rate:  1.424891472196673e-08  Varinance:  6.719510872856121e-09 \n",
      "\n",
      "Epoch:  15766  Learning Rate:  1.4234672929327884e-08  Varinance:  6.7101239624657285e-09 \n",
      "\n",
      "Epoch:  15767  Learning Rate:  1.4220445371363175e-08  Varinance:  6.70075016524508e-09 \n",
      "\n",
      "Epoch:  15768  Learning Rate:  1.4206232033845022e-08  Varinance:  6.69138946287556e-09 \n",
      "\n",
      "Epoch:  15769  Learning Rate:  1.4192032902560064e-08  Varinance:  6.682041837064163e-09 \n",
      "\n",
      "Epoch:  15770  Learning Rate:  1.4177847963309215e-08  Varinance:  6.672707269543369e-09 \n",
      "\n",
      "Epoch:  15771  Learning Rate:  1.4163677201907512e-08  Varinance:  6.663385742071252e-09 \n",
      "\n",
      "Epoch:  15772  Learning Rate:  1.4149520604184164e-08  Varinance:  6.654077236431338e-09 \n",
      "\n",
      "Epoch:  15773  Learning Rate:  1.4135378155982628e-08  Varinance:  6.644781734432606e-09 \n",
      "\n",
      "Epoch:  15774  Learning Rate:  1.4121249843160423e-08  Varinance:  6.635499217909446e-09 \n",
      "\n",
      "Epoch:  15775  Learning Rate:  1.4107135651589239e-08  Varinance:  6.626229668721626e-09 \n",
      "\n",
      "Epoch:  15776  Learning Rate:  1.4093035567154855e-08  Varinance:  6.616973068754274e-09 \n",
      "\n",
      "Epoch:  15777  Learning Rate:  1.407894957575724e-08  Varinance:  6.607729399917759e-09 \n",
      "\n",
      "Epoch:  15778  Learning Rate:  1.4064877663310374e-08  Varinance:  6.59849864414779e-09 \n",
      "\n",
      "Epoch:  15779  Learning Rate:  1.4050819815742317e-08  Varinance:  6.589280783405284e-09 \n",
      "\n",
      "Epoch:  15780  Learning Rate:  1.4036776018995273e-08  Varinance:  6.580075799676361e-09 \n",
      "\n",
      "Epoch:  15781  Learning Rate:  1.4022746259025416e-08  Varinance:  6.570883674972305e-09 \n",
      "\n",
      "Epoch:  15782  Learning Rate:  1.4008730521802962e-08  Varinance:  6.561704391329529e-09 \n",
      "\n",
      "Epoch:  15783  Learning Rate:  1.3994728793312225e-08  Varinance:  6.552537930809565e-09 \n",
      "\n",
      "Epoch:  15784  Learning Rate:  1.3980741059551444e-08  Varinance:  6.543384275498935e-09 \n",
      "\n",
      "Epoch:  15785  Learning Rate:  1.3966767306532865e-08  Varinance:  6.534243407509253e-09 \n",
      "\n",
      "Epoch:  15786  Learning Rate:  1.395280752028278e-08  Varinance:  6.5251153089771e-09 \n",
      "\n",
      "Epoch:  15787  Learning Rate:  1.3938861686841377e-08  Varinance:  6.515999962064012e-09 \n",
      "\n",
      "Epoch:  15788  Learning Rate:  1.3924929792262799e-08  Varinance:  6.506897348956445e-09 \n",
      "\n",
      "Epoch:  15789  Learning Rate:  1.3911011822615199e-08  Varinance:  6.497807451865739e-09 \n",
      "\n",
      "Epoch:  15790  Learning Rate:  1.389710776398058e-08  Varinance:  6.488730253028109e-09 \n",
      "\n",
      "Epoch:  15791  Learning Rate:  1.3883217602454886e-08  Varinance:  6.4796657347045124e-09 \n",
      "\n",
      "Epoch:  15792  Learning Rate:  1.3869341324147923e-08  Varinance:  6.470613879180761e-09 \n",
      "\n",
      "Epoch:  15793  Learning Rate:  1.3855478915183465e-08  Varinance:  6.461574668767388e-09 \n",
      "\n",
      "Epoch:  15794  Learning Rate:  1.3841630361699076e-08  Varinance:  6.452548085799635e-09 \n",
      "\n",
      "Epoch:  15795  Learning Rate:  1.382779564984618e-08  Varinance:  6.4435341126374256e-09 \n",
      "\n",
      "Epoch:  15796  Learning Rate:  1.3813974765790112e-08  Varinance:  6.434532731665322e-09 \n",
      "\n",
      "Epoch:  15797  Learning Rate:  1.3800167695709957e-08  Varinance:  6.4255439252925206e-09 \n",
      "\n",
      "Epoch:  15798  Learning Rate:  1.3786374425798626e-08  Varinance:  6.416567675952721e-09 \n",
      "\n",
      "Epoch:  15799  Learning Rate:  1.3772594942262893e-08  Varinance:  6.40760396610423e-09 \n",
      "\n",
      "Epoch:  15800  Learning Rate:  1.375882923132325e-08  Varinance:  6.39865277822984e-09 \n",
      "\n",
      "Epoch:  15801  Learning Rate:  1.374507727921396e-08  Varinance:  6.38971409483681e-09 \n",
      "\n",
      "Epoch:  15802  Learning Rate:  1.373133907218312e-08  Varinance:  6.380787898456839e-09 \n",
      "\n",
      "Epoch:  15803  Learning Rate:  1.3717614596492497e-08  Varinance:  6.3718741716460266e-09 \n",
      "\n",
      "Epoch:  15804  Learning Rate:  1.3703903838417587e-08  Varinance:  6.362972896984867e-09 \n",
      "\n",
      "Epoch:  15805  Learning Rate:  1.3690206784247684e-08  Varinance:  6.354084057078115e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15806  Learning Rate:  1.3676523420285705e-08  Varinance:  6.3452076345548984e-09 \n",
      "\n",
      "Epoch:  15807  Learning Rate:  1.3662853732848286e-08  Varinance:  6.336343612068589e-09 \n",
      "\n",
      "Epoch:  15808  Learning Rate:  1.3649197708265715e-08  Varinance:  6.327491972296787e-09 \n",
      "\n",
      "Epoch:  15809  Learning Rate:  1.3635555332882014e-08  Varinance:  6.318652697941296e-09 \n",
      "\n",
      "Epoch:  15810  Learning Rate:  1.362192659305478e-08  Varinance:  6.3098257717280815e-09 \n",
      "\n",
      "Epoch:  15811  Learning Rate:  1.3608311475155252e-08  Varinance:  6.301011176407265e-09 \n",
      "\n",
      "Epoch:  15812  Learning Rate:  1.3594709965568356e-08  Varinance:  6.292208894752998e-09 \n",
      "\n",
      "Epoch:  15813  Learning Rate:  1.358112205069256e-08  Varinance:  6.283418909563558e-09 \n",
      "\n",
      "Epoch:  15814  Learning Rate:  1.3567547716939922e-08  Varinance:  6.2746412036612384e-09 \n",
      "\n",
      "Epoch:  15815  Learning Rate:  1.3553986950736156e-08  Varinance:  6.265875759892323e-09 \n",
      "\n",
      "Epoch:  15816  Learning Rate:  1.3540439738520468e-08  Varinance:  6.257122561127063e-09 \n",
      "\n",
      "Epoch:  15817  Learning Rate:  1.3526906066745625e-08  Varinance:  6.248381590259634e-09 \n",
      "\n",
      "Epoch:  15818  Learning Rate:  1.3513385921878e-08  Varinance:  6.239652830208134e-09 \n",
      "\n",
      "Epoch:  15819  Learning Rate:  1.349987929039742e-08  Varinance:  6.230936263914459e-09 \n",
      "\n",
      "Epoch:  15820  Learning Rate:  1.3486386158797235e-08  Varinance:  6.222231874344396e-09 \n",
      "\n",
      "Epoch:  15821  Learning Rate:  1.3472906513584356e-08  Varinance:  6.213539644487509e-09 \n",
      "\n",
      "Epoch:  15822  Learning Rate:  1.3459440341279113e-08  Varinance:  6.2048595573571246e-09 \n",
      "\n",
      "Epoch:  15823  Learning Rate:  1.3445987628415333e-08  Varinance:  6.196191595990298e-09 \n",
      "\n",
      "Epoch:  15824  Learning Rate:  1.3432548361540277e-08  Varinance:  6.187535743447785e-09 \n",
      "\n",
      "Epoch:  15825  Learning Rate:  1.3419122527214725e-08  Varinance:  6.178891982814018e-09 \n",
      "\n",
      "Epoch:  15826  Learning Rate:  1.3405710112012823e-08  Varinance:  6.170260297197007e-09 \n",
      "\n",
      "Epoch:  15827  Learning Rate:  1.3392311102522122e-08  Varinance:  6.161640669728412e-09 \n",
      "\n",
      "Epoch:  15828  Learning Rate:  1.3378925485343663e-08  Varinance:  6.153033083563445e-09 \n",
      "\n",
      "Epoch:  15829  Learning Rate:  1.3365553247091808e-08  Varinance:  6.144437521880846e-09 \n",
      "\n",
      "Epoch:  15830  Learning Rate:  1.3352194374394288e-08  Varinance:  6.135853967882853e-09 \n",
      "\n",
      "Epoch:  15831  Learning Rate:  1.3338848853892279e-08  Varinance:  6.127282404795171e-09 \n",
      "\n",
      "Epoch:  15832  Learning Rate:  1.3325516672240233e-08  Varinance:  6.118722815866959e-09 \n",
      "\n",
      "Epoch:  15833  Learning Rate:  1.331219781610595e-08  Varinance:  6.110175184370712e-09 \n",
      "\n",
      "Epoch:  15834  Learning Rate:  1.3298892272170613e-08  Varinance:  6.101639493602359e-09 \n",
      "\n",
      "Epoch:  15835  Learning Rate:  1.328560002712866e-08  Varinance:  6.09311572688114e-09 \n",
      "\n",
      "Epoch:  15836  Learning Rate:  1.3272321067687813e-08  Varinance:  6.084603867549596e-09 \n",
      "\n",
      "Epoch:  15837  Learning Rate:  1.3259055380569168e-08  Varinance:  6.076103898973543e-09 \n",
      "\n",
      "Epoch:  15838  Learning Rate:  1.3245802952507005e-08  Varinance:  6.067615804542029e-09 \n",
      "\n",
      "Epoch:  15839  Learning Rate:  1.32325637702489e-08  Varinance:  6.0591395676673335e-09 \n",
      "\n",
      "Epoch:  15840  Learning Rate:  1.3219337820555644e-08  Varinance:  6.05067517178484e-09 \n",
      "\n",
      "Epoch:  15841  Learning Rate:  1.3206125090201332e-08  Varinance:  6.042222600353137e-09 \n",
      "\n",
      "Epoch:  15842  Learning Rate:  1.3192925565973215e-08  Varinance:  6.033781836853902e-09 \n",
      "\n",
      "Epoch:  15843  Learning Rate:  1.3179739234671738e-08  Varinance:  6.025352864791885e-09 \n",
      "\n",
      "Epoch:  15844  Learning Rate:  1.3166566083110617e-08  Varinance:  6.016935667694879e-09 \n",
      "\n",
      "Epoch:  15845  Learning Rate:  1.3153406098116673e-08  Varinance:  6.008530229113691e-09 \n",
      "\n",
      "Epoch:  15846  Learning Rate:  1.3140259266529905e-08  Varinance:  6.000136532622129e-09 \n",
      "\n",
      "Epoch:  15847  Learning Rate:  1.3127125575203517e-08  Varinance:  5.9917545618168776e-09 \n",
      "\n",
      "Epoch:  15848  Learning Rate:  1.3114005011003802e-08  Varinance:  5.9833843003176065e-09 \n",
      "\n",
      "Epoch:  15849  Learning Rate:  1.3100897560810168e-08  Varinance:  5.975025731766844e-09 \n",
      "\n",
      "Epoch:  15850  Learning Rate:  1.3087803211515207e-08  Varinance:  5.966678839829968e-09 \n",
      "\n",
      "Epoch:  15851  Learning Rate:  1.3074721950024549e-08  Varinance:  5.95834360819518e-09 \n",
      "\n",
      "Epoch:  15852  Learning Rate:  1.3061653763256909e-08  Varinance:  5.950020020573463e-09 \n",
      "\n",
      "Epoch:  15853  Learning Rate:  1.3048598638144141e-08  Varinance:  5.941708060698579e-09 \n",
      "\n",
      "Epoch:  15854  Learning Rate:  1.3035556561631101e-08  Varinance:  5.93340771232695e-09 \n",
      "\n",
      "Epoch:  15855  Learning Rate:  1.3022527520675707e-08  Varinance:  5.925118959237754e-09 \n",
      "\n",
      "Epoch:  15856  Learning Rate:  1.3009511502248899e-08  Varinance:  5.916841785232804e-09 \n",
      "\n",
      "Epoch:  15857  Learning Rate:  1.2996508493334699e-08  Varinance:  5.9085761741365445e-09 \n",
      "\n",
      "Epoch:  15858  Learning Rate:  1.2983518480930073e-08  Varinance:  5.900322109796018e-09 \n",
      "\n",
      "Epoch:  15859  Learning Rate:  1.297054145204499e-08  Varinance:  5.8920795760808265e-09 \n",
      "\n",
      "Epoch:  15860  Learning Rate:  1.2957577393702461e-08  Varinance:  5.8838485568831325e-09 \n",
      "\n",
      "Epoch:  15861  Learning Rate:  1.2944626292938407e-08  Varinance:  5.875629036117534e-09 \n",
      "\n",
      "Epoch:  15862  Learning Rate:  1.2931688136801702e-08  Varinance:  5.8674209977211645e-09 \n",
      "\n",
      "Epoch:  15863  Learning Rate:  1.291876291235423e-08  Varinance:  5.859224425653575e-09 \n",
      "\n",
      "Epoch:  15864  Learning Rate:  1.2905850606670754e-08  Varinance:  5.851039303896724e-09 \n",
      "\n",
      "Epoch:  15865  Learning Rate:  1.2892951206838932e-08  Varinance:  5.8428656164549475e-09 \n",
      "\n",
      "Epoch:  15866  Learning Rate:  1.2880064699959415e-08  Varinance:  5.834703347354925e-09 \n",
      "\n",
      "Epoch:  15867  Learning Rate:  1.286719107314567e-08  Varinance:  5.826552480645676e-09 \n",
      "\n",
      "Epoch:  15868  Learning Rate:  1.2854330313524052e-08  Varinance:  5.8184130003984336e-09 \n",
      "\n",
      "Epoch:  15869  Learning Rate:  1.2841482408233839e-08  Varinance:  5.81028489070675e-09 \n",
      "\n",
      "Epoch:  15870  Learning Rate:  1.2828647344427102e-08  Varinance:  5.802168135686375e-09 \n",
      "\n",
      "Epoch:  15871  Learning Rate:  1.2815825109268781e-08  Varinance:  5.79406271947525e-09 \n",
      "\n",
      "Epoch:  15872  Learning Rate:  1.2803015689936614e-08  Varinance:  5.785968626233473e-09 \n",
      "\n",
      "Epoch:  15873  Learning Rate:  1.2790219073621228e-08  Varinance:  5.777885840143292e-09 \n",
      "\n",
      "Epoch:  15874  Learning Rate:  1.2777435247525982e-08  Varinance:  5.7698143454089905e-09 \n",
      "\n",
      "Epoch:  15875  Learning Rate:  1.2764664198867023e-08  Varinance:  5.7617541262569766e-09 \n",
      "\n",
      "Epoch:  15876  Learning Rate:  1.2751905914873348e-08  Varinance:  5.753705166935677e-09 \n",
      "\n",
      "Epoch:  15877  Learning Rate:  1.2739160382786655e-08  Varinance:  5.74566745171552e-09 \n",
      "\n",
      "Epoch:  15878  Learning Rate:  1.2726427589861381e-08  Varinance:  5.737640964888909e-09 \n",
      "\n",
      "Epoch:  15879  Learning Rate:  1.2713707523364778e-08  Varinance:  5.729625690770189e-09 \n",
      "\n",
      "Epoch:  15880  Learning Rate:  1.2701000170576763e-08  Varinance:  5.721621613695639e-09 \n",
      "\n",
      "Epoch:  15881  Learning Rate:  1.2688305518789953e-08  Varinance:  5.713628718023359e-09 \n",
      "\n",
      "Epoch:  15882  Learning Rate:  1.2675623555309739e-08  Varinance:  5.705646988133358e-09 \n",
      "\n",
      "Epoch:  15883  Learning Rate:  1.2662954267454138e-08  Varinance:  5.697676408427449e-09 \n",
      "\n",
      "Epoch:  15884  Learning Rate:  1.2650297642553858e-08  Varinance:  5.689716963329233e-09 \n",
      "\n",
      "Epoch:  15885  Learning Rate:  1.2637653667952256e-08  Varinance:  5.681768637284072e-09 \n",
      "\n",
      "Epoch:  15886  Learning Rate:  1.2625022331005396e-08  Varinance:  5.673831414759054e-09 \n",
      "\n",
      "Epoch:  15887  Learning Rate:  1.2612403619081919e-08  Varinance:  5.665905280242991e-09 \n",
      "\n",
      "Epoch:  15888  Learning Rate:  1.2599797519563086e-08  Varinance:  5.6579902182463e-09 \n",
      "\n",
      "Epoch:  15889  Learning Rate:  1.258720401984285e-08  Varinance:  5.650086213301098e-09 \n",
      "\n",
      "Epoch:  15890  Learning Rate:  1.2574623107327681e-08  Varinance:  5.642193249961088e-09 \n",
      "\n",
      "Epoch:  15891  Learning Rate:  1.2562054769436644e-08  Varinance:  5.634311312801553e-09 \n",
      "\n",
      "Epoch:  15892  Learning Rate:  1.2549498993601446e-08  Varinance:  5.626440386419323e-09 \n",
      "\n",
      "Epoch:  15893  Learning Rate:  1.253695576726629e-08  Varinance:  5.618580455432746e-09 \n",
      "\n",
      "Epoch:  15894  Learning Rate:  1.2524425077887922e-08  Varinance:  5.610731504481676e-09 \n",
      "\n",
      "Epoch:  15895  Learning Rate:  1.2511906912935698e-08  Varinance:  5.602893518227367e-09 \n",
      "\n",
      "Epoch:  15896  Learning Rate:  1.2499401259891427e-08  Varinance:  5.595066481352561e-09 \n",
      "\n",
      "Epoch:  15897  Learning Rate:  1.2486908106249439e-08  Varinance:  5.587250378561373e-09 \n",
      "\n",
      "Epoch:  15898  Learning Rate:  1.2474427439516618e-08  Varinance:  5.5794451945792906e-09 \n",
      "\n",
      "Epoch:  15899  Learning Rate:  1.2461959247212276e-08  Varinance:  5.571650914153138e-09 \n",
      "\n",
      "Epoch:  15900  Learning Rate:  1.2449503516868219e-08  Varinance:  5.563867522051046e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15901  Learning Rate:  1.2437060236028695e-08  Varinance:  5.556095003062446e-09 \n",
      "\n",
      "Epoch:  15902  Learning Rate:  1.2424629392250467e-08  Varinance:  5.548333341997957e-09 \n",
      "\n",
      "Epoch:  15903  Learning Rate:  1.2412210973102666e-08  Varinance:  5.540582523689477e-09 \n",
      "\n",
      "Epoch:  15904  Learning Rate:  1.239980496616685e-08  Varinance:  5.532842532990073e-09 \n",
      "\n",
      "Epoch:  15905  Learning Rate:  1.2387411359037056e-08  Varinance:  5.525113354773973e-09 \n",
      "\n",
      "Epoch:  15906  Learning Rate:  1.2375030139319653e-08  Varinance:  5.517394973936532e-09 \n",
      "\n",
      "Epoch:  15907  Learning Rate:  1.2362661294633399e-08  Varinance:  5.50968737539421e-09 \n",
      "\n",
      "Epoch:  15908  Learning Rate:  1.235030481260949e-08  Varinance:  5.501990544084554e-09 \n",
      "\n",
      "Epoch:  15909  Learning Rate:  1.2337960680891423e-08  Varinance:  5.494304464966096e-09 \n",
      "\n",
      "Epoch:  15910  Learning Rate:  1.2325628887135047e-08  Varinance:  5.486629123018439e-09 \n",
      "\n",
      "Epoch:  15911  Learning Rate:  1.2313309419008602e-08  Varinance:  5.478964503242148e-09 \n",
      "\n",
      "Epoch:  15912  Learning Rate:  1.2301002264192606e-08  Varinance:  5.471310590658743e-09 \n",
      "\n",
      "Epoch:  15913  Learning Rate:  1.2288707410379874e-08  Varinance:  5.463667370310669e-09 \n",
      "\n",
      "Epoch:  15914  Learning Rate:  1.22764248452756e-08  Varinance:  5.456034827261264e-09 \n",
      "\n",
      "Epoch:  15915  Learning Rate:  1.2264154556597194e-08  Varinance:  5.448412946594753e-09 \n",
      "\n",
      "Epoch:  15916  Learning Rate:  1.225189653207437e-08  Varinance:  5.4408017134161385e-09 \n",
      "\n",
      "Epoch:  15917  Learning Rate:  1.2239650759449074e-08  Varinance:  5.433201112851291e-09 \n",
      "\n",
      "Epoch:  15918  Learning Rate:  1.2227417226475578e-08  Varinance:  5.425611130046837e-09 \n",
      "\n",
      "Epoch:  15919  Learning Rate:  1.2215195920920331e-08  Varinance:  5.4180317501701555e-09 \n",
      "\n",
      "Epoch:  15920  Learning Rate:  1.2202986830561999e-08  Varinance:  5.410462958409345e-09 \n",
      "\n",
      "Epoch:  15921  Learning Rate:  1.2190789943191536e-08  Varinance:  5.402904739973195e-09 \n",
      "\n",
      "Epoch:  15922  Learning Rate:  1.217860524661203e-08  Varinance:  5.395357080091179e-09 \n",
      "\n",
      "Epoch:  15923  Learning Rate:  1.2166432728638766e-08  Varinance:  5.387819964013345e-09 \n",
      "\n",
      "Epoch:  15924  Learning Rate:  1.2154272377099268e-08  Varinance:  5.380293377010403e-09 \n",
      "\n",
      "Epoch:  15925  Learning Rate:  1.2142124179833158e-08  Varinance:  5.372777304373623e-09 \n",
      "\n",
      "Epoch:  15926  Learning Rate:  1.2129988124692218e-08  Varinance:  5.36527173141482e-09 \n",
      "\n",
      "Epoch:  15927  Learning Rate:  1.2117864199540434e-08  Varinance:  5.3577766434663285e-09 \n",
      "\n",
      "Epoch:  15928  Learning Rate:  1.2105752392253862e-08  Varinance:  5.350292025880975e-09 \n",
      "\n",
      "Epoch:  15929  Learning Rate:  1.209365269072067e-08  Varinance:  5.342817864032063e-09 \n",
      "\n",
      "Epoch:  15930  Learning Rate:  1.2081565082841196e-08  Varinance:  5.3353541433132744e-09 \n",
      "\n",
      "Epoch:  15931  Learning Rate:  1.2069489556527812e-08  Varinance:  5.327900849138753e-09 \n",
      "\n",
      "Epoch:  15932  Learning Rate:  1.205742609970499e-08  Varinance:  5.3204579669429984e-09 \n",
      "\n",
      "Epoch:  15933  Learning Rate:  1.2045374700309251e-08  Varinance:  5.313025482180856e-09 \n",
      "\n",
      "Epoch:  15934  Learning Rate:  1.2033335346289237e-08  Varinance:  5.305603380327496e-09 \n",
      "\n",
      "Epoch:  15935  Learning Rate:  1.2021308025605572e-08  Varinance:  5.298191646878371e-09 \n",
      "\n",
      "Epoch:  15936  Learning Rate:  1.2009292726230916e-08  Varinance:  5.290790267349223e-09 \n",
      "\n",
      "Epoch:  15937  Learning Rate:  1.1997289436150006e-08  Varinance:  5.2833992272759655e-09 \n",
      "\n",
      "Epoch:  15938  Learning Rate:  1.1985298143359532e-08  Varinance:  5.2760185122147764e-09 \n",
      "\n",
      "Epoch:  15939  Learning Rate:  1.1973318835868179e-08  Varinance:  5.268648107741994e-09 \n",
      "\n",
      "Epoch:  15940  Learning Rate:  1.1961351501696681e-08  Varinance:  5.261287999454102e-09 \n",
      "\n",
      "Epoch:  15941  Learning Rate:  1.1949396128877681e-08  Varinance:  5.253938172967709e-09 \n",
      "\n",
      "Epoch:  15942  Learning Rate:  1.1937452705455787e-08  Varinance:  5.246598613919515e-09 \n",
      "\n",
      "Epoch:  15943  Learning Rate:  1.1925521219487613e-08  Varinance:  5.239269307966303e-09 \n",
      "\n",
      "Epoch:  15944  Learning Rate:  1.1913601659041651e-08  Varinance:  5.231950240784838e-09 \n",
      "\n",
      "Epoch:  15945  Learning Rate:  1.190169401219832e-08  Varinance:  5.224641398071951e-09 \n",
      "\n",
      "Epoch:  15946  Learning Rate:  1.1889798267050014e-08  Varinance:  5.217342765544432e-09 \n",
      "\n",
      "Epoch:  15947  Learning Rate:  1.1877914411700967e-08  Varinance:  5.2100543289390295e-09 \n",
      "\n",
      "Epoch:  15948  Learning Rate:  1.186604243426732e-08  Varinance:  5.202776074012412e-09 \n",
      "\n",
      "Epoch:  15949  Learning Rate:  1.1854182322877076e-08  Varinance:  5.195507986541146e-09 \n",
      "\n",
      "Epoch:  15950  Learning Rate:  1.1842334065670163e-08  Varinance:  5.188250052321691e-09 \n",
      "\n",
      "Epoch:  15951  Learning Rate:  1.1830497650798305e-08  Varinance:  5.181002257170286e-09 \n",
      "\n",
      "Epoch:  15952  Learning Rate:  1.181867306642506e-08  Varinance:  5.173764586923046e-09 \n",
      "\n",
      "Epoch:  15953  Learning Rate:  1.1806860300725889e-08  Varinance:  5.1665370274358485e-09 \n",
      "\n",
      "Epoch:  15954  Learning Rate:  1.1795059341888001e-08  Varinance:  5.159319564584335e-09 \n",
      "\n",
      "Epoch:  15955  Learning Rate:  1.1783270178110418e-08  Varinance:  5.152112184263873e-09 \n",
      "\n",
      "Epoch:  15956  Learning Rate:  1.1771492797604015e-08  Varinance:  5.144914872389537e-09 \n",
      "\n",
      "Epoch:  15957  Learning Rate:  1.1759727188591392e-08  Varinance:  5.137727614896095e-09 \n",
      "\n",
      "Epoch:  15958  Learning Rate:  1.1747973339306915e-08  Varinance:  5.1305503977379095e-09 \n",
      "\n",
      "Epoch:  15959  Learning Rate:  1.1736231237996779e-08  Varinance:  5.1233832068890186e-09 \n",
      "\n",
      "Epoch:  15960  Learning Rate:  1.1724500872918857e-08  Varinance:  5.1162260283430345e-09 \n",
      "\n",
      "Epoch:  15961  Learning Rate:  1.1712782232342766e-08  Varinance:  5.109078848113138e-09 \n",
      "\n",
      "Epoch:  15962  Learning Rate:  1.1701075304549903e-08  Varinance:  5.101941652232048e-09 \n",
      "\n",
      "Epoch:  15963  Learning Rate:  1.1689380077833322e-08  Varinance:  5.0948144267519945e-09 \n",
      "\n",
      "Epoch:  15964  Learning Rate:  1.167769654049779e-08  Varinance:  5.087697157744712e-09 \n",
      "\n",
      "Epoch:  15965  Learning Rate:  1.1666024680859752e-08  Varinance:  5.080589831301335e-09 \n",
      "\n",
      "Epoch:  15966  Learning Rate:  1.1654364487247388e-08  Varinance:  5.073492433532486e-09 \n",
      "\n",
      "Epoch:  15967  Learning Rate:  1.1642715948000481e-08  Varinance:  5.06640495056817e-09 \n",
      "\n",
      "Epoch:  15968  Learning Rate:  1.1631079051470474e-08  Varinance:  5.059327368557768e-09 \n",
      "\n",
      "Epoch:  15969  Learning Rate:  1.1619453786020506e-08  Varinance:  5.052259673670011e-09 \n",
      "\n",
      "Epoch:  15970  Learning Rate:  1.1607840140025294e-08  Varinance:  5.045201852092951e-09 \n",
      "\n",
      "Epoch:  15971  Learning Rate:  1.1596238101871167e-08  Varinance:  5.038153890033951e-09 \n",
      "\n",
      "Epoch:  15972  Learning Rate:  1.158464765995613e-08  Varinance:  5.031115773719593e-09 \n",
      "\n",
      "Epoch:  15973  Learning Rate:  1.157306880268972e-08  Varinance:  5.024087489395749e-09 \n",
      "\n",
      "Epoch:  15974  Learning Rate:  1.1561501518493053e-08  Varinance:  5.0170690233274876e-09 \n",
      "\n",
      "Epoch:  15975  Learning Rate:  1.154994579579889e-08  Varinance:  5.010060361799067e-09 \n",
      "\n",
      "Epoch:  15976  Learning Rate:  1.1538401623051487e-08  Varinance:  5.003061491113905e-09 \n",
      "\n",
      "Epoch:  15977  Learning Rate:  1.1526868988706646e-08  Varinance:  4.996072397594549e-09 \n",
      "\n",
      "Epoch:  15978  Learning Rate:  1.1515347881231776e-08  Varinance:  4.989093067582679e-09 \n",
      "\n",
      "Epoch:  15979  Learning Rate:  1.1503838289105747e-08  Varinance:  4.9821234874389944e-09 \n",
      "\n",
      "Epoch:  15980  Learning Rate:  1.1492340200818964e-08  Varinance:  4.975163643543306e-09 \n",
      "\n",
      "Epoch:  15981  Learning Rate:  1.148085360487332e-08  Varinance:  4.968213522294431e-09 \n",
      "\n",
      "Epoch:  15982  Learning Rate:  1.1469378489782258e-08  Varinance:  4.961273110110189e-09 \n",
      "\n",
      "Epoch:  15983  Learning Rate:  1.1457914844070643e-08  Varinance:  4.9543423934273734e-09 \n",
      "\n",
      "Epoch:  15984  Learning Rate:  1.1446462656274805e-08  Varinance:  4.947421358701722e-09 \n",
      "\n",
      "Epoch:  15985  Learning Rate:  1.1435021914942596e-08  Varinance:  4.940509992407915e-09 \n",
      "\n",
      "Epoch:  15986  Learning Rate:  1.1423592608633257e-08  Varinance:  4.933608281039471e-09 \n",
      "\n",
      "Epoch:  15987  Learning Rate:  1.1412174725917459e-08  Varinance:  4.926716211108831e-09 \n",
      "\n",
      "Epoch:  15988  Learning Rate:  1.1400768255377356e-08  Varinance:  4.91983376914726e-09 \n",
      "\n",
      "Epoch:  15989  Learning Rate:  1.138937318560646e-08  Varinance:  4.91296094170484e-09 \n",
      "\n",
      "Epoch:  15990  Learning Rate:  1.1377989505209677e-08  Varinance:  4.906097715350436e-09 \n",
      "\n",
      "Epoch:  15991  Learning Rate:  1.136661720280337e-08  Varinance:  4.899244076671684e-09 \n",
      "\n",
      "Epoch:  15992  Learning Rate:  1.1355256267015212e-08  Varinance:  4.8924000122749676e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15993  Learning Rate:  1.1343906686484247e-08  Varinance:  4.885565508785332e-09 \n",
      "\n",
      "Epoch:  15994  Learning Rate:  1.1332568449860933e-08  Varinance:  4.878740552846556e-09 \n",
      "\n",
      "Epoch:  15995  Learning Rate:  1.1321241545807014e-08  Varinance:  4.871925131121064e-09 \n",
      "\n",
      "Epoch:  15996  Learning Rate:  1.1309925962995584e-08  Varinance:  4.865119230289907e-09 \n",
      "\n",
      "Epoch:  15997  Learning Rate:  1.1298621690111039e-08  Varinance:  4.8583228370527464e-09 \n",
      "\n",
      "Epoch:  15998  Learning Rate:  1.1287328715849147e-08  Varinance:  4.851535938127821e-09 \n",
      "\n",
      "Epoch:  15999  Learning Rate:  1.127604702891691e-08  Varinance:  4.844758520251942e-09 \n",
      "\n",
      "Epoch:  16000  Learning Rate:  1.1264776618032624e-08  Varinance:  4.8379905701803965e-09 \n",
      "\n",
      "Epoch:  16001  Learning Rate:  1.1253517471925913e-08  Varinance:  4.831232074687026e-09 \n",
      "\n",
      "Epoch:  16002  Learning Rate:  1.1242269579337591e-08  Varinance:  4.824483020564133e-09 \n",
      "\n",
      "Epoch:  16003  Learning Rate:  1.1231032929019827e-08  Varinance:  4.817743394622466e-09 \n",
      "\n",
      "Epoch:  16004  Learning Rate:  1.1219807509735886e-08  Varinance:  4.811013183691205e-09 \n",
      "\n",
      "Epoch:  16005  Learning Rate:  1.120859331026039e-08  Varinance:  4.804292374617923e-09 \n",
      "\n",
      "Epoch:  16006  Learning Rate:  1.119739031937918e-08  Varinance:  4.7975809542685876e-09 \n",
      "\n",
      "Epoch:  16007  Learning Rate:  1.1186198525889182e-08  Varinance:  4.79087890952746e-09 \n",
      "\n",
      "Epoch:  16008  Learning Rate:  1.1175017918598641e-08  Varinance:  4.784186227297177e-09 \n",
      "\n",
      "Epoch:  16009  Learning Rate:  1.116384848632699e-08  Varinance:  4.7775028944986536e-09 \n",
      "\n",
      "Epoch:  16010  Learning Rate:  1.1152690217904716e-08  Varinance:  4.770828898071078e-09 \n",
      "\n",
      "Epoch:  16011  Learning Rate:  1.114154310217359e-08  Varinance:  4.76416422497188e-09 \n",
      "\n",
      "Epoch:  16012  Learning Rate:  1.1130407127986535e-08  Varinance:  4.757508862176731e-09 \n",
      "\n",
      "Epoch:  16013  Learning Rate:  1.1119282284207493e-08  Varinance:  4.75086279667944e-09 \n",
      "\n",
      "Epoch:  16014  Learning Rate:  1.1108168559711664e-08  Varinance:  4.7442260154920434e-09 \n",
      "\n",
      "Epoch:  16015  Learning Rate:  1.109706594338536e-08  Varinance:  4.737598505644698e-09 \n",
      "\n",
      "Epoch:  16016  Learning Rate:  1.1085974424125883e-08  Varinance:  4.730980254185684e-09 \n",
      "\n",
      "Epoch:  16017  Learning Rate:  1.1074893990841795e-08  Varinance:  4.7243712481813705e-09 \n",
      "\n",
      "Epoch:  16018  Learning Rate:  1.1063824632452581e-08  Varinance:  4.717771474716196e-09 \n",
      "\n",
      "Epoch:  16019  Learning Rate:  1.105276633788892e-08  Varinance:  4.711180920892659e-09 \n",
      "\n",
      "Epoch:  16020  Learning Rate:  1.104171909609256e-08  Varinance:  4.704599573831224e-09 \n",
      "\n",
      "Epoch:  16021  Learning Rate:  1.1030682896016174e-08  Varinance:  4.6980274206704e-09 \n",
      "\n",
      "Epoch:  16022  Learning Rate:  1.1019657726623606e-08  Varinance:  4.691464448566644e-09 \n",
      "\n",
      "Epoch:  16023  Learning Rate:  1.100864357688972e-08  Varinance:  4.684910644694355e-09 \n",
      "\n",
      "Epoch:  16024  Learning Rate:  1.099764043580029e-08  Varinance:  4.678365996245851e-09 \n",
      "\n",
      "Epoch:  16025  Learning Rate:  1.0986648292352212e-08  Varinance:  4.67183049043134e-09 \n",
      "\n",
      "Epoch:  16026  Learning Rate:  1.0975667135553382e-08  Varinance:  4.665304114478914e-09 \n",
      "\n",
      "Epoch:  16027  Learning Rate:  1.0964696954422562e-08  Varinance:  4.658786855634458e-09 \n",
      "\n",
      "Epoch:  16028  Learning Rate:  1.095373773798961e-08  Varinance:  4.652278701161721e-09 \n",
      "\n",
      "Epoch:  16029  Learning Rate:  1.094278947529535e-08  Varinance:  4.645779638342232e-09 \n",
      "\n",
      "Epoch:  16030  Learning Rate:  1.0931852155391436e-08  Varinance:  4.639289654475281e-09 \n",
      "\n",
      "Epoch:  16031  Learning Rate:  1.0920925767340589e-08  Varinance:  4.632808736877908e-09 \n",
      "\n",
      "Epoch:  16032  Learning Rate:  1.0910010300216458e-08  Varinance:  4.626336872884864e-09 \n",
      "\n",
      "Epoch:  16033  Learning Rate:  1.0899105743103498e-08  Varinance:  4.619874049848614e-09 \n",
      "\n",
      "Epoch:  16034  Learning Rate:  1.088821208509719e-08  Varinance:  4.613420255139238e-09 \n",
      "\n",
      "Epoch:  16035  Learning Rate:  1.0877329315303911e-08  Varinance:  4.606975476144513e-09 \n",
      "\n",
      "Epoch:  16036  Learning Rate:  1.0866457422840816e-08  Varinance:  4.600539700269814e-09 \n",
      "\n",
      "Epoch:  16037  Learning Rate:  1.0855596396836049e-08  Varinance:  4.594112914938113e-09 \n",
      "\n",
      "Epoch:  16038  Learning Rate:  1.0844746226428623e-08  Varinance:  4.58769510758995e-09 \n",
      "\n",
      "Epoch:  16039  Learning Rate:  1.0833906900768288e-08  Varinance:  4.581286265683414e-09 \n",
      "\n",
      "Epoch:  16040  Learning Rate:  1.0823078409015758e-08  Varinance:  4.574886376694125e-09 \n",
      "\n",
      "Epoch:  16041  Learning Rate:  1.0812260740342578e-08  Varinance:  4.568495428115154e-09 \n",
      "\n",
      "Epoch:  16042  Learning Rate:  1.0801453883930999e-08  Varinance:  4.562113407457093e-09 \n",
      "\n",
      "Epoch:  16043  Learning Rate:  1.0790657828974203e-08  Varinance:  4.555740302247962e-09 \n",
      "\n",
      "Epoch:  16044  Learning Rate:  1.0779872564676175e-08  Varinance:  4.549376100033207e-09 \n",
      "\n",
      "Epoch:  16045  Learning Rate:  1.0769098080251573e-08  Varinance:  4.54302078837567e-09 \n",
      "\n",
      "Epoch:  16046  Learning Rate:  1.0758334364925946e-08  Varinance:  4.53667435485557e-09 \n",
      "\n",
      "Epoch:  16047  Learning Rate:  1.074758140793562e-08  Varinance:  4.5303367870704884e-09 \n",
      "\n",
      "Epoch:  16048  Learning Rate:  1.073683919852756e-08  Varinance:  4.5240080726352885e-09 \n",
      "\n",
      "Epoch:  16049  Learning Rate:  1.072610772595963e-08  Varinance:  4.517688199182179e-09 \n",
      "\n",
      "Epoch:  16050  Learning Rate:  1.0715386979500282e-08  Varinance:  4.511377154360633e-09 \n",
      "\n",
      "Epoch:  16051  Learning Rate:  1.0704676948428808e-08  Varinance:  4.505074925837377e-09 \n",
      "\n",
      "Epoch:  16052  Learning Rate:  1.0693977622035212e-08  Varinance:  4.498781501296363e-09 \n",
      "\n",
      "Epoch:  16053  Learning Rate:  1.068328898962009e-08  Varinance:  4.492496868438752e-09 \n",
      "\n",
      "Epoch:  16054  Learning Rate:  1.067261104049485e-08  Varinance:  4.486221014982901e-09 \n",
      "\n",
      "Epoch:  16055  Learning Rate:  1.0661943763981575e-08  Varinance:  4.479953928664273e-09 \n",
      "\n",
      "Epoch:  16056  Learning Rate:  1.0651287149412915e-08  Varinance:  4.4736955972355175e-09 \n",
      "\n",
      "Epoch:  16057  Learning Rate:  1.0640641186132293e-08  Varinance:  4.467446008466373e-09 \n",
      "\n",
      "Epoch:  16058  Learning Rate:  1.0630005863493783e-08  Varinance:  4.461205150143665e-09 \n",
      "\n",
      "Epoch:  16059  Learning Rate:  1.0619381170861982e-08  Varinance:  4.454973010071279e-09 \n",
      "\n",
      "Epoch:  16060  Learning Rate:  1.0608767097612238e-08  Varinance:  4.448749576070141e-09 \n",
      "\n",
      "Epoch:  16061  Learning Rate:  1.0598163633130515e-08  Varinance:  4.442534835978201e-09 \n",
      "\n",
      "Epoch:  16062  Learning Rate:  1.0587570766813268e-08  Varinance:  4.436328777650357e-09 \n",
      "\n",
      "Epoch:  16063  Learning Rate:  1.057698848806767e-08  Varinance:  4.430131388958519e-09 \n",
      "\n",
      "Epoch:  16064  Learning Rate:  1.0566416786311482e-08  Varinance:  4.4239426577915214e-09 \n",
      "\n",
      "Epoch:  16065  Learning Rate:  1.055585565097292e-08  Varinance:  4.4177625720551215e-09 \n",
      "\n",
      "Epoch:  16066  Learning Rate:  1.0545305071490892e-08  Varinance:  4.4115911196719676e-09 \n",
      "\n",
      "Epoch:  16067  Learning Rate:  1.0534765037314848e-08  Varinance:  4.4054282885815836e-09 \n",
      "\n",
      "Epoch:  16068  Learning Rate:  1.0524235537904684e-08  Varinance:  4.399274066740354e-09 \n",
      "\n",
      "Epoch:  16069  Learning Rate:  1.0513716562730935e-08  Varinance:  4.393128442121442e-09 \n",
      "\n",
      "Epoch:  16070  Learning Rate:  1.0503208101274662e-08  Varinance:  4.386991402714859e-09 \n",
      "\n",
      "Epoch:  16071  Learning Rate:  1.0492710143027328e-08  Varinance:  4.380862936527378e-09 \n",
      "\n",
      "Epoch:  16072  Learning Rate:  1.048222267749101e-08  Varinance:  4.374743031582527e-09 \n",
      "\n",
      "Epoch:  16073  Learning Rate:  1.0471745694178282e-08  Varinance:  4.3686316759205624e-09 \n",
      "\n",
      "Epoch:  16074  Learning Rate:  1.0461279182612084e-08  Varinance:  4.36252885759845e-09 \n",
      "\n",
      "Epoch:  16075  Learning Rate:  1.045082313232594e-08  Varinance:  4.356434564689856e-09 \n",
      "\n",
      "Epoch:  16076  Learning Rate:  1.0440377532863837e-08  Varinance:  4.350348785285055e-09 \n",
      "\n",
      "Epoch:  16077  Learning Rate:  1.0429942373780098e-08  Varinance:  4.344271507491014e-09 \n",
      "\n",
      "Epoch:  16078  Learning Rate:  1.0419517644639603e-08  Varinance:  4.338202719431294e-09 \n",
      "\n",
      "Epoch:  16079  Learning Rate:  1.0409103335017658e-08  Varinance:  4.332142409246045e-09 \n",
      "\n",
      "Epoch:  16080  Learning Rate:  1.0398699434499876e-08  Varinance:  4.326090565091988e-09 \n",
      "\n",
      "Epoch:  16081  Learning Rate:  1.0388305932682435e-08  Varinance:  4.320047175142389e-09 \n",
      "\n",
      "Epoch:  16082  Learning Rate:  1.0377922819171756e-08  Varinance:  4.314012227587049e-09 \n",
      "\n",
      "Epoch:  16083  Learning Rate:  1.0367550083584759e-08  Varinance:  4.307985710632221e-09 \n",
      "\n",
      "Epoch:  16084  Learning Rate:  1.0357187715548747e-08  Varinance:  4.30196761250068e-09 \n",
      "\n",
      "Epoch:  16085  Learning Rate:  1.0346835704701277e-08  Varinance:  4.295957921431641e-09 \n",
      "\n",
      "Epoch:  16086  Learning Rate:  1.0336494040690374e-08  Varinance:  4.289956625680744e-09 \n",
      "\n",
      "Epoch:  16087  Learning Rate:  1.032616271317441e-08  Varinance:  4.283963713520038e-09 \n",
      "\n",
      "Epoch:  16088  Learning Rate:  1.0315841711821981e-08  Varinance:  4.277979173237955e-09 \n",
      "\n",
      "Epoch:  16089  Learning Rate:  1.0305531026312126e-08  Varinance:  4.2720029931393006e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16090  Learning Rate:  1.0295230646334192e-08  Varinance:  4.266035161545177e-09 \n",
      "\n",
      "Epoch:  16091  Learning Rate:  1.0284940561587726e-08  Varinance:  4.2600756667930435e-09 \n",
      "\n",
      "Epoch:  16092  Learning Rate:  1.0274660761782678e-08  Varinance:  4.254124497236637e-09 \n",
      "\n",
      "Epoch:  16093  Learning Rate:  1.0264391236639284e-08  Varinance:  4.248181641245966e-09 \n",
      "\n",
      "Epoch:  16094  Learning Rate:  1.0254131975887946e-08  Varinance:  4.242247087207283e-09 \n",
      "\n",
      "Epoch:  16095  Learning Rate:  1.024388296926944e-08  Varinance:  4.236320823523064e-09 \n",
      "\n",
      "Epoch:  16096  Learning Rate:  1.0233644206534791e-08  Varinance:  4.230402838612003e-09 \n",
      "\n",
      "Epoch:  16097  Learning Rate:  1.0223415677445167e-08  Varinance:  4.22449312090893e-09 \n",
      "\n",
      "Epoch:  16098  Learning Rate:  1.0213197371772071e-08  Varinance:  4.218591658864871e-09 \n",
      "\n",
      "Epoch:  16099  Learning Rate:  1.0202989279297235e-08  Varinance:  4.212698440946974e-09 \n",
      "\n",
      "Epoch:  16100  Learning Rate:  1.019279138981249e-08  Varinance:  4.206813455638496e-09 \n",
      "\n",
      "Epoch:  16101  Learning Rate:  1.0182603693119987e-08  Varinance:  4.200936691438784e-09 \n",
      "\n",
      "Epoch:  16102  Learning Rate:  1.0172426179032062e-08  Varinance:  4.19506813686325e-09 \n",
      "\n",
      "Epoch:  16103  Learning Rate:  1.0162258837371125e-08  Varinance:  4.189207780443365e-09 \n",
      "\n",
      "Epoch:  16104  Learning Rate:  1.0152101657969874e-08  Varinance:  4.1833556107265764e-09 \n",
      "\n",
      "Epoch:  16105  Learning Rate:  1.0141954630671163e-08  Varinance:  4.177511616276376e-09 \n",
      "\n",
      "Epoch:  16106  Learning Rate:  1.0131817745327892e-08  Varinance:  4.171675785672214e-09 \n",
      "\n",
      "Epoch:  16107  Learning Rate:  1.012169099180321e-08  Varinance:  4.1658481075094974e-09 \n",
      "\n",
      "Epoch:  16108  Learning Rate:  1.0111574359970399e-08  Varinance:  4.160028570399566e-09 \n",
      "\n",
      "Epoch:  16109  Learning Rate:  1.0101467839712755e-08  Varinance:  4.154217162969665e-09 \n",
      "\n",
      "Epoch:  16110  Learning Rate:  1.0091371420923793e-08  Varinance:  4.148413873862945e-09 \n",
      "\n",
      "Epoch:  16111  Learning Rate:  1.0081285093507128e-08  Varinance:  4.142618691738378e-09 \n",
      "\n",
      "Epoch:  16112  Learning Rate:  1.0071208847376361e-08  Varinance:  4.136831605270819e-09 \n",
      "\n",
      "Epoch:  16113  Learning Rate:  1.0061142672455316e-08  Varinance:  4.1310526031509346e-09 \n",
      "\n",
      "Epoch:  16114  Learning Rate:  1.0051086558677747e-08  Varinance:  4.125281674085187e-09 \n",
      "\n",
      "Epoch:  16115  Learning Rate:  1.0041040495987574e-08  Varinance:  4.119518806795814e-09 \n",
      "\n",
      "Epoch:  16116  Learning Rate:  1.0031004474338768e-08  Varinance:  4.11376399002081e-09 \n",
      "\n",
      "Epoch:  16117  Learning Rate:  1.002097848369524e-08  Varinance:  4.1080172125139175e-09 \n",
      "\n",
      "Epoch:  16118  Learning Rate:  1.0010962514031027e-08  Varinance:  4.1022784630445435e-09 \n",
      "\n",
      "Epoch:  16119  Learning Rate:  1.0000956555330201e-08  Varinance:  4.096547730397827e-09 \n",
      "\n",
      "Epoch:  16120  Learning Rate:  9.990960597586728e-09  Varinance:  4.090825003374563e-09 \n",
      "\n",
      "Epoch:  16121  Learning Rate:  9.980974630804683e-09  Varinance:  4.085110270791188e-09 \n",
      "\n",
      "Epoch:  16122  Learning Rate:  9.970998644998136e-09  Varinance:  4.079403521479762e-09 \n",
      "\n",
      "Epoch:  16123  Learning Rate:  9.961032630191031e-09  Varinance:  4.073704744287947e-09 \n",
      "\n",
      "Epoch:  16124  Learning Rate:  9.951076576417386e-09  Varinance:  4.068013928078999e-09 \n",
      "\n",
      "Epoch:  16125  Learning Rate:  9.941130473721181e-09  Varinance:  4.062331061731688e-09 \n",
      "\n",
      "Epoch:  16126  Learning Rate:  9.931194312156245e-09  Varinance:  4.056656134140362e-09 \n",
      "\n",
      "Epoch:  16127  Learning Rate:  9.921268081786447e-09  Varinance:  4.050989134214871e-09 \n",
      "\n",
      "Epoch:  16128  Learning Rate:  9.911351772685593e-09  Varinance:  4.045330050880556e-09 \n",
      "\n",
      "Epoch:  16129  Learning Rate:  9.901445374937303e-09  Varinance:  4.0396788730782286e-09 \n",
      "\n",
      "Epoch:  16130  Learning Rate:  9.891548878635212e-09  Varinance:  4.034035589764152e-09 \n",
      "\n",
      "Epoch:  16131  Learning Rate:  9.88166227388286e-09  Varinance:  4.0284001899100285e-09 \n",
      "\n",
      "Epoch:  16132  Learning Rate:  9.871785550793568e-09  Varinance:  4.022772662502925e-09 \n",
      "\n",
      "Epoch:  16133  Learning Rate:  9.861918699490653e-09  Varinance:  4.017152996545336e-09 \n",
      "\n",
      "Epoch:  16134  Learning Rate:  9.852061710107293e-09  Varinance:  4.011541181055105e-09 \n",
      "\n",
      "Epoch:  16135  Learning Rate:  9.842214572786428e-09  Varinance:  4.005937205065417e-09 \n",
      "\n",
      "Epoch:  16136  Learning Rate:  9.832377277680958e-09  Varinance:  4.000341057624776e-09 \n",
      "\n",
      "Epoch:  16137  Learning Rate:  9.822549814953619e-09  Varinance:  3.994752727796988e-09 \n",
      "\n",
      "Epoch:  16138  Learning Rate:  9.812732174776878e-09  Varinance:  3.989172204661146e-09 \n",
      "\n",
      "Epoch:  16139  Learning Rate:  9.80292434733313e-09  Varinance:  3.983599477311562e-09 \n",
      "\n",
      "Epoch:  16140  Learning Rate:  9.79312632281458e-09  Varinance:  3.978034534857818e-09 \n",
      "\n",
      "Epoch:  16141  Learning Rate:  9.783338091423136e-09  Varinance:  3.972477366424706e-09 \n",
      "\n",
      "Epoch:  16142  Learning Rate:  9.773559643370632e-09  Varinance:  3.966927961152199e-09 \n",
      "\n",
      "Epoch:  16143  Learning Rate:  9.763790968878552e-09  Varinance:  3.961386308195448e-09 \n",
      "\n",
      "Epoch:  16144  Learning Rate:  9.754032058178254e-09  Varinance:  3.955852396724767e-09 \n",
      "\n",
      "Epoch:  16145  Learning Rate:  9.744282901510862e-09  Varinance:  3.950326215925555e-09 \n",
      "\n",
      "Epoch:  16146  Learning Rate:  9.734543489127148e-09  Varinance:  3.94480775499836e-09 \n",
      "\n",
      "Epoch:  16147  Learning Rate:  9.724813811287736e-09  Varinance:  3.939297003158806e-09 \n",
      "\n",
      "Epoch:  16148  Learning Rate:  9.71509385826298e-09  Varinance:  3.9337939496375794e-09 \n",
      "\n",
      "Epoch:  16149  Learning Rate:  9.705383620332857e-09  Varinance:  3.928298583680411e-09 \n",
      "\n",
      "Epoch:  16150  Learning Rate:  9.695683087787161e-09  Varinance:  3.922810894548056e-09 \n",
      "\n",
      "Epoch:  16151  Learning Rate:  9.685992250925398e-09  Varinance:  3.917330871516285e-09 \n",
      "\n",
      "Epoch:  16152  Learning Rate:  9.676311100056657e-09  Varinance:  3.9118585038758095e-09 \n",
      "\n",
      "Epoch:  16153  Learning Rate:  9.666639625499823e-09  Varinance:  3.906393780932341e-09 \n",
      "\n",
      "Epoch:  16154  Learning Rate:  9.656977817583456e-09  Varinance:  3.90093669200652e-09 \n",
      "\n",
      "Epoch:  16155  Learning Rate:  9.647325666645675e-09  Varinance:  3.8954872264339016e-09 \n",
      "\n",
      "Epoch:  16156  Learning Rate:  9.637683163034365e-09  Varinance:  3.890045373564943e-09 \n",
      "\n",
      "Epoch:  16157  Learning Rate:  9.628050297107056e-09  Varinance:  3.884611122764975e-09 \n",
      "\n",
      "Epoch:  16158  Learning Rate:  9.618427059230811e-09  Varinance:  3.879184463414198e-09 \n",
      "\n",
      "Epoch:  16159  Learning Rate:  9.608813439782428e-09  Varinance:  3.873765384907611e-09 \n",
      "\n",
      "Epoch:  16160  Learning Rate:  9.599209429148319e-09  Varinance:  3.868353876655065e-09 \n",
      "\n",
      "Epoch:  16161  Learning Rate:  9.589615017724404e-09  Varinance:  3.862949928081193e-09 \n",
      "\n",
      "Epoch:  16162  Learning Rate:  9.580030195916309e-09  Varinance:  3.8575535286254e-09 \n",
      "\n",
      "Epoch:  16163  Learning Rate:  9.57045495413924e-09  Varinance:  3.852164667741846e-09 \n",
      "\n",
      "Epoch:  16164  Learning Rate:  9.560889282817889e-09  Varinance:  3.846783334899422e-09 \n",
      "\n",
      "Epoch:  16165  Learning Rate:  9.551333172386618e-09  Varinance:  3.8414095195817425e-09 \n",
      "\n",
      "Epoch:  16166  Learning Rate:  9.541786613289348e-09  Varinance:  3.836043211287076e-09 \n",
      "\n",
      "Epoch:  16167  Learning Rate:  9.532249595979455e-09  Varinance:  3.830684399528397e-09 \n",
      "\n",
      "Epoch:  16168  Learning Rate:  9.52272211091995e-09  Varinance:  3.825333073833322e-09 \n",
      "\n",
      "Epoch:  16169  Learning Rate:  9.513204148583382e-09  Varinance:  3.819989223744094e-09 \n",
      "\n",
      "Epoch:  16170  Learning Rate:  9.503695699451726e-09  Varinance:  3.814652838817566e-09 \n",
      "\n",
      "Epoch:  16171  Learning Rate:  9.494196754016557e-09  Varinance:  3.8093239086251796e-09 \n",
      "\n",
      "Epoch:  16172  Learning Rate:  9.48470730277897e-09  Varinance:  3.804002422752956e-09 \n",
      "\n",
      "Epoch:  16173  Learning Rate:  9.475227336249442e-09  Varinance:  3.798688370801427e-09 \n",
      "\n",
      "Epoch:  16174  Learning Rate:  9.465756844948072e-09  Varinance:  3.793381742385692e-09 \n",
      "\n",
      "Epoch:  16175  Learning Rate:  9.456295819404303e-09  Varinance:  3.788082527135343e-09 \n",
      "\n",
      "Epoch:  16176  Learning Rate:  9.44684425015714e-09  Varinance:  3.782790714694459e-09 \n",
      "\n",
      "Epoch:  16177  Learning Rate:  9.43740212775505e-09  Varinance:  3.777506294721587e-09 \n",
      "\n",
      "Epoch:  16178  Learning Rate:  9.427969442755841e-09  Varinance:  3.772229256889722e-09 \n",
      "\n",
      "Epoch:  16179  Learning Rate:  9.41854618572686e-09  Varinance:  3.766959590886295e-09 \n",
      "\n",
      "Epoch:  16180  Learning Rate:  9.409132347244881e-09  Varinance:  3.761697286413104e-09 \n",
      "\n",
      "Epoch:  16181  Learning Rate:  9.399727917896002e-09  Varinance:  3.756442333186376e-09 \n",
      "\n",
      "Epoch:  16182  Learning Rate:  9.390332888275825e-09  Varinance:  3.751194720936691e-09 \n",
      "\n",
      "Epoch:  16183  Learning Rate:  9.380947248989351e-09  Varinance:  3.74595443940897e-09 \n",
      "\n",
      "Epoch:  16184  Learning Rate:  9.371570990650874e-09  Varinance:  3.740721478362465e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16185  Learning Rate:  9.362204103884169e-09  Varinance:  3.735495827570731e-09 \n",
      "\n",
      "Epoch:  16186  Learning Rate:  9.35284657932238e-09  Varinance:  3.7302774768216234e-09 \n",
      "\n",
      "Epoch:  16187  Learning Rate:  9.343498407607917e-09  Varinance:  3.7250664159172243e-09 \n",
      "\n",
      "Epoch:  16188  Learning Rate:  9.334159579392642e-09  Varinance:  3.7198626346739004e-09 \n",
      "\n",
      "Epoch:  16189  Learning Rate:  9.324830085337757e-09  Varinance:  3.7146661229222335e-09 \n",
      "\n",
      "Epoch:  16190  Learning Rate:  9.3155099161137e-09  Varinance:  3.709476870507008e-09 \n",
      "\n",
      "Epoch:  16191  Learning Rate:  9.306199062400336e-09  Varinance:  3.7042948672871985e-09 \n",
      "\n",
      "Epoch:  16192  Learning Rate:  9.296897514886843e-09  Varinance:  3.6991201031359447e-09 \n",
      "\n",
      "Epoch:  16193  Learning Rate:  9.287605264271606e-09  Varinance:  3.6939525679405462e-09 \n",
      "\n",
      "Epoch:  16194  Learning Rate:  9.278322301262409e-09  Varinance:  3.68879225160239e-09 \n",
      "\n",
      "Epoch:  16195  Learning Rate:  9.269048616576318e-09  Varinance:  3.6836391440370093e-09 \n",
      "\n",
      "Epoch:  16196  Learning Rate:  9.259784200939583e-09  Varinance:  3.678493235174015e-09 \n",
      "\n",
      "Epoch:  16197  Learning Rate:  9.250529045087822e-09  Varinance:  3.673354514957083e-09 \n",
      "\n",
      "Epoch:  16198  Learning Rate:  9.241283139765908e-09  Varinance:  3.6682229733439365e-09 \n",
      "\n",
      "Epoch:  16199  Learning Rate:  9.232046475727872e-09  Varinance:  3.6630986003063317e-09 \n",
      "\n",
      "Epoch:  16200  Learning Rate:  9.222819043737082e-09  Varinance:  3.657981385830042e-09 \n",
      "\n",
      "Epoch:  16201  Learning Rate:  9.213600834566135e-09  Varinance:  3.6528713199147955e-09 \n",
      "\n",
      "Epoch:  16202  Learning Rate:  9.204391838996758e-09  Varinance:  3.6477683925743265e-09 \n",
      "\n",
      "Epoch:  16203  Learning Rate:  9.195192047819988e-09  Varinance:  3.6426725938363077e-09 \n",
      "\n",
      "Epoch:  16204  Learning Rate:  9.186001451836065e-09  Varinance:  3.6375839137423434e-09 \n",
      "\n",
      "Epoch:  16205  Learning Rate:  9.176820041854325e-09  Varinance:  3.6325023423479477e-09 \n",
      "\n",
      "Epoch:  16206  Learning Rate:  9.167647808693427e-09  Varinance:  3.6274278697225293e-09 \n",
      "\n",
      "Epoch:  16207  Learning Rate:  9.158484743181067e-09  Varinance:  3.6223604859493803e-09 \n",
      "\n",
      "Epoch:  16208  Learning Rate:  9.149330836154213e-09  Varinance:  3.617300181125607e-09 \n",
      "\n",
      "Epoch:  16209  Learning Rate:  9.14018607845899e-09  Varinance:  3.6122469453621915e-09 \n",
      "\n",
      "Epoch:  16210  Learning Rate:  9.131050460950577e-09  Varinance:  3.6072007687839142e-09 \n",
      "\n",
      "Epoch:  16211  Learning Rate:  9.121923974493384e-09  Varinance:  3.602161641529353e-09 \n",
      "\n",
      "Epoch:  16212  Learning Rate:  9.112806609960958e-09  Varinance:  3.5971295537508602e-09 \n",
      "\n",
      "Epoch:  16213  Learning Rate:  9.10369835823587e-09  Varinance:  3.5921044956145467e-09 \n",
      "\n",
      "Epoch:  16214  Learning Rate:  9.094599210209898e-09  Varinance:  3.587086457300273e-09 \n",
      "\n",
      "Epoch:  16215  Learning Rate:  9.085509156783928e-09  Varinance:  3.582075429001578e-09 \n",
      "\n",
      "Epoch:  16216  Learning Rate:  9.076428188867837e-09  Varinance:  3.577071400925741e-09 \n",
      "\n",
      "Epoch:  16217  Learning Rate:  9.067356297380693e-09  Varinance:  3.5720743632937063e-09 \n",
      "\n",
      "Epoch:  16218  Learning Rate:  9.058293473250633e-09  Varinance:  3.567084306340081e-09 \n",
      "\n",
      "Epoch:  16219  Learning Rate:  9.04923970741477e-09  Varinance:  3.5621012203131123e-09 \n",
      "\n",
      "Epoch:  16220  Learning Rate:  9.040194990819368e-09  Varinance:  3.5571250954746725e-09 \n",
      "\n",
      "Epoch:  16221  Learning Rate:  9.031159314419743e-09  Varinance:  3.552155922100248e-09 \n",
      "\n",
      "Epoch:  16222  Learning Rate:  9.022132669180153e-09  Varinance:  3.5471936904788728e-09 \n",
      "\n",
      "Epoch:  16223  Learning Rate:  9.013115046073983e-09  Varinance:  3.5422383909131855e-09 \n",
      "\n",
      "Epoch:  16224  Learning Rate:  9.004106436083643e-09  Varinance:  3.5372900137193594e-09 \n",
      "\n",
      "Epoch:  16225  Learning Rate:  8.995106830200457e-09  Varinance:  3.5323485492270933e-09 \n",
      "\n",
      "Epoch:  16226  Learning Rate:  8.98611621942485e-09  Varinance:  3.5274139877795963e-09 \n",
      "\n",
      "Epoch:  16227  Learning Rate:  8.977134594766244e-09  Varinance:  3.5224863197335693e-09 \n",
      "\n",
      "Epoch:  16228  Learning Rate:  8.96816194724295e-09  Varinance:  3.5175655354591948e-09 \n",
      "\n",
      "Epoch:  16229  Learning Rate:  8.95919826788235e-09  Varinance:  3.512651625340071e-09 \n",
      "\n",
      "Epoch:  16230  Learning Rate:  8.950243547720796e-09  Varinance:  3.507744579773268e-09 \n",
      "\n",
      "Epoch:  16231  Learning Rate:  8.941297777803503e-09  Varinance:  3.502844389169257e-09 \n",
      "\n",
      "Epoch:  16232  Learning Rate:  8.932360949184734e-09  Varinance:  3.497951043951907e-09 \n",
      "\n",
      "Epoch:  16233  Learning Rate:  8.92343305292769e-09  Varinance:  3.4930645345584634e-09 \n",
      "\n",
      "Epoch:  16234  Learning Rate:  8.91451408010441e-09  Varinance:  3.4881848514395314e-09 \n",
      "\n",
      "Epoch:  16235  Learning Rate:  8.905604021795954e-09  Varinance:  3.4833119850590666e-09 \n",
      "\n",
      "Epoch:  16236  Learning Rate:  8.896702869092294e-09  Varinance:  3.4784459258943118e-09 \n",
      "\n",
      "Epoch:  16237  Learning Rate:  8.887810613092212e-09  Varinance:  3.4735866644358485e-09 \n",
      "\n",
      "Epoch:  16238  Learning Rate:  8.878927244903516e-09  Varinance:  3.4687341911875296e-09 \n",
      "\n",
      "Epoch:  16239  Learning Rate:  8.870052755642772e-09  Varinance:  3.463888496666474e-09 \n",
      "\n",
      "Epoch:  16240  Learning Rate:  8.861187136435523e-09  Varinance:  3.4590495714030496e-09 \n",
      "\n",
      "Epoch:  16241  Learning Rate:  8.852330378416181e-09  Varinance:  3.4542174059408505e-09 \n",
      "\n",
      "Epoch:  16242  Learning Rate:  8.843482472727925e-09  Varinance:  3.4493919908366946e-09 \n",
      "\n",
      "Epoch:  16243  Learning Rate:  8.834643410522875e-09  Varinance:  3.444573316660554e-09 \n",
      "\n",
      "Epoch:  16244  Learning Rate:  8.825813182962006e-09  Varinance:  3.4397613739956135e-09 \n",
      "\n",
      "Epoch:  16245  Learning Rate:  8.816991781215024e-09  Varinance:  3.4349561534381972e-09 \n",
      "\n",
      "Epoch:  16246  Learning Rate:  8.808179196460558e-09  Varinance:  3.4301576455977677e-09 \n",
      "\n",
      "Epoch:  16247  Learning Rate:  8.799375419886053e-09  Varinance:  3.425365841096905e-09 \n",
      "\n",
      "Epoch:  16248  Learning Rate:  8.79058044268767e-09  Varinance:  3.42058073057129e-09 \n",
      "\n",
      "Epoch:  16249  Learning Rate:  8.781794256070462e-09  Varinance:  3.415802304669696e-09 \n",
      "\n",
      "Epoch:  16250  Learning Rate:  8.773016851248273e-09  Varinance:  3.4110305540539255e-09 \n",
      "\n",
      "Epoch:  16251  Learning Rate:  8.764248219443636e-09  Varinance:  3.406265469398859e-09 \n",
      "\n",
      "Epoch:  16252  Learning Rate:  8.75548835188795e-09  Varinance:  3.401507041392395e-09 \n",
      "\n",
      "Epoch:  16253  Learning Rate:  8.746737239821374e-09  Varinance:  3.3967552607354394e-09 \n",
      "\n",
      "Epoch:  16254  Learning Rate:  8.737994874492736e-09  Varinance:  3.3920101181418883e-09 \n",
      "\n",
      "Epoch:  16255  Learning Rate:  8.729261247159703e-09  Varinance:  3.387271604338611e-09 \n",
      "\n",
      "Epoch:  16256  Learning Rate:  8.720536349088673e-09  Varinance:  3.3825397100654415e-09 \n",
      "\n",
      "Epoch:  16257  Learning Rate:  8.71182017155469e-09  Varinance:  3.377814426075115e-09 \n",
      "\n",
      "Epoch:  16258  Learning Rate:  8.703112705841602e-09  Varinance:  3.373095743133321e-09 \n",
      "\n",
      "Epoch:  16259  Learning Rate:  8.694413943241978e-09  Varinance:  3.3683836520186373e-09 \n",
      "\n",
      "Epoch:  16260  Learning Rate:  8.68572387505699e-09  Varinance:  3.3636781435225223e-09 \n",
      "\n",
      "Epoch:  16261  Learning Rate:  8.6770424925966e-09  Varinance:  3.358979208449299e-09 \n",
      "\n",
      "Epoch:  16262  Learning Rate:  8.668369787179459e-09  Varinance:  3.354286837616137e-09 \n",
      "\n",
      "Epoch:  16263  Learning Rate:  8.659705750132795e-09  Varinance:  3.3496010218530447e-09 \n",
      "\n",
      "Epoch:  16264  Learning Rate:  8.651050372792604e-09  Varinance:  3.3449217520028067e-09 \n",
      "\n",
      "Epoch:  16265  Learning Rate:  8.642403646503537e-09  Varinance:  3.340249018921034e-09 \n",
      "\n",
      "Epoch:  16266  Learning Rate:  8.633765562618804e-09  Varinance:  3.3355828134761013e-09 \n",
      "\n",
      "Epoch:  16267  Learning Rate:  8.625136112500385e-09  Varinance:  3.330923126549138e-09 \n",
      "\n",
      "Epoch:  16268  Learning Rate:  8.616515287518767e-09  Varinance:  3.326269949034014e-09 \n",
      "\n",
      "Epoch:  16269  Learning Rate:  8.607903079053155e-09  Varinance:  3.3216232718373198e-09 \n",
      "\n",
      "Epoch:  16270  Learning Rate:  8.59929947849137e-09  Varinance:  3.316983085878361e-09 \n",
      "\n",
      "Epoch:  16271  Learning Rate:  8.590704477229748e-09  Varinance:  3.3123493820890915e-09 \n",
      "\n",
      "Epoch:  16272  Learning Rate:  8.58211806667332e-09  Varinance:  3.3077221514141704e-09 \n",
      "\n",
      "Epoch:  16273  Learning Rate:  8.573540238235704e-09  Varinance:  3.3031013848108947e-09 \n",
      "\n",
      "Epoch:  16274  Learning Rate:  8.564970983339012e-09  Varinance:  3.298487073249193e-09 \n",
      "\n",
      "Epoch:  16275  Learning Rate:  8.556410293414017e-09  Varinance:  3.2938792077116084e-09 \n",
      "\n",
      "Epoch:  16276  Learning Rate:  8.547858159900057e-09  Varinance:  3.289277779193293e-09 \n",
      "\n",
      "Epoch:  16277  Learning Rate:  8.53931457424494e-09  Varinance:  3.284682778701944e-09 \n",
      "\n",
      "Epoch:  16278  Learning Rate:  8.530779527905108e-09  Varinance:  3.2800941972578554e-09 \n",
      "\n",
      "Epoch:  16279  Learning Rate:  8.522253012345545e-09  Varinance:  3.2755120258938524e-09 \n",
      "\n",
      "Epoch:  16280  Learning Rate:  8.513735019039675e-09  Varinance:  3.270936255655289e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16281  Learning Rate:  8.505225539469533e-09  Varinance:  3.266366877600027e-09 \n",
      "\n",
      "Epoch:  16282  Learning Rate:  8.49672456512567e-09  Varinance:  3.261803882798421e-09 \n",
      "\n",
      "Epoch:  16283  Learning Rate:  8.488232087507049e-09  Varinance:  3.2572472623333105e-09 \n",
      "\n",
      "Epoch:  16284  Learning Rate:  8.479748098121224e-09  Varinance:  3.2526970072999587e-09 \n",
      "\n",
      "Epoch:  16285  Learning Rate:  8.471272588484234e-09  Varinance:  3.248153108806102e-09 \n",
      "\n",
      "Epoch:  16286  Learning Rate:  8.462805550120508e-09  Varinance:  3.243615557971888e-09 \n",
      "\n",
      "Epoch:  16287  Learning Rate:  8.454346974563035e-09  Varinance:  3.239084345929869e-09 \n",
      "\n",
      "Epoch:  16288  Learning Rate:  8.445896853353275e-09  Varinance:  3.2345594638249838e-09 \n",
      "\n",
      "Epoch:  16289  Learning Rate:  8.43745517804104e-09  Varinance:  3.2300409028145424e-09 \n",
      "\n",
      "Epoch:  16290  Learning Rate:  8.429021940184687e-09  Varinance:  3.22552865406822e-09 \n",
      "\n",
      "Epoch:  16291  Learning Rate:  8.420597131351006e-09  Varinance:  3.221022708767991e-09 \n",
      "\n",
      "Epoch:  16292  Learning Rate:  8.412180743115128e-09  Varinance:  3.2165230581081845e-09 \n",
      "\n",
      "Epoch:  16293  Learning Rate:  8.403772767060694e-09  Varinance:  3.212029693295418e-09 \n",
      "\n",
      "Epoch:  16294  Learning Rate:  8.395373194779757e-09  Varinance:  3.2075426055485934e-09 \n",
      "\n",
      "Epoch:  16295  Learning Rate:  8.386982017872686e-09  Varinance:  3.2030617860988808e-09 \n",
      "\n",
      "Epoch:  16296  Learning Rate:  8.378599227948332e-09  Varinance:  3.1985872261896976e-09 \n",
      "\n",
      "Epoch:  16297  Learning Rate:  8.370224816623934e-09  Varinance:  3.194118917076707e-09 \n",
      "\n",
      "Epoch:  16298  Learning Rate:  8.361858775525018e-09  Varinance:  3.1896568500277533e-09 \n",
      "\n",
      "Epoch:  16299  Learning Rate:  8.353501096285606e-09  Varinance:  3.185201016322913e-09 \n",
      "\n",
      "Epoch:  16300  Learning Rate:  8.345151770547956e-09  Varinance:  3.1807514072544334e-09 \n",
      "\n",
      "Epoch:  16301  Learning Rate:  8.336810789962772e-09  Varinance:  3.176308014126724e-09 \n",
      "\n",
      "Epoch:  16302  Learning Rate:  8.328478146189102e-09  Varinance:  3.1718708282563454e-09 \n",
      "\n",
      "Epoch:  16303  Learning Rate:  8.320153830894243e-09  Varinance:  3.1674398409719857e-09 \n",
      "\n",
      "Epoch:  16304  Learning Rate:  8.311837835753909e-09  Varinance:  3.1630150436144583e-09 \n",
      "\n",
      "Epoch:  16305  Learning Rate:  8.303530152452133e-09  Varinance:  3.1585964275366403e-09 \n",
      "\n",
      "Epoch:  16306  Learning Rate:  8.29523077268117e-09  Varinance:  3.1541839841035217e-09 \n",
      "\n",
      "Epoch:  16307  Learning Rate:  8.286939688141672e-09  Varinance:  3.1497777046921447e-09 \n",
      "\n",
      "Epoch:  16308  Learning Rate:  8.278656890542583e-09  Varinance:  3.145377580691596e-09 \n",
      "\n",
      "Epoch:  16309  Learning Rate:  8.270382371601045e-09  Varinance:  3.140983603502993e-09 \n",
      "\n",
      "Epoch:  16310  Learning Rate:  8.262116123042567e-09  Varinance:  3.1365957645394636e-09 \n",
      "\n",
      "Epoch:  16311  Learning Rate:  8.253858136600929e-09  Varinance:  3.1322140552261444e-09 \n",
      "\n",
      "Epoch:  16312  Learning Rate:  8.245608404018087e-09  Varinance:  3.1278384670001164e-09 \n",
      "\n",
      "Epoch:  16313  Learning Rate:  8.237366917044337e-09  Varinance:  3.123468991310456e-09 \n",
      "\n",
      "Epoch:  16314  Learning Rate:  8.229133667438219e-09  Varinance:  3.119105619618174e-09 \n",
      "\n",
      "Epoch:  16315  Learning Rate:  8.220908646966425e-09  Varinance:  3.1147483433962095e-09 \n",
      "\n",
      "Epoch:  16316  Learning Rate:  8.212691847403962e-09  Varinance:  3.1103971541294145e-09 \n",
      "\n",
      "Epoch:  16317  Learning Rate:  8.204483260534062e-09  Varinance:  3.106052043314535e-09 \n",
      "\n",
      "Epoch:  16318  Learning Rate:  8.196282878148076e-09  Varinance:  3.1017130024602086e-09 \n",
      "\n",
      "Epoch:  16319  Learning Rate:  8.188090692045652e-09  Varinance:  3.0973800230869e-09 \n",
      "\n",
      "Epoch:  16320  Learning Rate:  8.17990669403463e-09  Varinance:  3.093053096726953e-09 \n",
      "\n",
      "Epoch:  16321  Learning Rate:  8.171730875930956e-09  Varinance:  3.0887322149245305e-09 \n",
      "\n",
      "Epoch:  16322  Learning Rate:  8.163563229558839e-09  Varinance:  3.084417369235607e-09 \n",
      "\n",
      "Epoch:  16323  Learning Rate:  8.155403746750659e-09  Varinance:  3.0801085512279537e-09 \n",
      "\n",
      "Epoch:  16324  Learning Rate:  8.147252419346877e-09  Varinance:  3.0758057524811202e-09 \n",
      "\n",
      "Epoch:  16325  Learning Rate:  8.139109239196194e-09  Varinance:  3.071508964586431e-09 \n",
      "\n",
      "Epoch:  16326  Learning Rate:  8.130974198155457e-09  Varinance:  3.0672181791469235e-09 \n",
      "\n",
      "Epoch:  16327  Learning Rate:  8.122847288089567e-09  Varinance:  3.0629333877774e-09 \n",
      "\n",
      "Epoch:  16328  Learning Rate:  8.114728500871643e-09  Varinance:  3.0586545821043633e-09 \n",
      "\n",
      "Epoch:  16329  Learning Rate:  8.106617828382923e-09  Varinance:  3.0543817537660153e-09 \n",
      "\n",
      "Epoch:  16330  Learning Rate:  8.09851526251268e-09  Varinance:  3.050114894412238e-09 \n",
      "\n",
      "Epoch:  16331  Learning Rate:  8.090420795158401e-09  Varinance:  3.04585399570458e-09 \n",
      "\n",
      "Epoch:  16332  Learning Rate:  8.082334418225565e-09  Varinance:  3.041599049316246e-09 \n",
      "\n",
      "Epoch:  16333  Learning Rate:  8.07425612362782e-09  Varinance:  3.0373500469320443e-09 \n",
      "\n",
      "Epoch:  16334  Learning Rate:  8.066185903286899e-09  Varinance:  3.033106980248429e-09 \n",
      "\n",
      "Epoch:  16335  Learning Rate:  8.058123749132526e-09  Varinance:  3.028869840973444e-09 \n",
      "\n",
      "Epoch:  16336  Learning Rate:  8.050069653102573e-09  Varinance:  3.0246386208267166e-09 \n",
      "\n",
      "Epoch:  16337  Learning Rate:  8.042023607142974e-09  Varinance:  3.020413311539442e-09 \n",
      "\n",
      "Epoch:  16338  Learning Rate:  8.033985603207621e-09  Varinance:  3.016193904854366e-09 \n",
      "\n",
      "Epoch:  16339  Learning Rate:  8.025955633258543e-09  Varinance:  3.0119803925257808e-09 \n",
      "\n",
      "Epoch:  16340  Learning Rate:  8.017933689265794e-09  Varinance:  3.0077727663194636e-09 \n",
      "\n",
      "Epoch:  16341  Learning Rate:  8.009919763207376e-09  Varinance:  3.0035710180127294e-09 \n",
      "\n",
      "Epoch:  16342  Learning Rate:  8.001913847069386e-09  Varinance:  2.999375139394368e-09 \n",
      "\n",
      "Epoch:  16343  Learning Rate:  7.993915932845941e-09  Varinance:  2.995185122264639e-09 \n",
      "\n",
      "Epoch:  16344  Learning Rate:  7.985926012539065e-09  Varinance:  2.9910009584352583e-09 \n",
      "\n",
      "Epoch:  16345  Learning Rate:  7.977944078158867e-09  Varinance:  2.9868226397293796e-09 \n",
      "\n",
      "Epoch:  16346  Learning Rate:  7.96997012172344e-09  Varinance:  2.98265015798159e-09 \n",
      "\n",
      "Epoch:  16347  Learning Rate:  7.962004135258772e-09  Varinance:  2.9784835050378522e-09 \n",
      "\n",
      "Epoch:  16348  Learning Rate:  7.954046110798904e-09  Varinance:  2.974322672755551e-09 \n",
      "\n",
      "Epoch:  16349  Learning Rate:  7.946096040385836e-09  Varinance:  2.970167653003436e-09 \n",
      "\n",
      "Epoch:  16350  Learning Rate:  7.93815391606944e-09  Varinance:  2.9660184376616147e-09 \n",
      "\n",
      "Epoch:  16351  Learning Rate:  7.930219729907624e-09  Varinance:  2.9618750186215398e-09 \n",
      "\n",
      "Epoch:  16352  Learning Rate:  7.922293473966228e-09  Varinance:  2.95773738778599e-09 \n",
      "\n",
      "Epoch:  16353  Learning Rate:  7.914375140318936e-09  Varinance:  2.953605537069066e-09 \n",
      "\n",
      "Epoch:  16354  Learning Rate:  7.906464721047446e-09  Varinance:  2.9494794583961325e-09 \n",
      "\n",
      "Epoch:  16355  Learning Rate:  7.898562208241362e-09  Varinance:  2.9453591437038664e-09 \n",
      "\n",
      "Epoch:  16356  Learning Rate:  7.890667593998117e-09  Varinance:  2.9412445849401987e-09 \n",
      "\n",
      "Epoch:  16357  Learning Rate:  7.882780870423125e-09  Varinance:  2.937135774064307e-09 \n",
      "\n",
      "Epoch:  16358  Learning Rate:  7.874902029629685e-09  Varinance:  2.9330327030466036e-09 \n",
      "\n",
      "Epoch:  16359  Learning Rate:  7.867031063738905e-09  Varinance:  2.928935363868717e-09 \n",
      "\n",
      "Epoch:  16360  Learning Rate:  7.859167964879845e-09  Varinance:  2.9248437485234867e-09 \n",
      "\n",
      "Epoch:  16361  Learning Rate:  7.851312725189432e-09  Varinance:  2.9207578490149084e-09 \n",
      "\n",
      "Epoch:  16362  Learning Rate:  7.843465336812372e-09  Varinance:  2.9166776573581773e-09 \n",
      "\n",
      "Epoch:  16363  Learning Rate:  7.835625791901327e-09  Varinance:  2.912603165579635e-09 \n",
      "\n",
      "Epoch:  16364  Learning Rate:  7.827794082616701e-09  Varinance:  2.9085343657167586e-09 \n",
      "\n",
      "Epoch:  16365  Learning Rate:  7.81997020112681e-09  Varinance:  2.904471249818152e-09 \n",
      "\n",
      "Epoch:  16366  Learning Rate:  7.812154139607799e-09  Varinance:  2.9004138099435248e-09 \n",
      "\n",
      "Epoch:  16367  Learning Rate:  7.804345890243553e-09  Varinance:  2.896362038163691e-09 \n",
      "\n",
      "Epoch:  16368  Learning Rate:  7.796545445225845e-09  Varinance:  2.892315926560508e-09 \n",
      "\n",
      "Epoch:  16369  Learning Rate:  7.788752796754259e-09  Varinance:  2.888275467226927e-09 \n",
      "\n",
      "Epoch:  16370  Learning Rate:  7.780967937036094e-09  Varinance:  2.8842406522669345e-09 \n",
      "\n",
      "Epoch:  16371  Learning Rate:  7.773190858286513e-09  Varinance:  2.8802114737955485e-09 \n",
      "\n",
      "Epoch:  16372  Learning Rate:  7.765421552728465e-09  Varinance:  2.8761879239388e-09 \n",
      "\n",
      "Epoch:  16373  Learning Rate:  7.75766001259259e-09  Varinance:  2.872169994833721e-09 \n",
      "\n",
      "Epoch:  16374  Learning Rate:  7.749906230117375e-09  Varinance:  2.8681576786283374e-09 \n",
      "\n",
      "Epoch:  16375  Learning Rate:  7.742160197549061e-09  Varinance:  2.8641509674816146e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16376  Learning Rate:  7.734421907141565e-09  Varinance:  2.8601498535634997e-09 \n",
      "\n",
      "Epoch:  16377  Learning Rate:  7.726691351156619e-09  Varinance:  2.856154329054871e-09 \n",
      "\n",
      "Epoch:  16378  Learning Rate:  7.718968521863695e-09  Varinance:  2.852164386147527e-09 \n",
      "\n",
      "Epoch:  16379  Learning Rate:  7.71125341153991e-09  Varinance:  2.8481800170441764e-09 \n",
      "\n",
      "Epoch:  16380  Learning Rate:  7.70354601247018e-09  Varinance:  2.844201213958419e-09 \n",
      "\n",
      "Epoch:  16381  Learning Rate:  7.69584631694713e-09  Varinance:  2.840227969114742e-09 \n",
      "\n",
      "Epoch:  16382  Learning Rate:  7.68815431727101e-09  Varinance:  2.836260274748465e-09 \n",
      "\n",
      "Epoch:  16383  Learning Rate:  7.68047000574985e-09  Varinance:  2.8322981231057846e-09 \n",
      "\n",
      "Epoch:  16384  Learning Rate:  7.672793374699365e-09  Varinance:  2.82834150644372e-09 \n",
      "\n",
      "Epoch:  16385  Learning Rate:  7.665124416442863e-09  Varinance:  2.824390417030104e-09 \n",
      "\n",
      "Epoch:  16386  Learning Rate:  7.657463123311418e-09  Varinance:  2.820444847143575e-09 \n",
      "\n",
      "Epoch:  16387  Learning Rate:  7.64980948764376e-09  Varinance:  2.8165047890735552e-09 \n",
      "\n",
      "Epoch:  16388  Learning Rate:  7.642163501786201e-09  Varinance:  2.8125702351202496e-09 \n",
      "\n",
      "Epoch:  16389  Learning Rate:  7.634525158092781e-09  Varinance:  2.808641177594588e-09 \n",
      "\n",
      "Epoch:  16390  Learning Rate:  7.626894448925183e-09  Varinance:  2.804717608818273e-09 \n",
      "\n",
      "Epoch:  16391  Learning Rate:  7.619271366652643e-09  Varinance:  2.8007995211237224e-09 \n",
      "\n",
      "Epoch:  16392  Learning Rate:  7.61165590365213e-09  Varinance:  2.7968869068540666e-09 \n",
      "\n",
      "Epoch:  16393  Learning Rate:  7.604048052308128e-09  Varinance:  2.7929797583631318e-09 \n",
      "\n",
      "Epoch:  16394  Learning Rate:  7.59644780501281e-09  Varinance:  2.7890780680154245e-09 \n",
      "\n",
      "Epoch:  16395  Learning Rate:  7.58885515416596e-09  Varinance:  2.7851818281861296e-09 \n",
      "\n",
      "Epoch:  16396  Learning Rate:  7.58127009217487e-09  Varinance:  2.781291031261052e-09 \n",
      "\n",
      "Epoch:  16397  Learning Rate:  7.573692611454501e-09  Varinance:  2.7774056696366644e-09 \n",
      "\n",
      "Epoch:  16398  Learning Rate:  7.566122704427404e-09  Varinance:  2.7735257357200506e-09 \n",
      "\n",
      "Epoch:  16399  Learning Rate:  7.558560363523615e-09  Varinance:  2.7696512219289024e-09 \n",
      "\n",
      "Epoch:  16400  Learning Rate:  7.551005581180818e-09  Varinance:  2.765782120691502e-09 \n",
      "\n",
      "Epoch:  16401  Learning Rate:  7.543458349844258e-09  Varinance:  2.761918424446712e-09 \n",
      "\n",
      "Epoch:  16402  Learning Rate:  7.535918661966652e-09  Varinance:  2.7580601256439646e-09 \n",
      "\n",
      "Epoch:  16403  Learning Rate:  7.528386510008333e-09  Varinance:  2.7542072167432114e-09 \n",
      "\n",
      "Epoch:  16404  Learning Rate:  7.52086188643718e-09  Varinance:  2.750359690214967e-09 \n",
      "\n",
      "Epoch:  16405  Learning Rate:  7.513344783728515e-09  Varinance:  2.7465175385402536e-09 \n",
      "\n",
      "Epoch:  16406  Learning Rate:  7.505835194365257e-09  Varinance:  2.742680754210599e-09 \n",
      "\n",
      "Epoch:  16407  Learning Rate:  7.498333110837847e-09  Varinance:  2.738849329728018e-09 \n",
      "\n",
      "Epoch:  16408  Learning Rate:  7.490838525644146e-09  Varinance:  2.7350232576050106e-09 \n",
      "\n",
      "Epoch:  16409  Learning Rate:  7.483351431289593e-09  Varinance:  2.7312025303645076e-09 \n",
      "\n",
      "Epoch:  16410  Learning Rate:  7.475871820287125e-09  Varinance:  2.7273871405399133e-09 \n",
      "\n",
      "Epoch:  16411  Learning Rate:  7.46839968515707e-09  Varinance:  2.7235770806750535e-09 \n",
      "\n",
      "Epoch:  16412  Learning Rate:  7.460935018427325e-09  Varinance:  2.7197723433241697e-09 \n",
      "\n",
      "Epoch:  16413  Learning Rate:  7.453477812633246e-09  Varinance:  2.7159729210519056e-09 \n",
      "\n",
      "Epoch:  16414  Learning Rate:  7.446028060317574e-09  Varinance:  2.71217880643329e-09 \n",
      "\n",
      "Epoch:  16415  Learning Rate:  7.438585754030583e-09  Varinance:  2.708389992053736e-09 \n",
      "\n",
      "Epoch:  16416  Learning Rate:  7.431150886329992e-09  Varinance:  2.704606470508984e-09 \n",
      "\n",
      "Epoch:  16417  Learning Rate:  7.42372344978088e-09  Varinance:  2.700828234405148e-09 \n",
      "\n",
      "Epoch:  16418  Learning Rate:  7.416303436955838e-09  Varinance:  2.6970552763586606e-09 \n",
      "\n",
      "Epoch:  16419  Learning Rate:  7.408890840434877e-09  Varinance:  2.6932875889962686e-09 \n",
      "\n",
      "Epoch:  16420  Learning Rate:  7.4014856528053455e-09  Varinance:  2.68952516495502e-09 \n",
      "\n",
      "Epoch:  16421  Learning Rate:  7.394087866662086e-09  Varinance:  2.685767996882248e-09 \n",
      "\n",
      "Epoch:  16422  Learning Rate:  7.386697474607334e-09  Varinance:  2.682016077435567e-09 \n",
      "\n",
      "Epoch:  16423  Learning Rate:  7.3793144692506464e-09  Varinance:  2.6782693992828194e-09 \n",
      "\n",
      "Epoch:  16424  Learning Rate:  7.371938843209069e-09  Varinance:  2.674527955102119e-09 \n",
      "\n",
      "Epoch:  16425  Learning Rate:  7.3645705891069236e-09  Varinance:  2.670791737581799e-09 \n",
      "\n",
      "Epoch:  16426  Learning Rate:  7.3572096995759806e-09  Varinance:  2.667060739420407e-09 \n",
      "\n",
      "Epoch:  16427  Learning Rate:  7.349856167255376e-09  Varinance:  2.663334953326689e-09 \n",
      "\n",
      "Epoch:  16428  Learning Rate:  7.342509984791525e-09  Varinance:  2.6596143720195775e-09 \n",
      "\n",
      "Epoch:  16429  Learning Rate:  7.335171144838272e-09  Varinance:  2.6558989882281867e-09 \n",
      "\n",
      "Epoch:  16430  Learning Rate:  7.3278396400568e-09  Varinance:  2.652188794691758e-09 \n",
      "\n",
      "Epoch:  16431  Learning Rate:  7.3205154631155525e-09  Varinance:  2.648483784159705e-09 \n",
      "\n",
      "Epoch:  16432  Learning Rate:  7.3131986066903786e-09  Varinance:  2.644783949391561e-09 \n",
      "\n",
      "Epoch:  16433  Learning Rate:  7.305889063464447e-09  Varinance:  2.6410892831569734e-09 \n",
      "\n",
      "Epoch:  16434  Learning Rate:  7.298586826128163e-09  Varinance:  2.6373997782356898e-09 \n",
      "\n",
      "Epoch:  16435  Learning Rate:  7.291291887379311e-09  Varinance:  2.633715427417545e-09 \n",
      "\n",
      "Epoch:  16436  Learning Rate:  7.284004239922981e-09  Varinance:  2.630036223502455e-09 \n",
      "\n",
      "Epoch:  16437  Learning Rate:  7.276723876471471e-09  Varinance:  2.6263621593003666e-09 \n",
      "\n",
      "Epoch:  16438  Learning Rate:  7.2694507897444446e-09  Varinance:  2.6226932276312983e-09 \n",
      "\n",
      "Epoch:  16439  Learning Rate:  7.26218497246884e-09  Varinance:  2.6190294213252894e-09 \n",
      "\n",
      "Epoch:  16440  Learning Rate:  7.2549264173787875e-09  Varinance:  2.615370733222396e-09 \n",
      "\n",
      "Epoch:  16441  Learning Rate:  7.2476751172157565e-09  Varinance:  2.6117171561726753e-09 \n",
      "\n",
      "Epoch:  16442  Learning Rate:  7.240431064728472e-09  Varinance:  2.608068683036174e-09 \n",
      "\n",
      "Epoch:  16443  Learning Rate:  7.233194252672829e-09  Varinance:  2.6044253066829212e-09 \n",
      "\n",
      "Epoch:  16444  Learning Rate:  7.225964673812043e-09  Varinance:  2.6007870199928797e-09 \n",
      "\n",
      "Epoch:  16445  Learning Rate:  7.2187423209165585e-09  Varinance:  2.5971538158559847e-09 \n",
      "\n",
      "Epoch:  16446  Learning Rate:  7.211527186763971e-09  Varinance:  2.5935256871720964e-09 \n",
      "\n",
      "Epoch:  16447  Learning Rate:  7.204319264139171e-09  Varinance:  2.5899026268509936e-09 \n",
      "\n",
      "Epoch:  16448  Learning Rate:  7.197118545834261e-09  Varinance:  2.586284627812358e-09 \n",
      "\n",
      "Epoch:  16449  Learning Rate:  7.1899250246484704e-09  Varinance:  2.5826716829857644e-09 \n",
      "\n",
      "Epoch:  16450  Learning Rate:  7.182738693388305e-09  Varinance:  2.579063785310673e-09 \n",
      "\n",
      "Epoch:  16451  Learning Rate:  7.175559544867455e-09  Varinance:  2.5754609277363792e-09 \n",
      "\n",
      "Epoch:  16452  Learning Rate:  7.168387571906724e-09  Varinance:  2.5718631032220563e-09 \n",
      "\n",
      "Epoch:  16453  Learning Rate:  7.1612227673341614e-09  Varinance:  2.5682703047367046e-09 \n",
      "\n",
      "Epoch:  16454  Learning Rate:  7.1540651239849886e-09  Varinance:  2.564682525259145e-09 \n",
      "\n",
      "Epoch:  16455  Learning Rate:  7.146914634701511e-09  Varinance:  2.5610997577780084e-09 \n",
      "\n",
      "Epoch:  16456  Learning Rate:  7.1397712923332885e-09  Varinance:  2.557521995291719e-09 \n",
      "\n",
      "Epoch:  16457  Learning Rate:  7.132635089736928e-09  Varinance:  2.5539492308084916e-09 \n",
      "\n",
      "Epoch:  16458  Learning Rate:  7.125506019776252e-09  Varinance:  2.550381457346281e-09 \n",
      "\n",
      "Epoch:  16459  Learning Rate:  7.118384075322216e-09  Varinance:  2.546818667932823e-09 \n",
      "\n",
      "Epoch:  16460  Learning Rate:  7.1112692492528216e-09  Varinance:  2.5432608556055834e-09 \n",
      "\n",
      "Epoch:  16461  Learning Rate:  7.104161534453269e-09  Varinance:  2.5397080134117562e-09 \n",
      "\n",
      "Epoch:  16462  Learning Rate:  7.097060923815868e-09  Varinance:  2.5361601344082473e-09 \n",
      "\n",
      "Epoch:  16463  Learning Rate:  7.089967410239958e-09  Varinance:  2.5326172116616608e-09 \n",
      "\n",
      "Epoch:  16464  Learning Rate:  7.082880986632049e-09  Varinance:  2.5290792382482966e-09 \n",
      "\n",
      "Epoch:  16465  Learning Rate:  7.075801645905742e-09  Varinance:  2.5255462072541004e-09 \n",
      "\n",
      "Epoch:  16466  Learning Rate:  7.0687293809816446e-09  Varinance:  2.5220181117747024e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16467  Learning Rate:  7.061664184787518e-09  Varinance:  2.518494944915369e-09 \n",
      "\n",
      "Epoch:  16468  Learning Rate:  7.054606050258189e-09  Varinance:  2.514976699790999e-09 \n",
      "\n",
      "Epoch:  16469  Learning Rate:  7.047554970335474e-09  Varinance:  2.5114633695261085e-09 \n",
      "\n",
      "Epoch:  16470  Learning Rate:  7.040510937968316e-09  Varinance:  2.50795494725482e-09 \n",
      "\n",
      "Epoch:  16471  Learning Rate:  7.0334739461127074e-09  Varinance:  2.504451426120854e-09 \n",
      "\n",
      "Epoch:  16472  Learning Rate:  7.026443987731607e-09  Varinance:  2.500952799277485e-09 \n",
      "\n",
      "Epoch:  16473  Learning Rate:  7.019421055795079e-09  Varinance:  2.4974590598875755e-09 \n",
      "\n",
      "Epoch:  16474  Learning Rate:  7.012405143280217e-09  Varinance:  2.4939702011235337e-09 \n",
      "\n",
      "Epoch:  16475  Learning Rate:  7.005396243171058e-09  Varinance:  2.4904862161673036e-09 \n",
      "\n",
      "Epoch:  16476  Learning Rate:  6.998394348458725e-09  Varinance:  2.4870070982103538e-09 \n",
      "\n",
      "Epoch:  16477  Learning Rate:  6.991399452141351e-09  Varinance:  2.4835328404536654e-09 \n",
      "\n",
      "Epoch:  16478  Learning Rate:  6.984411547223985e-09  Varinance:  2.480063436107726e-09 \n",
      "\n",
      "Epoch:  16479  Learning Rate:  6.977430626718748e-09  Varinance:  2.47659887839248e-09 \n",
      "\n",
      "Epoch:  16480  Learning Rate:  6.970456683644744e-09  Varinance:  2.4731391605373713e-09 \n",
      "\n",
      "Epoch:  16481  Learning Rate:  6.96348971102798e-09  Varinance:  2.469684275781292e-09 \n",
      "\n",
      "Epoch:  16482  Learning Rate:  6.9565297019015076e-09  Varinance:  2.466234217372582e-09 \n",
      "\n",
      "Epoch:  16483  Learning Rate:  6.949576649305342e-09  Varinance:  2.462788978569008e-09 \n",
      "\n",
      "Epoch:  16484  Learning Rate:  6.942630546286379e-09  Varinance:  2.4593485526377604e-09 \n",
      "\n",
      "Epoch:  16485  Learning Rate:  6.935691385898541e-09  Varinance:  2.455912932855441e-09 \n",
      "\n",
      "Epoch:  16486  Learning Rate:  6.928759161202693e-09  Varinance:  2.452482112508019e-09 \n",
      "\n",
      "Epoch:  16487  Learning Rate:  6.921833865266557e-09  Varinance:  2.4490560848908672e-09 \n",
      "\n",
      "Epoch:  16488  Learning Rate:  6.9149154911648894e-09  Varinance:  2.4456348433087187e-09 \n",
      "\n",
      "Epoch:  16489  Learning Rate:  6.908004031979264e-09  Varinance:  2.4422183810756567e-09 \n",
      "\n",
      "Epoch:  16490  Learning Rate:  6.901099480798246e-09  Varinance:  2.4388066915151063e-09 \n",
      "\n",
      "Epoch:  16491  Learning Rate:  6.894201830717308e-09  Varinance:  2.435399767959819e-09 \n",
      "\n",
      "Epoch:  16492  Learning Rate:  6.8873110748387504e-09  Varinance:  2.4319976037518687e-09 \n",
      "\n",
      "Epoch:  16493  Learning Rate:  6.880427206271843e-09  Varinance:  2.4286001922426044e-09 \n",
      "\n",
      "Epoch:  16494  Learning Rate:  6.87355021813274e-09  Varinance:  2.4252075267926888e-09 \n",
      "\n",
      "Epoch:  16495  Learning Rate:  6.866680103544401e-09  Varinance:  2.4218196007720508e-09 \n",
      "\n",
      "Epoch:  16496  Learning Rate:  6.859816855636741e-09  Varinance:  2.4184364075598814e-09 \n",
      "\n",
      "Epoch:  16497  Learning Rate:  6.85296046754653e-09  Varinance:  2.4150579405446207e-09 \n",
      "\n",
      "Epoch:  16498  Learning Rate:  6.846110932417335e-09  Varinance:  2.4116841931239455e-09 \n",
      "\n",
      "Epoch:  16499  Learning Rate:  6.839268243399641e-09  Varinance:  2.408315158704763e-09 \n",
      "\n",
      "Epoch:  16500  Learning Rate:  6.832432393650787e-09  Varinance:  2.404950830703166e-09 \n",
      "\n",
      "Epoch:  16501  Learning Rate:  6.82560337633487e-09  Varinance:  2.4015912025444698e-09 \n",
      "\n",
      "Epoch:  16502  Learning Rate:  6.818781184622899e-09  Varinance:  2.3982362676631667e-09 \n",
      "\n",
      "Epoch:  16503  Learning Rate:  6.811965811692705e-09  Varinance:  2.39488601950292e-09 \n",
      "\n",
      "Epoch:  16504  Learning Rate:  6.805157250728866e-09  Varinance:  2.391540451516552e-09 \n",
      "\n",
      "Epoch:  16505  Learning Rate:  6.798355494922845e-09  Varinance:  2.3881995571660314e-09 \n",
      "\n",
      "Epoch:  16506  Learning Rate:  6.791560537472909e-09  Varinance:  2.3848633299224693e-09 \n",
      "\n",
      "Epoch:  16507  Learning Rate:  6.784772371584053e-09  Varinance:  2.3815317632660707e-09 \n",
      "\n",
      "Epoch:  16508  Learning Rate:  6.777990990468133e-09  Varinance:  2.3782048506861747e-09 \n",
      "\n",
      "Epoch:  16509  Learning Rate:  6.7712163873437936e-09  Varinance:  2.374882585681207e-09 \n",
      "\n",
      "Epoch:  16510  Learning Rate:  6.76444855543638e-09  Varinance:  2.371564961758676e-09 \n",
      "\n",
      "Epoch:  16511  Learning Rate:  6.7576874879780875e-09  Varinance:  2.36825197243516e-09 \n",
      "\n",
      "Epoch:  16512  Learning Rate:  6.75093317820787e-09  Varinance:  2.3649436112362934e-09 \n",
      "\n",
      "Epoch:  16513  Learning Rate:  6.744185619371368e-09  Varinance:  2.361639871696764e-09 \n",
      "\n",
      "Epoch:  16514  Learning Rate:  6.737444804721049e-09  Varinance:  2.358340747360267e-09 \n",
      "\n",
      "Epoch:  16515  Learning Rate:  6.730710727516118e-09  Varinance:  2.355046231779541e-09 \n",
      "\n",
      "Epoch:  16516  Learning Rate:  6.723983381022453e-09  Varinance:  2.351756318516323e-09 \n",
      "\n",
      "Epoch:  16517  Learning Rate:  6.717262758512753e-09  Varinance:  2.3484710011413444e-09 \n",
      "\n",
      "Epoch:  16518  Learning Rate:  6.7105488532663474e-09  Varinance:  2.345190273234317e-09 \n",
      "\n",
      "Epoch:  16519  Learning Rate:  6.7038416585693546e-09  Varinance:  2.341914128383924e-09 \n",
      "\n",
      "Epoch:  16520  Learning Rate:  6.697141167714602e-09  Varinance:  2.3386425601878096e-09 \n",
      "\n",
      "Epoch:  16521  Learning Rate:  6.690447374001552e-09  Varinance:  2.33537556225254e-09 \n",
      "\n",
      "Epoch:  16522  Learning Rate:  6.683760270736433e-09  Varinance:  2.3321131281936367e-09 \n",
      "\n",
      "Epoch:  16523  Learning Rate:  6.677079851232166e-09  Varinance:  2.328855251635531e-09 \n",
      "\n",
      "Epoch:  16524  Learning Rate:  6.670406108808284e-09  Varinance:  2.325601926211562e-09 \n",
      "\n",
      "Epoch:  16525  Learning Rate:  6.663739036791064e-09  Varinance:  2.3223531455639624e-09 \n",
      "\n",
      "Epoch:  16526  Learning Rate:  6.657078628513462e-09  Varinance:  2.3191089033438453e-09 \n",
      "\n",
      "Epoch:  16527  Learning Rate:  6.65042487731502e-09  Varinance:  2.315869193211202e-09 \n",
      "\n",
      "Epoch:  16528  Learning Rate:  6.643777776542007e-09  Varinance:  2.3126340088348574e-09 \n",
      "\n",
      "Epoch:  16529  Learning Rate:  6.63713731954735e-09  Varinance:  2.3094033438925026e-09 \n",
      "\n",
      "Epoch:  16530  Learning Rate:  6.6305034996905415e-09  Varinance:  2.3061771920706544e-09 \n",
      "\n",
      "Epoch:  16531  Learning Rate:  6.6238763103377846e-09  Varinance:  2.3029555470646494e-09 \n",
      "\n",
      "Epoch:  16532  Learning Rate:  6.617255744861913e-09  Varinance:  2.29973840257863e-09 \n",
      "\n",
      "Epoch:  16533  Learning Rate:  6.6106417966423155e-09  Varinance:  2.296525752325536e-09 \n",
      "\n",
      "Epoch:  16534  Learning Rate:  6.604034459065065e-09  Varinance:  2.293317590027096e-09 \n",
      "\n",
      "Epoch:  16535  Learning Rate:  6.5974337255228484e-09  Varinance:  2.290113909413786e-09 \n",
      "\n",
      "Epoch:  16536  Learning Rate:  6.590839589414883e-09  Varinance:  2.2869147042248636e-09 \n",
      "\n",
      "Epoch:  16537  Learning Rate:  6.584252044147055e-09  Varinance:  2.283719968208326e-09 \n",
      "\n",
      "Epoch:  16538  Learning Rate:  6.5776710831318455e-09  Varinance:  2.2805296951209025e-09 \n",
      "\n",
      "Epoch:  16539  Learning Rate:  6.571096699788242e-09  Varinance:  2.277343878728045e-09 \n",
      "\n",
      "Epoch:  16540  Learning Rate:  6.5645288875418865e-09  Varinance:  2.2741625128039224e-09 \n",
      "\n",
      "Epoch:  16541  Learning Rate:  6.557967639824989e-09  Varinance:  2.2709855911313774e-09 \n",
      "\n",
      "Epoch:  16542  Learning Rate:  6.551412950076255e-09  Varinance:  2.2678131075019603e-09 \n",
      "\n",
      "Epoch:  16543  Learning Rate:  6.5448648117410165e-09  Varinance:  2.264645055715889e-09 \n",
      "\n",
      "Epoch:  16544  Learning Rate:  6.538323218271158e-09  Varinance:  2.2614814295820396e-09 \n",
      "\n",
      "Epoch:  16545  Learning Rate:  6.53178816312504e-09  Varinance:  2.258322222917939e-09 \n",
      "\n",
      "Epoch:  16546  Learning Rate:  6.525259639767629e-09  Varinance:  2.2551674295497496e-09 \n",
      "\n",
      "Epoch:  16547  Learning Rate:  6.5187376416704255e-09  Varinance:  2.2520170433122664e-09 \n",
      "\n",
      "Epoch:  16548  Learning Rate:  6.5122221623113835e-09  Varinance:  2.248871058048874e-09 \n",
      "\n",
      "Epoch:  16549  Learning Rate:  6.505713195175069e-09  Varinance:  2.245729467611581e-09 \n",
      "\n",
      "Epoch:  16550  Learning Rate:  6.49921073375247e-09  Varinance:  2.2425922658609758e-09 \n",
      "\n",
      "Epoch:  16551  Learning Rate:  6.4927147715411446e-09  Varinance:  2.2394594466662246e-09 \n",
      "\n",
      "Epoch:  16552  Learning Rate:  6.486225302045155e-09  Varinance:  2.236331003905057e-09 \n",
      "\n",
      "Epoch:  16553  Learning Rate:  6.479742318774986e-09  Varinance:  2.2332069314637554e-09 \n",
      "\n",
      "Epoch:  16554  Learning Rate:  6.473265815247675e-09  Varinance:  2.230087223237152e-09 \n",
      "\n",
      "Epoch:  16555  Learning Rate:  6.4667957849867426e-09  Varinance:  2.2269718731285824e-09 \n",
      "\n",
      "Epoch:  16556  Learning Rate:  6.46033222152211e-09  Varinance:  2.2238608750499233e-09 \n",
      "\n",
      "Epoch:  16557  Learning Rate:  6.4538751183902385e-09  Varinance:  2.220754222921549e-09 \n",
      "\n",
      "Epoch:  16558  Learning Rate:  6.447424469134046e-09  Varinance:  2.2176519106723255e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16559  Learning Rate:  6.4409802673028366e-09  Varinance:  2.2145539322396013e-09 \n",
      "\n",
      "Epoch:  16560  Learning Rate:  6.434542506452431e-09  Varinance:  2.211460281569193e-09 \n",
      "\n",
      "Epoch:  16561  Learning Rate:  6.42811118014509e-09  Varinance:  2.208370952615384e-09 \n",
      "\n",
      "Epoch:  16562  Learning Rate:  6.421686281949443e-09  Varinance:  2.205285939340878e-09 \n",
      "\n",
      "Epoch:  16563  Learning Rate:  6.4152678054406134e-09  Varinance:  2.202205235716838e-09 \n",
      "\n",
      "Epoch:  16564  Learning Rate:  6.408855744200147e-09  Varinance:  2.1991288357228395e-09 \n",
      "\n",
      "Epoch:  16565  Learning Rate:  6.402450091815934e-09  Varinance:  2.196056733346869e-09 \n",
      "\n",
      "Epoch:  16566  Learning Rate:  6.396050841882349e-09  Varinance:  2.192988922585312e-09 \n",
      "\n",
      "Epoch:  16567  Learning Rate:  6.38965798800016e-09  Varinance:  2.189925397442941e-09 \n",
      "\n",
      "Epoch:  16568  Learning Rate:  6.383271523776469e-09  Varinance:  2.1868661519329097e-09 \n",
      "\n",
      "Epoch:  16569  Learning Rate:  6.376891442824834e-09  Varinance:  2.183811180076714e-09 \n",
      "\n",
      "Epoch:  16570  Learning Rate:  6.3705177387651954e-09  Varinance:  2.1807604759042238e-09 \n",
      "\n",
      "Epoch:  16571  Learning Rate:  6.364150405223806e-09  Varinance:  2.1777140334536406e-09 \n",
      "\n",
      "Epoch:  16572  Learning Rate:  6.35778943583335e-09  Varinance:  2.1746718467714955e-09 \n",
      "\n",
      "Epoch:  16573  Learning Rate:  6.351434824232883e-09  Varinance:  2.1716339099126366e-09 \n",
      "\n",
      "Epoch:  16574  Learning Rate:  6.345086564067747e-09  Varinance:  2.1686002169402158e-09 \n",
      "\n",
      "Epoch:  16575  Learning Rate:  6.338744648989702e-09  Varinance:  2.1655707619256873e-09 \n",
      "\n",
      "Epoch:  16576  Learning Rate:  6.332409072656859e-09  Varinance:  2.162545538948764e-09 \n",
      "\n",
      "Epoch:  16577  Learning Rate:  6.326079828733593e-09  Varinance:  2.159524542097452e-09 \n",
      "\n",
      "Epoch:  16578  Learning Rate:  6.319756910890682e-09  Varinance:  2.1565077654680083e-09 \n",
      "\n",
      "Epoch:  16579  Learning Rate:  6.313440312805232e-09  Varinance:  2.1534952031649384e-09 \n",
      "\n",
      "Epoch:  16580  Learning Rate:  6.3071300281605996e-09  Varinance:  2.1504868493009827e-09 \n",
      "\n",
      "Epoch:  16581  Learning Rate:  6.300826050646541e-09  Varinance:  2.147482697997106e-09 \n",
      "\n",
      "Epoch:  16582  Learning Rate:  6.294528373959037e-09  Varinance:  2.1444827433824927e-09 \n",
      "\n",
      "Epoch:  16583  Learning Rate:  6.288236991800432e-09  Varinance:  2.1414869795945074e-09 \n",
      "\n",
      "Epoch:  16584  Learning Rate:  6.281951897879364e-09  Varinance:  2.1384954007787262e-09 \n",
      "\n",
      "Epoch:  16585  Learning Rate:  6.2756730859106966e-09  Varinance:  2.135508001088897e-09 \n",
      "\n",
      "Epoch:  16586  Learning Rate:  6.2694005496156365e-09  Varinance:  2.1325247746869327e-09 \n",
      "\n",
      "Epoch:  16587  Learning Rate:  6.263134282721672e-09  Varinance:  2.1295457157429045e-09 \n",
      "\n",
      "Epoch:  16588  Learning Rate:  6.256874278962488e-09  Varinance:  2.126570818435025e-09 \n",
      "\n",
      "Epoch:  16589  Learning Rate:  6.250620532078106e-09  Varinance:  2.1236000769496502e-09 \n",
      "\n",
      "Epoch:  16590  Learning Rate:  6.244373035814799e-09  Varinance:  2.120633485481232e-09 \n",
      "\n",
      "Epoch:  16591  Learning Rate:  6.238131783925026e-09  Varinance:  2.117671038232357e-09 \n",
      "\n",
      "Epoch:  16592  Learning Rate:  6.231896770167556e-09  Varinance:  2.1147127294137022e-09 \n",
      "\n",
      "Epoch:  16593  Learning Rate:  6.225667988307399e-09  Varinance:  2.1117585532440325e-09 \n",
      "\n",
      "Epoch:  16594  Learning Rate:  6.2194454321157255e-09  Varinance:  2.108808503950188e-09 \n",
      "\n",
      "Epoch:  16595  Learning Rate:  6.213229095370003e-09  Varinance:  2.1058625757670756e-09 \n",
      "\n",
      "Epoch:  16596  Learning Rate:  6.207018971853916e-09  Varinance:  2.102920762937661e-09 \n",
      "\n",
      "Epoch:  16597  Learning Rate:  6.200815055357296e-09  Varinance:  2.099983059712932e-09 \n",
      "\n",
      "Epoch:  16598  Learning Rate:  6.194617339676247e-09  Varinance:  2.097049460351928e-09 \n",
      "\n",
      "Epoch:  16599  Learning Rate:  6.188425818613078e-09  Varinance:  2.0941199591217026e-09 \n",
      "\n",
      "Epoch:  16600  Learning Rate:  6.182240485976219e-09  Varinance:  2.091194550297317e-09 \n",
      "\n",
      "Epoch:  16601  Learning Rate:  6.176061335580363e-09  Varinance:  2.0882732281618305e-09 \n",
      "\n",
      "Epoch:  16602  Learning Rate:  6.169888361246379e-09  Varinance:  2.085355987006288e-09 \n",
      "\n",
      "Epoch:  16603  Learning Rate:  6.163721556801247e-09  Varinance:  2.0824428211297176e-09 \n",
      "\n",
      "Epoch:  16604  Learning Rate:  6.157560916078187e-09  Varinance:  2.0795337248390895e-09 \n",
      "\n",
      "Epoch:  16605  Learning Rate:  6.151406432916578e-09  Varinance:  2.0766286924493482e-09 \n",
      "\n",
      "Epoch:  16606  Learning Rate:  6.145258101161892e-09  Varinance:  2.073727718283374e-09 \n",
      "\n",
      "Epoch:  16607  Learning Rate:  6.139115914665819e-09  Varinance:  2.070830796671976e-09 \n",
      "\n",
      "Epoch:  16608  Learning Rate:  6.132979867286195e-09  Varinance:  2.0679379219538846e-09 \n",
      "\n",
      "Epoch:  16609  Learning Rate:  6.126849952886927e-09  Varinance:  2.065049088475738e-09 \n",
      "\n",
      "Epoch:  16610  Learning Rate:  6.1207261653381225e-09  Varinance:  2.06216429059208e-09 \n",
      "\n",
      "Epoch:  16611  Learning Rate:  6.114608498516016e-09  Varinance:  2.059283522665317e-09 \n",
      "\n",
      "Epoch:  16612  Learning Rate:  6.108496946302895e-09  Varinance:  2.056406779065756e-09 \n",
      "\n",
      "Epoch:  16613  Learning Rate:  6.10239150258725e-09  Varinance:  2.053534054171558e-09 \n",
      "\n",
      "Epoch:  16614  Learning Rate:  6.096292161263596e-09  Varinance:  2.0506653423687395e-09 \n",
      "\n",
      "Epoch:  16615  Learning Rate:  6.090198916232611e-09  Varinance:  2.047800638051159e-09 \n",
      "\n",
      "Epoch:  16616  Learning Rate:  6.084111761401072e-09  Varinance:  2.0449399356205066e-09 \n",
      "\n",
      "Epoch:  16617  Learning Rate:  6.078030690681779e-09  Varinance:  2.0420832294863e-09 \n",
      "\n",
      "Epoch:  16618  Learning Rate:  6.071955697993682e-09  Varinance:  2.039230514065845e-09 \n",
      "\n",
      "Epoch:  16619  Learning Rate:  6.065886777261812e-09  Varinance:  2.036381783784268e-09 \n",
      "\n",
      "Epoch:  16620  Learning Rate:  6.059823922417203e-09  Varinance:  2.0335370330744763e-09 \n",
      "\n",
      "Epoch:  16621  Learning Rate:  6.053767127397022e-09  Varinance:  2.030696256377154e-09 \n",
      "\n",
      "Epoch:  16622  Learning Rate:  6.047716386144494e-09  Varinance:  2.0278594481407506e-09 \n",
      "\n",
      "Epoch:  16623  Learning Rate:  6.041671692608834e-09  Varinance:  2.0250266028214723e-09 \n",
      "\n",
      "Epoch:  16624  Learning Rate:  6.035633040745371e-09  Varinance:  2.0221977148832767e-09 \n",
      "\n",
      "Epoch:  16625  Learning Rate:  6.029600424515473e-09  Varinance:  2.019372778797832e-09 \n",
      "\n",
      "Epoch:  16626  Learning Rate:  6.023573837886479e-09  Varinance:  2.0165517890445534e-09 \n",
      "\n",
      "Epoch:  16627  Learning Rate:  6.017553274831827e-09  Varinance:  2.0137347401105583e-09 \n",
      "\n",
      "Epoch:  16628  Learning Rate:  6.0115387293309706e-09  Varinance:  2.010921626490667e-09 \n",
      "\n",
      "Epoch:  16629  Learning Rate:  6.005530195369324e-09  Varinance:  2.0081124426873904e-09 \n",
      "\n",
      "Epoch:  16630  Learning Rate:  5.999527666938375e-09  Varinance:  2.0053071832109177e-09 \n",
      "\n",
      "Epoch:  16631  Learning Rate:  5.993531138035612e-09  Varinance:  2.002505842579116e-09 \n",
      "\n",
      "Epoch:  16632  Learning Rate:  5.987540602664466e-09  Varinance:  1.999708415317489e-09 \n",
      "\n",
      "Epoch:  16633  Learning Rate:  5.981556054834422e-09  Varinance:  1.9969148959592084e-09 \n",
      "\n",
      "Epoch:  16634  Learning Rate:  5.975577488560952e-09  Varinance:  1.9941252790450767e-09 \n",
      "\n",
      "Epoch:  16635  Learning Rate:  5.969604897865447e-09  Varinance:  1.991339559123523e-09 \n",
      "\n",
      "Epoch:  16636  Learning Rate:  5.963638276775338e-09  Varinance:  1.9885577307505907e-09 \n",
      "\n",
      "Epoch:  16637  Learning Rate:  5.957677619324024e-09  Varinance:  1.98577978848993e-09 \n",
      "\n",
      "Epoch:  16638  Learning Rate:  5.951722919550804e-09  Varinance:  1.9830057269127907e-09 \n",
      "\n",
      "Epoch:  16639  Learning Rate:  5.945774171501e-09  Varinance:  1.980235540597986e-09 \n",
      "\n",
      "Epoch:  16640  Learning Rate:  5.939831369225883e-09  Varinance:  1.977469224131924e-09 \n",
      "\n",
      "Epoch:  16641  Learning Rate:  5.93389450678261e-09  Varinance:  1.9747067721085677e-09 \n",
      "\n",
      "Epoch:  16642  Learning Rate:  5.92796357823436e-09  Varinance:  1.9719481791294323e-09 \n",
      "\n",
      "Epoch:  16643  Learning Rate:  5.9220385776501604e-09  Varinance:  1.9691934398035743e-09 \n",
      "\n",
      "Epoch:  16644  Learning Rate:  5.916119499105032e-09  Varinance:  1.9664425487475816e-09 \n",
      "\n",
      "Epoch:  16645  Learning Rate:  5.910206336679917e-09  Varinance:  1.9636955005855694e-09 \n",
      "\n",
      "Epoch:  16646  Learning Rate:  5.9042990844616096e-09  Varinance:  1.960952289949141e-09 \n",
      "\n",
      "Epoch:  16647  Learning Rate:  5.898397736542879e-09  Varinance:  1.958212911477421e-09 \n",
      "\n",
      "Epoch:  16648  Learning Rate:  5.892502287022398e-09  Varinance:  1.9554773598170167e-09 \n",
      "\n",
      "Epoch:  16649  Learning Rate:  5.886612730004674e-09  Varinance:  1.952745629622012e-09 \n",
      "\n",
      "Epoch:  16650  Learning Rate:  5.88072905960017e-09  Varinance:  1.9500177155539597e-09 \n",
      "\n",
      "Epoch:  16651  Learning Rate:  5.874851269925237e-09  Varinance:  1.9472936122818707e-09 \n",
      "\n",
      "Epoch:  16652  Learning Rate:  5.868979355102042e-09  Varinance:  1.94457331448221e-09 \n",
      "\n",
      "Epoch:  16653  Learning Rate:  5.863113309258691e-09  Varinance:  1.9418568168388578e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16654  Learning Rate:  5.85725312652916e-09  Varinance:  1.939144114043142e-09 \n",
      "\n",
      "Epoch:  16655  Learning Rate:  5.851398801053222e-09  Varinance:  1.9364352007937994e-09 \n",
      "\n",
      "Epoch:  16656  Learning Rate:  5.845550326976573e-09  Varinance:  1.933730071796973e-09 \n",
      "\n",
      "Epoch:  16657  Learning Rate:  5.8397076984507585e-09  Varinance:  1.9310287217662008e-09 \n",
      "\n",
      "Epoch:  16658  Learning Rate:  5.8338709096331085e-09  Varinance:  1.928331145422405e-09 \n",
      "\n",
      "Epoch:  16659  Learning Rate:  5.828039954686855e-09  Varinance:  1.9256373374938906e-09 \n",
      "\n",
      "Epoch:  16660  Learning Rate:  5.822214827781062e-09  Varinance:  1.922947292716306e-09 \n",
      "\n",
      "Epoch:  16661  Learning Rate:  5.816395523090561e-09  Varinance:  1.920261005832674e-09 \n",
      "\n",
      "Epoch:  16662  Learning Rate:  5.810582034796068e-09  Varinance:  1.917578471593354e-09 \n",
      "\n",
      "Epoch:  16663  Learning Rate:  5.804774357084114e-09  Varinance:  1.9148996847560407e-09 \n",
      "\n",
      "Epoch:  16664  Learning Rate:  5.798972484146981e-09  Varinance:  1.91222464008575e-09 \n",
      "\n",
      "Epoch:  16665  Learning Rate:  5.793176410182815e-09  Varinance:  1.909553332354811e-09 \n",
      "\n",
      "Epoch:  16666  Learning Rate:  5.787386129395563e-09  Varinance:  1.9068857563428645e-09 \n",
      "\n",
      "Epoch:  16667  Learning Rate:  5.781601635994901e-09  Varinance:  1.9042219068368215e-09 \n",
      "\n",
      "Epoch:  16668  Learning Rate:  5.775822924196358e-09  Varinance:  1.9015617786308977e-09 \n",
      "\n",
      "Epoch:  16669  Learning Rate:  5.77004998822124e-09  Varinance:  1.8989053665265713e-09 \n",
      "\n",
      "Epoch:  16670  Learning Rate:  5.764282822296572e-09  Varinance:  1.8962526653325867e-09 \n",
      "\n",
      "Epoch:  16671  Learning Rate:  5.7585214206552054e-09  Varinance:  1.8936036698649365e-09 \n",
      "\n",
      "Epoch:  16672  Learning Rate:  5.75276577753576e-09  Varinance:  1.8909583749468646e-09 \n",
      "\n",
      "Epoch:  16673  Learning Rate:  5.747015887182552e-09  Varinance:  1.8883167754088247e-09 \n",
      "\n",
      "Epoch:  16674  Learning Rate:  5.74127174384573e-09  Varinance:  1.8856788660885137e-09 \n",
      "\n",
      "Epoch:  16675  Learning Rate:  5.735533341781109e-09  Varinance:  1.8830446418308325e-09 \n",
      "\n",
      "Epoch:  16676  Learning Rate:  5.7298006752503094e-09  Varinance:  1.8804140974878837e-09 \n",
      "\n",
      "Epoch:  16677  Learning Rate:  5.724073738520681e-09  Varinance:  1.877787227918961e-09 \n",
      "\n",
      "Epoch:  16678  Learning Rate:  5.71835252586525e-09  Varinance:  1.87516402799054e-09 \n",
      "\n",
      "Epoch:  16679  Learning Rate:  5.71263703156282e-09  Varinance:  1.8725444925762745e-09 \n",
      "\n",
      "Epoch:  16680  Learning Rate:  5.706927249897917e-09  Varinance:  1.8699286165569583e-09 \n",
      "\n",
      "Epoch:  16681  Learning Rate:  5.701223175160721e-09  Varinance:  1.8673163948205584e-09 \n",
      "\n",
      "Epoch:  16682  Learning Rate:  5.695524801647174e-09  Varinance:  1.864707822262175e-09 \n",
      "\n",
      "Epoch:  16683  Learning Rate:  5.689832123658924e-09  Varinance:  1.86210289378404e-09 \n",
      "\n",
      "Epoch:  16684  Learning Rate:  5.684145135503252e-09  Varinance:  1.8595016042955073e-09 \n",
      "\n",
      "Epoch:  16685  Learning Rate:  5.678463831493189e-09  Varinance:  1.8569039487130415e-09 \n",
      "\n",
      "Epoch:  16686  Learning Rate:  5.672788205947451e-09  Varinance:  1.854309921960216e-09 \n",
      "\n",
      "Epoch:  16687  Learning Rate:  5.667118253190371e-09  Varinance:  1.8517195189676751e-09 \n",
      "\n",
      "Epoch:  16688  Learning Rate:  5.661453967552017e-09  Varinance:  1.8491327346731655e-09 \n",
      "\n",
      "Epoch:  16689  Learning Rate:  5.655795343368121e-09  Varinance:  1.8465495640214987e-09 \n",
      "\n",
      "Epoch:  16690  Learning Rate:  5.650142374980021e-09  Varinance:  1.8439700019645475e-09 \n",
      "\n",
      "Epoch:  16691  Learning Rate:  5.644495056734766e-09  Varinance:  1.8413940434612384e-09 \n",
      "\n",
      "Epoch:  16692  Learning Rate:  5.638853382985059e-09  Varinance:  1.8388216834775383e-09 \n",
      "\n",
      "Epoch:  16693  Learning Rate:  5.633217348089185e-09  Varinance:  1.8362529169864536e-09 \n",
      "\n",
      "Epoch:  16694  Learning Rate:  5.627586946411128e-09  Varinance:  1.8336877389679941e-09 \n",
      "\n",
      "Epoch:  16695  Learning Rate:  5.621962172320506e-09  Varinance:  1.8311261444092017e-09 \n",
      "\n",
      "Epoch:  16696  Learning Rate:  5.616343020192506e-09  Varinance:  1.8285681283041147e-09 \n",
      "\n",
      "Epoch:  16697  Learning Rate:  5.610729484407993e-09  Varinance:  1.8260136856537638e-09 \n",
      "\n",
      "Epoch:  16698  Learning Rate:  5.6051215593534526e-09  Varinance:  1.8234628114661645e-09 \n",
      "\n",
      "Epoch:  16699  Learning Rate:  5.599519239420919e-09  Varinance:  1.8209155007563046e-09 \n",
      "\n",
      "Epoch:  16700  Learning Rate:  5.593922519008091e-09  Varinance:  1.8183717485461427e-09 \n",
      "\n",
      "Epoch:  16701  Learning Rate:  5.588331392518268e-09  Varinance:  1.8158315498645725e-09 \n",
      "\n",
      "Epoch:  16702  Learning Rate:  5.5827458543602836e-09  Varinance:  1.8132948997474516e-09 \n",
      "\n",
      "Epoch:  16703  Learning Rate:  5.577165898948619e-09  Varinance:  1.810761793237565e-09 \n",
      "\n",
      "Epoch:  16704  Learning Rate:  5.571591520703337e-09  Varinance:  1.8082322253846236e-09 \n",
      "\n",
      "Epoch:  16705  Learning Rate:  5.566022714050021e-09  Varinance:  1.805706191245253e-09 \n",
      "\n",
      "Epoch:  16706  Learning Rate:  5.560459473419902e-09  Varinance:  1.8031836858829849e-09 \n",
      "\n",
      "Epoch:  16707  Learning Rate:  5.554901793249701e-09  Varinance:  1.800664704368254e-09 \n",
      "\n",
      "Epoch:  16708  Learning Rate:  5.549349667981755e-09  Varinance:  1.7981492417783611e-09 \n",
      "\n",
      "Epoch:  16709  Learning Rate:  5.54380309206396e-09  Varinance:  1.795637293197504e-09 \n",
      "\n",
      "Epoch:  16710  Learning Rate:  5.538262059949699e-09  Varinance:  1.7931288537167403e-09 \n",
      "\n",
      "Epoch:  16711  Learning Rate:  5.532726566097959e-09  Varinance:  1.790623918433986e-09 \n",
      "\n",
      "Epoch:  16712  Learning Rate:  5.5271966049732665e-09  Varinance:  1.7881224824540055e-09 \n",
      "\n",
      "Epoch:  16713  Learning Rate:  5.52167217104562e-09  Varinance:  1.7856245408884005e-09 \n",
      "\n",
      "Epoch:  16714  Learning Rate:  5.516153258790604e-09  Varinance:  1.7831300888556085e-09 \n",
      "\n",
      "Epoch:  16715  Learning Rate:  5.510639862689327e-09  Varinance:  1.7806391214808677e-09 \n",
      "\n",
      "Epoch:  16716  Learning Rate:  5.5051319772283516e-09  Varinance:  1.7781516338962448e-09 \n",
      "\n",
      "Epoch:  16717  Learning Rate:  5.499629596899813e-09  Varinance:  1.7756676212406002e-09 \n",
      "\n",
      "Epoch:  16718  Learning Rate:  5.4941327162013484e-09  Varinance:  1.7731870786595863e-09 \n",
      "\n",
      "Epoch:  16719  Learning Rate:  5.4886413296360386e-09  Varinance:  1.770710001305635e-09 \n",
      "\n",
      "Epoch:  16720  Learning Rate:  5.483155431712516e-09  Varinance:  1.7682363843379522e-09 \n",
      "\n",
      "Epoch:  16721  Learning Rate:  5.477675016944901e-09  Varinance:  1.7657662229225103e-09 \n",
      "\n",
      "Epoch:  16722  Learning Rate:  5.47220007985274e-09  Varinance:  1.7632995122320174e-09 \n",
      "\n",
      "Epoch:  16723  Learning Rate:  5.466730614961115e-09  Varinance:  1.7608362474459437e-09 \n",
      "\n",
      "Epoch:  16724  Learning Rate:  5.461266616800581e-09  Varinance:  1.758376423750487e-09 \n",
      "\n",
      "Epoch:  16725  Learning Rate:  5.455808079907097e-09  Varinance:  1.7559200363385699e-09 \n",
      "\n",
      "Epoch:  16726  Learning Rate:  5.45035499882215e-09  Varinance:  1.75346708040983e-09 \n",
      "\n",
      "Epoch:  16727  Learning Rate:  5.4449073680926746e-09  Varinance:  1.751017551170611e-09 \n",
      "\n",
      "Epoch:  16728  Learning Rate:  5.439465182271001e-09  Varinance:  1.7485714438339595e-09 \n",
      "\n",
      "Epoch:  16729  Learning Rate:  5.434028435914965e-09  Varinance:  1.7461287536195908e-09 \n",
      "\n",
      "Epoch:  16730  Learning Rate:  5.428597123587834e-09  Varinance:  1.7436894757539166e-09 \n",
      "\n",
      "Epoch:  16731  Learning Rate:  5.423171239858262e-09  Varinance:  1.7412536054700108e-09 \n",
      "\n",
      "Epoch:  16732  Learning Rate:  5.417750779300381e-09  Varinance:  1.7388211380076062e-09 \n",
      "\n",
      "Epoch:  16733  Learning Rate:  5.4123357364937505e-09  Varinance:  1.7363920686130863e-09 \n",
      "\n",
      "Epoch:  16734  Learning Rate:  5.406926106023287e-09  Varinance:  1.733966392539475e-09 \n",
      "\n",
      "Epoch:  16735  Learning Rate:  5.4015218824793815e-09  Varinance:  1.7315441050464327e-09 \n",
      "\n",
      "Epoch:  16736  Learning Rate:  5.396123060457827e-09  Varinance:  1.7291252014002252e-09 \n",
      "\n",
      "Epoch:  16737  Learning Rate:  5.390729634559764e-09  Varinance:  1.7267096768737482e-09 \n",
      "\n",
      "Epoch:  16738  Learning Rate:  5.385341599391804e-09  Varinance:  1.724297526746495e-09 \n",
      "\n",
      "Epoch:  16739  Learning Rate:  5.379958949565873e-09  Varinance:  1.7218887463045542e-09 \n",
      "\n",
      "Epoch:  16740  Learning Rate:  5.374581679699339e-09  Varinance:  1.7194833308405984e-09 \n",
      "\n",
      "Epoch:  16741  Learning Rate:  5.369209784414953e-09  Varinance:  1.7170812756538768e-09 \n",
      "\n",
      "Epoch:  16742  Learning Rate:  5.363843258340779e-09  Varinance:  1.714682576050212e-09 \n",
      "\n",
      "Epoch:  16743  Learning Rate:  5.3584820961103095e-09  Varinance:  1.7122872273419642e-09 \n",
      "\n",
      "Epoch:  16744  Learning Rate:  5.353126292362404e-09  Varinance:  1.7098952248480624e-09 \n",
      "\n",
      "Epoch:  16745  Learning Rate:  5.347775841741215e-09  Varinance:  1.7075065638939672e-09 \n",
      "\n",
      "Epoch:  16746  Learning Rate:  5.342430738896315e-09  Varinance:  1.7051212398116701e-09 \n",
      "\n",
      "Epoch:  16747  Learning Rate:  5.337090978482619e-09  Varinance:  1.7027392479396836e-09 \n",
      "\n",
      "Epoch:  16748  Learning Rate:  5.331756555160325e-09  Varinance:  1.7003605836230323e-09 \n",
      "\n",
      "Epoch:  16749  Learning Rate:  5.326427463595033e-09  Varinance:  1.6979852422132488e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16750  Learning Rate:  5.321103698457666e-09  Varinance:  1.6956132190683425e-09 \n",
      "\n",
      "Epoch:  16751  Learning Rate:  5.315785254424422e-09  Varinance:  1.6932445095528246e-09 \n",
      "\n",
      "Epoch:  16752  Learning Rate:  5.310472126176875e-09  Varinance:  1.6908791090376765e-09 \n",
      "\n",
      "Epoch:  16753  Learning Rate:  5.305164308401916e-09  Varinance:  1.6885170129003459e-09 \n",
      "\n",
      "Epoch:  16754  Learning Rate:  5.299861795791689e-09  Varinance:  1.6861582165247382e-09 \n",
      "\n",
      "Epoch:  16755  Learning Rate:  5.2945645830437e-09  Varinance:  1.6838027153012072e-09 \n",
      "\n",
      "Epoch:  16756  Learning Rate:  5.289272664860753e-09  Varinance:  1.6814505046265525e-09 \n",
      "\n",
      "Epoch:  16757  Learning Rate:  5.283986035950892e-09  Varinance:  1.6791015799039854e-09 \n",
      "\n",
      "Epoch:  16758  Learning Rate:  5.27870469102751e-09  Varinance:  1.6767559365431574e-09 \n",
      "\n",
      "Epoch:  16759  Learning Rate:  5.273428624809275e-09  Varinance:  1.6744135699601264e-09 \n",
      "\n",
      "Epoch:  16760  Learning Rate:  5.268157832020087e-09  Varinance:  1.6720744755773547e-09 \n",
      "\n",
      "Epoch:  16761  Learning Rate:  5.26289230738917e-09  Varinance:  1.6697386488236973e-09 \n",
      "\n",
      "Epoch:  16762  Learning Rate:  5.257632045651017e-09  Varinance:  1.667406085134397e-09 \n",
      "\n",
      "Epoch:  16763  Learning Rate:  5.252377041545329e-09  Varinance:  1.6650767799510778e-09 \n",
      "\n",
      "Epoch:  16764  Learning Rate:  5.247127289817121e-09  Varinance:  1.6627507287217148e-09 \n",
      "\n",
      "Epoch:  16765  Learning Rate:  5.2418827852166585e-09  Varinance:  1.6604279269006596e-09 \n",
      "\n",
      "Epoch:  16766  Learning Rate:  5.236643522499399e-09  Varinance:  1.6581083699486076e-09 \n",
      "\n",
      "Epoch:  16767  Learning Rate:  5.231409496426118e-09  Varinance:  1.655792053332596e-09 \n",
      "\n",
      "Epoch:  16768  Learning Rate:  5.22618070176275e-09  Varinance:  1.653478972525994e-09 \n",
      "\n",
      "Epoch:  16769  Learning Rate:  5.220957133280519e-09  Varinance:  1.651169123008494e-09 \n",
      "\n",
      "Epoch:  16770  Learning Rate:  5.215738785755875e-09  Varinance:  1.6488625002661106e-09 \n",
      "\n",
      "Epoch:  16771  Learning Rate:  5.2105256539704335e-09  Varinance:  1.6465590997911444e-09 \n",
      "\n",
      "Epoch:  16772  Learning Rate:  5.2053177327110795e-09  Varinance:  1.644258917082212e-09 \n",
      "\n",
      "Epoch:  16773  Learning Rate:  5.20011501676991e-09  Varinance:  1.641961947644213e-09 \n",
      "\n",
      "Epoch:  16774  Learning Rate:  5.1949175009441734e-09  Varinance:  1.6396681869883243e-09 \n",
      "\n",
      "Epoch:  16775  Learning Rate:  5.18972518003637e-09  Varinance:  1.6373776306319964e-09 \n",
      "\n",
      "Epoch:  16776  Learning Rate:  5.184538048854198e-09  Varinance:  1.6350902740989388e-09 \n",
      "\n",
      "Epoch:  16777  Learning Rate:  5.179356102210489e-09  Varinance:  1.6328061129191228e-09 \n",
      "\n",
      "Epoch:  16778  Learning Rate:  5.174179334923313e-09  Varinance:  1.6305251426287446e-09 \n",
      "\n",
      "Epoch:  16779  Learning Rate:  5.169007741815921e-09  Varinance:  1.6282473587702548e-09 \n",
      "\n",
      "Epoch:  16780  Learning Rate:  5.163841317716683e-09  Varinance:  1.6259727568923248e-09 \n",
      "\n",
      "Epoch:  16781  Learning Rate:  5.1586800574591945e-09  Varinance:  1.6237013325498442e-09 \n",
      "\n",
      "Epoch:  16782  Learning Rate:  5.153523955882211e-09  Varinance:  1.6214330813039126e-09 \n",
      "\n",
      "Epoch:  16783  Learning Rate:  5.148373007829594e-09  Varinance:  1.6191679987218303e-09 \n",
      "\n",
      "Epoch:  16784  Learning Rate:  5.143227208150415e-09  Varinance:  1.6169060803770962e-09 \n",
      "\n",
      "Epoch:  16785  Learning Rate:  5.13808655169889e-09  Varinance:  1.6146473218493745e-09 \n",
      "\n",
      "Epoch:  16786  Learning Rate:  5.132951033334327e-09  Varinance:  1.6123917187245231e-09 \n",
      "\n",
      "Epoch:  16787  Learning Rate:  5.127820647921225e-09  Varinance:  1.6101392665945595e-09 \n",
      "\n",
      "Epoch:  16788  Learning Rate:  5.122695390329217e-09  Varinance:  1.6078899610576595e-09 \n",
      "\n",
      "Epoch:  16789  Learning Rate:  5.117575255433007e-09  Varinance:  1.6056437977181476e-09 \n",
      "\n",
      "Epoch:  16790  Learning Rate:  5.11246023811248e-09  Varinance:  1.6034007721864896e-09 \n",
      "\n",
      "Epoch:  16791  Learning Rate:  5.107350333252635e-09  Varinance:  1.6011608800792884e-09 \n",
      "\n",
      "Epoch:  16792  Learning Rate:  5.102245535743529e-09  Varinance:  1.5989241170192537e-09 \n",
      "\n",
      "Epoch:  16793  Learning Rate:  5.097145840480386e-09  Varinance:  1.5966904786352267e-09 \n",
      "\n",
      "Epoch:  16794  Learning Rate:  5.092051242363526e-09  Varinance:  1.5944599605621498e-09 \n",
      "\n",
      "Epoch:  16795  Learning Rate:  5.0869617362983145e-09  Varinance:  1.5922325584410628e-09 \n",
      "\n",
      "Epoch:  16796  Learning Rate:  5.081877317195263e-09  Varinance:  1.5900082679190952e-09 \n",
      "\n",
      "Epoch:  16797  Learning Rate:  5.07679797996997e-09  Varinance:  1.5877870846494569e-09 \n",
      "\n",
      "Epoch:  16798  Learning Rate:  5.071723719543062e-09  Varinance:  1.585569004291436e-09 \n",
      "\n",
      "Epoch:  16799  Learning Rate:  5.066654530840315e-09  Varinance:  1.5833540225103675e-09 \n",
      "\n",
      "Epoch:  16800  Learning Rate:  5.0615904087925025e-09  Varinance:  1.5811421349776586e-09 \n",
      "\n",
      "Epoch:  16801  Learning Rate:  5.0565313483355204e-09  Varinance:  1.5789333373707577e-09 \n",
      "\n",
      "Epoch:  16802  Learning Rate:  5.051477344410326e-09  Varinance:  1.5767276253731516e-09 \n",
      "\n",
      "Epoch:  16803  Learning Rate:  5.04642839196288e-09  Varinance:  1.5745249946743575e-09 \n",
      "\n",
      "Epoch:  16804  Learning Rate:  5.041384485944246e-09  Varinance:  1.572325440969919e-09 \n",
      "\n",
      "Epoch:  16805  Learning Rate:  5.0363456213105356e-09  Varinance:  1.570128959961377e-09 \n",
      "\n",
      "Epoch:  16806  Learning Rate:  5.031311793022848e-09  Varinance:  1.5679355473562936e-09 \n",
      "\n",
      "Epoch:  16807  Learning Rate:  5.0262829960473735e-09  Varinance:  1.5657451988682215e-09 \n",
      "\n",
      "Epoch:  16808  Learning Rate:  5.021259225355331e-09  Varinance:  1.5635579102167014e-09 \n",
      "\n",
      "Epoch:  16809  Learning Rate:  5.016240475922915e-09  Varinance:  1.561373677127254e-09 \n",
      "\n",
      "Epoch:  16810  Learning Rate:  5.011226742731393e-09  Varinance:  1.5591924953313712e-09 \n",
      "\n",
      "Epoch:  16811  Learning Rate:  5.0062180207670495e-09  Varinance:  1.5570143605665128e-09 \n",
      "\n",
      "Epoch:  16812  Learning Rate:  5.001214305021125e-09  Varinance:  1.5548392685760775e-09 \n",
      "\n",
      "Epoch:  16813  Learning Rate:  4.996215590489923e-09  Varinance:  1.552667215109426e-09 \n",
      "\n",
      "Epoch:  16814  Learning Rate:  4.9912218721747454e-09  Varinance:  1.5504981959218528e-09 \n",
      "\n",
      "Epoch:  16815  Learning Rate:  4.986233145081839e-09  Varinance:  1.5483322067745803e-09 \n",
      "\n",
      "Epoch:  16816  Learning Rate:  4.981249404222492e-09  Varinance:  1.5461692434347543e-09 \n",
      "\n",
      "Epoch:  16817  Learning Rate:  4.976270644612982e-09  Varinance:  1.5440093016754317e-09 \n",
      "\n",
      "Epoch:  16818  Learning Rate:  4.9712968612745146e-09  Varinance:  1.5418523772755812e-09 \n",
      "\n",
      "Epoch:  16819  Learning Rate:  4.966328049233322e-09  Varinance:  1.5396984660200513e-09 \n",
      "\n",
      "Epoch:  16820  Learning Rate:  4.9613642035206106e-09  Varinance:  1.5375475636995955e-09 \n",
      "\n",
      "Epoch:  16821  Learning Rate:  4.956405319172498e-09  Varinance:  1.535399666110841e-09 \n",
      "\n",
      "Epoch:  16822  Learning Rate:  4.9514513912301186e-09  Varinance:  1.5332547690562889e-09 \n",
      "\n",
      "Epoch:  16823  Learning Rate:  4.946502414739559e-09  Varinance:  1.531112868344302e-09 \n",
      "\n",
      "Epoch:  16824  Learning Rate:  4.94155838475181e-09  Varinance:  1.5289739597891002e-09 \n",
      "\n",
      "Epoch:  16825  Learning Rate:  4.936619296322857e-09  Varinance:  1.5268380392107547e-09 \n",
      "\n",
      "Epoch:  16826  Learning Rate:  4.931685144513631e-09  Varinance:  1.5247051024351615e-09 \n",
      "\n",
      "Epoch:  16827  Learning Rate:  4.926755924389941e-09  Varinance:  1.522575145294062e-09 \n",
      "\n",
      "Epoch:  16828  Learning Rate:  4.921831631022586e-09  Varinance:  1.5204481636250172e-09 \n",
      "\n",
      "Epoch:  16829  Learning Rate:  4.9169122594872906e-09  Varinance:  1.5183241532714003e-09 \n",
      "\n",
      "Epoch:  16830  Learning Rate:  4.911997804864646e-09  Varinance:  1.5162031100823938e-09 \n",
      "\n",
      "Epoch:  16831  Learning Rate:  4.907088262240234e-09  Varinance:  1.514085029912977e-09 \n",
      "\n",
      "Epoch:  16832  Learning Rate:  4.902183626704475e-09  Varinance:  1.5119699086239252e-09 \n",
      "\n",
      "Epoch:  16833  Learning Rate:  4.897283893352751e-09  Varinance:  1.5098577420817808e-09 \n",
      "\n",
      "Epoch:  16834  Learning Rate:  4.892389057285347e-09  Varinance:  1.5077485261588753e-09 \n",
      "\n",
      "Epoch:  16835  Learning Rate:  4.88749911360739e-09  Varinance:  1.5056422567333026e-09 \n",
      "\n",
      "Epoch:  16836  Learning Rate:  4.882614057428954e-09  Varinance:  1.5035389296889137e-09 \n",
      "\n",
      "Epoch:  16837  Learning Rate:  4.8777338838649995e-09  Varinance:  1.5014385409153095e-09 \n",
      "\n",
      "Epoch:  16838  Learning Rate:  4.872858588035318e-09  Varinance:  1.499341086307834e-09 \n",
      "\n",
      "Epoch:  16839  Learning Rate:  4.86798816506463e-09  Varinance:  1.49724656176757e-09 \n",
      "\n",
      "Epoch:  16840  Learning Rate:  4.8631226100825314e-09  Varinance:  1.4951549632013095e-09 \n",
      "\n",
      "Epoch:  16841  Learning Rate:  4.8582619182234295e-09  Varinance:  1.4930662865215803e-09 \n",
      "\n",
      "Epoch:  16842  Learning Rate:  4.853406084626652e-09  Varinance:  1.4909805276466137e-09 \n",
      "\n",
      "Epoch:  16843  Learning Rate:  4.8485551044363795e-09  Varinance:  1.4888976825003433e-09 \n",
      "\n",
      "Epoch:  16844  Learning Rate:  4.843708972801599e-09  Varinance:  1.4868177470123969e-09 \n",
      "\n",
      "Epoch:  16845  Learning Rate:  4.838867684876195e-09  Varinance:  1.4847407171180888e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16846  Learning Rate:  4.834031235818896e-09  Varinance:  1.482666588758416e-09 \n",
      "\n",
      "Epoch:  16847  Learning Rate:  4.829199620793219e-09  Varinance:  1.480595357880031e-09 \n",
      "\n",
      "Epoch:  16848  Learning Rate:  4.824372834967565e-09  Varinance:  1.4785270204352633e-09 \n",
      "\n",
      "Epoch:  16849  Learning Rate:  4.819550873515165e-09  Varinance:  1.4764615723820927e-09 \n",
      "\n",
      "Epoch:  16850  Learning Rate:  4.814733731614022e-09  Varinance:  1.4743990096841445e-09 \n",
      "\n",
      "Epoch:  16851  Learning Rate:  4.8099214044470134e-09  Varinance:  1.4723393283106838e-09 \n",
      "\n",
      "Epoch:  16852  Learning Rate:  4.8051138872018266e-09  Varinance:  1.4702825242366056e-09 \n",
      "\n",
      "Epoch:  16853  Learning Rate:  4.80031117507091e-09  Varinance:  1.4682285934424332e-09 \n",
      "\n",
      "Epoch:  16854  Learning Rate:  4.795513263251569e-09  Varinance:  1.4661775319142899e-09 \n",
      "\n",
      "Epoch:  16855  Learning Rate:  4.790720146945908e-09  Varinance:  1.4641293356439208e-09 \n",
      "\n",
      "Epoch:  16856  Learning Rate:  4.785931821360777e-09  Varinance:  1.4620840006286668e-09 \n",
      "\n",
      "Epoch:  16857  Learning Rate:  4.7811482817078645e-09  Varinance:  1.4600415228714584e-09 \n",
      "\n",
      "Epoch:  16858  Learning Rate:  4.77636952320365e-09  Varinance:  1.4580018983808112e-09 \n",
      "\n",
      "Epoch:  16859  Learning Rate:  4.7715955410693385e-09  Varinance:  1.4559651231708164e-09 \n",
      "\n",
      "Epoch:  16860  Learning Rate:  4.7668263305309666e-09  Varinance:  1.4539311932611388e-09 \n",
      "\n",
      "Epoch:  16861  Learning Rate:  4.76206188681934e-09  Varinance:  1.4519001046769873e-09 \n",
      "\n",
      "Epoch:  16862  Learning Rate:  4.757302205169979e-09  Varinance:  1.44987185344914e-09 \n",
      "\n",
      "Epoch:  16863  Learning Rate:  4.752547280823238e-09  Varinance:  1.4478464356139137e-09 \n",
      "\n",
      "Epoch:  16864  Learning Rate:  4.747797109024156e-09  Varinance:  1.4458238472131628e-09 \n",
      "\n",
      "Epoch:  16865  Learning Rate:  4.743051685022579e-09  Varinance:  1.4438040842942714e-09 \n",
      "\n",
      "Epoch:  16866  Learning Rate:  4.738311004073099e-09  Varinance:  1.4417871429101446e-09 \n",
      "\n",
      "Epoch:  16867  Learning Rate:  4.733575061435001e-09  Varinance:  1.439773019119207e-09 \n",
      "\n",
      "Epoch:  16868  Learning Rate:  4.728843852372359e-09  Varinance:  1.4377617089853738e-09 \n",
      "\n",
      "Epoch:  16869  Learning Rate:  4.7241173721539805e-09  Varinance:  1.4357532085780744e-09 \n",
      "\n",
      "Epoch:  16870  Learning Rate:  4.71939561605335e-09  Varinance:  1.4337475139722239e-09 \n",
      "\n",
      "Epoch:  16871  Learning Rate:  4.71467857934873e-09  Varinance:  1.4317446212482205e-09 \n",
      "\n",
      "Epoch:  16872  Learning Rate:  4.709966257323098e-09  Varinance:  1.429744526491938e-09 \n",
      "\n",
      "Epoch:  16873  Learning Rate:  4.7052586452641e-09  Varinance:  1.4277472257947176e-09 \n",
      "\n",
      "Epoch:  16874  Learning Rate:  4.700555738464139e-09  Varinance:  1.4257527152533668e-09 \n",
      "\n",
      "Epoch:  16875  Learning Rate:  4.695857532220325e-09  Varinance:  1.4237609909701297e-09 \n",
      "\n",
      "Epoch:  16876  Learning Rate:  4.691164021834418e-09  Varinance:  1.421772049052711e-09 \n",
      "\n",
      "Epoch:  16877  Learning Rate:  4.686475202612924e-09  Varinance:  1.419785885614248e-09 \n",
      "\n",
      "Epoch:  16878  Learning Rate:  4.681791069867038e-09  Varinance:  1.4178024967733068e-09 \n",
      "\n",
      "Epoch:  16879  Learning Rate:  4.6771116189125974e-09  Varinance:  1.4158218786538767e-09 \n",
      "\n",
      "Epoch:  16880  Learning Rate:  4.672436845070165e-09  Varinance:  1.4138440273853612e-09 \n",
      "\n",
      "Epoch:  16881  Learning Rate:  4.667766743664984e-09  Varinance:  1.4118689391025763e-09 \n",
      "\n",
      "Epoch:  16882  Learning Rate:  4.663101310026917e-09  Varinance:  1.409896609945722e-09 \n",
      "\n",
      "Epoch:  16883  Learning Rate:  4.6584405394905504e-09  Varinance:  1.407927036060405e-09 \n",
      "\n",
      "Epoch:  16884  Learning Rate:  4.653784427395128e-09  Varinance:  1.4059602135976127e-09 \n",
      "\n",
      "Epoch:  16885  Learning Rate:  4.6491329690845045e-09  Varinance:  1.4039961387137086e-09 \n",
      "\n",
      "Epoch:  16886  Learning Rate:  4.644486159907237e-09  Varinance:  1.4020348075704253e-09 \n",
      "\n",
      "Epoch:  16887  Learning Rate:  4.639843995216532e-09  Varinance:  1.400076216334858e-09 \n",
      "\n",
      "Epoch:  16888  Learning Rate:  4.635206470370194e-09  Varinance:  1.398120361179461e-09 \n",
      "\n",
      "Epoch:  16889  Learning Rate:  4.630573580730712e-09  Varinance:  1.3961672382820206e-09 \n",
      "\n",
      "Epoch:  16890  Learning Rate:  4.625945321665213e-09  Varinance:  1.3942168438256774e-09 \n",
      "\n",
      "Epoch:  16891  Learning Rate:  4.621321688545405e-09  Varinance:  1.3922691739988994e-09 \n",
      "\n",
      "Epoch:  16892  Learning Rate:  4.616702676747686e-09  Varinance:  1.3903242249954792e-09 \n",
      "\n",
      "Epoch:  16893  Learning Rate:  4.6120882816530135e-09  Varinance:  1.3883819930145258e-09 \n",
      "\n",
      "Epoch:  16894  Learning Rate:  4.607478498647006e-09  Varinance:  1.3864424742604587e-09 \n",
      "\n",
      "Epoch:  16895  Learning Rate:  4.602873323119898e-09  Varinance:  1.3845056649430042e-09 \n",
      "\n",
      "Epoch:  16896  Learning Rate:  4.59827275046648e-09  Varinance:  1.382571561277169e-09 \n",
      "\n",
      "Epoch:  16897  Learning Rate:  4.593676776086196e-09  Varinance:  1.3806401594832617e-09 \n",
      "\n",
      "Epoch:  16898  Learning Rate:  4.589085395383088e-09  Varinance:  1.378711455786866e-09 \n",
      "\n",
      "Epoch:  16899  Learning Rate:  4.58449860376574e-09  Varinance:  1.3767854464188392e-09 \n",
      "\n",
      "Epoch:  16900  Learning Rate:  4.579916396647379e-09  Varinance:  1.374862127615303e-09 \n",
      "\n",
      "Epoch:  16901  Learning Rate:  4.575338769445812e-09  Varinance:  1.3729414956176373e-09 \n",
      "\n",
      "Epoch:  16902  Learning Rate:  4.57076571758338e-09  Varinance:  1.3710235466724778e-09 \n",
      "\n",
      "Epoch:  16903  Learning Rate:  4.566197236487045e-09  Varinance:  1.3691082770316889e-09 \n",
      "\n",
      "Epoch:  16904  Learning Rate:  4.561633321588344e-09  Varinance:  1.3671956829523852e-09 \n",
      "\n",
      "Epoch:  16905  Learning Rate:  4.55707396832333e-09  Varinance:  1.3652857606969053e-09 \n",
      "\n",
      "Epoch:  16906  Learning Rate:  4.5525191721326624e-09  Varinance:  1.3633785065328097e-09 \n",
      "\n",
      "Epoch:  16907  Learning Rate:  4.547968928461562e-09  Varinance:  1.3614739167328723e-09 \n",
      "\n",
      "Epoch:  16908  Learning Rate:  4.543423232759754e-09  Varinance:  1.3595719875750738e-09 \n",
      "\n",
      "Epoch:  16909  Learning Rate:  4.538882080481557e-09  Varinance:  1.3576727153425997e-09 \n",
      "\n",
      "Epoch:  16910  Learning Rate:  4.534345467085835e-09  Varinance:  1.355776096323813e-09 \n",
      "\n",
      "Epoch:  16911  Learning Rate:  4.5298133880359425e-09  Varinance:  1.3538821268122766e-09 \n",
      "\n",
      "Epoch:  16912  Learning Rate:  4.525285838799814e-09  Varinance:  1.3519908031067254e-09 \n",
      "\n",
      "Epoch:  16913  Learning Rate:  4.520762814849919e-09  Varinance:  1.3501021215110657e-09 \n",
      "\n",
      "Epoch:  16914  Learning Rate:  4.5162443116631985e-09  Varinance:  1.348216078334367e-09 \n",
      "\n",
      "Epoch:  16915  Learning Rate:  4.511730324721166e-09  Varinance:  1.3463326698908543e-09 \n",
      "\n",
      "Epoch:  16916  Learning Rate:  4.507220849509851e-09  Varinance:  1.3444518924999073e-09 \n",
      "\n",
      "Epoch:  16917  Learning Rate:  4.502715881519745e-09  Varinance:  1.3425737424860317e-09 \n",
      "\n",
      "Epoch:  16918  Learning Rate:  4.498215416245895e-09  Varinance:  1.3406982161788835e-09 \n",
      "\n",
      "Epoch:  16919  Learning Rate:  4.493719449187852e-09  Varinance:  1.3388253099132401e-09 \n",
      "\n",
      "Epoch:  16920  Learning Rate:  4.489227975849618e-09  Varinance:  1.3369550200289998e-09 \n",
      "\n",
      "Epoch:  16921  Learning Rate:  4.484740991739733e-09  Varinance:  1.3350873428711738e-09 \n",
      "\n",
      "Epoch:  16922  Learning Rate:  4.48025849237123e-09  Varinance:  1.3332222747898787e-09 \n",
      "\n",
      "Epoch:  16923  Learning Rate:  4.475780473261576e-09  Varinance:  1.331359812140335e-09 \n",
      "\n",
      "Epoch:  16924  Learning Rate:  4.471306929932785e-09  Varinance:  1.329499951282841e-09 \n",
      "\n",
      "Epoch:  16925  Learning Rate:  4.4668378579112794e-09  Varinance:  1.3276426885827929e-09 \n",
      "\n",
      "Epoch:  16926  Learning Rate:  4.462373252728005e-09  Varinance:  1.32578802041066e-09 \n",
      "\n",
      "Epoch:  16927  Learning Rate:  4.457913109918371e-09  Varinance:  1.3239359431419819e-09 \n",
      "\n",
      "Epoch:  16928  Learning Rate:  4.453457425022202e-09  Varinance:  1.3220864531573617e-09 \n",
      "\n",
      "Epoch:  16929  Learning Rate:  4.449006193583829e-09  Varinance:  1.3202395468424583e-09 \n",
      "\n",
      "Epoch:  16930  Learning Rate:  4.444559411152037e-09  Varinance:  1.3183952205879843e-09 \n",
      "\n",
      "Epoch:  16931  Learning Rate:  4.44011707328001e-09  Varinance:  1.3165534707896807e-09 \n",
      "\n",
      "Epoch:  16932  Learning Rate:  4.435679175525427e-09  Varinance:  1.3147142938483372e-09 \n",
      "\n",
      "Epoch:  16933  Learning Rate:  4.431245713450405e-09  Varinance:  1.3128776861697672e-09 \n",
      "\n",
      "Epoch:  16934  Learning Rate:  4.426816682621449e-09  Varinance:  1.3110436441648045e-09 \n",
      "\n",
      "Epoch:  16935  Learning Rate:  4.422392078609545e-09  Varinance:  1.309212164249297e-09 \n",
      "\n",
      "Epoch:  16936  Learning Rate:  4.417971896990104e-09  Varinance:  1.3073832428441049e-09 \n",
      "\n",
      "Epoch:  16937  Learning Rate:  4.413556133342913e-09  Varinance:  1.305556876375073e-09 \n",
      "\n",
      "Epoch:  16938  Learning Rate:  4.409144783252222e-09  Varinance:  1.3037330612730543e-09 \n",
      "\n",
      "Epoch:  16939  Learning Rate:  4.404737842306699e-09  Varinance:  1.3019117939738824e-09 \n",
      "\n",
      "Epoch:  16940  Learning Rate:  4.4003353060993675e-09  Varinance:  1.3000930709183703e-09 \n",
      "\n",
      "Epoch:  16941  Learning Rate:  4.39593717022771e-09  Varinance:  1.2982768885523026e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16942  Learning Rate:  4.3915434302936045e-09  Varinance:  1.2964632433264295e-09 \n",
      "\n",
      "Epoch:  16943  Learning Rate:  4.38715408190328e-09  Varinance:  1.2946521316964633e-09 \n",
      "\n",
      "Epoch:  16944  Learning Rate:  4.382769120667402e-09  Varinance:  1.292843550123055e-09 \n",
      "\n",
      "Epoch:  16945  Learning Rate:  4.378388542201027e-09  Varinance:  1.2910374950718121e-09 \n",
      "\n",
      "Epoch:  16946  Learning Rate:  4.374012342123542e-09  Varinance:  1.2892339630132762e-09 \n",
      "\n",
      "Epoch:  16947  Learning Rate:  4.369640516058765e-09  Varinance:  1.287432950422919e-09 \n",
      "\n",
      "Epoch:  16948  Learning Rate:  4.365273059634883e-09  Varinance:  1.2856344537811356e-09 \n",
      "\n",
      "Epoch:  16949  Learning Rate:  4.360909968484409e-09  Varinance:  1.283838469573238e-09 \n",
      "\n",
      "Epoch:  16950  Learning Rate:  4.356551238244267e-09  Varinance:  1.282044994289453e-09 \n",
      "\n",
      "Epoch:  16951  Learning Rate:  4.3521968645557415e-09  Varinance:  1.280254024424896e-09 \n",
      "\n",
      "Epoch:  16952  Learning Rate:  4.3478468430644275e-09  Varinance:  1.2784655564795935e-09 \n",
      "\n",
      "Epoch:  16953  Learning Rate:  4.3435011694203195e-09  Varinance:  1.2766795869584552e-09 \n",
      "\n",
      "Epoch:  16954  Learning Rate:  4.339159839277759e-09  Varinance:  1.2748961123712748e-09 \n",
      "\n",
      "Epoch:  16955  Learning Rate:  4.334822848295382e-09  Varinance:  1.2731151292327204e-09 \n",
      "\n",
      "Epoch:  16956  Learning Rate:  4.330490192136231e-09  Varinance:  1.2713366340623299e-09 \n",
      "\n",
      "Epoch:  16957  Learning Rate:  4.326161866467618e-09  Varinance:  1.2695606233845075e-09 \n",
      "\n",
      "Epoch:  16958  Learning Rate:  4.321837866961231e-09  Varinance:  1.2677870937284991e-09 \n",
      "\n",
      "Epoch:  16959  Learning Rate:  4.317518189293087e-09  Varinance:  1.2660160416284124e-09 \n",
      "\n",
      "Epoch:  16960  Learning Rate:  4.313202829143477e-09  Varinance:  1.264247463623193e-09 \n",
      "\n",
      "Epoch:  16961  Learning Rate:  4.308891782197055e-09  Varinance:  1.262481356256621e-09 \n",
      "\n",
      "Epoch:  16962  Learning Rate:  4.304585044142791e-09  Varinance:  1.2607177160773045e-09 \n",
      "\n",
      "Epoch:  16963  Learning Rate:  4.300282610673913e-09  Varinance:  1.2589565396386737e-09 \n",
      "\n",
      "Epoch:  16964  Learning Rate:  4.295984477488005e-09  Varinance:  1.2571978234989773e-09 \n",
      "\n",
      "Epoch:  16965  Learning Rate:  4.291690640286947e-09  Varinance:  1.2554415642212593e-09 \n",
      "\n",
      "Epoch:  16966  Learning Rate:  4.287401094776872e-09  Varinance:  1.2536877583733776e-09 \n",
      "\n",
      "Epoch:  16967  Learning Rate:  4.283115836668249e-09  Varinance:  1.2519364025279808e-09 \n",
      "\n",
      "Epoch:  16968  Learning Rate:  4.278834861675835e-09  Varinance:  1.2501874932625054e-09 \n",
      "\n",
      "Epoch:  16969  Learning Rate:  4.274558165518623e-09  Varinance:  1.2484410271591689e-09 \n",
      "\n",
      "Epoch:  16970  Learning Rate:  4.270285743919934e-09  Varinance:  1.246697000804963e-09 \n",
      "\n",
      "Epoch:  16971  Learning Rate:  4.266017592607359e-09  Varinance:  1.2449554107916524e-09 \n",
      "\n",
      "Epoch:  16972  Learning Rate:  4.261753707312718e-09  Varinance:  1.2432162537157493e-09 \n",
      "\n",
      "Epoch:  16973  Learning Rate:  4.257494083772139e-09  Varinance:  1.2414795261785336e-09 \n",
      "\n",
      "Epoch:  16974  Learning Rate:  4.253238717726013e-09  Varinance:  1.239745224786029e-09 \n",
      "\n",
      "Epoch:  16975  Learning Rate:  4.248987604918945e-09  Varinance:  1.2380133461489998e-09 \n",
      "\n",
      "Epoch:  16976  Learning Rate:  4.244740741099836e-09  Varinance:  1.236283886882946e-09 \n",
      "\n",
      "Epoch:  16977  Learning Rate:  4.240498122021837e-09  Varinance:  1.2345568436080945e-09 \n",
      "\n",
      "Epoch:  16978  Learning Rate:  4.236259743442298e-09  Varinance:  1.232832212949399e-09 \n",
      "\n",
      "Epoch:  16979  Learning Rate:  4.232025601122856e-09  Varinance:  1.2311099915365141e-09 \n",
      "\n",
      "Epoch:  16980  Learning Rate:  4.227795690829381e-09  Varinance:  1.2293901760038162e-09 \n",
      "\n",
      "Epoch:  16981  Learning Rate:  4.223570008331936e-09  Varinance:  1.2276727629903787e-09 \n",
      "\n",
      "Epoch:  16982  Learning Rate:  4.21934854940485e-09  Varinance:  1.2259577491399707e-09 \n",
      "\n",
      "Epoch:  16983  Learning Rate:  4.215131309826681e-09  Varinance:  1.2242451311010487e-09 \n",
      "\n",
      "Epoch:  16984  Learning Rate:  4.2109182853801584e-09  Varinance:  1.2225349055267527e-09 \n",
      "\n",
      "Epoch:  16985  Learning Rate:  4.206709471852271e-09  Varinance:  1.2208270690749008e-09 \n",
      "\n",
      "Epoch:  16986  Learning Rate:  4.202504865034222e-09  Varinance:  1.2191216184079685e-09 \n",
      "\n",
      "Epoch:  16987  Learning Rate:  4.1983044607213725e-09  Varinance:  1.2174185501931056e-09 \n",
      "\n",
      "Epoch:  16988  Learning Rate:  4.194108254713349e-09  Varinance:  1.2157178611021144e-09 \n",
      "\n",
      "Epoch:  16989  Learning Rate:  4.189916242813914e-09  Varinance:  1.2140195478114453e-09 \n",
      "\n",
      "Epoch:  16990  Learning Rate:  4.185728420831072e-09  Varinance:  1.212323607002193e-09 \n",
      "\n",
      "Epoch:  16991  Learning Rate:  4.1815447845770135e-09  Varinance:  1.2106300353600877e-09 \n",
      "\n",
      "Epoch:  16992  Learning Rate:  4.177365329868074e-09  Varinance:  1.2089388295754941e-09 \n",
      "\n",
      "Epoch:  16993  Learning Rate:  4.173190052524812e-09  Varinance:  1.2072499863433872e-09 \n",
      "\n",
      "Epoch:  16994  Learning Rate:  4.169018948371966e-09  Varinance:  1.2055635023633723e-09 \n",
      "\n",
      "Epoch:  16995  Learning Rate:  4.1648520132384e-09  Varinance:  1.2038793743396605e-09 \n",
      "\n",
      "Epoch:  16996  Learning Rate:  4.160689242957195e-09  Varinance:  1.2021975989810672e-09 \n",
      "\n",
      "Epoch:  16997  Learning Rate:  4.156530633365594e-09  Varinance:  1.2005181730010055e-09 \n",
      "\n",
      "Epoch:  16998  Learning Rate:  4.152376180304958e-09  Varinance:  1.19884109311748e-09 \n",
      "\n",
      "Epoch:  16999  Learning Rate:  4.148225879620848e-09  Varinance:  1.1971663560530838e-09 \n",
      "\n",
      "Epoch:  17000  Learning Rate:  4.144079727162979e-09  Varinance:  1.195493958534976e-09 \n",
      "\n",
      "Epoch:  17001  Learning Rate:  4.139937718785167e-09  Varinance:  1.1938238972949003e-09 \n",
      "\n",
      "Epoch:  17002  Learning Rate:  4.135799850345419e-09  Varinance:  1.1921561690691621e-09 \n",
      "\n",
      "Epoch:  17003  Learning Rate:  4.131666117705881e-09  Varinance:  1.1904907705986259e-09 \n",
      "\n",
      "Epoch:  17004  Learning Rate:  4.127536516732789e-09  Varinance:  1.1888276986287089e-09 \n",
      "\n",
      "Epoch:  17005  Learning Rate:  4.123411043296559e-09  Varinance:  1.1871669499093751e-09 \n",
      "\n",
      "Epoch:  17006  Learning Rate:  4.11928969327173e-09  Varinance:  1.185508521195133e-09 \n",
      "\n",
      "Epoch:  17007  Learning Rate:  4.115172462536924e-09  Varinance:  1.183852409245012e-09 \n",
      "\n",
      "Epoch:  17008  Learning Rate:  4.111059346974922e-09  Varinance:  1.1821986108225816e-09 \n",
      "\n",
      "Epoch:  17009  Learning Rate:  4.106950342472625e-09  Varinance:  1.1805471226959285e-09 \n",
      "\n",
      "Epoch:  17010  Learning Rate:  4.1028454449209985e-09  Varinance:  1.178897941637654e-09 \n",
      "\n",
      "Epoch:  17011  Learning Rate:  4.098744650215158e-09  Varinance:  1.1772510644248687e-09 \n",
      "\n",
      "Epoch:  17012  Learning Rate:  4.094647954254325e-09  Varinance:  1.1756064878391841e-09 \n",
      "\n",
      "Epoch:  17013  Learning Rate:  4.090555352941771e-09  Varinance:  1.1739642086667134e-09 \n",
      "\n",
      "Epoch:  17014  Learning Rate:  4.086466842184912e-09  Varinance:  1.1723242236980457e-09 \n",
      "\n",
      "Epoch:  17015  Learning Rate:  4.08238241789525e-09  Varinance:  1.1706865297282669e-09 \n",
      "\n",
      "Epoch:  17016  Learning Rate:  4.078302075988332e-09  Varinance:  1.1690511235569352e-09 \n",
      "\n",
      "Epoch:  17017  Learning Rate:  4.074225812383844e-09  Varinance:  1.16741800198808e-09 \n",
      "\n",
      "Epoch:  17018  Learning Rate:  4.070153623005493e-09  Varinance:  1.1657871618301958e-09 \n",
      "\n",
      "Epoch:  17019  Learning Rate:  4.066085503781105e-09  Varinance:  1.164158599896235e-09 \n",
      "\n",
      "Epoch:  17020  Learning Rate:  4.062021450642574e-09  Varinance:  1.162532313003606e-09 \n",
      "\n",
      "Epoch:  17021  Learning Rate:  4.057961459525818e-09  Varinance:  1.1609082979741517e-09 \n",
      "\n",
      "Epoch:  17022  Learning Rate:  4.053905526370859e-09  Varinance:  1.159286551634166e-09 \n",
      "\n",
      "Epoch:  17023  Learning Rate:  4.049853647121778e-09  Varinance:  1.1576670708143736e-09 \n",
      "\n",
      "Epoch:  17024  Learning Rate:  4.045805817726668e-09  Varinance:  1.1560498523499255e-09 \n",
      "\n",
      "Epoch:  17025  Learning Rate:  4.0417620341377126e-09  Varinance:  1.1544348930803943e-09 \n",
      "\n",
      "Epoch:  17026  Learning Rate:  4.037722292311143e-09  Varinance:  1.152822189849767e-09 \n",
      "\n",
      "Epoch:  17027  Learning Rate:  4.033686588207187e-09  Varinance:  1.151211739506445e-09 \n",
      "\n",
      "Epoch:  17028  Learning Rate:  4.0296549177901564e-09  Varinance:  1.1496035389032182e-09 \n",
      "\n",
      "Epoch:  17029  Learning Rate:  4.025627277028394e-09  Varinance:  1.147997584897287e-09 \n",
      "\n",
      "Epoch:  17030  Learning Rate:  4.021603661894228e-09  Varinance:  1.1463938743502366e-09 \n",
      "\n",
      "Epoch:  17031  Learning Rate:  4.01758406836406e-09  Varinance:  1.1447924041280377e-09 \n",
      "\n",
      "Epoch:  17032  Learning Rate:  4.01356849241831e-09  Varinance:  1.1431931711010385e-09 \n",
      "\n",
      "Epoch:  17033  Learning Rate:  4.009556930041372e-09  Varinance:  1.141596172143959e-09 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17034  Learning Rate:  4.005549377221698e-09  Varinance:  1.1400014041358899e-09 \n",
      "\n",
      "Epoch:  17035  Learning Rate:  4.00154582995175e-09  Varinance:  1.138408863960269e-09 \n",
      "\n",
      "Epoch:  17036  Learning Rate:  3.997546284227951e-09  Varinance:  1.1368185485048997e-09 \n",
      "\n",
      "Epoch:  17037  Learning Rate:  3.9935507360507686e-09  Varinance:  1.13523045466193e-09 \n",
      "\n",
      "Epoch:  17038  Learning Rate:  3.98955918142467e-09  Varinance:  1.1336445793278483e-09 \n",
      "\n",
      "Epoch:  17039  Learning Rate:  3.985571616358071e-09  Varinance:  1.132060919403479e-09 \n",
      "\n",
      "Epoch:  17040  Learning Rate:  3.98158803686342e-09  Varinance:  1.1304794717939764e-09 \n",
      "\n",
      "Epoch:  17041  Learning Rate:  3.977608438957152e-09  Varinance:  1.1289002334088211e-09 \n",
      "\n",
      "Epoch:  17042  Learning Rate:  3.9736328186596405e-09  Varinance:  1.1273232011618002e-09 \n",
      "\n",
      "Epoch:  17043  Learning Rate:  3.969661171995279e-09  Varinance:  1.1257483719710233e-09 \n",
      "\n",
      "Epoch:  17044  Learning Rate:  3.9656934949924335e-09  Varinance:  1.1241757427589017e-09 \n",
      "\n",
      "Epoch:  17045  Learning Rate:  3.961729783683401e-09  Varinance:  1.1226053104521457e-09 \n",
      "\n",
      "Epoch:  17046  Learning Rate:  3.957770034104481e-09  Varinance:  1.1210370719817592e-09 \n",
      "\n",
      "Epoch:  17047  Learning Rate:  3.953814242295939e-09  Varinance:  1.1194710242830331e-09 \n",
      "\n",
      "Epoch:  17048  Learning Rate:  3.949862404301955e-09  Varinance:  1.1179071642955436e-09 \n",
      "\n",
      "Epoch:  17049  Learning Rate:  3.945914516170719e-09  Varinance:  1.1163454889631307e-09 \n",
      "\n",
      "Epoch:  17050  Learning Rate:  3.941970573954313e-09  Varinance:  1.114785995233915e-09 \n",
      "\n",
      "Epoch:  17051  Learning Rate:  3.93803057370881e-09  Varinance:  1.1132286800602767e-09 \n",
      "\n",
      "Epoch:  17052  Learning Rate:  3.934094511494223e-09  Varinance:  1.1116735403988539e-09 \n",
      "\n",
      "Epoch:  17053  Learning Rate:  3.930162383374461e-09  Varinance:  1.1101205732105354e-09 \n",
      "\n",
      "Epoch:  17054  Learning Rate:  3.9262341854174106e-09  Varinance:  1.108569775460456e-09 \n",
      "\n",
      "Epoch:  17055  Learning Rate:  3.922309913694886e-09  Varinance:  1.107021144117994e-09 \n",
      "\n",
      "Epoch:  17056  Learning Rate:  3.9183895642825885e-09  Varinance:  1.1054746761567498e-09 \n",
      "\n",
      "Epoch:  17057  Learning Rate:  3.914473133260182e-09  Varinance:  1.1039303685545624e-09 \n",
      "\n",
      "Epoch:  17058  Learning Rate:  3.910560616711248e-09  Varinance:  1.1023882182934899e-09 \n",
      "\n",
      "Epoch:  17059  Learning Rate:  3.9066520107232425e-09  Varinance:  1.1008482223598054e-09 \n",
      "\n",
      "Epoch:  17060  Learning Rate:  3.902747311387575e-09  Varinance:  1.0993103777439931e-09 \n",
      "\n",
      "Epoch:  17061  Learning Rate:  3.898846514799556e-09  Varinance:  1.0977746814407404e-09 \n",
      "\n",
      "Epoch:  17062  Learning Rate:  3.894949617058364e-09  Varinance:  1.0962411304489374e-09 \n",
      "\n",
      "Epoch:  17063  Learning Rate:  3.891056614267113e-09  Varinance:  1.0947097217716546e-09 \n",
      "\n",
      "Epoch:  17064  Learning Rate:  3.887167502532815e-09  Varinance:  1.0931804524161614e-09 \n",
      "\n",
      "Epoch:  17065  Learning Rate:  3.883282277966329e-09  Varinance:  1.0916533193939034e-09 \n",
      "\n",
      "Epoch:  17066  Learning Rate:  3.879400936682446e-09  Varinance:  1.0901283197205017e-09 \n",
      "\n",
      "Epoch:  17067  Learning Rate:  3.875523474799836e-09  Varinance:  1.0886054504157458e-09 \n",
      "\n",
      "Epoch:  17068  Learning Rate:  3.8716498884410095e-09  Varinance:  1.0870847085035929e-09 \n",
      "\n",
      "Epoch:  17069  Learning Rate:  3.867780173732394e-09  Varinance:  1.0855660910121456e-09 \n",
      "\n",
      "Epoch:  17070  Learning Rate:  3.863914326804289e-09  Varinance:  1.08404959497367e-09 \n",
      "\n",
      "Epoch:  17071  Learning Rate:  3.860052343790818e-09  Varinance:  1.082535217424574e-09 \n",
      "\n",
      "Epoch:  17072  Learning Rate:  3.856194220830013e-09  Varinance:  1.0810229554054057e-09 \n",
      "\n",
      "Epoch:  17073  Learning Rate:  3.852339954063764e-09  Varinance:  1.0795128059608473e-09 \n",
      "\n",
      "Epoch:  17074  Learning Rate:  3.848489539637777e-09  Varinance:  1.0780047661397093e-09 \n",
      "\n",
      "Epoch:  17075  Learning Rate:  3.844642973701649e-09  Varinance:  1.0764988329949293e-09 \n",
      "\n",
      "Epoch:  17076  Learning Rate:  3.84080025240883e-09  Varinance:  1.0749950035835497e-09 \n",
      "\n",
      "Epoch:  17077  Learning Rate:  3.836961371916569e-09  Varinance:  1.0734932749667362e-09 \n",
      "\n",
      "Epoch:  17078  Learning Rate:  3.833126328386e-09  Varinance:  1.0719936442097552e-09 \n",
      "\n",
      "Epoch:  17079  Learning Rate:  3.829295117982092e-09  Varinance:  1.070496108381974e-09 \n",
      "\n",
      "Epoch:  17080  Learning Rate:  3.825467736873608e-09  Varinance:  1.069000664556853e-09 \n",
      "\n",
      "Epoch:  17081  Learning Rate:  3.821644181233194e-09  Varinance:  1.0675073098119413e-09 \n",
      "\n",
      "Epoch:  17082  Learning Rate:  3.817824447237265e-09  Varinance:  1.0660160412288741e-09 \n",
      "\n",
      "Epoch:  17083  Learning Rate:  3.814008531066102e-09  Varinance:  1.0645268558933524e-09 \n",
      "\n",
      "Epoch:  17084  Learning Rate:  3.8101964289038e-09  Varinance:  1.0630397508951591e-09 \n",
      "\n",
      "Epoch:  17085  Learning Rate:  3.8063881369382324e-09  Varinance:  1.0615547233281397e-09 \n",
      "\n",
      "Epoch:  17086  Learning Rate:  3.802583651361119e-09  Varinance:  1.0600717702901988e-09 \n",
      "\n",
      "Epoch:  17087  Learning Rate:  3.7987829683679865e-09  Varinance:  1.0585908888832952e-09 \n",
      "\n",
      "Epoch:  17088  Learning Rate:  3.794986084158126e-09  Varinance:  1.0571120762134362e-09 \n",
      "\n",
      "Epoch:  17089  Learning Rate:  3.791192994934665e-09  Varinance:  1.0556353293906762e-09 \n",
      "\n",
      "Epoch:  17090  Learning Rate:  3.7874036969045296e-09  Varinance:  1.0541606455290946e-09 \n",
      "\n",
      "Epoch:  17091  Learning Rate:  3.783618186278393e-09  Varinance:  1.052688021746814e-09 \n",
      "\n",
      "Epoch:  17092  Learning Rate:  3.779836459270758e-09  Varinance:  1.0512174551659793e-09 \n",
      "\n",
      "Epoch:  17093  Learning Rate:  3.776058512099909e-09  Varinance:  1.0497489429127557e-09 \n",
      "\n",
      "Epoch:  17094  Learning Rate:  3.772284340987876e-09  Varinance:  1.0482824821173221e-09 \n",
      "\n",
      "Epoch:  17095  Learning Rate:  3.768513942160496e-09  Varinance:  1.0468180699138682e-09 \n",
      "\n",
      "Epoch:  17096  Learning Rate:  3.764747311847388e-09  Varinance:  1.0453557034405888e-09 \n",
      "\n",
      "Epoch:  17097  Learning Rate:  3.760984446281891e-09  Varinance:  1.043895379839667e-09 \n",
      "\n",
      "Epoch:  17098  Learning Rate:  3.7572253417011525e-09  Varinance:  1.0424370962572884e-09 \n",
      "\n",
      "Epoch:  17099  Learning Rate:  3.753469994346083e-09  Varinance:  1.040980849843622e-09 \n",
      "\n",
      "Epoch:  17100  Learning Rate:  3.749718400461308e-09  Varinance:  1.039526637752818e-09 \n",
      "\n",
      "Epoch:  17101  Learning Rate:  3.7459705562952454e-09  Varinance:  1.0380744571430016e-09 \n",
      "\n",
      "Epoch:  17102  Learning Rate:  3.742226458100064e-09  Varinance:  1.0366243051762684e-09 \n",
      "\n",
      "Epoch:  17103  Learning Rate:  3.738486102131641e-09  Varinance:  1.0351761790186822e-09 \n",
      "\n",
      "Epoch:  17104  Learning Rate:  3.73474948464963e-09  Varinance:  1.0337300758402543e-09 \n",
      "\n",
      "Epoch:  17105  Learning Rate:  3.731016601917429e-09  Varinance:  1.0322859928149609e-09 \n",
      "\n",
      "Epoch:  17106  Learning Rate:  3.727287450202127e-09  Varinance:  1.0308439271207219e-09 \n",
      "\n",
      "Epoch:  17107  Learning Rate:  3.723562025774586e-09  Varinance:  1.0294038759393998e-09 \n",
      "\n",
      "Epoch:  17108  Learning Rate:  3.7198403249093947e-09  Varinance:  1.027965836456794e-09 \n",
      "\n",
      "Epoch:  17109  Learning Rate:  3.7161223438848247e-09  Varinance:  1.026529805862635e-09 \n",
      "\n",
      "Epoch:  17110  Learning Rate:  3.7124080789829084e-09  Varinance:  1.025095781350583e-09 \n",
      "\n",
      "Epoch:  17111  Learning Rate:  3.708697526489394e-09  Varinance:  1.0236637601182076e-09 \n",
      "\n",
      "Epoch:  17112  Learning Rate:  3.7049906826937014e-09  Varinance:  1.022233739367004e-09 \n",
      "\n",
      "Epoch:  17113  Learning Rate:  3.7012875438890136e-09  Varinance:  1.0208057163023735e-09 \n",
      "\n",
      "Epoch:  17114  Learning Rate:  3.697588106372165e-09  Varinance:  1.0193796881336211e-09 \n",
      "\n",
      "Epoch:  17115  Learning Rate:  3.6938923664437305e-09  Varinance:  1.0179556520739504e-09 \n",
      "\n",
      "Epoch:  17116  Learning Rate:  3.690200320407984e-09  Varinance:  1.0165336053404579e-09 \n",
      "\n",
      "Epoch:  17117  Learning Rate:  3.686511964572852e-09  Varinance:  1.0151135451541317e-09 \n",
      "\n",
      "Epoch:  17118  Learning Rate:  3.6828272952499916e-09  Varinance:  1.0136954687398307e-09 \n",
      "\n",
      "Epoch:  17119  Learning Rate:  3.679146308754747e-09  Varinance:  1.012279373326302e-09 \n",
      "\n",
      "Epoch:  17120  Learning Rate:  3.675469001406104e-09  Varinance:  1.01086525614616e-09 \n",
      "\n",
      "Epoch:  17121  Learning Rate:  3.6717953695267695e-09  Varinance:  1.0094531144358851e-09 \n",
      "\n",
      "Epoch:  17122  Learning Rate:  3.668125409443123e-09  Varinance:  1.0080429454358183e-09 \n",
      "\n",
      "Epoch:  17123  Learning Rate:  3.664459117485179e-09  Varinance:  1.006634746390156e-09 \n",
      "\n",
      "Epoch:  17124  Learning Rate:  3.660796489986657e-09  Varinance:  1.0052285145469475e-09 \n",
      "\n",
      "Epoch:  17125  Learning Rate:  3.657137523284944e-09  Varinance:  1.003824247158076e-09 \n",
      "\n",
      "Epoch:  17126  Learning Rate:  3.6534822137210457e-09  Varinance:  1.002421941479274e-09 \n",
      "\n",
      "Epoch:  17127  Learning Rate:  3.6498305576396654e-09  Varinance:  1.001021594770105e-09 \n",
      "\n",
      "Epoch:  17128  Learning Rate:  3.64618255138916e-09  Varinance:  9.996232042939602e-10 \n",
      "\n",
      "Epoch:  17129  Learning Rate:  3.6425381913214973e-09  Varinance:  9.982267673180532e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17130  Learning Rate:  3.6388974737923286e-09  Varinance:  9.968322811134163e-10 \n",
      "\n",
      "Epoch:  17131  Learning Rate:  3.6352603951609507e-09  Varinance:  9.954397429548969e-10 \n",
      "\n",
      "Epoch:  17132  Learning Rate:  3.631626951790257e-09  Varinance:  9.940491501211386e-10 \n",
      "\n",
      "Epoch:  17133  Learning Rate:  3.6279971400468184e-09  Varinance:  9.92660499894598e-10 \n",
      "\n",
      "Epoch:  17134  Learning Rate:  3.6243709563008348e-09  Varinance:  9.91273789561524e-10 \n",
      "\n",
      "Epoch:  17135  Learning Rate:  3.6207483969260974e-09  Varinance:  9.898890164119564e-10 \n",
      "\n",
      "Epoch:  17136  Learning Rate:  3.6171294583000578e-09  Varinance:  9.885061777397208e-10 \n",
      "\n",
      "Epoch:  17137  Learning Rate:  3.6135141368037908e-09  Varinance:  9.871252708424234e-10 \n",
      "\n",
      "Epoch:  17138  Learning Rate:  3.6099024288219496e-09  Varinance:  9.857462930214487e-10 \n",
      "\n",
      "Epoch:  17139  Learning Rate:  3.6062943307428376e-09  Varinance:  9.84369241581941e-10 \n",
      "\n",
      "Epoch:  17140  Learning Rate:  3.6026898389583697e-09  Varinance:  9.829941138328194e-10 \n",
      "\n",
      "Epoch:  17141  Learning Rate:  3.599088949864028e-09  Varinance:  9.816209070867589e-10 \n",
      "\n",
      "Epoch:  17142  Learning Rate:  3.595491659858949e-09  Varinance:  9.802496186601888e-10 \n",
      "\n",
      "Epoch:  17143  Learning Rate:  3.591897965345817e-09  Varinance:  9.78880245873287e-10 \n",
      "\n",
      "Epoch:  17144  Learning Rate:  3.5883078627309496e-09  Varinance:  9.775127860499753e-10 \n",
      "\n",
      "Epoch:  17145  Learning Rate:  3.5847213484242565e-09  Varinance:  9.761472365179165e-10 \n",
      "\n",
      "Epoch:  17146  Learning Rate:  3.581138418839198e-09  Varinance:  9.747835946084977e-10 \n",
      "\n",
      "Epoch:  17147  Learning Rate:  3.577559070392856e-09  Varinance:  9.734218576568427e-10 \n",
      "\n",
      "Epoch:  17148  Learning Rate:  3.5739832995058964e-09  Varinance:  9.720620230017958e-10 \n",
      "\n",
      "Epoch:  17149  Learning Rate:  3.5704111026025205e-09  Varinance:  9.707040879859182e-10 \n",
      "\n",
      "Epoch:  17150  Learning Rate:  3.5668424761105454e-09  Varinance:  9.693480499554836e-10 \n",
      "\n",
      "Epoch:  17151  Learning Rate:  3.5632774164613563e-09  Varinance:  9.679939062604727e-10 \n",
      "\n",
      "Epoch:  17152  Learning Rate:  3.559715920089867e-09  Varinance:  9.666416542545722e-10 \n",
      "\n",
      "Epoch:  17153  Learning Rate:  3.5561579834345957e-09  Varinance:  9.652912912951546e-10 \n",
      "\n",
      "Epoch:  17154  Learning Rate:  3.5526036029376164e-09  Varinance:  9.639428147432947e-10 \n",
      "\n",
      "Epoch:  17155  Learning Rate:  3.549052775044523e-09  Varinance:  9.625962219637502e-10 \n",
      "\n",
      "Epoch:  17156  Learning Rate:  3.5455054962045007e-09  Varinance:  9.612515103249605e-10 \n",
      "\n",
      "Epoch:  17157  Learning Rate:  3.5419617628702824e-09  Varinance:  9.599086771990407e-10 \n",
      "\n",
      "Epoch:  17158  Learning Rate:  3.53842157149811e-09  Varinance:  9.58567719961777e-10 \n",
      "\n",
      "Epoch:  17159  Learning Rate:  3.534884918547804e-09  Varinance:  9.572286359926252e-10 \n",
      "\n",
      "Epoch:  17160  Learning Rate:  3.5313518004827235e-09  Varinance:  9.55891422674692e-10 \n",
      "\n",
      "Epoch:  17161  Learning Rate:  3.5278222137697247e-09  Varinance:  9.545560773947493e-10 \n",
      "\n",
      "Epoch:  17162  Learning Rate:  3.5242961548792348e-09  Varinance:  9.532225975432164e-10 \n",
      "\n",
      "Epoch:  17163  Learning Rate:  3.5207736202852053e-09  Varinance:  9.518909805141586e-10 \n",
      "\n",
      "Epoch:  17164  Learning Rate:  3.5172546064650764e-09  Varinance:  9.505612237052806e-10 \n",
      "\n",
      "Epoch:  17165  Learning Rate:  3.513739109899848e-09  Varinance:  9.492333245179233e-10 \n",
      "\n",
      "Epoch:  17166  Learning Rate:  3.5102271270740347e-09  Varinance:  9.479072803570612e-10 \n",
      "\n",
      "Epoch:  17167  Learning Rate:  3.5067186544756283e-09  Varinance:  9.465830886312828e-10 \n",
      "\n",
      "Epoch:  17168  Learning Rate:  3.5032136885961687e-09  Varinance:  9.452607467528083e-10 \n",
      "\n",
      "Epoch:  17169  Learning Rate:  3.499712225930702e-09  Varinance:  9.439402521374684e-10 \n",
      "\n",
      "Epoch:  17170  Learning Rate:  3.49621426297774e-09  Varinance:  9.426216022047042e-10 \n",
      "\n",
      "Epoch:  17171  Learning Rate:  3.4927197962393328e-09  Varinance:  9.413047943775621e-10 \n",
      "\n",
      "Epoch:  17172  Learning Rate:  3.489228822221025e-09  Varinance:  9.399898260826877e-10 \n",
      "\n",
      "Epoch:  17173  Learning Rate:  3.485741337431818e-09  Varinance:  9.386766947503254e-10 \n",
      "\n",
      "Epoch:  17174  Learning Rate:  3.4822573383842516e-09  Varinance:  9.373653978142991e-10 \n",
      "\n",
      "Epoch:  17175  Learning Rate:  3.478776821594301e-09  Varinance:  9.360559327120279e-10 \n",
      "\n",
      "Epoch:  17176  Learning Rate:  3.475299783581462e-09  Varinance:  9.347482968845067e-10 \n",
      "\n",
      "Epoch:  17177  Learning Rate:  3.4718262208687085e-09  Varinance:  9.334424877763065e-10 \n",
      "\n",
      "Epoch:  17178  Learning Rate:  3.4683561299824536e-09  Varinance:  9.321385028355666e-10 \n",
      "\n",
      "Epoch:  17179  Learning Rate:  3.4648895074526168e-09  Varinance:  9.308363395139925e-10 \n",
      "\n",
      "Epoch:  17180  Learning Rate:  3.4614263498125888e-09  Varinance:  9.295359952668518e-10 \n",
      "\n",
      "Epoch:  17181  Learning Rate:  3.4579666535991865e-09  Varinance:  9.282374675529581e-10 \n",
      "\n",
      "Epoch:  17182  Learning Rate:  3.454510415352726e-09  Varinance:  9.269407538346839e-10 \n",
      "\n",
      "Epoch:  17183  Learning Rate:  3.451057631616981e-09  Varinance:  9.256458515779441e-10 \n",
      "\n",
      "Epoch:  17184  Learning Rate:  3.447608298939144e-09  Varinance:  9.243527582521931e-10 \n",
      "\n",
      "Epoch:  17185  Learning Rate:  3.4441624138698917e-09  Varinance:  9.230614713304209e-10 \n",
      "\n",
      "Epoch:  17186  Learning Rate:  3.440719972963353e-09  Varinance:  9.217719882891471e-10 \n",
      "\n",
      "Epoch:  17187  Learning Rate:  3.4372809727770624e-09  Varinance:  9.204843066084201e-10 \n",
      "\n",
      "Epoch:  17188  Learning Rate:  3.43384540987203e-09  Varinance:  9.191984237717988e-10 \n",
      "\n",
      "Epoch:  17189  Learning Rate:  3.4304132808127067e-09  Varinance:  9.17914337266367e-10 \n",
      "\n",
      "Epoch:  17190  Learning Rate:  3.4269845821669376e-09  Varinance:  9.166320445827161e-10 \n",
      "\n",
      "Epoch:  17191  Learning Rate:  3.4235593105060366e-09  Varinance:  9.153515432149427e-10 \n",
      "\n",
      "Epoch:  17192  Learning Rate:  3.4201374624047427e-09  Varinance:  9.140728306606442e-10 \n",
      "\n",
      "Epoch:  17193  Learning Rate:  3.416719034441185e-09  Varinance:  9.127959044209138e-10 \n",
      "\n",
      "Epoch:  17194  Learning Rate:  3.4133040231969454e-09  Varinance:  9.115207620003387e-10 \n",
      "\n",
      "Epoch:  17195  Learning Rate:  3.4098924252570266e-09  Varinance:  9.102474009069824e-10 \n",
      "\n",
      "Epoch:  17196  Learning Rate:  3.4064842372098047e-09  Varinance:  9.089758186523992e-10 \n",
      "\n",
      "Epoch:  17197  Learning Rate:  3.4030794556471038e-09  Varinance:  9.077060127516168e-10 \n",
      "\n",
      "Epoch:  17198  Learning Rate:  3.3996780771641545e-09  Varinance:  9.064379807231339e-10 \n",
      "\n",
      "Epoch:  17199  Learning Rate:  3.396280098359553e-09  Varinance:  9.05171720088916e-10 \n",
      "\n",
      "Epoch:  17200  Learning Rate:  3.3928855158353335e-09  Varinance:  9.039072283743903e-10 \n",
      "\n",
      "Epoch:  17201  Learning Rate:  3.3894943261969243e-09  Varinance:  9.026445031084439e-10 \n",
      "\n",
      "Epoch:  17202  Learning Rate:  3.3861065260531115e-09  Varinance:  9.013835418234064e-10 \n",
      "\n",
      "Epoch:  17203  Learning Rate:  3.3827221120161077e-09  Varinance:  9.001243420550647e-10 \n",
      "\n",
      "Epoch:  17204  Learning Rate:  3.379341080701509e-09  Varinance:  8.988669013426441e-10 \n",
      "\n",
      "Epoch:  17205  Learning Rate:  3.3759634287282617e-09  Varinance:  8.97611217228808e-10 \n",
      "\n",
      "Epoch:  17206  Learning Rate:  3.3725891527187354e-09  Varinance:  8.963572872596529e-10 \n",
      "\n",
      "Epoch:  17207  Learning Rate:  3.3692182492986314e-09  Varinance:  8.951051089847059e-10 \n",
      "\n",
      "Epoch:  17208  Learning Rate:  3.365850715097057e-09  Varinance:  8.938546799569081e-10 \n",
      "\n",
      "Epoch:  17209  Learning Rate:  3.3624865467464904e-09  Varinance:  8.926059977326285e-10 \n",
      "\n",
      "Epoch:  17210  Learning Rate:  3.359125740882739e-09  Varinance:  8.913590598716468e-10 \n",
      "\n",
      "Epoch:  17211  Learning Rate:  3.355768294145008e-09  Varinance:  8.901138639371511e-10 \n",
      "\n",
      "Epoch:  17212  Learning Rate:  3.352414203175863e-09  Varinance:  8.888704074957341e-10 \n",
      "\n",
      "Epoch:  17213  Learning Rate:  3.3490634646211885e-09  Varinance:  8.876286881173876e-10 \n",
      "\n",
      "Epoch:  17214  Learning Rate:  3.345716075130258e-09  Varinance:  8.863887033755016e-10 \n",
      "\n",
      "Epoch:  17215  Learning Rate:  3.3423720313556932e-09  Varinance:  8.851504508468461e-10 \n",
      "\n",
      "Epoch:  17216  Learning Rate:  3.3390313299534266e-09  Varinance:  8.839139281115859e-10 \n",
      "\n",
      "Epoch:  17217  Learning Rate:  3.3356939675827675e-09  Varinance:  8.826791327532629e-10 \n",
      "\n",
      "Epoch:  17218  Learning Rate:  3.332359940906366e-09  Varinance:  8.81446062358795e-10 \n",
      "\n",
      "Epoch:  17219  Learning Rate:  3.3290292465901715e-09  Varinance:  8.802147145184707e-10 \n",
      "\n",
      "Epoch:  17220  Learning Rate:  3.3257018813035017e-09  Varinance:  8.789850868259452e-10 \n",
      "\n",
      "Epoch:  17221  Learning Rate:  3.3223778417190014e-09  Varinance:  8.777571768782382e-10 \n",
      "\n",
      "Epoch:  17222  Learning Rate:  3.3190571245126076e-09  Varinance:  8.765309822757168e-10 \n",
      "\n",
      "Epoch:  17223  Learning Rate:  3.3157397263636154e-09  Varinance:  8.753065006221097e-10 \n",
      "\n",
      "Epoch:  17224  Learning Rate:  3.3124256439546375e-09  Varinance:  8.740837295244903e-10 \n",
      "\n",
      "Epoch:  17225  Learning Rate:  3.309114873971568e-09  Varinance:  8.728626665932744e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17226  Learning Rate:  3.3058074131036483e-09  Varinance:  8.716433094422161e-10 \n",
      "\n",
      "Epoch:  17227  Learning Rate:  3.302503258043428e-09  Varinance:  8.704256556884032e-10 \n",
      "\n",
      "Epoch:  17228  Learning Rate:  3.2992024054867307e-09  Varinance:  8.692097029522554e-10 \n",
      "\n",
      "Epoch:  17229  Learning Rate:  3.295904852132713e-09  Varinance:  8.679954488575069e-10 \n",
      "\n",
      "Epoch:  17230  Learning Rate:  3.2926105946838335e-09  Varinance:  8.66782891031221e-10 \n",
      "\n",
      "Epoch:  17231  Learning Rate:  3.289319629845812e-09  Varinance:  8.655720271037732e-10 \n",
      "\n",
      "Epoch:  17232  Learning Rate:  3.286031954327694e-09  Varinance:  8.643628547088486e-10 \n",
      "\n",
      "Epoch:  17233  Learning Rate:  3.2827475648418156e-09  Varinance:  8.631553714834381e-10 \n",
      "\n",
      "Epoch:  17234  Learning Rate:  3.2794664581037645e-09  Varinance:  8.619495750678343e-10 \n",
      "\n",
      "Epoch:  17235  Learning Rate:  3.276188630832445e-09  Varinance:  8.607454631056283e-10 \n",
      "\n",
      "Epoch:  17236  Learning Rate:  3.272914079750041e-09  Varinance:  8.595430332436947e-10 \n",
      "\n",
      "Epoch:  17237  Learning Rate:  3.269642801581977e-09  Varinance:  8.583422831322042e-10 \n",
      "\n",
      "Epoch:  17238  Learning Rate:  3.2663747930569996e-09  Varinance:  8.571432104246064e-10 \n",
      "\n",
      "Epoch:  17239  Learning Rate:  3.263110050907075e-09  Varinance:  8.559458127776302e-10 \n",
      "\n",
      "Epoch:  17240  Learning Rate:  3.2598485718674737e-09  Varinance:  8.547500878512769e-10 \n",
      "\n",
      "Epoch:  17241  Learning Rate:  3.256590352676727e-09  Varinance:  8.535560333088175e-10 \n",
      "\n",
      "Epoch:  17242  Learning Rate:  3.2533353900765934e-09  Varinance:  8.523636468167895e-10 \n",
      "\n",
      "Epoch:  17243  Learning Rate:  3.250083680812121e-09  Varinance:  8.511729260449818e-10 \n",
      "\n",
      "Epoch:  17244  Learning Rate:  3.246835221631612e-09  Varinance:  8.499838686664472e-10 \n",
      "\n",
      "Epoch:  17245  Learning Rate:  3.243590009286583e-09  Varinance:  8.487964723574864e-10 \n",
      "\n",
      "Epoch:  17246  Learning Rate:  3.240348040531834e-09  Varinance:  8.476107347976461e-10 \n",
      "\n",
      "Epoch:  17247  Learning Rate:  3.237109312125407e-09  Varinance:  8.464266536697144e-10 \n",
      "\n",
      "Epoch:  17248  Learning Rate:  3.23387382082855e-09  Varinance:  8.452442266597168e-10 \n",
      "\n",
      "Epoch:  17249  Learning Rate:  3.2306415634057837e-09  Varinance:  8.44063451456914e-10 \n",
      "\n",
      "Epoch:  17250  Learning Rate:  3.2274125366248614e-09  Varinance:  8.428843257537862e-10 \n",
      "\n",
      "Epoch:  17251  Learning Rate:  3.224186737256734e-09  Varinance:  8.417068472460456e-10 \n",
      "\n",
      "Epoch:  17252  Learning Rate:  3.2209641620756116e-09  Varinance:  8.405310136326206e-10 \n",
      "\n",
      "Epoch:  17253  Learning Rate:  3.217744807858931e-09  Varinance:  8.393568226156543e-10 \n",
      "\n",
      "Epoch:  17254  Learning Rate:  3.2145286713873156e-09  Varinance:  8.381842719004995e-10 \n",
      "\n",
      "Epoch:  17255  Learning Rate:  3.211315749444639e-09  Varinance:  8.370133591957148e-10 \n",
      "\n",
      "Epoch:  17256  Learning Rate:  3.208106038817991e-09  Varinance:  8.358440822130624e-10 \n",
      "\n",
      "Epoch:  17257  Learning Rate:  3.2048995362976376e-09  Varinance:  8.34676438667493e-10 \n",
      "\n",
      "Epoch:  17258  Learning Rate:  3.201696238677088e-09  Varinance:  8.335104262771576e-10 \n",
      "\n",
      "Epoch:  17259  Learning Rate:  3.1984961427530554e-09  Varinance:  8.323460427633921e-10 \n",
      "\n",
      "Epoch:  17260  Learning Rate:  3.19529924532542e-09  Varinance:  8.311832858507156e-10 \n",
      "\n",
      "Epoch:  17261  Learning Rate:  3.1921055431972967e-09  Varinance:  8.300221532668261e-10 \n",
      "\n",
      "Epoch:  17262  Learning Rate:  3.1889150331749936e-09  Varinance:  8.288626427425958e-10 \n",
      "\n",
      "Epoch:  17263  Learning Rate:  3.1857277120679783e-09  Varinance:  8.277047520120698e-10 \n",
      "\n",
      "Epoch:  17264  Learning Rate:  3.182543576688941e-09  Varinance:  8.265484788124496e-10 \n",
      "\n",
      "Epoch:  17265  Learning Rate:  3.179362623853756e-09  Varinance:  8.25393820884107e-10 \n",
      "\n",
      "Epoch:  17266  Learning Rate:  3.1761848503814494e-09  Varinance:  8.24240775970567e-10 \n",
      "\n",
      "Epoch:  17267  Learning Rate:  3.1730102530942685e-09  Varinance:  8.230893418185072e-10 \n",
      "\n",
      "Epoch:  17268  Learning Rate:  3.169838828817594e-09  Varinance:  8.219395161777529e-10 \n",
      "\n",
      "Epoch:  17269  Learning Rate:  3.1666705743800124e-09  Varinance:  8.207912968012725e-10 \n",
      "\n",
      "Epoch:  17270  Learning Rate:  3.16350548661328e-09  Varinance:  8.19644681445177e-10 \n",
      "\n",
      "Epoch:  17271  Learning Rate:  3.1603435623522874e-09  Varinance:  8.184996678687026e-10 \n",
      "\n",
      "Epoch:  17272  Learning Rate:  3.1571847984351206e-09  Varinance:  8.173562538342247e-10 \n",
      "\n",
      "Epoch:  17273  Learning Rate:  3.1540291917030255e-09  Varinance:  8.162144371072421e-10 \n",
      "\n",
      "Epoch:  17274  Learning Rate:  3.1508767390003747e-09  Varinance:  8.150742154563746e-10 \n",
      "\n",
      "Epoch:  17275  Learning Rate:  3.147727437174725e-09  Varinance:  8.139355866533591e-10 \n",
      "\n",
      "Epoch:  17276  Learning Rate:  3.144581283076786e-09  Varinance:  8.12798548473046e-10 \n",
      "\n",
      "Epoch:  17277  Learning Rate:  3.1414382735603816e-09  Varinance:  8.116630986933962e-10 \n",
      "\n",
      "Epoch:  17278  Learning Rate:  3.1382984054825114e-09  Varinance:  8.105292350954667e-10 \n",
      "\n",
      "Epoch:  17279  Learning Rate:  3.13516167570332e-09  Varinance:  8.093969554634225e-10 \n",
      "\n",
      "Epoch:  17280  Learning Rate:  3.1320280810860548e-09  Varinance:  8.082662575845213e-10 \n",
      "\n",
      "Epoch:  17281  Learning Rate:  3.1288976184971307e-09  Varinance:  8.071371392491121e-10 \n",
      "\n",
      "Epoch:  17282  Learning Rate:  3.1257702848060973e-09  Varinance:  8.060095982506301e-10 \n",
      "\n",
      "Epoch:  17283  Learning Rate:  3.122646076885599e-09  Varinance:  8.04883632385594e-10 \n",
      "\n",
      "Epoch:  17284  Learning Rate:  3.119524991611437e-09  Varinance:  8.037592394536028e-10 \n",
      "\n",
      "Epoch:  17285  Learning Rate:  3.1164070258625375e-09  Varinance:  8.026364172573207e-10 \n",
      "\n",
      "Epoch:  17286  Learning Rate:  3.1132921765209126e-09  Varinance:  8.015151636024906e-10 \n",
      "\n",
      "Epoch:  17287  Learning Rate:  3.110180440471724e-09  Varinance:  8.003954762979176e-10 \n",
      "\n",
      "Epoch:  17288  Learning Rate:  3.107071814603246e-09  Varinance:  7.992773531554676e-10 \n",
      "\n",
      "Epoch:  17289  Learning Rate:  3.1039662958068304e-09  Varinance:  7.981607919900637e-10 \n",
      "\n",
      "Epoch:  17290  Learning Rate:  3.1008638809769693e-09  Varinance:  7.970457906196811e-10 \n",
      "\n",
      "Epoch:  17291  Learning Rate:  3.0977645670112585e-09  Varinance:  7.95932346865346e-10 \n",
      "\n",
      "Epoch:  17292  Learning Rate:  3.0946683508103623e-09  Varinance:  7.948204585511204e-10 \n",
      "\n",
      "Epoch:  17293  Learning Rate:  3.0915752292780742e-09  Varinance:  7.937101235041146e-10 \n",
      "\n",
      "Epoch:  17294  Learning Rate:  3.088485199321284e-09  Varinance:  7.926013395544705e-10 \n",
      "\n",
      "Epoch:  17295  Learning Rate:  3.08539825784994e-09  Varinance:  7.914941045353625e-10 \n",
      "\n",
      "Epoch:  17296  Learning Rate:  3.0823144017771107e-09  Varinance:  7.903884162829913e-10 \n",
      "\n",
      "Epoch:  17297  Learning Rate:  3.079233628018951e-09  Varinance:  7.892842726365805e-10 \n",
      "\n",
      "Epoch:  17298  Learning Rate:  3.076155933494665e-09  Varinance:  7.881816714383748e-10 \n",
      "\n",
      "Epoch:  17299  Learning Rate:  3.0730813151265794e-09  Varinance:  7.870806105336251e-10 \n",
      "\n",
      "Epoch:  17300  Learning Rate:  3.070009769840054e-09  Varinance:  7.859810877706009e-10 \n",
      "\n",
      "Epoch:  17301  Learning Rate:  3.0669412945635554e-09  Varinance:  7.848831010005743e-10 \n",
      "\n",
      "Epoch:  17302  Learning Rate:  3.063875886228617e-09  Varinance:  7.837866480778193e-10 \n",
      "\n",
      "Epoch:  17303  Learning Rate:  3.060813541769809e-09  Varinance:  7.826917268596079e-10 \n",
      "\n",
      "Epoch:  17304  Learning Rate:  3.057754258124798e-09  Varinance:  7.815983352062046e-10 \n",
      "\n",
      "Epoch:  17305  Learning Rate:  3.0546980322343115e-09  Varinance:  7.805064709808665e-10 \n",
      "\n",
      "Epoch:  17306  Learning Rate:  3.051644861042101e-09  Varinance:  7.794161320498269e-10 \n",
      "\n",
      "Epoch:  17307  Learning Rate:  3.0485947414950054e-09  Varinance:  7.783273162823084e-10 \n",
      "\n",
      "Epoch:  17308  Learning Rate:  3.045547670542916e-09  Varinance:  7.772400215505071e-10 \n",
      "\n",
      "Epoch:  17309  Learning Rate:  3.04250364513874e-09  Varinance:  7.761542457295923e-10 \n",
      "\n",
      "Epoch:  17310  Learning Rate:  3.0394626622384624e-09  Varinance:  7.750699866977009e-10 \n",
      "\n",
      "Epoch:  17311  Learning Rate:  3.0364247188011122e-09  Varinance:  7.739872423359344e-10 \n",
      "\n",
      "Epoch:  17312  Learning Rate:  3.033389811788722e-09  Varinance:  7.729060105283566e-10 \n",
      "\n",
      "Epoch:  17313  Learning Rate:  3.0303579381663976e-09  Varinance:  7.718262891619796e-10 \n",
      "\n",
      "Epoch:  17314  Learning Rate:  3.027329094902274e-09  Varinance:  7.707480761267749e-10 \n",
      "\n",
      "Epoch:  17315  Learning Rate:  3.024303278967486e-09  Varinance:  7.696713693156594e-10 \n",
      "\n",
      "Epoch:  17316  Learning Rate:  3.0212804873362303e-09  Varinance:  7.685961666244929e-10 \n",
      "\n",
      "Epoch:  17317  Learning Rate:  3.0182607169857234e-09  Varinance:  7.675224659520752e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17318  Learning Rate:  3.0152439648961746e-09  Varinance:  7.664502652001411e-10 \n",
      "\n",
      "Epoch:  17319  Learning Rate:  3.012230228050843e-09  Varinance:  7.653795622733595e-10 \n",
      "\n",
      "Epoch:  17320  Learning Rate:  3.0092195034360002e-09  Varinance:  7.64310355079318e-10 \n",
      "\n",
      "Epoch:  17321  Learning Rate:  3.006211788040901e-09  Varinance:  7.632426415285355e-10 \n",
      "\n",
      "Epoch:  17322  Learning Rate:  3.00320707885784e-09  Varinance:  7.62176419534447e-10 \n",
      "\n",
      "Epoch:  17323  Learning Rate:  3.0002053728821195e-09  Varinance:  7.611116870134028e-10 \n",
      "\n",
      "Epoch:  17324  Learning Rate:  2.9972066671120106e-09  Varinance:  7.600484418846635e-10 \n",
      "\n",
      "Epoch:  17325  Learning Rate:  2.994210958548819e-09  Varinance:  7.589866820703964e-10 \n",
      "\n",
      "Epoch:  17326  Learning Rate:  2.991218244196846e-09  Varinance:  7.579264054956745e-10 \n",
      "\n",
      "Epoch:  17327  Learning Rate:  2.988228521063356e-09  Varinance:  7.568676100884611e-10 \n",
      "\n",
      "Epoch:  17328  Learning Rate:  2.9852417861586355e-09  Varinance:  7.558102937796223e-10 \n",
      "\n",
      "Epoch:  17329  Learning Rate:  2.982258036495961e-09  Varinance:  7.547544545029116e-10 \n",
      "\n",
      "Epoch:  17330  Learning Rate:  2.9792772690915614e-09  Varinance:  7.537000901949696e-10 \n",
      "\n",
      "Epoch:  17331  Learning Rate:  2.976299480964689e-09  Varinance:  7.526471987953185e-10 \n",
      "\n",
      "Epoch:  17332  Learning Rate:  2.973324669137535e-09  Varinance:  7.515957782463599e-10 \n",
      "\n",
      "Epoch:  17333  Learning Rate:  2.9703528306352985e-09  Varinance:  7.505458264933717e-10 \n",
      "\n",
      "Epoch:  17334  Learning Rate:  2.96738396248615e-09  Varinance:  7.494973414844947e-10 \n",
      "\n",
      "Epoch:  17335  Learning Rate:  2.9644180617212016e-09  Varinance:  7.484503211707435e-10 \n",
      "\n",
      "Epoch:  17336  Learning Rate:  2.9614551253745614e-09  Varinance:  7.474047635059928e-10 \n",
      "\n",
      "Epoch:  17337  Learning Rate:  2.9584951504833036e-09  Varinance:  7.463606664469754e-10 \n",
      "\n",
      "Epoch:  17338  Learning Rate:  2.955538134087433e-09  Varinance:  7.453180279532791e-10 \n",
      "\n",
      "Epoch:  17339  Learning Rate:  2.952584073229942e-09  Varinance:  7.442768459873437e-10 \n",
      "\n",
      "Epoch:  17340  Learning Rate:  2.9496329649567816e-09  Varinance:  7.432371185144483e-10 \n",
      "\n",
      "Epoch:  17341  Learning Rate:  2.946684806316821e-09  Varinance:  7.421988435027221e-10 \n",
      "\n",
      "Epoch:  17342  Learning Rate:  2.943739594361912e-09  Varinance:  7.411620189231301e-10 \n",
      "\n",
      "Epoch:  17343  Learning Rate:  2.9407973261468537e-09  Varinance:  7.401266427494717e-10 \n",
      "\n",
      "Epoch:  17344  Learning Rate:  2.937857998729356e-09  Varinance:  7.390927129583769e-10 \n",
      "\n",
      "Epoch:  17345  Learning Rate:  2.934921609170102e-09  Varinance:  7.380602275293021e-10 \n",
      "\n",
      "Epoch:  17346  Learning Rate:  2.9319881545327117e-09  Varinance:  7.370291844445295e-10 \n",
      "\n",
      "Epoch:  17347  Learning Rate:  2.92905763188371e-09  Varinance:  7.359995816891516e-10 \n",
      "\n",
      "Epoch:  17348  Learning Rate:  2.9261300382925845e-09  Varinance:  7.349714172510834e-10 \n",
      "\n",
      "Epoch:  17349  Learning Rate:  2.9232053708317513e-09  Varinance:  7.339446891210487e-10 \n",
      "\n",
      "Epoch:  17350  Learning Rate:  2.9202836265765224e-09  Varinance:  7.329193952925776e-10 \n",
      "\n",
      "Epoch:  17351  Learning Rate:  2.9173648026051634e-09  Varinance:  7.318955337620033e-10 \n",
      "\n",
      "Epoch:  17352  Learning Rate:  2.9144488959988604e-09  Varinance:  7.308731025284584e-10 \n",
      "\n",
      "Epoch:  17353  Learning Rate:  2.9115359038416857e-09  Varinance:  7.29852099593873e-10 \n",
      "\n",
      "Epoch:  17354  Learning Rate:  2.908625823220658e-09  Varinance:  7.288325229629602e-10 \n",
      "\n",
      "Epoch:  17355  Learning Rate:  2.9057186512257056e-09  Varinance:  7.278143706432291e-10 \n",
      "\n",
      "Epoch:  17356  Learning Rate:  2.902814384949637e-09  Varinance:  7.267976406449689e-10 \n",
      "\n",
      "Epoch:  17357  Learning Rate:  2.8999130214881947e-09  Varinance:  7.257823309812488e-10 \n",
      "\n",
      "Epoch:  17358  Learning Rate:  2.8970145579400255e-09  Varinance:  7.247684396679134e-10 \n",
      "\n",
      "Epoch:  17359  Learning Rate:  2.8941189914066465e-09  Varinance:  7.237559647235793e-10 \n",
      "\n",
      "Epoch:  17360  Learning Rate:  2.891226318992499e-09  Varinance:  7.227449041696335e-10 \n",
      "\n",
      "Epoch:  17361  Learning Rate:  2.8883365378049223e-09  Varinance:  7.217352560302191e-10 \n",
      "\n",
      "Epoch:  17362  Learning Rate:  2.8854496449541136e-09  Varinance:  7.207270183322476e-10 \n",
      "\n",
      "Epoch:  17363  Learning Rate:  2.8825656375532005e-09  Varinance:  7.197201891053839e-10 \n",
      "\n",
      "Epoch:  17364  Learning Rate:  2.8796845127181554e-09  Varinance:  7.187147663820455e-10 \n",
      "\n",
      "Epoch:  17365  Learning Rate:  2.876806267567862e-09  Varinance:  7.177107481973986e-10 \n",
      "\n",
      "Epoch:  17366  Learning Rate:  2.8739308992240873e-09  Varinance:  7.16708132589354e-10 \n",
      "\n",
      "Epoch:  17367  Learning Rate:  2.87105840481144e-09  Varinance:  7.157069175985663e-10 \n",
      "\n",
      "Epoch:  17368  Learning Rate:  2.8681887814574378e-09  Varinance:  7.147071012684192e-10 \n",
      "\n",
      "Epoch:  17369  Learning Rate:  2.8653220262924656e-09  Varinance:  7.137086816450379e-10 \n",
      "\n",
      "Epoch:  17370  Learning Rate:  2.8624581364497483e-09  Varinance:  7.12711656777274e-10 \n",
      "\n",
      "Epoch:  17371  Learning Rate:  2.8595971090654063e-09  Varinance:  7.117160247167051e-10 \n",
      "\n",
      "Epoch:  17372  Learning Rate:  2.8567389412784217e-09  Varinance:  7.107217835176306e-10 \n",
      "\n",
      "Epoch:  17373  Learning Rate:  2.8538836302306063e-09  Varinance:  7.09728931237068e-10 \n",
      "\n",
      "Epoch:  17374  Learning Rate:  2.851031173066659e-09  Varinance:  7.087374659347517e-10 \n",
      "\n",
      "Epoch:  17375  Learning Rate:  2.8481815669341326e-09  Varinance:  7.077473856731186e-10 \n",
      "\n",
      "Epoch:  17376  Learning Rate:  2.8453348089834003e-09  Varinance:  7.067586885173203e-10 \n",
      "\n",
      "Epoch:  17377  Learning Rate:  2.8424908963677135e-09  Varinance:  7.057713725352087e-10 \n",
      "\n",
      "Epoch:  17378  Learning Rate:  2.8396498262431707e-09  Varinance:  7.047854357973348e-10 \n",
      "\n",
      "Epoch:  17379  Learning Rate:  2.8368115957686804e-09  Varinance:  7.038008763769448e-10 \n",
      "\n",
      "Epoch:  17380  Learning Rate:  2.8339762021060227e-09  Varinance:  7.028176923499767e-10 \n",
      "\n",
      "Epoch:  17381  Learning Rate:  2.831143642419813e-09  Varinance:  7.018358817950588e-10 \n",
      "\n",
      "Epoch:  17382  Learning Rate:  2.8283139138774715e-09  Varinance:  7.008554427934962e-10 \n",
      "\n",
      "Epoch:  17383  Learning Rate:  2.8254870136492794e-09  Varinance:  6.998763734292814e-10 \n",
      "\n",
      "Epoch:  17384  Learning Rate:  2.822662938908347e-09  Varinance:  6.988986717890813e-10 \n",
      "\n",
      "Epoch:  17385  Learning Rate:  2.8198416868305784e-09  Varinance:  6.979223359622357e-10 \n",
      "\n",
      "Epoch:  17386  Learning Rate:  2.8170232545947315e-09  Varinance:  6.969473640407532e-10 \n",
      "\n",
      "Epoch:  17387  Learning Rate:  2.8142076393823845e-09  Varinance:  6.95973754119308e-10 \n",
      "\n",
      "Epoch:  17388  Learning Rate:  2.8113948383779007e-09  Varinance:  6.950015042952387e-10 \n",
      "\n",
      "Epoch:  17389  Learning Rate:  2.80858484876849e-09  Varinance:  6.940306126685339e-10 \n",
      "\n",
      "Epoch:  17390  Learning Rate:  2.805777667744172e-09  Varinance:  6.930610773418443e-10 \n",
      "\n",
      "Epoch:  17391  Learning Rate:  2.8029732924977453e-09  Varinance:  6.920928964204685e-10 \n",
      "\n",
      "Epoch:  17392  Learning Rate:  2.800171720224855e-09  Varinance:  6.911260680123517e-10 \n",
      "\n",
      "Epoch:  17393  Learning Rate:  2.7973729481239084e-09  Varinance:  6.901605902280827e-10 \n",
      "\n",
      "Epoch:  17394  Learning Rate:  2.7945769733961426e-09  Varinance:  6.891964611808895e-10 \n",
      "\n",
      "Epoch:  17395  Learning Rate:  2.7917837932455935e-09  Varinance:  6.882336789866381e-10 \n",
      "\n",
      "Epoch:  17396  Learning Rate:  2.78899340487906e-09  Varinance:  6.872722417638196e-10 \n",
      "\n",
      "Epoch:  17397  Learning Rate:  2.786205805506164e-09  Varinance:  6.863121476335604e-10 \n",
      "\n",
      "Epoch:  17398  Learning Rate:  2.7834209923393155e-09  Varinance:  6.853533947196096e-10 \n",
      "\n",
      "Epoch:  17399  Learning Rate:  2.780638962593681e-09  Varinance:  6.843959811483373e-10 \n",
      "\n",
      "Epoch:  17400  Learning Rate:  2.7778597134872416e-09  Varinance:  6.834399050487307e-10 \n",
      "\n",
      "Epoch:  17401  Learning Rate:  2.7750832422407564e-09  Varinance:  6.82485164552391e-10 \n",
      "\n",
      "Epoch:  17402  Learning Rate:  2.772309546077735e-09  Varinance:  6.815317577935317e-10 \n",
      "\n",
      "Epoch:  17403  Learning Rate:  2.7695386222244913e-09  Varinance:  6.805796829089657e-10 \n",
      "\n",
      "Epoch:  17404  Learning Rate:  2.7667704679101098e-09  Varinance:  6.796289380381157e-10 \n",
      "\n",
      "Epoch:  17405  Learning Rate:  2.7640050803664173e-09  Varinance:  6.786795213230015e-10 \n",
      "\n",
      "Epoch:  17406  Learning Rate:  2.761242456828035e-09  Varinance:  6.77731430908238e-10 \n",
      "\n",
      "Epoch:  17407  Learning Rate:  2.75848259453235e-09  Varinance:  6.767846649410324e-10 \n",
      "\n",
      "Epoch:  17408  Learning Rate:  2.755725490719479e-09  Varinance:  6.758392215711798e-10 \n",
      "\n",
      "Epoch:  17409  Learning Rate:  2.752971142632329e-09  Varinance:  6.748950989510629e-10 \n",
      "\n",
      "Epoch:  17410  Learning Rate:  2.7502195475165605e-09  Varinance:  6.739522952356376e-10 \n",
      "\n",
      "Epoch:  17411  Learning Rate:  2.7474707026205587e-09  Varinance:  6.730108085824451e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17412  Learning Rate:  2.744724605195489e-09  Varinance:  6.720706371515975e-10 \n",
      "\n",
      "Epoch:  17413  Learning Rate:  2.7419812524952628e-09  Varinance:  6.711317791057774e-10 \n",
      "\n",
      "Epoch:  17414  Learning Rate:  2.7392406417765078e-09  Varinance:  6.701942326102341e-10 \n",
      "\n",
      "Epoch:  17415  Learning Rate:  2.736502770298623e-09  Varinance:  6.692579958327801e-10 \n",
      "\n",
      "Epoch:  17416  Learning Rate:  2.7337676353237457e-09  Varinance:  6.683230669437895e-10 \n",
      "\n",
      "Epoch:  17417  Learning Rate:  2.7310352341167226e-09  Varinance:  6.673894441161852e-10 \n",
      "\n",
      "Epoch:  17418  Learning Rate:  2.7283055639451607e-09  Varinance:  6.664571255254496e-10 \n",
      "\n",
      "Epoch:  17419  Learning Rate:  2.7255786220793998e-09  Varinance:  6.65526109349612e-10 \n",
      "\n",
      "Epoch:  17420  Learning Rate:  2.7228544057924784e-09  Varinance:  6.645963937692463e-10 \n",
      "\n",
      "Epoch:  17421  Learning Rate:  2.72013291236019e-09  Varinance:  6.636679769674681e-10 \n",
      "\n",
      "Epoch:  17422  Learning Rate:  2.71741413906105e-09  Varinance:  6.627408571299318e-10 \n",
      "\n",
      "Epoch:  17423  Learning Rate:  2.7146980831762663e-09  Varinance:  6.618150324448277e-10 \n",
      "\n",
      "Epoch:  17424  Learning Rate:  2.7119847419898016e-09  Varinance:  6.60890501102871e-10 \n",
      "\n",
      "Epoch:  17425  Learning Rate:  2.7092741127882947e-09  Varinance:  6.599672612973109e-10 \n",
      "\n",
      "Epoch:  17426  Learning Rate:  2.7065661928611266e-09  Varinance:  6.590453112239185e-10 \n",
      "\n",
      "Epoch:  17427  Learning Rate:  2.7038609795003867e-09  Varinance:  6.581246490809852e-10 \n",
      "\n",
      "Epoch:  17428  Learning Rate:  2.701158470000842e-09  Varinance:  6.572052730693193e-10 \n",
      "\n",
      "Epoch:  17429  Learning Rate:  2.6984586616599923e-09  Varinance:  6.562871813922426e-10 \n",
      "\n",
      "Epoch:  17430  Learning Rate:  2.6957615517780387e-09  Varinance:  6.553703722555892e-10 \n",
      "\n",
      "Epoch:  17431  Learning Rate:  2.693067137657852e-09  Varinance:  6.544548438676922e-10 \n",
      "\n",
      "Epoch:  17432  Learning Rate:  2.6903754166050273e-09  Varinance:  6.535405944393953e-10 \n",
      "\n",
      "Epoch:  17433  Learning Rate:  2.687686385927853e-09  Varinance:  6.526276221840383e-10 \n",
      "\n",
      "Epoch:  17434  Learning Rate:  2.685000042937279e-09  Varinance:  6.517159253174577e-10 \n",
      "\n",
      "Epoch:  17435  Learning Rate:  2.6823163849469717e-09  Varinance:  6.508055020579823e-10 \n",
      "\n",
      "Epoch:  17436  Learning Rate:  2.6796354092732826e-09  Varinance:  6.498963506264292e-10 \n",
      "\n",
      "Epoch:  17437  Learning Rate:  2.6769571132352167e-09  Varinance:  6.489884692461041e-10 \n",
      "\n",
      "Epoch:  17438  Learning Rate:  2.6742814941544867e-09  Varinance:  6.480818561427873e-10 \n",
      "\n",
      "Epoch:  17439  Learning Rate:  2.671608549355483e-09  Varinance:  6.471765095447446e-10 \n",
      "\n",
      "Epoch:  17440  Learning Rate:  2.6689382761652423e-09  Varinance:  6.462724276827144e-10 \n",
      "\n",
      "Epoch:  17441  Learning Rate:  2.6662706719135e-09  Varinance:  6.453696087899071e-10 \n",
      "\n",
      "Epoch:  17442  Learning Rate:  2.6636057339326613e-09  Varinance:  6.444680511020006e-10 \n",
      "\n",
      "Epoch:  17443  Learning Rate:  2.660943459557769e-09  Varinance:  6.435677528571383e-10 \n",
      "\n",
      "Epoch:  17444  Learning Rate:  2.658283846126558e-09  Varinance:  6.426687122959266e-10 \n",
      "\n",
      "Epoch:  17445  Learning Rate:  2.6556268909794243e-09  Varinance:  6.417709276614228e-10 \n",
      "\n",
      "Epoch:  17446  Learning Rate:  2.6529725914593935e-09  Varinance:  6.408743971991458e-10 \n",
      "\n",
      "Epoch:  17447  Learning Rate:  2.650320944912175e-09  Varinance:  6.399791191570628e-10 \n",
      "\n",
      "Epoch:  17448  Learning Rate:  2.6476719486861316e-09  Varinance:  6.39085091785589e-10 \n",
      "\n",
      "Epoch:  17449  Learning Rate:  2.6450256001322477e-09  Varinance:  6.381923133375832e-10 \n",
      "\n",
      "Epoch:  17450  Learning Rate:  2.6423818966041853e-09  Varinance:  6.37300782068345e-10 \n",
      "\n",
      "Epoch:  17451  Learning Rate:  2.6397408354582485e-09  Varinance:  6.364104962356137e-10 \n",
      "\n",
      "Epoch:  17452  Learning Rate:  2.637102414053358e-09  Varinance:  6.355214540995557e-10 \n",
      "\n",
      "Epoch:  17453  Learning Rate:  2.6344666297511013e-09  Varinance:  6.346336539227744e-10 \n",
      "\n",
      "Epoch:  17454  Learning Rate:  2.631833479915703e-09  Varinance:  6.337470939702985e-10 \n",
      "\n",
      "Epoch:  17455  Learning Rate:  2.629202961913995e-09  Varinance:  6.3286177250958e-10 \n",
      "\n",
      "Epoch:  17456  Learning Rate:  2.626575073115477e-09  Varinance:  6.319776878104912e-10 \n",
      "\n",
      "Epoch:  17457  Learning Rate:  2.6239498108922417e-09  Varinance:  6.310948381453216e-10 \n",
      "\n",
      "Epoch:  17458  Learning Rate:  2.621327172619036e-09  Varinance:  6.302132217887762e-10 \n",
      "\n",
      "Epoch:  17459  Learning Rate:  2.618707155673231e-09  Varinance:  6.293328370179636e-10 \n",
      "\n",
      "Epoch:  17460  Learning Rate:  2.61608975743479e-09  Varinance:  6.284536821124061e-10 \n",
      "\n",
      "Epoch:  17461  Learning Rate:  2.613474975286325e-09  Varinance:  6.275757553540266e-10 \n",
      "\n",
      "Epoch:  17462  Learning Rate:  2.610862806613062e-09  Varinance:  6.266990550271489e-10 \n",
      "\n",
      "Epoch:  17463  Learning Rate:  2.6082532488028142e-09  Varinance:  6.25823579418493e-10 \n",
      "\n",
      "Epoch:  17464  Learning Rate:  2.605646299246032e-09  Varinance:  6.249493268171725e-10 \n",
      "\n",
      "Epoch:  17465  Learning Rate:  2.603041955335776e-09  Varinance:  6.240762955146931e-10 \n",
      "\n",
      "Epoch:  17466  Learning Rate:  2.600440214467683e-09  Varinance:  6.232044838049406e-10 \n",
      "\n",
      "Epoch:  17467  Learning Rate:  2.5978410740400206e-09  Varinance:  6.22333889984191e-10 \n",
      "\n",
      "Epoch:  17468  Learning Rate:  2.5952445314536586e-09  Varinance:  6.21464512351098e-10 \n",
      "\n",
      "Epoch:  17469  Learning Rate:  2.592650584112035e-09  Varinance:  6.205963492066921e-10 \n",
      "\n",
      "Epoch:  17470  Learning Rate:  2.590059229421211e-09  Varinance:  6.19729398854377e-10 \n",
      "\n",
      "Epoch:  17471  Learning Rate:  2.5874704647898422e-09  Varinance:  6.188636595999287e-10 \n",
      "\n",
      "Epoch:  17472  Learning Rate:  2.584884287629144e-09  Varinance:  6.179991297514837e-10 \n",
      "\n",
      "Epoch:  17473  Learning Rate:  2.5823006953529497e-09  Varinance:  6.171358076195482e-10 \n",
      "\n",
      "Epoch:  17474  Learning Rate:  2.579719685377674e-09  Varinance:  6.162736915169865e-10 \n",
      "\n",
      "Epoch:  17475  Learning Rate:  2.5771412551222903e-09  Varinance:  6.154127797590199e-10 \n",
      "\n",
      "Epoch:  17476  Learning Rate:  2.5745654020083762e-09  Varinance:  6.145530706632229e-10 \n",
      "\n",
      "Epoch:  17477  Learning Rate:  2.571992123460088e-09  Varinance:  6.136945625495208e-10 \n",
      "\n",
      "Epoch:  17478  Learning Rate:  2.569421416904128e-09  Varinance:  6.128372537401878e-10 \n",
      "\n",
      "Epoch:  17479  Learning Rate:  2.5668532797697996e-09  Varinance:  6.119811425598351e-10 \n",
      "\n",
      "Epoch:  17480  Learning Rate:  2.5642877094889736e-09  Varinance:  6.111262273354212e-10 \n",
      "\n",
      "Epoch:  17481  Learning Rate:  2.561724703496062e-09  Varinance:  6.102725063962395e-10 \n",
      "\n",
      "Epoch:  17482  Learning Rate:  2.559164259228067e-09  Varinance:  6.094199780739172e-10 \n",
      "\n",
      "Epoch:  17483  Learning Rate:  2.556606374124554e-09  Varinance:  6.085686407024125e-10 \n",
      "\n",
      "Epoch:  17484  Learning Rate:  2.5540510456276186e-09  Varinance:  6.077184926180106e-10 \n",
      "\n",
      "Epoch:  17485  Learning Rate:  2.551498271181942e-09  Varinance:  6.068695321593231e-10 \n",
      "\n",
      "Epoch:  17486  Learning Rate:  2.548948048234758e-09  Varinance:  6.060217576672761e-10 \n",
      "\n",
      "Epoch:  17487  Learning Rate:  2.5464003742358257e-09  Varinance:  6.051751674851202e-10 \n",
      "\n",
      "Epoch:  17488  Learning Rate:  2.543855246637489e-09  Varinance:  6.043297599584173e-10 \n",
      "\n",
      "Epoch:  17489  Learning Rate:  2.5413126628946016e-09  Varinance:  6.034855334350415e-10 \n",
      "\n",
      "Epoch:  17490  Learning Rate:  2.5387726204645893e-09  Varinance:  6.026424862651743e-10 \n",
      "\n",
      "Epoch:  17491  Learning Rate:  2.536235116807418e-09  Varinance:  6.018006168013021e-10 \n",
      "\n",
      "Epoch:  17492  Learning Rate:  2.5337001493855658e-09  Varinance:  6.00959923398215e-10 \n",
      "\n",
      "Epoch:  17493  Learning Rate:  2.5311677156640737e-09  Varinance:  6.001204044129947e-10 \n",
      "\n",
      "Epoch:  17494  Learning Rate:  2.5286378131105176e-09  Varinance:  5.992820582050249e-10 \n",
      "\n",
      "Epoch:  17495  Learning Rate:  2.5261104391949762e-09  Varinance:  5.984448831359787e-10 \n",
      "\n",
      "Epoch:  17496  Learning Rate:  2.5235855913900844e-09  Varinance:  5.976088775698176e-10 \n",
      "\n",
      "Epoch:  17497  Learning Rate:  2.5210632671710037e-09  Varinance:  5.967740398727894e-10 \n",
      "\n",
      "Epoch:  17498  Learning Rate:  2.518543464015391e-09  Varinance:  5.959403684134236e-10 \n",
      "\n",
      "Epoch:  17499  Learning Rate:  2.5160261794034522e-09  Varinance:  5.95107861562531e-10 \n",
      "\n",
      "Epoch:  17500  Learning Rate:  2.5135114108179114e-09  Varinance:  5.94276517693192e-10 \n",
      "\n",
      "Epoch:  17501  Learning Rate:  2.510999155743982e-09  Varinance:  5.934463351807661e-10 \n",
      "\n",
      "Epoch:  17502  Learning Rate:  2.5084894116694175e-09  Varinance:  5.926173124028804e-10 \n",
      "\n",
      "Epoch:  17503  Learning Rate:  2.505982176084483e-09  Varinance:  5.91789447739428e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17504  Learning Rate:  2.503477446481924e-09  Varinance:  5.909627395725658e-10 \n",
      "\n",
      "Epoch:  17505  Learning Rate:  2.5009752203570202e-09  Varinance:  5.901371862867101e-10 \n",
      "\n",
      "Epoch:  17506  Learning Rate:  2.4984754952075542e-09  Varinance:  5.893127862685369e-10 \n",
      "\n",
      "Epoch:  17507  Learning Rate:  2.4959782685337825e-09  Varinance:  5.884895379069692e-10 \n",
      "\n",
      "Epoch:  17508  Learning Rate:  2.4934835378384877e-09  Varinance:  5.87667439593187e-10 \n",
      "\n",
      "Epoch:  17509  Learning Rate:  2.4909913006269474e-09  Varinance:  5.868464897206156e-10 \n",
      "\n",
      "Epoch:  17510  Learning Rate:  2.4885015544069064e-09  Varinance:  5.860266866849249e-10 \n",
      "\n",
      "Epoch:  17511  Learning Rate:  2.4860142966886268e-09  Varinance:  5.852080288840257e-10 \n",
      "\n",
      "Epoch:  17512  Learning Rate:  2.48352952498486e-09  Varinance:  5.843905147180672e-10 \n",
      "\n",
      "Epoch:  17513  Learning Rate:  2.4810472368108167e-09  Varinance:  5.835741425894351e-10 \n",
      "\n",
      "Epoch:  17514  Learning Rate:  2.4785674296842168e-09  Varinance:  5.82758910902741e-10 \n",
      "\n",
      "Epoch:  17515  Learning Rate:  2.4760901011252617e-09  Varinance:  5.819448180648316e-10 \n",
      "\n",
      "Epoch:  17516  Learning Rate:  2.4736152486566055e-09  Varinance:  5.811318624847767e-10 \n",
      "\n",
      "Epoch:  17517  Learning Rate:  2.471142869803413e-09  Varinance:  5.803200425738688e-10 \n",
      "\n",
      "Epoch:  17518  Learning Rate:  2.468672962093287e-09  Varinance:  5.795093567456197e-10 \n",
      "\n",
      "Epoch:  17519  Learning Rate:  2.466205523056329e-09  Varinance:  5.786998034157576e-10 \n",
      "\n",
      "Epoch:  17520  Learning Rate:  2.4637405502251087e-09  Varinance:  5.778913810022258e-10 \n",
      "\n",
      "Epoch:  17521  Learning Rate:  2.461278041134635e-09  Varinance:  5.770840879251715e-10 \n",
      "\n",
      "Epoch:  17522  Learning Rate:  2.4588179933224073e-09  Varinance:  5.762779226069549e-10 \n",
      "\n",
      "Epoch:  17523  Learning Rate:  2.4563604043283864e-09  Varinance:  5.754728834721386e-10 \n",
      "\n",
      "Epoch:  17524  Learning Rate:  2.4539052716949664e-09  Varinance:  5.746689689474855e-10 \n",
      "\n",
      "Epoch:  17525  Learning Rate:  2.4514525929670224e-09  Varinance:  5.738661774619562e-10 \n",
      "\n",
      "Epoch:  17526  Learning Rate:  2.4490023656918843e-09  Varinance:  5.730645074467067e-10 \n",
      "\n",
      "Epoch:  17527  Learning Rate:  2.446554587419307e-09  Varinance:  5.722639573350857e-10 \n",
      "\n",
      "Epoch:  17528  Learning Rate:  2.4441092557015214e-09  Varinance:  5.71464525562625e-10 \n",
      "\n",
      "Epoch:  17529  Learning Rate:  2.4416663680932036e-09  Varinance:  5.706662105670478e-10 \n",
      "\n",
      "Epoch:  17530  Learning Rate:  2.439225922151449e-09  Varinance:  5.698690107882578e-10 \n",
      "\n",
      "Epoch:  17531  Learning Rate:  2.4367879154358197e-09  Varinance:  5.69072924668338e-10 \n",
      "\n",
      "Epoch:  17532  Learning Rate:  2.4343523455083174e-09  Varinance:  5.682779506515477e-10 \n",
      "\n",
      "Epoch:  17533  Learning Rate:  2.4319192099333545e-09  Varinance:  5.674840871843197e-10 \n",
      "\n",
      "Epoch:  17534  Learning Rate:  2.4294885062778045e-09  Varinance:  5.666913327152589e-10 \n",
      "\n",
      "Epoch:  17535  Learning Rate:  2.427060232110972e-09  Varinance:  5.658996856951314e-10 \n",
      "\n",
      "Epoch:  17536  Learning Rate:  2.4246343850045653e-09  Varinance:  5.651091445768738e-10 \n",
      "\n",
      "Epoch:  17537  Learning Rate:  2.4222109625327458e-09  Varinance:  5.643197078155818e-10 \n",
      "\n",
      "Epoch:  17538  Learning Rate:  2.419789962272099e-09  Varinance:  5.635313738685089e-10 \n",
      "\n",
      "Epoch:  17539  Learning Rate:  2.4173713818016075e-09  Varinance:  5.627441411950643e-10 \n",
      "\n",
      "Epoch:  17540  Learning Rate:  2.414955218702699e-09  Varinance:  5.61958008256809e-10 \n",
      "\n",
      "Epoch:  17541  Learning Rate:  2.4125414705592198e-09  Varinance:  5.611729735174553e-10 \n",
      "\n",
      "Epoch:  17542  Learning Rate:  2.410130134957403e-09  Varinance:  5.603890354428556e-10 \n",
      "\n",
      "Epoch:  17543  Learning Rate:  2.407721209485922e-09  Varinance:  5.596061925010115e-10 \n",
      "\n",
      "Epoch:  17544  Learning Rate:  2.4053146917358603e-09  Varinance:  5.588244431620624e-10 \n",
      "\n",
      "Epoch:  17545  Learning Rate:  2.4029105793006817e-09  Varinance:  5.580437858982854e-10 \n",
      "\n",
      "Epoch:  17546  Learning Rate:  2.4005088697762825e-09  Varinance:  5.572642191840915e-10 \n",
      "\n",
      "Epoch:  17547  Learning Rate:  2.3981095607609617e-09  Varinance:  5.564857414960229e-10 \n",
      "\n",
      "Epoch:  17548  Learning Rate:  2.3957126498553933e-09  Varinance:  5.557083513127518e-10 \n",
      "\n",
      "Epoch:  17549  Learning Rate:  2.393318134662683e-09  Varinance:  5.549320471150702e-10 \n",
      "\n",
      "Epoch:  17550  Learning Rate:  2.390926012788298e-09  Varinance:  5.541568273858977e-10 \n",
      "\n",
      "Epoch:  17551  Learning Rate:  2.388536281840125e-09  Varinance:  5.533826906102719e-10 \n",
      "\n",
      "Epoch:  17552  Learning Rate:  2.3861489394284413e-09  Varinance:  5.526096352753461e-10 \n",
      "\n",
      "Epoch:  17553  Learning Rate:  2.3837639831658878e-09  Varinance:  5.518376598703875e-10 \n",
      "\n",
      "Epoch:  17554  Learning Rate:  2.381381410667516e-09  Varinance:  5.510667628867734e-10 \n",
      "\n",
      "Epoch:  17555  Learning Rate:  2.3790012195507613e-09  Varinance:  5.502969428179906e-10 \n",
      "\n",
      "Epoch:  17556  Learning Rate:  2.3766234074354168e-09  Varinance:  5.495281981596247e-10 \n",
      "\n",
      "Epoch:  17557  Learning Rate:  2.374247971943677e-09  Varinance:  5.487605274093687e-10 \n",
      "\n",
      "Epoch:  17558  Learning Rate:  2.371874910700116e-09  Varinance:  5.479939290670125e-10 \n",
      "\n",
      "Epoch:  17559  Learning Rate:  2.3695042213316544e-09  Varinance:  5.472284016344413e-10 \n",
      "\n",
      "Epoch:  17560  Learning Rate:  2.367135901467612e-09  Varinance:  5.464639436156336e-10 \n",
      "\n",
      "Epoch:  17561  Learning Rate:  2.3647699487396765e-09  Varinance:  5.457005535166575e-10 \n",
      "\n",
      "Epoch:  17562  Learning Rate:  2.3624063607818787e-09  Varinance:  5.449382298456701e-10 \n",
      "\n",
      "Epoch:  17563  Learning Rate:  2.360045135230638e-09  Varinance:  5.441769711129066e-10 \n",
      "\n",
      "Epoch:  17564  Learning Rate:  2.357686269724738e-09  Varinance:  5.434167758306895e-10 \n",
      "\n",
      "Epoch:  17565  Learning Rate:  2.3553297619052957e-09  Varinance:  5.426576425134171e-10 \n",
      "\n",
      "Epoch:  17566  Learning Rate:  2.352975609415812e-09  Varinance:  5.418995696775635e-10 \n",
      "\n",
      "Epoch:  17567  Learning Rate:  2.3506238099021413e-09  Varinance:  5.41142555841675e-10 \n",
      "\n",
      "Epoch:  17568  Learning Rate:  2.348274361012469e-09  Varinance:  5.403865995263673e-10 \n",
      "\n",
      "Epoch:  17569  Learning Rate:  2.345927260397353e-09  Varinance:  5.39631699254325e-10 \n",
      "\n",
      "Epoch:  17570  Learning Rate:  2.343582505709701e-09  Varinance:  5.388778535502905e-10 \n",
      "\n",
      "Epoch:  17571  Learning Rate:  2.341240094604742e-09  Varinance:  5.381250609410728e-10 \n",
      "\n",
      "Epoch:  17572  Learning Rate:  2.338900024740072e-09  Varinance:  5.373733199555372e-10 \n",
      "\n",
      "Epoch:  17573  Learning Rate:  2.3365622937756305e-09  Varinance:  5.366226291246037e-10 \n",
      "\n",
      "Epoch:  17574  Learning Rate:  2.3342268993736695e-09  Varinance:  5.358729869812451e-10 \n",
      "\n",
      "Epoch:  17575  Learning Rate:  2.331893839198802e-09  Varinance:  5.351243920604831e-10 \n",
      "\n",
      "Epoch:  17576  Learning Rate:  2.3295631109179765e-09  Varinance:  5.343768428993881e-10 \n",
      "\n",
      "Epoch:  17577  Learning Rate:  2.327234712200448e-09  Varinance:  5.336303380370683e-10 \n",
      "\n",
      "Epoch:  17578  Learning Rate:  2.324908640717825e-09  Varinance:  5.328848760146786e-10 \n",
      "\n",
      "Epoch:  17579  Learning Rate:  2.3225848941440447e-09  Varinance:  5.321404553754098e-10 \n",
      "\n",
      "Epoch:  17580  Learning Rate:  2.3202634701553445e-09  Varinance:  5.313970746644878e-10 \n",
      "\n",
      "Epoch:  17581  Learning Rate:  2.317944366430316e-09  Varinance:  5.306547324291709e-10 \n",
      "\n",
      "Epoch:  17582  Learning Rate:  2.3156275806498382e-09  Varinance:  5.299134272187466e-10 \n",
      "\n",
      "Epoch:  17583  Learning Rate:  2.3133131104971345e-09  Varinance:  5.291731575845312e-10 \n",
      "\n",
      "Epoch:  17584  Learning Rate:  2.311000953657742e-09  Varinance:  5.284339220798589e-10 \n",
      "\n",
      "Epoch:  17585  Learning Rate:  2.308691107819488e-09  Varinance:  5.276957192600904e-10 \n",
      "\n",
      "Epoch:  17586  Learning Rate:  2.306383570672534e-09  Varinance:  5.269585476826028e-10 \n",
      "\n",
      "Epoch:  17587  Learning Rate:  2.3040783399093507e-09  Varinance:  5.262224059067884e-10 \n",
      "\n",
      "Epoch:  17588  Learning Rate:  2.3017754132246916e-09  Varinance:  5.254872924940522e-10 \n",
      "\n",
      "Epoch:  17589  Learning Rate:  2.2994747883156375e-09  Varinance:  5.247532060078083e-10 \n",
      "\n",
      "Epoch:  17590  Learning Rate:  2.297176462881571e-09  Varinance:  5.240201450134803e-10 \n",
      "\n",
      "Epoch:  17591  Learning Rate:  2.294880434624151e-09  Varinance:  5.232881080784895e-10 \n",
      "\n",
      "Epoch:  17592  Learning Rate:  2.292586701247357e-09  Varinance:  5.225570937722648e-10 \n",
      "\n",
      "Epoch:  17593  Learning Rate:  2.2902952604574634e-09  Varinance:  5.21827100666231e-10 \n",
      "\n",
      "Epoch:  17594  Learning Rate:  2.2880061099630127e-09  Varinance:  5.210981273338088e-10 \n",
      "\n",
      "Epoch:  17595  Learning Rate:  2.285719247474863e-09  Varinance:  5.203701723504122e-10 \n",
      "\n",
      "Epoch:  17596  Learning Rate:  2.283434670706159e-09  Varinance:  5.196432342934446e-10 \n",
      "\n",
      "Epoch:  17597  Learning Rate:  2.2811523773723086e-09  Varinance:  5.18917311742299e-10 \n",
      "\n",
      "Epoch:  17598  Learning Rate:  2.2788723651910246e-09  Varinance:  5.181924032783474e-10 \n",
      "\n",
      "Epoch:  17599  Learning Rate:  2.2765946318823045e-09  Varinance:  5.174685074849488e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17600  Learning Rate:  2.2743191751683976e-09  Varinance:  5.167456229474398e-10 \n",
      "\n",
      "Epoch:  17601  Learning Rate:  2.272045992773856e-09  Varinance:  5.160237482531329e-10 \n",
      "\n",
      "Epoch:  17602  Learning Rate:  2.2697750824255037e-09  Varinance:  5.15302881991314e-10 \n",
      "\n",
      "Epoch:  17603  Learning Rate:  2.2675064418524156e-09  Varinance:  5.145830227532419e-10 \n",
      "\n",
      "Epoch:  17604  Learning Rate:  2.2652400687859582e-09  Varinance:  5.138641691321377e-10 \n",
      "\n",
      "Epoch:  17605  Learning Rate:  2.262975960959766e-09  Varinance:  5.13146319723193e-10 \n",
      "\n",
      "Epoch:  17606  Learning Rate:  2.2607141161097157e-09  Varinance:  5.124294731235603e-10 \n",
      "\n",
      "Epoch:  17607  Learning Rate:  2.2584545319739697e-09  Varinance:  5.11713627932352e-10 \n",
      "\n",
      "Epoch:  17608  Learning Rate:  2.256197206292952e-09  Varinance:  5.109987827506367e-10 \n",
      "\n",
      "Epoch:  17609  Learning Rate:  2.2539421368093206e-09  Varinance:  5.102849361814383e-10 \n",
      "\n",
      "Epoch:  17610  Learning Rate:  2.2516893212680137e-09  Varinance:  5.095720868297332e-10 \n",
      "\n",
      "Epoch:  17611  Learning Rate:  2.249438757416224e-09  Varinance:  5.088602333024417e-10 \n",
      "\n",
      "Epoch:  17612  Learning Rate:  2.247190443003371e-09  Varinance:  5.081493742084354e-10 \n",
      "\n",
      "Epoch:  17613  Learning Rate:  2.244944375781156e-09  Varinance:  5.074395081585274e-10 \n",
      "\n",
      "Epoch:  17614  Learning Rate:  2.242700553503496e-09  Varinance:  5.067306337654713e-10 \n",
      "\n",
      "Epoch:  17615  Learning Rate:  2.240458973926577e-09  Varinance:  5.060227496439592e-10 \n",
      "\n",
      "Epoch:  17616  Learning Rate:  2.238219634808826e-09  Varinance:  5.053158544106176e-10 \n",
      "\n",
      "Epoch:  17617  Learning Rate:  2.2359825339108883e-09  Varinance:  5.046099466840082e-10 \n",
      "\n",
      "Epoch:  17618  Learning Rate:  2.233747668995671e-09  Varinance:  5.039050250846165e-10 \n",
      "\n",
      "Epoch:  17619  Learning Rate:  2.2315150378283167e-09  Varinance:  5.032010882348608e-10 \n",
      "\n",
      "Epoch:  17620  Learning Rate:  2.2292846381761784e-09  Varinance:  5.024981347590817e-10 \n",
      "\n",
      "Epoch:  17621  Learning Rate:  2.227056467808864e-09  Varinance:  5.017961632835423e-10 \n",
      "\n",
      "Epoch:  17622  Learning Rate:  2.224830524498211e-09  Varinance:  5.010951724364237e-10 \n",
      "\n",
      "Epoch:  17623  Learning Rate:  2.22260680601826e-09  Varinance:  5.003951608478243e-10 \n",
      "\n",
      "Epoch:  17624  Learning Rate:  2.2203853101453e-09  Varinance:  4.996961271497575e-10 \n",
      "\n",
      "Epoch:  17625  Learning Rate:  2.218166034657843e-09  Varinance:  4.989980699761425e-10 \n",
      "\n",
      "Epoch:  17626  Learning Rate:  2.215948977336598e-09  Varinance:  4.983009879628122e-10 \n",
      "\n",
      "Epoch:  17627  Learning Rate:  2.213734135964515e-09  Varinance:  4.976048797475036e-10 \n",
      "\n",
      "Epoch:  17628  Learning Rate:  2.21152150832676e-09  Varinance:  4.969097439698563e-10 \n",
      "\n",
      "Epoch:  17629  Learning Rate:  2.20931109221069e-09  Varinance:  4.962155792714107e-10 \n",
      "\n",
      "Epoch:  17630  Learning Rate:  2.207102885405896e-09  Varinance:  4.955223842956047e-10 \n",
      "\n",
      "Epoch:  17631  Learning Rate:  2.20489688570418e-09  Varinance:  4.948301576877731e-10 \n",
      "\n",
      "Epoch:  17632  Learning Rate:  2.202693090899525e-09  Varinance:  4.941388980951377e-10 \n",
      "\n",
      "Epoch:  17633  Learning Rate:  2.200491498788144e-09  Varinance:  4.934486041668157e-10 \n",
      "\n",
      "Epoch:  17634  Learning Rate:  2.1982921071684536e-09  Varinance:  4.927592745538093e-10 \n",
      "\n",
      "Epoch:  17635  Learning Rate:  2.1960949138410456e-09  Varinance:  4.920709079090054e-10 \n",
      "\n",
      "Epoch:  17636  Learning Rate:  2.1938999166087345e-09  Varinance:  4.913835028871727e-10 \n",
      "\n",
      "Epoch:  17637  Learning Rate:  2.1917071132765305e-09  Varinance:  4.90697058144959e-10 \n",
      "\n",
      "Epoch:  17638  Learning Rate:  2.189516501651615e-09  Varinance:  4.900115723408908e-10 \n",
      "\n",
      "Epoch:  17639  Learning Rate:  2.1873280795433833e-09  Varinance:  4.893270441353633e-10 \n",
      "\n",
      "Epoch:  17640  Learning Rate:  2.185141844763421e-09  Varinance:  4.886434721906479e-10 \n",
      "\n",
      "Epoch:  17641  Learning Rate:  2.1829577951254782e-09  Varinance:  4.879608551708834e-10 \n",
      "\n",
      "Epoch:  17642  Learning Rate:  2.18077592844552e-09  Varinance:  4.872791917420747e-10 \n",
      "\n",
      "Epoch:  17643  Learning Rate:  2.1785962425416643e-09  Varinance:  4.865984805720901e-10 \n",
      "\n",
      "Epoch:  17644  Learning Rate:  2.1764187352342324e-09  Varinance:  4.85918720330659e-10 \n",
      "\n",
      "Epoch:  17645  Learning Rate:  2.1742434043457253e-09  Varinance:  4.852399096893708e-10 \n",
      "\n",
      "Epoch:  17646  Learning Rate:  2.1720702477007957e-09  Varinance:  4.845620473216654e-10 \n",
      "\n",
      "Epoch:  17647  Learning Rate:  2.1698992631262945e-09  Varinance:  4.838851319028412e-10 \n",
      "\n",
      "Epoch:  17648  Learning Rate:  2.1677304484512454e-09  Varinance:  4.832091621100451e-10 \n",
      "\n",
      "Epoch:  17649  Learning Rate:  2.1655638015068175e-09  Varinance:  4.825341366222724e-10 \n",
      "\n",
      "Epoch:  17650  Learning Rate:  2.1633993201263718e-09  Varinance:  4.818600541203635e-10 \n",
      "\n",
      "Epoch:  17651  Learning Rate:  2.1612370021454345e-09  Varinance:  4.811869132870018e-10 \n",
      "\n",
      "Epoch:  17652  Learning Rate:  2.1590768454016713e-09  Varinance:  4.805147128067125e-10 \n",
      "\n",
      "Epoch:  17653  Learning Rate:  2.156918847734933e-09  Varinance:  4.798434513658533e-10 \n",
      "\n",
      "Epoch:  17654  Learning Rate:  2.154763006987231e-09  Varinance:  4.791731276526224e-10 \n",
      "\n",
      "Epoch:  17655  Learning Rate:  2.1526093210027066e-09  Varinance:  4.785037403570486e-10 \n",
      "\n",
      "Epoch:  17656  Learning Rate:  2.1504577876276836e-09  Varinance:  4.778352881709909e-10 \n",
      "\n",
      "Epoch:  17657  Learning Rate:  2.1483084047106347e-09  Varinance:  4.771677697881355e-10 \n",
      "\n",
      "Epoch:  17658  Learning Rate:  2.1461611701021618e-09  Varinance:  4.765011839039934e-10 \n",
      "\n",
      "Epoch:  17659  Learning Rate:  2.144016081655038e-09  Varinance:  4.758355292159e-10 \n",
      "\n",
      "Epoch:  17660  Learning Rate:  2.141873137224182e-09  Varinance:  4.75170804423005e-10 \n",
      "\n",
      "Epoch:  17661  Learning Rate:  2.1397323346666344e-09  Varinance:  4.745070082262807e-10 \n",
      "\n",
      "Epoch:  17662  Learning Rate:  2.1375936718415992e-09  Varinance:  4.738441393285122e-10 \n",
      "\n",
      "Epoch:  17663  Learning Rate:  2.135457146610422e-09  Varinance:  4.731821964342969e-10 \n",
      "\n",
      "Epoch:  17664  Learning Rate:  2.133322756836562e-09  Varinance:  4.725211782500417e-10 \n",
      "\n",
      "Epoch:  17665  Learning Rate:  2.1311905003856363e-09  Varinance:  4.718610834839609e-10 \n",
      "\n",
      "Epoch:  17666  Learning Rate:  2.1290603751253962e-09  Varinance:  4.712019108460746e-10 \n",
      "\n",
      "Epoch:  17667  Learning Rate:  2.126932378925701e-09  Varinance:  4.705436590482002e-10 \n",
      "\n",
      "Epoch:  17668  Learning Rate:  2.124806509658562e-09  Varinance:  4.698863268039597e-10 \n",
      "\n",
      "Epoch:  17669  Learning Rate:  2.1226827651981174e-09  Varinance:  4.692299128287703e-10 \n",
      "\n",
      "Epoch:  17670  Learning Rate:  2.1205611434206074e-09  Varinance:  4.685744158398439e-10 \n",
      "\n",
      "Epoch:  17671  Learning Rate:  2.118441642204417e-09  Varinance:  4.679198345561843e-10 \n",
      "\n",
      "Epoch:  17672  Learning Rate:  2.1163242594300535e-09  Varinance:  4.672661676985846e-10 \n",
      "\n",
      "Epoch:  17673  Learning Rate:  2.114208992980118e-09  Varinance:  4.66613413989627e-10 \n",
      "\n",
      "Epoch:  17674  Learning Rate:  2.112095840739359e-09  Varinance:  4.65961572153673e-10 \n",
      "\n",
      "Epoch:  17675  Learning Rate:  2.1099848005946093e-09  Varinance:  4.6531064091687085e-10 \n",
      "\n",
      "Epoch:  17676  Learning Rate:  2.1078758704348364e-09  Varinance:  4.6466061900714726e-10 \n",
      "\n",
      "Epoch:  17677  Learning Rate:  2.105769048151117e-09  Varinance:  4.6401150515420535e-10 \n",
      "\n",
      "Epoch:  17678  Learning Rate:  2.1036643316366135e-09  Varinance:  4.633632980895232e-10 \n",
      "\n",
      "Epoch:  17679  Learning Rate:  2.1015617187866168e-09  Varinance:  4.627159965463508e-10 \n",
      "\n",
      "Epoch:  17680  Learning Rate:  2.0994612074985218e-09  Varinance:  4.620695992597095e-10 \n",
      "\n",
      "Epoch:  17681  Learning Rate:  2.097362795671802e-09  Varinance:  4.6142410496638274e-10 \n",
      "\n",
      "Epoch:  17682  Learning Rate:  2.095266481208052e-09  Varinance:  4.607795124049237e-10 \n",
      "\n",
      "Epoch:  17683  Learning Rate:  2.093172262010966e-09  Varinance:  4.6013582031564583e-10 \n",
      "\n",
      "Epoch:  17684  Learning Rate:  2.0910801359863087e-09  Varinance:  4.594930274406226e-10 \n",
      "\n",
      "Epoch:  17685  Learning Rate:  2.0889901010419612e-09  Varinance:  4.588511325236847e-10 \n",
      "\n",
      "Epoch:  17686  Learning Rate:  2.0869021550878967e-09  Varinance:  4.5821013431041757e-10 \n",
      "\n",
      "Epoch:  17687  Learning Rate:  2.084816296036154e-09  Varinance:  4.5757003154816057e-10 \n",
      "\n",
      "Epoch:  17688  Learning Rate:  2.082732521800881e-09  Varinance:  4.569308229859984e-10 \n",
      "\n",
      "Epoch:  17689  Learning Rate:  2.0806508302983103e-09  Varinance:  4.562925073747678e-10 \n",
      "\n",
      "Epoch:  17690  Learning Rate:  2.078571219446736e-09  Varinance:  4.55655083467049e-10 \n",
      "\n",
      "Epoch:  17691  Learning Rate:  2.0764936871665547e-09  Varinance:  4.55018550017165e-10 \n",
      "\n",
      "Epoch:  17692  Learning Rate:  2.0744182313802407e-09  Varinance:  4.543829057811788e-10 \n",
      "\n",
      "Epoch:  17693  Learning Rate:  2.0723448500123235e-09  Varinance:  4.537481495168912e-10 \n",
      "\n",
      "Epoch:  17694  Learning Rate:  2.0702735409894294e-09  Varinance:  4.5311427998383983e-10 \n",
      "\n",
      "Epoch:  17695  Learning Rate:  2.068204302240256e-09  Varinance:  4.5248129594329046e-10 \n",
      "\n",
      "Epoch:  17696  Learning Rate:  2.0661371316955497e-09  Varinance:  4.5184919615824417e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17697  Learning Rate:  2.064072027288147e-09  Varinance:  4.512179793934284e-10 \n",
      "\n",
      "Epoch:  17698  Learning Rate:  2.0620089869529514e-09  Varinance:  4.505876444152963e-10 \n",
      "\n",
      "Epoch:  17699  Learning Rate:  2.0599480086269076e-09  Varinance:  4.499581899920242e-10 \n",
      "\n",
      "Epoch:  17700  Learning Rate:  2.0578890902490435e-09  Varinance:  4.4932961489350916e-10 \n",
      "\n",
      "Epoch:  17701  Learning Rate:  2.0558322297604486e-09  Varinance:  4.4870191789136833e-10 \n",
      "\n",
      "Epoch:  17702  Learning Rate:  2.0537774251042474e-09  Varinance:  4.4807509775893017e-10 \n",
      "\n",
      "Epoch:  17703  Learning Rate:  2.0517246742256426e-09  Varinance:  4.474491532712413e-10 \n",
      "\n",
      "Epoch:  17704  Learning Rate:  2.04967397507189e-09  Varinance:  4.4682408320505815e-10 \n",
      "\n",
      "Epoch:  17705  Learning Rate:  2.0476253255922763e-09  Varinance:  4.461998863388461e-10 \n",
      "\n",
      "Epoch:  17706  Learning Rate:  2.045578723738166e-09  Varinance:  4.4557656145277644e-10 \n",
      "\n",
      "Epoch:  17707  Learning Rate:  2.0435341674629424e-09  Varinance:  4.449541073287252e-10 \n",
      "\n",
      "Epoch:  17708  Learning Rate:  2.0414916547220567e-09  Varinance:  4.4433252275027117e-10 \n",
      "\n",
      "Epoch:  17709  Learning Rate:  2.0394511834730033e-09  Varinance:  4.43711806502688e-10 \n",
      "\n",
      "Epoch:  17710  Learning Rate:  2.037412751675296e-09  Varinance:  4.430919573729507e-10 \n",
      "\n",
      "Epoch:  17711  Learning Rate:  2.0353763572905103e-09  Varinance:  4.424729741497275e-10 \n",
      "\n",
      "Epoch:  17712  Learning Rate:  2.0333419982822587e-09  Varinance:  4.418548556233788e-10 \n",
      "\n",
      "Epoch:  17713  Learning Rate:  2.031309672616167e-09  Varinance:  4.412376005859546e-10 \n",
      "\n",
      "Epoch:  17714  Learning Rate:  2.029279378259918e-09  Varinance:  4.406212078311928e-10 \n",
      "\n",
      "Epoch:  17715  Learning Rate:  2.0272511131832232e-09  Varinance:  4.400056761545174e-10 \n",
      "\n",
      "Epoch:  17716  Learning Rate:  2.0252248753578032e-09  Varinance:  4.3939100435303087e-10 \n",
      "\n",
      "Epoch:  17717  Learning Rate:  2.0232006627574275e-09  Varinance:  4.387771912255207e-10 \n",
      "\n",
      "Epoch:  17718  Learning Rate:  2.0211784733578903e-09  Varinance:  4.3816423557245077e-10 \n",
      "\n",
      "Epoch:  17719  Learning Rate:  2.019158305136988e-09  Varinance:  4.375521361959607e-10 \n",
      "\n",
      "Epoch:  17720  Learning Rate:  2.0171401560745588e-09  Varinance:  4.369408918998634e-10 \n",
      "\n",
      "Epoch:  17721  Learning Rate:  2.015124024152461e-09  Varinance:  4.3633050148964313e-10 \n",
      "\n",
      "Epoch:  17722  Learning Rate:  2.0131099073545483e-09  Varinance:  4.35720963772454e-10 \n",
      "\n",
      "Epoch:  17723  Learning Rate:  2.01109780366671e-09  Varinance:  4.3511227755711214e-10 \n",
      "\n",
      "Epoch:  17724  Learning Rate:  2.009087711076851e-09  Varinance:  4.3450444165410214e-10 \n",
      "\n",
      "Epoch:  17725  Learning Rate:  2.0070796275748626e-09  Varinance:  4.338974548755688e-10 \n",
      "\n",
      "Epoch:  17726  Learning Rate:  2.0050735511526694e-09  Varinance:  4.3329131603531634e-10 \n",
      "\n",
      "Epoch:  17727  Learning Rate:  2.0030694798042017e-09  Varinance:  4.3268602394880617e-10 \n",
      "\n",
      "Epoch:  17728  Learning Rate:  2.001067411525374e-09  Varinance:  4.320815774331543e-10 \n",
      "\n",
      "Epoch:  17729  Learning Rate:  1.9990673443141236e-09  Varinance:  4.314779753071308e-10 \n",
      "\n",
      "Epoch:  17730  Learning Rate:  1.997069276170392e-09  Varinance:  4.3087521639115106e-10 \n",
      "\n",
      "Epoch:  17731  Learning Rate:  1.995073205096095e-09  Varinance:  4.302732995072834e-10 \n",
      "\n",
      "Epoch:  17732  Learning Rate:  1.9930791290951702e-09  Varinance:  4.2967222347923957e-10 \n",
      "\n",
      "Epoch:  17733  Learning Rate:  1.9910870461735476e-09  Varinance:  4.2907198713237494e-10 \n",
      "\n",
      "Epoch:  17734  Learning Rate:  1.9890969543391297e-09  Varinance:  4.2847258929368555e-10 \n",
      "\n",
      "Epoch:  17735  Learning Rate:  1.987108851601832e-09  Varinance:  4.278740287918079e-10 \n",
      "\n",
      "Epoch:  17736  Learning Rate:  1.9851227359735586e-09  Varinance:  4.2727630445701003e-10 \n",
      "\n",
      "Epoch:  17737  Learning Rate:  1.9831386054681794e-09  Varinance:  4.266794151211987e-10 \n",
      "\n",
      "Epoch:  17738  Learning Rate:  1.981156458101578e-09  Varinance:  4.2608335961791097e-10 \n",
      "\n",
      "Epoch:  17739  Learning Rate:  1.979176291891593e-09  Varinance:  4.2548813678231326e-10 \n",
      "\n",
      "Epoch:  17740  Learning Rate:  1.977198104858065e-09  Varinance:  4.248937454511994e-10 \n",
      "\n",
      "Epoch:  17741  Learning Rate:  1.9752218950228132e-09  Varinance:  4.243001844629882e-10 \n",
      "\n",
      "Epoch:  17742  Learning Rate:  1.973247660409614e-09  Varinance:  4.2370745265772236e-10 \n",
      "\n",
      "Epoch:  17743  Learning Rate:  1.9712753990442396e-09  Varinance:  4.2311554887706087e-10 \n",
      "\n",
      "Epoch:  17744  Learning Rate:  1.9693051089544355e-09  Varinance:  4.2252447196428513e-10 \n",
      "\n",
      "Epoch:  17745  Learning Rate:  1.967336788169898e-09  Varinance:  4.21934220764291e-10 \n",
      "\n",
      "Epoch:  17746  Learning Rate:  1.9653704347223123e-09  Varinance:  4.213447941235881e-10 \n",
      "\n",
      "Epoch:  17747  Learning Rate:  1.963406046645332e-09  Varinance:  4.207561908902971e-10 \n",
      "\n",
      "Epoch:  17748  Learning Rate:  1.9614436219745548e-09  Varinance:  4.201684099141483e-10 \n",
      "\n",
      "Epoch:  17749  Learning Rate:  1.9594831587475637e-09  Varinance:  4.1958145004647986e-10 \n",
      "\n",
      "Epoch:  17750  Learning Rate:  1.9575246550039012e-09  Varinance:  4.1899531014023053e-10 \n",
      "\n",
      "Epoch:  17751  Learning Rate:  1.9555681087850497e-09  Varinance:  4.184099890499457e-10 \n",
      "\n",
      "Epoch:  17752  Learning Rate:  1.95361351813447e-09  Varinance:  4.178254856317694e-10 \n",
      "\n",
      "Epoch:  17753  Learning Rate:  1.9516608810975786e-09  Varinance:  4.172417987434437e-10 \n",
      "\n",
      "Epoch:  17754  Learning Rate:  1.9497101957217234e-09  Varinance:  4.1665892724430644e-10 \n",
      "\n",
      "Epoch:  17755  Learning Rate:  1.9477614600562266e-09  Varinance:  4.160768699952887e-10 \n",
      "\n",
      "Epoch:  17756  Learning Rate:  1.945814672152359e-09  Varinance:  4.154956258589144e-10 \n",
      "\n",
      "Epoch:  17757  Learning Rate:  1.943869830063319e-09  Varinance:  4.14915193699292e-10 \n",
      "\n",
      "Epoch:  17758  Learning Rate:  1.941926931844271e-09  Varinance:  4.1433557238212135e-10 \n",
      "\n",
      "Epoch:  17759  Learning Rate:  1.939985975552324e-09  Varinance:  4.1375676077468506e-10 \n",
      "\n",
      "Epoch:  17760  Learning Rate:  1.9380469592465068e-09  Varinance:  4.131787577458485e-10 \n",
      "\n",
      "Epoch:  17761  Learning Rate:  1.9361098809878105e-09  Varinance:  4.1260156216605693e-10 \n",
      "\n",
      "Epoch:  17762  Learning Rate:  1.9341747388391634e-09  Varinance:  4.1202517290733354e-10 \n",
      "\n",
      "Epoch:  17763  Learning Rate:  1.932241530865409e-09  Varinance:  4.11449588843279e-10 \n",
      "\n",
      "Epoch:  17764  Learning Rate:  1.930310255133347e-09  Varinance:  4.108748088490628e-10 \n",
      "\n",
      "Epoch:  17765  Learning Rate:  1.9283809097117077e-09  Varinance:  4.1030083180143037e-10 \n",
      "\n",
      "Epoch:  17766  Learning Rate:  1.926453492671132e-09  Varinance:  4.0972765657869476e-10 \n",
      "\n",
      "Epoch:  17767  Learning Rate:  1.9245280020842163e-09  Varinance:  4.09155282060736e-10 \n",
      "\n",
      "Epoch:  17768  Learning Rate:  1.9226044360254566e-09  Varinance:  4.085837071289988e-10 \n",
      "\n",
      "Epoch:  17769  Learning Rate:  1.920682792571293e-09  Varinance:  4.080129306664906e-10 \n",
      "\n",
      "Epoch:  17770  Learning Rate:  1.918763069800088e-09  Varinance:  4.074429515577805e-10 \n",
      "\n",
      "Epoch:  17771  Learning Rate:  1.916845265792107e-09  Varinance:  4.068737686889917e-10 \n",
      "\n",
      "Epoch:  17772  Learning Rate:  1.914929378629551e-09  Varinance:  4.063053809478075e-10 \n",
      "\n",
      "Epoch:  17773  Learning Rate:  1.91301540639654e-09  Varinance:  4.057377872234639e-10 \n",
      "\n",
      "Epoch:  17774  Learning Rate:  1.9111033471790883e-09  Varinance:  4.051709864067486e-10 \n",
      "\n",
      "Epoch:  17775  Learning Rate:  1.9091931990651427e-09  Varinance:  4.046049773899984e-10 \n",
      "\n",
      "Epoch:  17776  Learning Rate:  1.9072849601445625e-09  Varinance:  4.04039759067098e-10 \n",
      "\n",
      "Epoch:  17777  Learning Rate:  1.905378628509094e-09  Varinance:  4.034753303334784e-10 \n",
      "\n",
      "Epoch:  17778  Learning Rate:  1.9034742022524136e-09  Varinance:  4.0291169008610954e-10 \n",
      "\n",
      "Epoch:  17779  Learning Rate:  1.9015716794701e-09  Varinance:  4.023488372235064e-10 \n",
      "\n",
      "Epoch:  17780  Learning Rate:  1.899671058259618e-09  Varinance:  4.017867706457213e-10 \n",
      "\n",
      "Epoch:  17781  Learning Rate:  1.8977723367203524e-09  Varinance:  4.0122548925434325e-10 \n",
      "\n",
      "Epoch:  17782  Learning Rate:  1.8958755129535885e-09  Varinance:  4.0066499195249556e-10 \n",
      "\n",
      "Epoch:  17783  Learning Rate:  1.8939805850624887e-09  Varinance:  4.0010527764483393e-10 \n",
      "\n",
      "Epoch:  17784  Learning Rate:  1.8920875511521318e-09  Varinance:  3.995463452375456e-10 \n",
      "\n",
      "Epoch:  17785  Learning Rate:  1.8901964093294906e-09  Varinance:  3.989881936383415e-10 \n",
      "\n",
      "Epoch:  17786  Learning Rate:  1.8883071577034098e-09  Varinance:  3.984308217564629e-10 \n",
      "\n",
      "Epoch:  17787  Learning Rate:  1.8864197943846436e-09  Varinance:  3.978742285026731e-10 \n",
      "\n",
      "Epoch:  17788  Learning Rate:  1.8845343174858355e-09  Varinance:  3.9731841278925735e-10 \n",
      "\n",
      "Epoch:  17789  Learning Rate:  1.882650725121496e-09  Varinance:  3.9676337353002025e-10 \n",
      "\n",
      "Epoch:  17790  Learning Rate:  1.880769015408038e-09  Varinance:  3.9620910964028375e-10 \n",
      "\n",
      "Epoch:  17791  Learning Rate:  1.878889186463759e-09  Varinance:  3.9565562003688635e-10 \n",
      "\n",
      "Epoch:  17792  Learning Rate:  1.877011236408816e-09  Varinance:  3.9510290363817583e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17793  Learning Rate:  1.875135163365266e-09  Varinance:  3.9455095936401483e-10 \n",
      "\n",
      "Epoch:  17794  Learning Rate:  1.8732609654570424e-09  Varinance:  3.939997861357737e-10 \n",
      "\n",
      "Epoch:  17795  Learning Rate:  1.8713886408099337e-09  Varinance:  3.9344938287632955e-10 \n",
      "\n",
      "Epoch:  17796  Learning Rate:  1.869518187551622e-09  Varinance:  3.928997485100642e-10 \n",
      "\n",
      "Epoch:  17797  Learning Rate:  1.86764960381166e-09  Varinance:  3.92350881962862e-10 \n",
      "\n",
      "Epoch:  17798  Learning Rate:  1.8657828877214508e-09  Varinance:  3.918027821621094e-10 \n",
      "\n",
      "Epoch:  17799  Learning Rate:  1.8639180374142916e-09  Varinance:  3.9125544803668676e-10 \n",
      "\n",
      "Epoch:  17800  Learning Rate:  1.8620550510253182e-09  Varinance:  3.9070887851697524e-10 \n",
      "\n",
      "Epoch:  17801  Learning Rate:  1.8601939266915512e-09  Varinance:  3.901630725348486e-10 \n",
      "\n",
      "Epoch:  17802  Learning Rate:  1.8583346625518725e-09  Varinance:  3.8961802902367294e-10 \n",
      "\n",
      "Epoch:  17803  Learning Rate:  1.8564772567470047e-09  Varinance:  3.8907374691830413e-10 \n",
      "\n",
      "Epoch:  17804  Learning Rate:  1.854621707419548e-09  Varinance:  3.8853022515508625e-10 \n",
      "\n",
      "Epoch:  17805  Learning Rate:  1.8527680127139602e-09  Varinance:  3.879874626718505e-10 \n",
      "\n",
      "Epoch:  17806  Learning Rate:  1.850916170776533e-09  Varinance:  3.8744545840790793e-10 \n",
      "\n",
      "Epoch:  17807  Learning Rate:  1.8490661797554307e-09  Varinance:  3.869042113040553e-10 \n",
      "\n",
      "Epoch:  17808  Learning Rate:  1.8472180378006687e-09  Varinance:  3.8636372030256765e-10 \n",
      "\n",
      "Epoch:  17809  Learning Rate:  1.8453717430640918e-09  Varinance:  3.858239843471979e-10 \n",
      "\n",
      "Epoch:  17810  Learning Rate:  1.8435272936994121e-09  Varinance:  3.85285002383174e-10 \n",
      "\n",
      "Epoch:  17811  Learning Rate:  1.841684687862186e-09  Varinance:  3.8474677335719795e-10 \n",
      "\n",
      "Epoch:  17812  Learning Rate:  1.8398439237097948e-09  Varinance:  3.8420929621744414e-10 \n",
      "\n",
      "Epoch:  17813  Learning Rate:  1.8380049994014806e-09  Varinance:  3.836725699135523e-10 \n",
      "\n",
      "Epoch:  17814  Learning Rate:  1.8361679130983252e-09  Varinance:  3.8313659339663365e-10 \n",
      "\n",
      "Epoch:  17815  Learning Rate:  1.8343326629632299e-09  Varinance:  3.826013656192633e-10 \n",
      "\n",
      "Epoch:  17816  Learning Rate:  1.83249924716095e-09  Varinance:  3.820668855354796e-10 \n",
      "\n",
      "Epoch:  17817  Learning Rate:  1.8306676638580768e-09  Varinance:  3.815331521007819e-10 \n",
      "\n",
      "Epoch:  17818  Learning Rate:  1.8288379112230135e-09  Varinance:  3.810001642721288e-10 \n",
      "\n",
      "Epoch:  17819  Learning Rate:  1.8270099874260137e-09  Varinance:  3.8046792100793743e-10 \n",
      "\n",
      "Epoch:  17820  Learning Rate:  1.8251838906391602e-09  Varinance:  3.7993642126807575e-10 \n",
      "\n",
      "Epoch:  17821  Learning Rate:  1.823359619036343e-09  Varinance:  3.7940566401386894e-10 \n",
      "\n",
      "Epoch:  17822  Learning Rate:  1.8215371707932963e-09  Varinance:  3.788756482080917e-10 \n",
      "\n",
      "Epoch:  17823  Learning Rate:  1.8197165440875793e-09  Varinance:  3.783463728149678e-10 \n",
      "\n",
      "Epoch:  17824  Learning Rate:  1.8178977370985513e-09  Varinance:  3.7781783680016785e-10 \n",
      "\n",
      "Epoch:  17825  Learning Rate:  1.8160807480074117e-09  Varinance:  3.772900391308075e-10 \n",
      "\n",
      "Epoch:  17826  Learning Rate:  1.814265574997178e-09  Varinance:  3.7676297877544646e-10 \n",
      "\n",
      "Epoch:  17827  Learning Rate:  1.812452216252664e-09  Varinance:  3.762366547040816e-10 \n",
      "\n",
      "Epoch:  17828  Learning Rate:  1.8106406699605174e-09  Varinance:  3.757110658881523e-10 \n",
      "\n",
      "Epoch:  17829  Learning Rate:  1.8088309343091983e-09  Varinance:  3.7518621130053373e-10 \n",
      "\n",
      "Epoch:  17830  Learning Rate:  1.8070230074889574e-09  Varinance:  3.74662089915536e-10 \n",
      "\n",
      "Epoch:  17831  Learning Rate:  1.805216887691881e-09  Varinance:  3.741387007089017e-10 \n",
      "\n",
      "Epoch:  17832  Learning Rate:  1.8034125731118367e-09  Varinance:  3.7361604265780466e-10 \n",
      "\n",
      "Epoch:  17833  Learning Rate:  1.8016100619445157e-09  Varinance:  3.730941147408486e-10 \n",
      "\n",
      "Epoch:  17834  Learning Rate:  1.7998093523874128e-09  Varinance:  3.725729159380604e-10 \n",
      "\n",
      "Epoch:  17835  Learning Rate:  1.7980104426398063e-09  Varinance:  3.7205244523089543e-10 \n",
      "\n",
      "Epoch:  17836  Learning Rate:  1.796213330902792e-09  Varinance:  3.715327016022309e-10 \n",
      "\n",
      "Epoch:  17837  Learning Rate:  1.7944180153792648e-09  Varinance:  3.710136840363648e-10 \n",
      "\n",
      "Epoch:  17838  Learning Rate:  1.7926244942738961e-09  Varinance:  3.7049539151901395e-10 \n",
      "\n",
      "Epoch:  17839  Learning Rate:  1.790832765793171e-09  Varinance:  3.6997782303731225e-10 \n",
      "\n",
      "Epoch:  17840  Learning Rate:  1.7890428281453674e-09  Varinance:  3.694609775798096e-10 \n",
      "\n",
      "Epoch:  17841  Learning Rate:  1.7872546795405348e-09  Varinance:  3.6894485413646525e-10 \n",
      "\n",
      "Epoch:  17842  Learning Rate:  1.7854683181905304e-09  Varinance:  3.684294516986531e-10 \n",
      "\n",
      "Epoch:  17843  Learning Rate:  1.7836837423089993e-09  Varinance:  3.679147692591548e-10 \n",
      "\n",
      "Epoch:  17844  Learning Rate:  1.781900950111353e-09  Varinance:  3.674008058121591e-10 \n",
      "\n",
      "Epoch:  17845  Learning Rate:  1.7801199398148054e-09  Varinance:  3.6688756035326e-10 \n",
      "\n",
      "Epoch:  17846  Learning Rate:  1.778340709638352e-09  Varinance:  3.663750318794543e-10 \n",
      "\n",
      "Epoch:  17847  Learning Rate:  1.7765632578027504e-09  Varinance:  3.658632193891414e-10 \n",
      "\n",
      "Epoch:  17848  Learning Rate:  1.7747875825305546e-09  Varinance:  3.6535212188211607e-10 \n",
      "\n",
      "Epoch:  17849  Learning Rate:  1.7730136820460953e-09  Varinance:  3.6484173835957413e-10 \n",
      "\n",
      "Epoch:  17850  Learning Rate:  1.7712415545754599e-09  Varinance:  3.6433206782410537e-10 \n",
      "\n",
      "Epoch:  17851  Learning Rate:  1.7694711983465264e-09  Varinance:  3.63823109279693e-10 \n",
      "\n",
      "Epoch:  17852  Learning Rate:  1.767702611588945e-09  Varinance:  3.633148617317118e-10 \n",
      "\n",
      "Epoch:  17853  Learning Rate:  1.765935792534116e-09  Varinance:  3.628073241869256e-10 \n",
      "\n",
      "Epoch:  17854  Learning Rate:  1.764170739415227e-09  Varinance:  3.6230049565348733e-10 \n",
      "\n",
      "Epoch:  17855  Learning Rate:  1.7624074504672306e-09  Varinance:  3.617943751409315e-10 \n",
      "\n",
      "Epoch:  17856  Learning Rate:  1.7606459239268252e-09  Varinance:  3.6128896166018025e-10 \n",
      "\n",
      "Epoch:  17857  Learning Rate:  1.7588861580324907e-09  Varinance:  3.6078425422353605e-10 \n",
      "\n",
      "Epoch:  17858  Learning Rate:  1.757128151024467e-09  Varinance:  3.602802518446811e-10 \n",
      "\n",
      "Epoch:  17859  Learning Rate:  1.7553719011447343e-09  Varinance:  3.597769535386756e-10 \n",
      "\n",
      "Epoch:  17860  Learning Rate:  1.7536174066370493e-09  Varinance:  3.5927435832195553e-10 \n",
      "\n",
      "Epoch:  17861  Learning Rate:  1.751864665746923e-09  Varinance:  3.587724652123322e-10 \n",
      "\n",
      "Epoch:  17862  Learning Rate:  1.7501136767216023e-09  Varinance:  3.582712732289852e-10 \n",
      "\n",
      "Epoch:  17863  Learning Rate:  1.7483644378101103e-09  Varinance:  3.577707813924681e-10 \n",
      "\n",
      "Epoch:  17864  Learning Rate:  1.7466169472631958e-09  Varinance:  3.5727098872470144e-10 \n",
      "\n",
      "Epoch:  17865  Learning Rate:  1.7448712033333743e-09  Varinance:  3.567718942489721e-10 \n",
      "\n",
      "Epoch:  17866  Learning Rate:  1.7431272042749071e-09  Varinance:  3.5627349698993135e-10 \n",
      "\n",
      "Epoch:  17867  Learning Rate:  1.741384948343784e-09  Varinance:  3.557757959735944e-10 \n",
      "\n",
      "Epoch:  17868  Learning Rate:  1.7396444337977538e-09  Varinance:  3.5527879022733304e-10 \n",
      "\n",
      "Epoch:  17869  Learning Rate:  1.7379056588963086e-09  Varinance:  3.547824787798819e-10 \n",
      "\n",
      "Epoch:  17870  Learning Rate:  1.7361686219006611e-09  Varinance:  3.5428686066133084e-10 \n",
      "\n",
      "Epoch:  17871  Learning Rate:  1.7344333210737804e-09  Varinance:  3.537919349031249e-10 \n",
      "\n",
      "Epoch:  17872  Learning Rate:  1.732699754680371e-09  Varinance:  3.5329770053806196e-10 \n",
      "\n",
      "Epoch:  17873  Learning Rate:  1.7309679209868547e-09  Varinance:  3.5280415660029123e-10 \n",
      "\n",
      "Epoch:  17874  Learning Rate:  1.729237818261404e-09  Varinance:  3.5231130212531237e-10 \n",
      "\n",
      "Epoch:  17875  Learning Rate:  1.7275094447739213e-09  Varinance:  3.518191361499685e-10 \n",
      "\n",
      "Epoch:  17876  Learning Rate:  1.7257827987960215e-09  Varinance:  3.513276577124522e-10 \n",
      "\n",
      "Epoch:  17877  Learning Rate:  1.7240578786010639e-09  Varinance:  3.5083686585229826e-10 \n",
      "\n",
      "Epoch:  17878  Learning Rate:  1.7223346824641354e-09  Varinance:  3.503467596103834e-10 \n",
      "\n",
      "Epoch:  17879  Learning Rate:  1.7206132086620262e-09  Varinance:  3.4985733802892394e-10 \n",
      "\n",
      "Epoch:  17880  Learning Rate:  1.7188934554732691e-09  Varinance:  3.4936860015147437e-10 \n",
      "\n",
      "Epoch:  17881  Learning Rate:  1.717175421178117e-09  Varinance:  3.4888054502292654e-10 \n",
      "\n",
      "Epoch:  17882  Learning Rate:  1.715459104058523e-09  Varinance:  3.483931716895027e-10 \n",
      "\n",
      "Epoch:  17883  Learning Rate:  1.713744502398176e-09  Varinance:  3.479064791987613e-10 \n",
      "\n",
      "Epoch:  17884  Learning Rate:  1.7120316144824802e-09  Varinance:  3.474204665995901e-10 \n",
      "\n",
      "Epoch:  17885  Learning Rate:  1.7103204385985358e-09  Varinance:  3.4693513294220556e-10 \n",
      "\n",
      "Epoch:  17886  Learning Rate:  1.7086109730351725e-09  Varinance:  3.464504772781507e-10 \n",
      "\n",
      "Epoch:  17887  Learning Rate:  1.7069032160829303e-09  Varinance:  3.459664986602939e-10 \n",
      "\n",
      "Epoch:  17888  Learning Rate:  1.7051971660340404e-09  Varinance:  3.4548319614282756e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17889  Learning Rate:  1.703492821182459e-09  Varinance:  3.4500056878126176e-10 \n",
      "\n",
      "Epoch:  17890  Learning Rate:  1.7017901798238467e-09  Varinance:  3.445186156324297e-10 \n",
      "\n",
      "Epoch:  17891  Learning Rate:  1.7000892402555496e-09  Varinance:  3.440373357544809e-10 \n",
      "\n",
      "Epoch:  17892  Learning Rate:  1.6983900007766409e-09  Varinance:  3.4355672820688064e-10 \n",
      "\n",
      "Epoch:  17893  Learning Rate:  1.6966924596878682e-09  Varinance:  3.43076792050408e-10 \n",
      "\n",
      "Epoch:  17894  Learning Rate:  1.694996615291697e-09  Varinance:  3.425975263471543e-10 \n",
      "\n",
      "Epoch:  17895  Learning Rate:  1.693302465892288e-09  Varinance:  3.4211893016052204e-10 \n",
      "\n",
      "Epoch:  17896  Learning Rate:  1.6916100097954798e-09  Varinance:  3.416410025552187e-10 \n",
      "\n",
      "Epoch:  17897  Learning Rate:  1.6899192453088226e-09  Varinance:  3.4116374259726187e-10 \n",
      "\n",
      "Epoch:  17898  Learning Rate:  1.6882301707415577e-09  Varinance:  3.406871493539726e-10 \n",
      "\n",
      "Epoch:  17899  Learning Rate:  1.6865427844045982e-09  Varinance:  3.4021122189397514e-10 \n",
      "\n",
      "Epoch:  17900  Learning Rate:  1.6848570846105634e-09  Varinance:  3.3973595928719443e-10 \n",
      "\n",
      "Epoch:  17901  Learning Rate:  1.6831730696737597e-09  Varinance:  3.3926136060485497e-10 \n",
      "\n",
      "Epoch:  17902  Learning Rate:  1.68149073791016e-09  Varinance:  3.3878742491947995e-10 \n",
      "\n",
      "Epoch:  17903  Learning Rate:  1.6798100876374382e-09  Varinance:  3.383141513048843e-10 \n",
      "\n",
      "Epoch:  17904  Learning Rate:  1.6781311171749502e-09  Varinance:  3.378415388361808e-10 \n",
      "\n",
      "Epoch:  17905  Learning Rate:  1.6764538248437132e-09  Varinance:  3.3736958658977275e-10 \n",
      "\n",
      "Epoch:  17906  Learning Rate:  1.6747782089664409e-09  Varinance:  3.3689829364335385e-10 \n",
      "\n",
      "Epoch:  17907  Learning Rate:  1.673104267867523e-09  Varinance:  3.364276590759062e-10 \n",
      "\n",
      "Epoch:  17908  Learning Rate:  1.6714319998730064e-09  Varinance:  3.3595768196769853e-10 \n",
      "\n",
      "Epoch:  17909  Learning Rate:  1.6697614033106288e-09  Varinance:  3.354883614002855e-10 \n",
      "\n",
      "Epoch:  17910  Learning Rate:  1.6680924765097996e-09  Varinance:  3.350196964565013e-10 \n",
      "\n",
      "Epoch:  17911  Learning Rate:  1.66642521780158e-09  Varinance:  3.3455168622046504e-10 \n",
      "\n",
      "Epoch:  17912  Learning Rate:  1.6647596255187173e-09  Varinance:  3.340843297775739e-10 \n",
      "\n",
      "Epoch:  17913  Learning Rate:  1.6630956979956244e-09  Varinance:  3.3361762621450287e-10 \n",
      "\n",
      "Epoch:  17914  Learning Rate:  1.6614334335683628e-09  Varinance:  3.3315157461920277e-10 \n",
      "\n",
      "Epoch:  17915  Learning Rate:  1.6597728305746727e-09  Varinance:  3.3268617408089846e-10 \n",
      "\n",
      "Epoch:  17916  Learning Rate:  1.6581138873539575e-09  Varinance:  3.322214236900884e-10 \n",
      "\n",
      "Epoch:  17917  Learning Rate:  1.656456602247262e-09  Varinance:  3.3175732253853805e-10 \n",
      "\n",
      "Epoch:  17918  Learning Rate:  1.6548009735973068e-09  Varinance:  3.312938697192851e-10 \n",
      "\n",
      "Epoch:  17919  Learning Rate:  1.653146999748469e-09  Varinance:  3.308310643266332e-10 \n",
      "\n",
      "Epoch:  17920  Learning Rate:  1.6514946790467627e-09  Varinance:  3.303689054561509e-10 \n",
      "\n",
      "Epoch:  17921  Learning Rate:  1.6498440098398732e-09  Varinance:  3.2990739220467064e-10 \n",
      "\n",
      "Epoch:  17922  Learning Rate:  1.6481949904771364e-09  Varinance:  3.294465236702862e-10 \n",
      "\n",
      "Epoch:  17923  Learning Rate:  1.6465476193095222e-09  Varinance:  3.2898629895235267e-10 \n",
      "\n",
      "Epoch:  17924  Learning Rate:  1.6449018946896702e-09  Varinance:  3.285267171514797e-10 \n",
      "\n",
      "Epoch:  17925  Learning Rate:  1.643257814971844e-09  Varinance:  3.280677773695369e-10 \n",
      "\n",
      "Epoch:  17926  Learning Rate:  1.64161537851197e-09  Varinance:  3.2760947870964734e-10 \n",
      "\n",
      "Epoch:  17927  Learning Rate:  1.6399745836676167e-09  Varinance:  3.27151820276187e-10 \n",
      "\n",
      "Epoch:  17928  Learning Rate:  1.6383354287979784e-09  Varinance:  3.2669480117478317e-10 \n",
      "\n",
      "Epoch:  17929  Learning Rate:  1.636697912263905e-09  Varinance:  3.262384205123122e-10 \n",
      "\n",
      "Epoch:  17930  Learning Rate:  1.6350620324278862e-09  Varinance:  3.2578267739689956e-10 \n",
      "\n",
      "Epoch:  17931  Learning Rate:  1.6334277876540303e-09  Varinance:  3.2532757093791304e-10 \n",
      "\n",
      "Epoch:  17932  Learning Rate:  1.631795176308098e-09  Varinance:  3.248731002459681e-10 \n",
      "\n",
      "Epoch:  17933  Learning Rate:  1.6301641967574842e-09  Varinance:  3.244192644329215e-10 \n",
      "\n",
      "Epoch:  17934  Learning Rate:  1.628534847371197e-09  Varinance:  3.239660626118707e-10 \n",
      "\n",
      "Epoch:  17935  Learning Rate:  1.6269071265198927e-09  Varinance:  3.2351349389715215e-10 \n",
      "\n",
      "Epoch:  17936  Learning Rate:  1.6252810325758564e-09  Varinance:  3.2306155740433944e-10 \n",
      "\n",
      "Epoch:  17937  Learning Rate:  1.6236565639129826e-09  Varinance:  3.226102522502429e-10 \n",
      "\n",
      "Epoch:  17938  Learning Rate:  1.6220337189068079e-09  Varinance:  3.2215957755290325e-10 \n",
      "\n",
      "Epoch:  17939  Learning Rate:  1.6204124959344929e-09  Varinance:  3.2170953243159656e-10 \n",
      "\n",
      "Epoch:  17940  Learning Rate:  1.6187928933748032e-09  Varinance:  3.212601160068283e-10 \n",
      "\n",
      "Epoch:  17941  Learning Rate:  1.6171749096081417e-09  Varinance:  3.208113274003324e-10 \n",
      "\n",
      "Epoch:  17942  Learning Rate:  1.6155585430165303e-09  Varinance:  3.203631657350697e-10 \n",
      "\n",
      "Epoch:  17943  Learning Rate:  1.6139437919835907e-09  Varinance:  3.1991563013522634e-10 \n",
      "\n",
      "Epoch:  17944  Learning Rate:  1.6123306548945778e-09  Varinance:  3.19468719726213e-10 \n",
      "\n",
      "Epoch:  17945  Learning Rate:  1.6107191301363597e-09  Varinance:  3.190224336346587e-10 \n",
      "\n",
      "Epoch:  17946  Learning Rate:  1.6091092160974004e-09  Varinance:  3.1857677098841604e-10 \n",
      "\n",
      "Epoch:  17947  Learning Rate:  1.6075009111677913e-09  Varinance:  3.181317309165547e-10 \n",
      "\n",
      "Epoch:  17948  Learning Rate:  1.6058942137392328e-09  Varinance:  3.176873125493611e-10 \n",
      "\n",
      "Epoch:  17949  Learning Rate:  1.6042891222050167e-09  Varinance:  3.1724351501833666e-10 \n",
      "\n",
      "Epoch:  17950  Learning Rate:  1.602685634960056e-09  Varinance:  3.168003374561959e-10 \n",
      "\n",
      "Epoch:  17951  Learning Rate:  1.6010837504008697e-09  Varinance:  3.163577789968663e-10 \n",
      "\n",
      "Epoch:  17952  Learning Rate:  1.5994834669255615e-09  Varinance:  3.1591583877548153e-10 \n",
      "\n",
      "Epoch:  17953  Learning Rate:  1.5978847829338534e-09  Varinance:  3.1547451592838704e-10 \n",
      "\n",
      "Epoch:  17954  Learning Rate:  1.5962876968270673e-09  Varinance:  3.150338095931336e-10 \n",
      "\n",
      "Epoch:  17955  Learning Rate:  1.594692207008105e-09  Varinance:  3.1459371890847663e-10 \n",
      "\n",
      "Epoch:  17956  Learning Rate:  1.5930983118814888e-09  Varinance:  3.14154243014375e-10 \n",
      "\n",
      "Epoch:  17957  Learning Rate:  1.591506009853311e-09  Varinance:  3.1371538105198873e-10 \n",
      "\n",
      "Epoch:  17958  Learning Rate:  1.589915299331276e-09  Varinance:  3.132771321636788e-10 \n",
      "\n",
      "Epoch:  17959  Learning Rate:  1.5883261787246786e-09  Varinance:  3.1283949549300104e-10 \n",
      "\n",
      "Epoch:  17960  Learning Rate:  1.5867386464443865e-09  Varinance:  3.12402470184711e-10 \n",
      "\n",
      "Epoch:  17961  Learning Rate:  1.585152700902873e-09  Varinance:  3.1196605538475775e-10 \n",
      "\n",
      "Epoch:  17962  Learning Rate:  1.5835683405141982e-09  Varinance:  3.115302502402835e-10 \n",
      "\n",
      "Epoch:  17963  Learning Rate:  1.58198556369399e-09  Varinance:  3.11095053899622e-10 \n",
      "\n",
      "Epoch:  17964  Learning Rate:  1.5804043688594779e-09  Varinance:  3.1066046551229657e-10 \n",
      "\n",
      "Epoch:  17965  Learning Rate:  1.5788247544294715e-09  Varinance:  3.1022648422901983e-10 \n",
      "\n",
      "Epoch:  17966  Learning Rate:  1.5772467188243455e-09  Varinance:  3.097931092016874e-10 \n",
      "\n",
      "Epoch:  17967  Learning Rate:  1.5756702604660699e-09  Varinance:  3.093603395833831e-10 \n",
      "\n",
      "Epoch:  17968  Learning Rate:  1.5740953777781917e-09  Varinance:  3.0892817452837276e-10 \n",
      "\n",
      "Epoch:  17969  Learning Rate:  1.5725220691858167e-09  Varinance:  3.0849661319210354e-10 \n",
      "\n",
      "Epoch:  17970  Learning Rate:  1.5709503331156422e-09  Varinance:  3.0806565473120247e-10 \n",
      "\n",
      "Epoch:  17971  Learning Rate:  1.5693801679959374e-09  Varinance:  3.0763529830347483e-10 \n",
      "\n",
      "Epoch:  17972  Learning Rate:  1.5678115722565255e-09  Varinance:  3.072055430679034e-10 \n",
      "\n",
      "Epoch:  17973  Learning Rate:  1.5662445443288164e-09  Varinance:  3.0677638818464254e-10 \n",
      "\n",
      "Epoch:  17974  Learning Rate:  1.564679082645788e-09  Varinance:  3.0634783281502325e-10 \n",
      "\n",
      "Epoch:  17975  Learning Rate:  1.5631151856419668e-09  Varinance:  3.0591987612154697e-10 \n",
      "\n",
      "Epoch:  17976  Learning Rate:  1.5615528517534617e-09  Varinance:  3.05492517267885e-10 \n",
      "\n",
      "Epoch:  17977  Learning Rate:  1.559992079417944e-09  Varinance:  3.05065755418877e-10 \n",
      "\n",
      "Epoch:  17978  Learning Rate:  1.5584328670746302e-09  Varinance:  3.0463958974052945e-10 \n",
      "\n",
      "Epoch:  17979  Learning Rate:  1.5568752131643133e-09  Varinance:  3.0421401940001483e-10 \n",
      "\n",
      "Epoch:  17980  Learning Rate:  1.555319116129345e-09  Varinance:  3.0378904356566585e-10 \n",
      "\n",
      "Epoch:  17981  Learning Rate:  1.5537645744136167e-09  Varinance:  3.0336466140698025e-10 \n",
      "\n",
      "Epoch:  17982  Learning Rate:  1.5522115864625923e-09  Varinance:  3.0294087209461486e-10 \n",
      "\n",
      "Epoch:  17983  Learning Rate:  1.5506601507232894e-09  Varinance:  3.025176748003852e-10 \n",
      "\n",
      "Epoch:  17984  Learning Rate:  1.5491102656442607e-09  Varinance:  3.0209506869726357e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17985  Learning Rate:  1.5475619296756266e-09  Varinance:  3.0167305295937765e-10 \n",
      "\n",
      "Epoch:  17986  Learning Rate:  1.5460151412690573e-09  Varinance:  3.0125162676200996e-10 \n",
      "\n",
      "Epoch:  17987  Learning Rate:  1.544469898877752e-09  Varinance:  3.008307892815918e-10 \n",
      "\n",
      "Epoch:  17988  Learning Rate:  1.54292620095648e-09  Varinance:  3.004105396957083e-10 \n",
      "\n",
      "Epoch:  17989  Learning Rate:  1.5413840459615321e-09  Varinance:  2.9999087718309237e-10 \n",
      "\n",
      "Epoch:  17990  Learning Rate:  1.5398434323507585e-09  Varinance:  2.9957180092362407e-10 \n",
      "\n",
      "Epoch:  17991  Learning Rate:  1.538304358583551e-09  Varinance:  2.991533100983293e-10 \n",
      "\n",
      "Epoch:  17992  Learning Rate:  1.536766823120825e-09  Varinance:  2.98735403889378e-10 \n",
      "\n",
      "Epoch:  17993  Learning Rate:  1.53523082442505e-09  Varinance:  2.9831808148008356e-10 \n",
      "\n",
      "Epoch:  17994  Learning Rate:  1.533696360960233e-09  Varinance:  2.979013420548972e-10 \n",
      "\n",
      "Epoch:  17995  Learning Rate:  1.5321634311918992e-09  Varinance:  2.974851847994126e-10 \n",
      "\n",
      "Epoch:  17996  Learning Rate:  1.5306320335871244e-09  Varinance:  2.970696089003599e-10 \n",
      "\n",
      "Epoch:  17997  Learning Rate:  1.529102166614516e-09  Varinance:  2.9665461354560554e-10 \n",
      "\n",
      "Epoch:  17998  Learning Rate:  1.5275738287441961e-09  Varinance:  2.9624019792415036e-10 \n",
      "\n",
      "Epoch:  17999  Learning Rate:  1.5260470184478325e-09  Varinance:  2.958263612261294e-10 \n",
      "\n",
      "Epoch:  18000  Learning Rate:  1.5245217341986196e-09  Varinance:  2.9541310264280553e-10 \n",
      "\n",
      "Epoch:  18001  Learning Rate:  1.522997974471263e-09  Varinance:  2.950004213665749e-10 \n",
      "\n",
      "Epoch:  18002  Learning Rate:  1.5214757377420075e-09  Varinance:  2.945883165909606e-10 \n",
      "\n",
      "Epoch:  18003  Learning Rate:  1.5199550224886218e-09  Varinance:  2.9417678751061244e-10 \n",
      "\n",
      "Epoch:  18004  Learning Rate:  1.5184358271903801e-09  Varinance:  2.9376583332130515e-10 \n",
      "\n",
      "Epoch:  18005  Learning Rate:  1.516918150328092e-09  Varinance:  2.933554532199372e-10 \n",
      "\n",
      "Epoch:  18006  Learning Rate:  1.5154019903840864e-09  Varinance:  2.929456464045296e-10 \n",
      "\n",
      "Epoch:  18007  Learning Rate:  1.5138873458421918e-09  Varinance:  2.92536412074221e-10 \n",
      "\n",
      "Epoch:  18008  Learning Rate:  1.512374215187769e-09  Varinance:  2.9212774942927155e-10 \n",
      "\n",
      "Epoch:  18009  Learning Rate:  1.5108625969076931e-09  Varinance:  2.917196576710579e-10 \n",
      "\n",
      "Epoch:  18010  Learning Rate:  1.5093524894903346e-09  Varinance:  2.9131213600207213e-10 \n",
      "\n",
      "Epoch:  18011  Learning Rate:  1.5078438914255914e-09  Varinance:  2.909051836259205e-10 \n",
      "\n",
      "Epoch:  18012  Learning Rate:  1.5063368012048703e-09  Varinance:  2.904987997473218e-10 \n",
      "\n",
      "Epoch:  18013  Learning Rate:  1.5048312173210709e-09  Varinance:  2.9009298357210686e-10 \n",
      "\n",
      "Epoch:  18014  Learning Rate:  1.503327138268614e-09  Varinance:  2.8968773430721277e-10 \n",
      "\n",
      "Epoch:  18015  Learning Rate:  1.501824562543426e-09  Varinance:  2.8928305116068753e-10 \n",
      "\n",
      "Epoch:  18016  Learning Rate:  1.5003234886429205e-09  Varinance:  2.8887893334168465e-10 \n",
      "\n",
      "Epoch:  18017  Learning Rate:  1.498823915066034e-09  Varinance:  2.8847538006046227e-10 \n",
      "\n",
      "Epoch:  18018  Learning Rate:  1.4973258403131823e-09  Varinance:  2.8807239052838174e-10 \n",
      "\n",
      "Epoch:  18019  Learning Rate:  1.4958292628862954e-09  Varinance:  2.8766996395790623e-10 \n",
      "\n",
      "Epoch:  18020  Learning Rate:  1.4943341812888013e-09  Varinance:  2.872680995626001e-10 \n",
      "\n",
      "Epoch:  18021  Learning Rate:  1.492840594025608e-09  Varinance:  2.868667965571231e-10 \n",
      "\n",
      "Epoch:  18022  Learning Rate:  1.4913484996031332e-09  Varinance:  2.8646605415723525e-10 \n",
      "\n",
      "Epoch:  18023  Learning Rate:  1.4898578965292873e-09  Varinance:  2.8606587157979113e-10 \n",
      "\n",
      "Epoch:  18024  Learning Rate:  1.4883687833134569e-09  Varinance:  2.856662480427393e-10 \n",
      "\n",
      "Epoch:  18025  Learning Rate:  1.4868811584665338e-09  Varinance:  2.852671827651208e-10 \n",
      "\n",
      "Epoch:  18026  Learning Rate:  1.4853950205008984e-09  Varinance:  2.848686749670676e-10 \n",
      "\n",
      "Epoch:  18027  Learning Rate:  1.4839103679304019e-09  Varinance:  2.8447072386980237e-10 \n",
      "\n",
      "Epoch:  18028  Learning Rate:  1.482427199270397e-09  Varinance:  2.8407332869563236e-10 \n",
      "\n",
      "Epoch:  18029  Learning Rate:  1.4809455130377205e-09  Varinance:  2.836764886679544e-10 \n",
      "\n",
      "Epoch:  18030  Learning Rate:  1.4794653077506747e-09  Varinance:  2.832802030112492e-10 \n",
      "\n",
      "Epoch:  18031  Learning Rate:  1.4779865819290603e-09  Varinance:  2.828844709510809e-10 \n",
      "\n",
      "Epoch:  18032  Learning Rate:  1.476509334094156e-09  Varinance:  2.824892917140953e-10 \n",
      "\n",
      "Epoch:  18033  Learning Rate:  1.475033562768704e-09  Varinance:  2.820946645280188e-10 \n",
      "\n",
      "Epoch:  18034  Learning Rate:  1.4735592664769371e-09  Varinance:  2.817005886216575e-10 \n",
      "\n",
      "Epoch:  18035  Learning Rate:  1.472086443744565e-09  Varinance:  2.813070632248917e-10 \n",
      "\n",
      "Epoch:  18036  Learning Rate:  1.4706150930987543e-09  Varinance:  2.809140875686809e-10 \n",
      "\n",
      "Epoch:  18037  Learning Rate:  1.4691452130681589e-09  Varinance:  2.805216608850575e-10 \n",
      "\n",
      "Epoch:  18038  Learning Rate:  1.4676768021829045e-09  Varinance:  2.80129782407127e-10 \n",
      "\n",
      "Epoch:  18039  Learning Rate:  1.4662098589745693e-09  Varinance:  2.7973845136906615e-10 \n",
      "\n",
      "Epoch:  18040  Learning Rate:  1.464744381976215e-09  Varinance:  2.793476670061215e-10 \n",
      "\n",
      "Epoch:  18041  Learning Rate:  1.4632803697223702e-09  Varinance:  2.7895742855460896e-10 \n",
      "\n",
      "Epoch:  18042  Learning Rate:  1.4618178207490117e-09  Varinance:  2.785677352519083e-10 \n",
      "\n",
      "Epoch:  18043  Learning Rate:  1.460356733593596e-09  Varinance:  2.781785863364675e-10 \n",
      "\n",
      "Epoch:  18044  Learning Rate:  1.4588971067950407e-09  Varinance:  2.7778998104779764e-10 \n",
      "\n",
      "Epoch:  18045  Learning Rate:  1.4574389388937085e-09  Varinance:  2.77401918626472e-10 \n",
      "\n",
      "Epoch:  18046  Learning Rate:  1.4559822284314368e-09  Varinance:  2.7701439831412476e-10 \n",
      "\n",
      "Epoch:  18047  Learning Rate:  1.4545269739515198e-09  Varinance:  2.766274193534496e-10 \n",
      "\n",
      "Epoch:  18048  Learning Rate:  1.453073173998693e-09  Varinance:  2.7624098098819906e-10 \n",
      "\n",
      "Epoch:  18049  Learning Rate:  1.4516208271191663e-09  Varinance:  2.758550824631791e-10 \n",
      "\n",
      "Epoch:  18050  Learning Rate:  1.4501699318605826e-09  Varinance:  2.7546972302425387e-10 \n",
      "\n",
      "Epoch:  18051  Learning Rate:  1.4487204867720516e-09  Varinance:  2.7508490191833964e-10 \n",
      "\n",
      "Epoch:  18052  Learning Rate:  1.4472724904041332e-09  Varinance:  2.7470061839340506e-10 \n",
      "\n",
      "Epoch:  18053  Learning Rate:  1.4458259413088207e-09  Varinance:  2.743168716984692e-10 \n",
      "\n",
      "Epoch:  18054  Learning Rate:  1.44438083803957e-09  Varinance:  2.7393366108360016e-10 \n",
      "\n",
      "Epoch:  18055  Learning Rate:  1.442937179151283e-09  Varinance:  2.735509857999147e-10 \n",
      "\n",
      "Epoch:  18056  Learning Rate:  1.44149496320029e-09  Varinance:  2.731688450995729e-10 \n",
      "\n",
      "Epoch:  18057  Learning Rate:  1.4400541887443804e-09  Varinance:  2.727872382357824e-10 \n",
      "\n",
      "Epoch:  18058  Learning Rate:  1.4386148543427846e-09  Varinance:  2.724061644627931e-10 \n",
      "\n",
      "Epoch:  18059  Learning Rate:  1.437176958556158e-09  Varinance:  2.7202562303589664e-10 \n",
      "\n",
      "Epoch:  18060  Learning Rate:  1.4357404999466098e-09  Varinance:  2.716456132114251e-10 \n",
      "\n",
      "Epoch:  18061  Learning Rate:  1.4343054770776864e-09  Varinance:  2.7126613424674936e-10 \n",
      "\n",
      "Epoch:  18062  Learning Rate:  1.4328718885143542e-09  Varinance:  2.7088718540027865e-10 \n",
      "\n",
      "Epoch:  18063  Learning Rate:  1.4314397328230301e-09  Varinance:  2.705087659314555e-10 \n",
      "\n",
      "Epoch:  18064  Learning Rate:  1.4300090085715631e-09  Varinance:  2.7013087510075944e-10 \n",
      "\n",
      "Epoch:  18065  Learning Rate:  1.4285797143292189e-09  Varinance:  2.697535121697026e-10 \n",
      "\n",
      "Epoch:  18066  Learning Rate:  1.4271518486667078e-09  Varinance:  2.693766764008285e-10 \n",
      "\n",
      "Epoch:  18067  Learning Rate:  1.4257254101561699e-09  Varinance:  2.6900036705771085e-10 \n",
      "\n",
      "Epoch:  18068  Learning Rate:  1.4243003973711553e-09  Varinance:  2.686245834049522e-10 \n",
      "\n",
      "Epoch:  18069  Learning Rate:  1.422876808886657e-09  Varinance:  2.6824932470818326e-10 \n",
      "\n",
      "Epoch:  18070  Learning Rate:  1.4214546432790915e-09  Varinance:  2.6787459023405794e-10 \n",
      "\n",
      "Epoch:  18071  Learning Rate:  1.4200338991262823e-09  Varinance:  2.6750037925025734e-10 \n",
      "\n",
      "Epoch:  18072  Learning Rate:  1.4186145750074907e-09  Varinance:  2.671266910254847e-10 \n",
      "\n",
      "Epoch:  18073  Learning Rate:  1.4171966695033972e-09  Varinance:  2.6675352482946474e-10 \n",
      "\n",
      "Epoch:  18074  Learning Rate:  1.4157801811960864e-09  Varinance:  2.663808799329425e-10 \n",
      "\n",
      "Epoch:  18075  Learning Rate:  1.414365108669075e-09  Varinance:  2.660087556076816e-10 \n",
      "\n",
      "Epoch:  18076  Learning Rate:  1.412951450507295e-09  Varinance:  2.6563715112646417e-10 \n",
      "\n",
      "Epoch:  18077  Learning Rate:  1.4115392052970781e-09  Varinance:  2.6526606576308513e-10 \n",
      "\n",
      "Epoch:  18078  Learning Rate:  1.410128371626184e-09  Varinance:  2.6489549879235684e-10 \n",
      "\n",
      "Epoch:  18079  Learning Rate:  1.4087189480837844e-09  Varinance:  2.6452544949010376e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18080  Learning Rate:  1.407310933260445e-09  Varinance:  2.64155917133162e-10 \n",
      "\n",
      "Epoch:  18081  Learning Rate:  1.4059043257481615e-09  Varinance:  2.6378690099937803e-10 \n",
      "\n",
      "Epoch:  18082  Learning Rate:  1.4044991241403157e-09  Varinance:  2.634184003676069e-10 \n",
      "\n",
      "Epoch:  18083  Learning Rate:  1.4030953270317109e-09  Varinance:  2.630504145177121e-10 \n",
      "\n",
      "Epoch:  18084  Learning Rate:  1.4016929330185553e-09  Varinance:  2.6268294273056046e-10 \n",
      "\n",
      "Epoch:  18085  Learning Rate:  1.4002919406984445e-09  Varinance:  2.62315984288026e-10 \n",
      "\n",
      "Epoch:  18086  Learning Rate:  1.398892348670391e-09  Varinance:  2.619495384729852e-10 \n",
      "\n",
      "Epoch:  18087  Learning Rate:  1.3974941555348077e-09  Varinance:  2.615836045693162e-10 \n",
      "\n",
      "Epoch:  18088  Learning Rate:  1.3960973598934914e-09  Varinance:  2.6121818186189754e-10 \n",
      "\n",
      "Epoch:  18089  Learning Rate:  1.3947019603496515e-09  Varinance:  2.608532696366068e-10 \n",
      "\n",
      "Epoch:  18090  Learning Rate:  1.3933079555078931e-09  Varinance:  2.604888671803202e-10 \n",
      "\n",
      "Epoch:  18091  Learning Rate:  1.3919153439742013e-09  Varinance:  2.601249737809071e-10 \n",
      "\n",
      "Epoch:  18092  Learning Rate:  1.3905241243559695e-09  Varinance:  2.597615887272348e-10 \n",
      "\n",
      "Epoch:  18093  Learning Rate:  1.389134295261983e-09  Varinance:  2.5939871130916283e-10 \n",
      "\n",
      "Epoch:  18094  Learning Rate:  1.387745855302402e-09  Varinance:  2.5903634081754287e-10 \n",
      "\n",
      "Epoch:  18095  Learning Rate:  1.3863588030887927e-09  Varinance:  2.5867447654421724e-10 \n",
      "\n",
      "Epoch:  18096  Learning Rate:  1.3849731372341065e-09  Varinance:  2.583131177820176e-10 \n",
      "\n",
      "Epoch:  18097  Learning Rate:  1.3835888563526683e-09  Varinance:  2.5795226382476436e-10 \n",
      "\n",
      "Epoch:  18098  Learning Rate:  1.3822059590602016e-09  Varinance:  2.575919139672616e-10 \n",
      "\n",
      "Epoch:  18099  Learning Rate:  1.380824443973814e-09  Varinance:  2.5723206750530136e-10 \n",
      "\n",
      "Epoch:  18100  Learning Rate:  1.3794443097119806e-09  Varinance:  2.5687272373565857e-10 \n",
      "\n",
      "Epoch:  18101  Learning Rate:  1.3780655548945718e-09  Varinance:  2.565138819560905e-10 \n",
      "\n",
      "Epoch:  18102  Learning Rate:  1.3766881781428378e-09  Varinance:  2.5615554146533537e-10 \n",
      "\n",
      "Epoch:  18103  Learning Rate:  1.3753121780793915e-09  Varinance:  2.5579770156311113e-10 \n",
      "\n",
      "Epoch:  18104  Learning Rate:  1.3739375533282383e-09  Varinance:  2.554403615501148e-10 \n",
      "\n",
      "Epoch:  18105  Learning Rate:  1.3725643025147575e-09  Varinance:  2.550835207280177e-10 \n",
      "\n",
      "Epoch:  18106  Learning Rate:  1.3711924242656887e-09  Varinance:  2.547271783994693e-10 \n",
      "\n",
      "Epoch:  18107  Learning Rate:  1.3698219172091586e-09  Varinance:  2.5437133386809243e-10 \n",
      "\n",
      "Epoch:  18108  Learning Rate:  1.3684527799746646e-09  Varinance:  2.540159864384826e-10 \n",
      "\n",
      "Epoch:  18109  Learning Rate:  1.3670850111930597e-09  Varinance:  2.53661135416207e-10 \n",
      "\n",
      "Epoch:  18110  Learning Rate:  1.3657186094965803e-09  Varinance:  2.533067801078027e-10 \n",
      "\n",
      "Epoch:  18111  Learning Rate:  1.3643535735188285e-09  Varinance:  2.5295291982077635e-10 \n",
      "\n",
      "Epoch:  18112  Learning Rate:  1.3629899018947594e-09  Varinance:  2.525995538635997e-10 \n",
      "\n",
      "Epoch:  18113  Learning Rate:  1.3616275932607106e-09  Varinance:  2.5224668154571284e-10 \n",
      "\n",
      "Epoch:  18114  Learning Rate:  1.3602666462543637e-09  Varinance:  2.5189430217751974e-10 \n",
      "\n",
      "Epoch:  18115  Learning Rate:  1.3589070595147764e-09  Varinance:  2.5154241507038784e-10 \n",
      "\n",
      "Epoch:  18116  Learning Rate:  1.3575488316823666e-09  Varinance:  2.511910195366464e-10 \n",
      "\n",
      "Epoch:  18117  Learning Rate:  1.356191961398897e-09  Varinance:  2.508401148895854e-10 \n",
      "\n",
      "Epoch:  18118  Learning Rate:  1.3548364473075014e-09  Varinance:  2.504897004434551e-10 \n",
      "\n",
      "Epoch:  18119  Learning Rate:  1.353482288052671e-09  Varinance:  2.50139775513461e-10 \n",
      "\n",
      "Epoch:  18120  Learning Rate:  1.3521294822802368e-09  Varinance:  2.497903394157678e-10 \n",
      "\n",
      "Epoch:  18121  Learning Rate:  1.3507780286373977e-09  Varinance:  2.494413914674947e-10 \n",
      "\n",
      "Epoch:  18122  Learning Rate:  1.349427925772704e-09  Varinance:  2.49092930986715e-10 \n",
      "\n",
      "Epoch:  18123  Learning Rate:  1.3480791723360443e-09  Varinance:  2.487449572924543e-10 \n",
      "\n",
      "Epoch:  18124  Learning Rate:  1.346731766978669e-09  Varinance:  2.4839746970468974e-10 \n",
      "\n",
      "Epoch:  18125  Learning Rate:  1.3453857083531779e-09  Varinance:  2.480504675443493e-10 \n",
      "\n",
      "Epoch:  18126  Learning Rate:  1.3440409951135022e-09  Varinance:  2.477039501333068e-10 \n",
      "\n",
      "Epoch:  18127  Learning Rate:  1.3426976259149337e-09  Varinance:  2.4735791679438623e-10 \n",
      "\n",
      "Epoch:  18128  Learning Rate:  1.341355599414108e-09  Varinance:  2.4701236685135646e-10 \n",
      "\n",
      "Epoch:  18129  Learning Rate:  1.3400149142689885e-09  Varinance:  2.4666729962893123e-10 \n",
      "\n",
      "Epoch:  18130  Learning Rate:  1.3386755691388951e-09  Varinance:  2.463227144527676e-10 \n",
      "\n",
      "Epoch:  18131  Learning Rate:  1.337337562684487e-09  Varinance:  2.4597861064946533e-10 \n",
      "\n",
      "Epoch:  18132  Learning Rate:  1.3360008935677482e-09  Varinance:  2.456349875465625e-10 \n",
      "\n",
      "Epoch:  18133  Learning Rate:  1.3346655604520145e-09  Varinance:  2.452918444725392e-10 \n",
      "\n",
      "Epoch:  18134  Learning Rate:  1.3333315620019573e-09  Varinance:  2.449491807568127e-10 \n",
      "\n",
      "Epoch:  18135  Learning Rate:  1.3319988968835683e-09  Varinance:  2.446069957297369e-10 \n",
      "\n",
      "Epoch:  18136  Learning Rate:  1.3306675637641873e-09  Varinance:  2.4426528872260135e-10 \n",
      "\n",
      "Epoch:  18137  Learning Rate:  1.3293375613124856e-09  Varinance:  2.4392405906762974e-10 \n",
      "\n",
      "Epoch:  18138  Learning Rate:  1.3280088881984512e-09  Varinance:  2.435833060979795e-10 \n",
      "\n",
      "Epoch:  18139  Learning Rate:  1.3266815430934157e-09  Varinance:  2.4324302914773693e-10 \n",
      "\n",
      "Epoch:  18140  Learning Rate:  1.3253555246700386e-09  Varinance:  2.4290322755192126e-10 \n",
      "\n",
      "Epoch:  18141  Learning Rate:  1.324030831602292e-09  Varinance:  2.425639006464797e-10 \n",
      "\n",
      "Epoch:  18142  Learning Rate:  1.322707462565492e-09  Varinance:  2.4222504776828726e-10 \n",
      "\n",
      "Epoch:  18143  Learning Rate:  1.32138541623626e-09  Varinance:  2.4188666825514525e-10 \n",
      "\n",
      "Epoch:  18144  Learning Rate:  1.3200646912925544e-09  Varinance:  2.4154876144578e-10 \n",
      "\n",
      "Epoch:  18145  Learning Rate:  1.3187452864136546e-09  Varinance:  2.412113266798426e-10 \n",
      "\n",
      "Epoch:  18146  Learning Rate:  1.3174272002801466e-09  Varinance:  2.408743632979039e-10 \n",
      "\n",
      "Epoch:  18147  Learning Rate:  1.3161104315739487e-09  Varinance:  2.405378706414586e-10 \n",
      "\n",
      "Epoch:  18148  Learning Rate:  1.3147949789782967e-09  Varinance:  2.4020184805292047e-10 \n",
      "\n",
      "Epoch:  18149  Learning Rate:  1.3134808411777285e-09  Varinance:  2.398662948756218e-10 \n",
      "\n",
      "Epoch:  18150  Learning Rate:  1.312168016858111e-09  Varinance:  2.3953121045381244e-10 \n",
      "\n",
      "Epoch:  18151  Learning Rate:  1.3108565047066245e-09  Varinance:  2.3919659413265805e-10 \n",
      "\n",
      "Epoch:  18152  Learning Rate:  1.3095463034117471e-09  Varinance:  2.3886244525823997e-10 \n",
      "\n",
      "Epoch:  18153  Learning Rate:  1.3082374116632822e-09  Varinance:  2.3852876317755064e-10 \n",
      "\n",
      "Epoch:  18154  Learning Rate:  1.3069298281523428e-09  Varinance:  2.381955472384971e-10 \n",
      "\n",
      "Epoch:  18155  Learning Rate:  1.3056235515713356e-09  Varinance:  2.378627967898967e-10 \n",
      "\n",
      "Epoch:  18156  Learning Rate:  1.304318580613989e-09  Varinance:  2.3753051118147624e-10 \n",
      "\n",
      "Epoch:  18157  Learning Rate:  1.3030149139753363e-09  Varinance:  2.3719868976387103e-10 \n",
      "\n",
      "Epoch:  18158  Learning Rate:  1.3017125503517015e-09  Varinance:  2.3686733188862356e-10 \n",
      "\n",
      "Epoch:  18159  Learning Rate:  1.3004114884407255e-09  Varinance:  2.365364369081829e-10 \n",
      "\n",
      "Epoch:  18160  Learning Rate:  1.299111726941351e-09  Varinance:  2.362060041759004e-10 \n",
      "\n",
      "Epoch:  18161  Learning Rate:  1.297813264553807e-09  Varinance:  2.3587603304603306e-10 \n",
      "\n",
      "Epoch:  18162  Learning Rate:  1.2965160999796358e-09  Varinance:  2.355465228737394e-10 \n",
      "\n",
      "Epoch:  18163  Learning Rate:  1.2952202319216772e-09  Varinance:  2.352174730150784e-10 \n",
      "\n",
      "Epoch:  18164  Learning Rate:  1.2939256590840538e-09  Varinance:  2.348888828270088e-10 \n",
      "\n",
      "Epoch:  18165  Learning Rate:  1.2926323801721973e-09  Varinance:  2.345607516673877e-10 \n",
      "\n",
      "Epoch:  18166  Learning Rate:  1.2913403938928332e-09  Varinance:  2.342330788949701e-10 \n",
      "\n",
      "Epoch:  18167  Learning Rate:  1.2900496989539662e-09  Varinance:  2.3390586386940407e-10 \n",
      "\n",
      "Epoch:  18168  Learning Rate:  1.2887602940649056e-09  Varinance:  2.335791059512349e-10 \n",
      "\n",
      "Epoch:  18169  Learning Rate:  1.287472177936251e-09  Varinance:  2.332528045019003e-10 \n",
      "\n",
      "Epoch:  18170  Learning Rate:  1.286185349279877e-09  Varinance:  2.3292695888373015e-10 \n",
      "\n",
      "Epoch:  18171  Learning Rate:  1.2848998068089597e-09  Varinance:  2.3260156845994487e-10 \n",
      "\n",
      "Epoch:  18172  Learning Rate:  1.2836155492379607e-09  Varinance:  2.3227663259465468e-10 \n",
      "\n",
      "Epoch:  18173  Learning Rate:  1.2823325752826133e-09  Varinance:  2.3195215065285884e-10 \n",
      "\n",
      "Epoch:  18174  Learning Rate:  1.2810508836599527e-09  Varinance:  2.3162812200044117e-10 \n",
      "\n",
      "Epoch:  18175  Learning Rate:  1.2797704730882778e-09  Varinance:  2.3130454600417392e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18176  Learning Rate:  1.278491342287183e-09  Varinance:  2.30981422031713e-10 \n",
      "\n",
      "Epoch:  18177  Learning Rate:  1.2772134899775412e-09  Varinance:  2.306587494515977e-10 \n",
      "\n",
      "Epoch:  18178  Learning Rate:  1.2759369148814913e-09  Varinance:  2.3033652763324953e-10 \n",
      "\n",
      "Epoch:  18179  Learning Rate:  1.2746616157224627e-09  Varinance:  2.3001475594697074e-10 \n",
      "\n",
      "Epoch:  18180  Learning Rate:  1.2733875912251605e-09  Varinance:  2.2969343376394417e-10 \n",
      "\n",
      "Epoch:  18181  Learning Rate:  1.2721148401155513e-09  Varinance:  2.2937256045622863e-10 \n",
      "\n",
      "Epoch:  18182  Learning Rate:  1.270843361120888e-09  Varinance:  2.290521353967626e-10 \n",
      "\n",
      "Epoch:  18183  Learning Rate:  1.2695731529696962e-09  Varinance:  2.287321579593597e-10 \n",
      "\n",
      "Epoch:  18184  Learning Rate:  1.268304214391759e-09  Varinance:  2.2841262751870828e-10 \n",
      "\n",
      "Epoch:  18185  Learning Rate:  1.2670365441181417e-09  Varinance:  2.280935434503703e-10 \n",
      "\n",
      "Epoch:  18186  Learning Rate:  1.2657701408811786e-09  Varinance:  2.2777490513077996e-10 \n",
      "\n",
      "Epoch:  18187  Learning Rate:  1.2645050034144573e-09  Varinance:  2.2745671193724346e-10 \n",
      "\n",
      "Epoch:  18188  Learning Rate:  1.2632411304528447e-09  Varinance:  2.2713896324793437e-10 \n",
      "\n",
      "Epoch:  18189  Learning Rate:  1.2619785207324726e-09  Varinance:  2.2682165844189734e-10 \n",
      "\n",
      "Epoch:  18190  Learning Rate:  1.2607171729907217e-09  Varinance:  2.2650479689904379e-10 \n",
      "\n",
      "Epoch:  18191  Learning Rate:  1.259457085966249e-09  Varinance:  2.261883780001513e-10 \n",
      "\n",
      "Epoch:  18192  Learning Rate:  1.2581982583989716e-09  Varinance:  2.258724011268624e-10 \n",
      "\n",
      "Epoch:  18193  Learning Rate:  1.2569406890300532e-09  Varinance:  2.255568656616836e-10 \n",
      "\n",
      "Epoch:  18194  Learning Rate:  1.2556843766019282e-09  Varinance:  2.252417709879847e-10 \n",
      "\n",
      "Epoch:  18195  Learning Rate:  1.2544293198582891e-09  Varinance:  2.2492711648999464e-10 \n",
      "\n",
      "Epoch:  18196  Learning Rate:  1.25317551754407e-09  Varinance:  2.2461290155280473e-10 \n",
      "\n",
      "Epoch:  18197  Learning Rate:  1.2519229684054726e-09  Varinance:  2.242991255623648e-10 \n",
      "\n",
      "Epoch:  18198  Learning Rate:  1.2506716711899526e-09  Varinance:  2.239857879054823e-10 \n",
      "\n",
      "Epoch:  18199  Learning Rate:  1.2494216246462036e-09  Varinance:  2.2367288796982127e-10 \n",
      "\n",
      "Epoch:  18200  Learning Rate:  1.2481728275241833e-09  Varinance:  2.233604251439012e-10 \n",
      "\n",
      "Epoch:  18201  Learning Rate:  1.246925278575099e-09  Varinance:  2.2304839881709665e-10 \n",
      "\n",
      "Epoch:  18202  Learning Rate:  1.2456789765513928e-09  Varinance:  2.2273680837963275e-10 \n",
      "\n",
      "Epoch:  18203  Learning Rate:  1.244433920206767e-09  Varinance:  2.224256532225889e-10 \n",
      "\n",
      "Epoch:  18204  Learning Rate:  1.2431901082961692e-09  Varinance:  2.221149327378942e-10 \n",
      "\n",
      "Epoch:  18205  Learning Rate:  1.2419475395757792e-09  Varinance:  2.218046463183274e-10 \n",
      "\n",
      "Epoch:  18206  Learning Rate:  1.2407062128030366e-09  Varinance:  2.214947933575154e-10 \n",
      "\n",
      "Epoch:  18207  Learning Rate:  1.2394661267366058e-09  Varinance:  2.2118537324993218e-10 \n",
      "\n",
      "Epoch:  18208  Learning Rate:  1.2382272801364048e-09  Varinance:  2.208763853908985e-10 \n",
      "\n",
      "Epoch:  18209  Learning Rate:  1.2369896717635917e-09  Varinance:  2.2056782917657734e-10 \n",
      "\n",
      "Epoch:  18210  Learning Rate:  1.2357533003805492e-09  Varinance:  2.202597040039777e-10 \n",
      "\n",
      "Epoch:  18211  Learning Rate:  1.2345181647509098e-09  Varinance:  2.1995200927095006e-10 \n",
      "\n",
      "Epoch:  18212  Learning Rate:  1.2332842636395426e-09  Varinance:  2.1964474437618616e-10 \n",
      "\n",
      "Epoch:  18213  Learning Rate:  1.2320515958125373e-09  Varinance:  2.1933790871921762e-10 \n",
      "\n",
      "Epoch:  18214  Learning Rate:  1.2308201600372306e-09  Varinance:  2.19031501700415e-10 \n",
      "\n",
      "Epoch:  18215  Learning Rate:  1.229589955082191e-09  Varinance:  2.1872552272098734e-10 \n",
      "\n",
      "Epoch:  18216  Learning Rate:  1.2283609797172042e-09  Varinance:  2.1841997118297767e-10 \n",
      "\n",
      "Epoch:  18217  Learning Rate:  1.2271332327132999e-09  Varinance:  2.1811484648926687e-10 \n",
      "\n",
      "Epoch:  18218  Learning Rate:  1.2259067128427348e-09  Varinance:  2.17810148043569e-10 \n",
      "\n",
      "Epoch:  18219  Learning Rate:  1.22468141887898e-09  Varinance:  2.175058752504313e-10 \n",
      "\n",
      "Epoch:  18220  Learning Rate:  1.2234573495967466e-09  Varinance:  2.1720202751523268e-10 \n",
      "\n",
      "Epoch:  18221  Learning Rate:  1.2222345037719688e-09  Varinance:  2.1689860424418283e-10 \n",
      "\n",
      "Epoch:  18222  Learning Rate:  1.2210128801817923e-09  Varinance:  2.165956048443216e-10 \n",
      "\n",
      "Epoch:  18223  Learning Rate:  1.219792477604598e-09  Varinance:  2.16293028723515e-10 \n",
      "\n",
      "Epoch:  18224  Learning Rate:  1.218573294819987e-09  Varinance:  2.159908752904585e-10 \n",
      "\n",
      "Epoch:  18225  Learning Rate:  1.2173553306087682e-09  Varinance:  2.1568914395467268e-10 \n",
      "\n",
      "Epoch:  18226  Learning Rate:  1.2161385837529813e-09  Varinance:  2.153878341265032e-10 \n",
      "\n",
      "Epoch:  18227  Learning Rate:  1.214923053035884e-09  Varinance:  2.1508694521711942e-10 \n",
      "\n",
      "Epoch:  18228  Learning Rate:  1.2137087372419368e-09  Varinance:  2.1478647663851317e-10 \n",
      "\n",
      "Epoch:  18229  Learning Rate:  1.2124956351568274e-09  Varinance:  2.144864278034985e-10 \n",
      "\n",
      "Epoch:  18230  Learning Rate:  1.2112837455674591e-09  Varinance:  2.141867981257075e-10 \n",
      "\n",
      "Epoch:  18231  Learning Rate:  1.2100730672619327e-09  Varinance:  2.138875870195937e-10 \n",
      "\n",
      "Epoch:  18232  Learning Rate:  1.2088635990295747e-09  Varinance:  2.1358879390042774e-10 \n",
      "\n",
      "Epoch:  18233  Learning Rate:  1.2076553396609203e-09  Varinance:  2.132904181842972e-10 \n",
      "\n",
      "Epoch:  18234  Learning Rate:  1.2064482879477022e-09  Varinance:  2.129924592881053e-10 \n",
      "\n",
      "Epoch:  18235  Learning Rate:  1.2052424426828726e-09  Varinance:  2.1269491662956994e-10 \n",
      "\n",
      "Epoch:  18236  Learning Rate:  1.2040378026605903e-09  Varinance:  2.123977896272231e-10 \n",
      "\n",
      "Epoch:  18237  Learning Rate:  1.2028343666762068e-09  Varinance:  2.121010777004068e-10 \n",
      "\n",
      "Epoch:  18238  Learning Rate:  1.2016321335262944e-09  Varinance:  2.118047802692765e-10 \n",
      "\n",
      "Epoch:  18239  Learning Rate:  1.2004311020086114e-09  Varinance:  2.115088967547969e-10 \n",
      "\n",
      "Epoch:  18240  Learning Rate:  1.1992312709221305e-09  Varinance:  2.1121342657874163e-10 \n",
      "\n",
      "Epoch:  18241  Learning Rate:  1.1980326390670247e-09  Varinance:  2.1091836916369208e-10 \n",
      "\n",
      "Epoch:  18242  Learning Rate:  1.1968352052446536e-09  Varinance:  2.1062372393303618e-10 \n",
      "\n",
      "Epoch:  18243  Learning Rate:  1.1956389682575874e-09  Varinance:  2.1032949031096825e-10 \n",
      "\n",
      "Epoch:  18244  Learning Rate:  1.1944439269095935e-09  Varinance:  2.1003566772248472e-10 \n",
      "\n",
      "Epoch:  18245  Learning Rate:  1.1932500800056217e-09  Varinance:  2.0974225559338738e-10 \n",
      "\n",
      "Epoch:  18246  Learning Rate:  1.1920574263518291e-09  Varinance:  2.094492533502796e-10 \n",
      "\n",
      "Epoch:  18247  Learning Rate:  1.1908659647555668e-09  Varinance:  2.0915666042056563e-10 \n",
      "\n",
      "Epoch:  18248  Learning Rate:  1.189675694025364e-09  Varinance:  2.0886447623244968e-10 \n",
      "\n",
      "Epoch:  18249  Learning Rate:  1.1884866129709545e-09  Varinance:  2.0857270021493473e-10 \n",
      "\n",
      "Epoch:  18250  Learning Rate:  1.1872987204032612e-09  Varinance:  2.0828133179782217e-10 \n",
      "\n",
      "Epoch:  18251  Learning Rate:  1.186112015134383e-09  Varinance:  2.0799037041170763e-10 \n",
      "\n",
      "Epoch:  18252  Learning Rate:  1.1849264959776189e-09  Varinance:  2.0769981548798452e-10 \n",
      "\n",
      "Epoch:  18253  Learning Rate:  1.1837421617474536e-09  Varinance:  2.0740966645883979e-10 \n",
      "\n",
      "Epoch:  18254  Learning Rate:  1.1825590112595444e-09  Varinance:  2.0711992275725353e-10 \n",
      "\n",
      "Epoch:  18255  Learning Rate:  1.1813770433307453e-09  Varinance:  2.0683058381699805e-10 \n",
      "\n",
      "Epoch:  18256  Learning Rate:  1.1801962567790919e-09  Varinance:  2.0654164907263657e-10 \n",
      "\n",
      "Epoch:  18257  Learning Rate:  1.1790166504237897e-09  Varinance:  2.0625311795952302e-10 \n",
      "\n",
      "Epoch:  18258  Learning Rate:  1.177838223085236e-09  Varinance:  2.0596498991379783e-10 \n",
      "\n",
      "Epoch:  18259  Learning Rate:  1.1766609735850077e-09  Varinance:  2.056772643723914e-10 \n",
      "\n",
      "Epoch:  18260  Learning Rate:  1.175484900745847e-09  Varinance:  2.0538994077301992e-10 \n",
      "\n",
      "Epoch:  18261  Learning Rate:  1.1743100033916848e-09  Varinance:  2.0510301855418513e-10 \n",
      "\n",
      "Epoch:  18262  Learning Rate:  1.1731362803476282e-09  Varinance:  2.048164971551731e-10 \n",
      "\n",
      "Epoch:  18263  Learning Rate:  1.1719637304399453e-09  Varinance:  2.0453037601605325e-10 \n",
      "\n",
      "Epoch:  18264  Learning Rate:  1.1707923524960909e-09  Varinance:  2.042446545776779e-10 \n",
      "\n",
      "Epoch:  18265  Learning Rate:  1.1696221453446904e-09  Varinance:  2.0395933228167833e-10 \n",
      "\n",
      "Epoch:  18266  Learning Rate:  1.1684531078155287e-09  Varinance:  2.03674408570468e-10 \n",
      "\n",
      "Epoch:  18267  Learning Rate:  1.1672852387395762e-09  Varinance:  2.0338988288723853e-10 \n",
      "\n",
      "Epoch:  18268  Learning Rate:  1.1661185369489557e-09  Varinance:  2.0310575467595946e-10 \n",
      "\n",
      "Epoch:  18269  Learning Rate:  1.1649530012769693e-09  Varinance:  2.0282202338137699e-10 \n",
      "\n",
      "Epoch:  18270  Learning Rate:  1.1637886305580854e-09  Varinance:  2.025386884490138e-10 \n",
      "\n",
      "Epoch:  18271  Learning Rate:  1.1626254236279248e-09  Varinance:  2.0225574932516496e-10 \n",
      "\n",
      "Epoch:  18272  Learning Rate:  1.1614633793232848e-09  Varinance:  2.0197320545690123e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18273  Learning Rate:  1.1603024964821251e-09  Varinance:  2.01691056292065e-10 \n",
      "\n",
      "Epoch:  18274  Learning Rate:  1.1591427739435543e-09  Varinance:  2.0140930127927004e-10 \n",
      "\n",
      "Epoch:  18275  Learning Rate:  1.1579842105478542e-09  Varinance:  2.0112793986790045e-10 \n",
      "\n",
      "Epoch:  18276  Learning Rate:  1.1568268051364651e-09  Varinance:  2.0084697150810944e-10 \n",
      "\n",
      "Epoch:  18277  Learning Rate:  1.1556705565519735e-09  Varinance:  2.005663956508191e-10 \n",
      "\n",
      "Epoch:  18278  Learning Rate:  1.154515463638135e-09  Varinance:  2.002862117477163e-10 \n",
      "\n",
      "Epoch:  18279  Learning Rate:  1.1533615252398602e-09  Varinance:  2.000064192512562e-10 \n",
      "\n",
      "Epoch:  18280  Learning Rate:  1.1522087402032026e-09  Varinance:  1.997270176146581e-10 \n",
      "\n",
      "Epoch:  18281  Learning Rate:  1.1510571073753813e-09  Varinance:  1.9944800629190505e-10 \n",
      "\n",
      "Epoch:  18282  Learning Rate:  1.1499066256047674e-09  Varinance:  1.991693847377429e-10 \n",
      "\n",
      "Epoch:  18283  Learning Rate:  1.1487572937408708e-09  Varinance:  1.9889115240767924e-10 \n",
      "\n",
      "Epoch:  18284  Learning Rate:  1.1476091106343637e-09  Varinance:  1.9861330875798297e-10 \n",
      "\n",
      "Epoch:  18285  Learning Rate:  1.146462075137067e-09  Varinance:  1.9833585324568044e-10 \n",
      "\n",
      "Epoch:  18286  Learning Rate:  1.1453161861019367e-09  Varinance:  1.980587853285587e-10 \n",
      "\n",
      "Epoch:  18287  Learning Rate:  1.1441714423830881e-09  Varinance:  1.977821044651614e-10 \n",
      "\n",
      "Epoch:  18288  Learning Rate:  1.1430278428357813e-09  Varinance:  1.9750581011478875e-10 \n",
      "\n",
      "Epoch:  18289  Learning Rate:  1.1418853863164086e-09  Varinance:  1.9722990173749618e-10 \n",
      "\n",
      "Epoch:  18290  Learning Rate:  1.1407440716825173e-09  Varinance:  1.9695437879409347e-10 \n",
      "\n",
      "Epoch:  18291  Learning Rate:  1.1396038977927968e-09  Varinance:  1.9667924074614427e-10 \n",
      "\n",
      "Epoch:  18292  Learning Rate:  1.138464863507065e-09  Varinance:  1.964044870559624e-10 \n",
      "\n",
      "Epoch:  18293  Learning Rate:  1.1373269676862916e-09  Varinance:  1.961301171866148e-10 \n",
      "\n",
      "Epoch:  18294  Learning Rate:  1.1361902091925845e-09  Varinance:  1.958561306019178e-10 \n",
      "\n",
      "Epoch:  18295  Learning Rate:  1.1350545868891775e-09  Varinance:  1.9558252676643687e-10 \n",
      "\n",
      "Epoch:  18296  Learning Rate:  1.1339200996404519e-09  Varinance:  1.9530930514548535e-10 \n",
      "\n",
      "Epoch:  18297  Learning Rate:  1.1327867463119244e-09  Varinance:  1.9503646520512355e-10 \n",
      "\n",
      "Epoch:  18298  Learning Rate:  1.1316545257702337e-09  Varinance:  1.9476400641215839e-10 \n",
      "\n",
      "Epoch:  18299  Learning Rate:  1.130523436883167e-09  Varinance:  1.944919282341395e-10 \n",
      "\n",
      "Epoch:  18300  Learning Rate:  1.1293934785196277e-09  Varinance:  1.9422023013936245e-10 \n",
      "\n",
      "Epoch:  18301  Learning Rate:  1.1282646495496606e-09  Varinance:  1.9394891159686493e-10 \n",
      "\n",
      "Epoch:  18302  Learning Rate:  1.1271369488444413e-09  Varinance:  1.9367797207642626e-10 \n",
      "\n",
      "Epoch:  18303  Learning Rate:  1.1260103752762606e-09  Varinance:  1.9340741104856653e-10 \n",
      "\n",
      "Epoch:  18304  Learning Rate:  1.1248849277185492e-09  Varinance:  1.9313722798454555e-10 \n",
      "\n",
      "Epoch:  18305  Learning Rate:  1.1237606050458632e-09  Varinance:  1.9286742235636228e-10 \n",
      "\n",
      "Epoch:  18306  Learning Rate:  1.1226374061338718e-09  Varinance:  1.9259799363675137e-10 \n",
      "\n",
      "Epoch:  18307  Learning Rate:  1.12151532985938e-09  Varinance:  1.9232894129918603e-10 \n",
      "\n",
      "Epoch:  18308  Learning Rate:  1.1203943751003158e-09  Varinance:  1.9206026481787434e-10 \n",
      "\n",
      "Epoch:  18309  Learning Rate:  1.119274540735716e-09  Varinance:  1.9179196366775891e-10 \n",
      "\n",
      "Epoch:  18310  Learning Rate:  1.1181558256457502e-09  Varinance:  1.9152403732451577e-10 \n",
      "\n",
      "Epoch:  18311  Learning Rate:  1.117038228711707e-09  Varinance:  1.9125648526455353e-10 \n",
      "\n",
      "Epoch:  18312  Learning Rate:  1.1159217488159817e-09  Varinance:  1.9098930696501277e-10 \n",
      "\n",
      "Epoch:  18313  Learning Rate:  1.1148063848420984e-09  Varinance:  1.9072250190376257e-10 \n",
      "\n",
      "Epoch:  18314  Learning Rate:  1.1136921356746967e-09  Varinance:  1.9045606955940336e-10 \n",
      "\n",
      "Epoch:  18315  Learning Rate:  1.1125790001995195e-09  Varinance:  1.9019000941126336e-10 \n",
      "\n",
      "Epoch:  18316  Learning Rate:  1.1114669773034351e-09  Varinance:  1.8992432093939807e-10 \n",
      "\n",
      "Epoch:  18317  Learning Rate:  1.1103560658744249e-09  Varinance:  1.896590036245893e-10 \n",
      "\n",
      "Epoch:  18318  Learning Rate:  1.109246264801569e-09  Varinance:  1.8939405694834432e-10 \n",
      "\n",
      "Epoch:  18319  Learning Rate:  1.1081375729750702e-09  Varinance:  1.891294803928953e-10 \n",
      "\n",
      "Epoch:  18320  Learning Rate:  1.1070299892862407e-09  Varinance:  1.888652734411957e-10 \n",
      "\n",
      "Epoch:  18321  Learning Rate:  1.105923512627489e-09  Varinance:  1.886014355769233e-10 \n",
      "\n",
      "Epoch:  18322  Learning Rate:  1.1048181418923418e-09  Varinance:  1.8833796628447648e-10 \n",
      "\n",
      "Epoch:  18323  Learning Rate:  1.1037138759754328e-09  Varinance:  1.8807486504897394e-10 \n",
      "\n",
      "Epoch:  18324  Learning Rate:  1.1026107137724875e-09  Varinance:  1.8781213135625363e-10 \n",
      "\n",
      "Epoch:  18325  Learning Rate:  1.1015086541803481e-09  Varinance:  1.875497646928717e-10 \n",
      "\n",
      "Epoch:  18326  Learning Rate:  1.1004076960969586e-09  Varinance:  1.8728776454610232e-10 \n",
      "\n",
      "Epoch:  18327  Learning Rate:  1.099307838421353e-09  Varinance:  1.8702613040393388e-10 \n",
      "\n",
      "Epoch:  18328  Learning Rate:  1.0982090800536773e-09  Varinance:  1.8676486175507201e-10 \n",
      "\n",
      "Epoch:  18329  Learning Rate:  1.097111419895177e-09  Varinance:  1.86503958088936e-10 \n",
      "\n",
      "Epoch:  18330  Learning Rate:  1.0960148568481844e-09  Varinance:  1.862434188956583e-10 \n",
      "\n",
      "Epoch:  18331  Learning Rate:  1.0949193898161435e-09  Varinance:  1.8598324366608378e-10 \n",
      "\n",
      "Epoch:  18332  Learning Rate:  1.09382501770358e-09  Varinance:  1.8572343189176843e-10 \n",
      "\n",
      "Epoch:  18333  Learning Rate:  1.0927317394161254e-09  Varinance:  1.854639830649792e-10 \n",
      "\n",
      "Epoch:  18334  Learning Rate:  1.091639553860505e-09  Varinance:  1.8520489667869045e-10 \n",
      "\n",
      "Epoch:  18335  Learning Rate:  1.0905484599445256e-09  Varinance:  1.849461722265867e-10 \n",
      "\n",
      "Epoch:  18336  Learning Rate:  1.089458456577097e-09  Varinance:  1.8468780920305922e-10 \n",
      "\n",
      "Epoch:  18337  Learning Rate:  1.0883695426682198e-09  Varinance:  1.8442980710320544e-10 \n",
      "\n",
      "Epoch:  18338  Learning Rate:  1.0872817171289721e-09  Varinance:  1.8417216542282828e-10 \n",
      "\n",
      "Epoch:  18339  Learning Rate:  1.086194978871532e-09  Varinance:  1.8391488365843494e-10 \n",
      "\n",
      "Epoch:  18340  Learning Rate:  1.0851093268091654e-09  Varinance:  1.8365796130723661e-10 \n",
      "\n",
      "Epoch:  18341  Learning Rate:  1.0840247598562119e-09  Varinance:  1.8340139786714497e-10 \n",
      "\n",
      "Epoch:  18342  Learning Rate:  1.0829412769281086e-09  Varinance:  1.8314519283677498e-10 \n",
      "\n",
      "Epoch:  18343  Learning Rate:  1.0818588769413764e-09  Varinance:  1.828893457154414e-10 \n",
      "\n",
      "Epoch:  18344  Learning Rate:  1.0807775588136073e-09  Varinance:  1.8263385600315835e-10 \n",
      "\n",
      "Epoch:  18345  Learning Rate:  1.0796973214634872e-09  Varinance:  1.8237872320063864e-10 \n",
      "\n",
      "Epoch:  18346  Learning Rate:  1.0786181638107824e-09  Varinance:  1.8212394680929232e-10 \n",
      "\n",
      "Epoch:  18347  Learning Rate:  1.0775400847763275e-09  Varinance:  1.818695263312267e-10 \n",
      "\n",
      "Epoch:  18348  Learning Rate:  1.0764630832820472e-09  Varinance:  1.8161546126924271e-10 \n",
      "\n",
      "Epoch:  18349  Learning Rate:  1.0753871582509435e-09  Varinance:  1.813617511268377e-10 \n",
      "\n",
      "Epoch:  18350  Learning Rate:  1.074312308607084e-09  Varinance:  1.811083954082021e-10 \n",
      "\n",
      "Epoch:  18351  Learning Rate:  1.0732385332756226e-09  Varinance:  1.8085539361821882e-10 \n",
      "\n",
      "Epoch:  18352  Learning Rate:  1.0721658311827877e-09  Varinance:  1.8060274526246255e-10 \n",
      "\n",
      "Epoch:  18353  Learning Rate:  1.0710942012558696e-09  Varinance:  1.8035044984719864e-10 \n",
      "\n",
      "Epoch:  18354  Learning Rate:  1.070023642423242e-09  Varinance:  1.8009850687938277e-10 \n",
      "\n",
      "Epoch:  18355  Learning Rate:  1.0689541536143498e-09  Varinance:  1.7984691586665748e-10 \n",
      "\n",
      "Epoch:  18356  Learning Rate:  1.0678857337596965e-09  Varinance:  1.7959567631735512e-10 \n",
      "\n",
      "Epoch:  18357  Learning Rate:  1.0668183817908657e-09  Varinance:  1.793447877404941e-10 \n",
      "\n",
      "Epoch:  18358  Learning Rate:  1.0657520966405098e-09  Varinance:  1.790942496457788e-10 \n",
      "\n",
      "Epoch:  18359  Learning Rate:  1.0646868772423353e-09  Varinance:  1.7884406154359855e-10 \n",
      "\n",
      "Epoch:  18360  Learning Rate:  1.0636227225311268e-09  Varinance:  1.7859422294502655e-10 \n",
      "\n",
      "Epoch:  18361  Learning Rate:  1.0625596314427334e-09  Varinance:  1.7834473336181972e-10 \n",
      "\n",
      "Epoch:  18362  Learning Rate:  1.061497602914056e-09  Varinance:  1.7809559230641515e-10 \n",
      "\n",
      "Epoch:  18363  Learning Rate:  1.0604366358830737e-09  Varinance:  1.7784679929193288e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18364  Learning Rate:  1.059376729288812e-09  Varinance:  1.775983538321725e-10 \n",
      "\n",
      "Epoch:  18365  Learning Rate:  1.058317882071368e-09  Varinance:  1.7735025544161282e-10 \n",
      "\n",
      "Epoch:  18366  Learning Rate:  1.0572600931718978e-09  Varinance:  1.7710250363541088e-10 \n",
      "\n",
      "Epoch:  18367  Learning Rate:  1.0562033615326052e-09  Varinance:  1.7685509792940104e-10 \n",
      "\n",
      "Epoch:  18368  Learning Rate:  1.0551476860967622e-09  Varinance:  1.7660803784009463e-10 \n",
      "\n",
      "Epoch:  18369  Learning Rate:  1.054093065808697e-09  Varinance:  1.7636132288467654e-10 \n",
      "\n",
      "Epoch:  18370  Learning Rate:  1.0530394996137817e-09  Varinance:  1.76114952581008e-10 \n",
      "\n",
      "Epoch:  18371  Learning Rate:  1.0519869864584535e-09  Varinance:  1.7586892644762318e-10 \n",
      "\n",
      "Epoch:  18372  Learning Rate:  1.0509355252902036e-09  Varinance:  1.7562324400372874e-10 \n",
      "\n",
      "Epoch:  18373  Learning Rate:  1.0498851150575625e-09  Varinance:  1.7537790476920313e-10 \n",
      "\n",
      "Epoch:  18374  Learning Rate:  1.0488357547101242e-09  Varinance:  1.751329082645954e-10 \n",
      "\n",
      "Epoch:  18375  Learning Rate:  1.0477874431985316e-09  Varinance:  1.7488825401112505e-10 \n",
      "\n",
      "Epoch:  18376  Learning Rate:  1.046740179474466e-09  Varinance:  1.7464394153067857e-10 \n",
      "\n",
      "Epoch:  18377  Learning Rate:  1.0456939624906667e-09  Varinance:  1.7439997034581214e-10 \n",
      "\n",
      "Epoch:  18378  Learning Rate:  1.0446487912009208e-09  Varinance:  1.7415633997974837e-10 \n",
      "\n",
      "Epoch:  18379  Learning Rate:  1.0436046645600497e-09  Varinance:  1.739130499563759e-10 \n",
      "\n",
      "Epoch:  18380  Learning Rate:  1.04256158152393e-09  Varinance:  1.7367009980024848e-10 \n",
      "\n",
      "Epoch:  18381  Learning Rate:  1.0415195410494825e-09  Varinance:  1.7342748903658398e-10 \n",
      "\n",
      "Epoch:  18382  Learning Rate:  1.040478542094659e-09  Varinance:  1.7318521719126421e-10 \n",
      "\n",
      "Epoch:  18383  Learning Rate:  1.0394385836184643e-09  Varinance:  1.7294328379083142e-10 \n",
      "\n",
      "Epoch:  18384  Learning Rate:  1.0383996645809438e-09  Varinance:  1.7270168836249115e-10 \n",
      "\n",
      "Epoch:  18385  Learning Rate:  1.0373617839431705e-09  Varinance:  1.7246043043410877e-10 \n",
      "\n",
      "Epoch:  18386  Learning Rate:  1.0363249406672677e-09  Varinance:  1.7221950953420918e-10 \n",
      "\n",
      "Epoch:  18387  Learning Rate:  1.0352891337163954e-09  Varinance:  1.7197892519197596e-10 \n",
      "\n",
      "Epoch:  18388  Learning Rate:  1.0342543620547396e-09  Varinance:  1.7173867693725041e-10 \n",
      "\n",
      "Epoch:  18389  Learning Rate:  1.033220624647532e-09  Varinance:  1.7149876430053117e-10 \n",
      "\n",
      "Epoch:  18390  Learning Rate:  1.0321879204610386e-09  Varinance:  1.71259186812971e-10 \n",
      "\n",
      "Epoch:  18391  Learning Rate:  1.0311562484625484e-09  Varinance:  1.7101994400637939e-10 \n",
      "\n",
      "Epoch:  18392  Learning Rate:  1.0301256076203959e-09  Varinance:  1.707810354132193e-10 \n",
      "\n",
      "Epoch:  18393  Learning Rate:  1.0290959969039335e-09  Varinance:  1.7054246056660684e-10 \n",
      "\n",
      "Epoch:  18394  Learning Rate:  1.0280674152835535e-09  Varinance:  1.703042190003103e-10 \n",
      "\n",
      "Epoch:  18395  Learning Rate:  1.0270398617306784e-09  Varinance:  1.700663102487493e-10 \n",
      "\n",
      "Epoch:  18396  Learning Rate:  1.0260133352177467e-09  Varinance:  1.6982873384699447e-10 \n",
      "\n",
      "Epoch:  18397  Learning Rate:  1.0249878347182358e-09  Varinance:  1.6959148933076407e-10 \n",
      "\n",
      "Epoch:  18398  Learning Rate:  1.0239633592066487e-09  Varinance:  1.6935457623642682e-10 \n",
      "\n",
      "Epoch:  18399  Learning Rate:  1.0229399076585025e-09  Varinance:  1.691179941009985e-10 \n",
      "\n",
      "Epoch:  18400  Learning Rate:  1.0219174790503492e-09  Varinance:  1.688817424621416e-10 \n",
      "\n",
      "Epoch:  18401  Learning Rate:  1.0208960723597638e-09  Varinance:  1.686458208581645e-10 \n",
      "\n",
      "Epoch:  18402  Learning Rate:  1.019875686565332e-09  Varinance:  1.6841022882802125e-10 \n",
      "\n",
      "Epoch:  18403  Learning Rate:  1.018856320646672e-09  Varinance:  1.6817496591130808e-10 \n",
      "\n",
      "Epoch:  18404  Learning Rate:  1.0178379735844213e-09  Varinance:  1.6794003164826611e-10 \n",
      "\n",
      "Epoch:  18405  Learning Rate:  1.0168206443602251e-09  Varinance:  1.6770542557977828e-10 \n",
      "\n",
      "Epoch:  18406  Learning Rate:  1.015804331956758e-09  Varinance:  1.6747114724736882e-10 \n",
      "\n",
      "Epoch:  18407  Learning Rate:  1.014789035357711e-09  Varinance:  1.672371961932024e-10 \n",
      "\n",
      "Epoch:  18408  Learning Rate:  1.0137747535477807e-09  Varinance:  1.6700357196008335e-10 \n",
      "\n",
      "Epoch:  18409  Learning Rate:  1.0127614855126882e-09  Varinance:  1.6677027409145523e-10 \n",
      "\n",
      "Epoch:  18410  Learning Rate:  1.0117492302391692e-09  Varinance:  1.6653730213139762e-10 \n",
      "\n",
      "Epoch:  18411  Learning Rate:  1.0107379867149613e-09  Varinance:  1.6630465562462876e-10 \n",
      "\n",
      "Epoch:  18412  Learning Rate:  1.009727753928824e-09  Varinance:  1.6607233411650238e-10 \n",
      "\n",
      "Epoch:  18413  Learning Rate:  1.0087185308705289e-09  Varinance:  1.658403371530073e-10 \n",
      "\n",
      "Epoch:  18414  Learning Rate:  1.0077103165308446e-09  Varinance:  1.656086642807665e-10 \n",
      "\n",
      "Epoch:  18415  Learning Rate:  1.006703109901561e-09  Varinance:  1.6537731504703642e-10 \n",
      "\n",
      "Epoch:  18416  Learning Rate:  1.005696909975475e-09  Varinance:  1.651462889997065e-10 \n",
      "\n",
      "Epoch:  18417  Learning Rate:  1.004691715746379e-09  Varinance:  1.64915585687296e-10 \n",
      "\n",
      "Epoch:  18418  Learning Rate:  1.0036875262090826e-09  Varinance:  1.646852046589567e-10 \n",
      "\n",
      "Epoch:  18419  Learning Rate:  1.0026843403593994e-09  Varinance:  1.6445514546446952e-10 \n",
      "\n",
      "Epoch:  18420  Learning Rate:  1.0016821571941367e-09  Varinance:  1.6422540765424437e-10 \n",
      "\n",
      "Epoch:  18421  Learning Rate:  1.0006809757111146e-09  Varinance:  1.6399599077931925e-10 \n",
      "\n",
      "Epoch:  18422  Learning Rate:  9.996807949091552e-10  Varinance:  1.6376689439135926e-10 \n",
      "\n",
      "Epoch:  18423  Learning Rate:  9.986816137880706e-10  Varinance:  1.6353811804265649e-10 \n",
      "\n",
      "Epoch:  18424  Learning Rate:  9.976834313486863e-10  Varinance:  1.6330966128612662e-10 \n",
      "\n",
      "Epoch:  18425  Learning Rate:  9.96686246592813e-10  Varinance:  1.630815236753117e-10 \n",
      "\n",
      "Epoch:  18426  Learning Rate:  9.956900585232696e-10  Varinance:  1.6285370476437685e-10 \n",
      "\n",
      "Epoch:  18427  Learning Rate:  9.94694866143871e-10  Varinance:  1.6262620410811004e-10 \n",
      "\n",
      "Epoch:  18428  Learning Rate:  9.93700668459418e-10  Varinance:  1.6239902126192116e-10 \n",
      "\n",
      "Epoch:  18429  Learning Rate:  9.927074644757161e-10  Varinance:  1.621721557818412e-10 \n",
      "\n",
      "Epoch:  18430  Learning Rate:  9.917152531995651e-10  Varinance:  1.6194560722452186e-10 \n",
      "\n",
      "Epoch:  18431  Learning Rate:  9.907240336387462e-10  Varinance:  1.6171937514723253e-10 \n",
      "\n",
      "Epoch:  18432  Learning Rate:  9.897338048020436e-10  Varinance:  1.6149345910786284e-10 \n",
      "\n",
      "Epoch:  18433  Learning Rate:  9.887445656992321e-10  Varinance:  1.612678586649193e-10 \n",
      "\n",
      "Epoch:  18434  Learning Rate:  9.877563153410647e-10  Varinance:  1.6104257337752539e-10 \n",
      "\n",
      "Epoch:  18435  Learning Rate:  9.867690527392952e-10  Varinance:  1.6081760280542025e-10 \n",
      "\n",
      "Epoch:  18436  Learning Rate:  9.857827769066643e-10  Varinance:  1.605929465089582e-10 \n",
      "\n",
      "Epoch:  18437  Learning Rate:  9.847974868568888e-10  Varinance:  1.6036860404910827e-10 \n",
      "\n",
      "Epoch:  18438  Learning Rate:  9.838131816046822e-10  Varinance:  1.6014457498745106e-10 \n",
      "\n",
      "Epoch:  18439  Learning Rate:  9.828298601657428e-10  Varinance:  1.5992085888618136e-10 \n",
      "\n",
      "Epoch:  18440  Learning Rate:  9.81847521556742e-10  Varinance:  1.5969745530810496e-10 \n",
      "\n",
      "Epoch:  18441  Learning Rate:  9.808661647953445e-10  Varinance:  1.5947436381663847e-10 \n",
      "\n",
      "Epoch:  18442  Learning Rate:  9.798857889001969e-10  Varinance:  1.5925158397580828e-10 \n",
      "\n",
      "Epoch:  18443  Learning Rate:  9.789063928909166e-10  Varinance:  1.5902911535024986e-10 \n",
      "\n",
      "Epoch:  18444  Learning Rate:  9.779279757881105e-10  Varinance:  1.588069575052075e-10 \n",
      "\n",
      "Epoch:  18445  Learning Rate:  9.769505366133654e-10  Varinance:  1.5858511000653106e-10 \n",
      "\n",
      "Epoch:  18446  Learning Rate:  9.759740743892348e-10  Varinance:  1.583635724206786e-10 \n",
      "\n",
      "Epoch:  18447  Learning Rate:  9.749985881392597e-10  Varinance:  1.5814234431471326e-10 \n",
      "\n",
      "Epoch:  18448  Learning Rate:  9.740240768879576e-10  Varinance:  1.5792142525630302e-10 \n",
      "\n",
      "Epoch:  18449  Learning Rate:  9.730505396608102e-10  Varinance:  1.577008148137197e-10 \n",
      "\n",
      "Epoch:  18450  Learning Rate:  9.720779754842834e-10  Varinance:  1.5748051255583838e-10 \n",
      "\n",
      "Epoch:  18451  Learning Rate:  9.711063833858167e-10  Varinance:  1.5726051805213684e-10 \n",
      "\n",
      "Epoch:  18452  Learning Rate:  9.701357623938107e-10  Varinance:  1.570408308726927e-10 \n",
      "\n",
      "Epoch:  18453  Learning Rate:  9.691661115376482e-10  Varinance:  1.5682145058818573e-10 \n",
      "\n",
      "Epoch:  18454  Learning Rate:  9.681974298476813e-10  Varinance:  1.5660237676989498e-10 \n",
      "\n",
      "Epoch:  18455  Learning Rate:  9.672297163552213e-10  Varinance:  1.563836089896984e-10 \n",
      "\n",
      "Epoch:  18456  Learning Rate:  9.662629700925618e-10  Varinance:  1.5616514682007197e-10 \n",
      "\n",
      "Epoch:  18457  Learning Rate:  9.652971900929497e-10  Varinance:  1.5594698983408894e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18458  Learning Rate:  9.643323753906078e-10  Varinance:  1.5572913760541946e-10 \n",
      "\n",
      "Epoch:  18459  Learning Rate:  9.633685250207253e-10  Varinance:  1.555115897083277e-10 \n",
      "\n",
      "Epoch:  18460  Learning Rate:  9.624056380194447e-10  Varinance:  1.5529434571767408e-10 \n",
      "\n",
      "Epoch:  18461  Learning Rate:  9.614437134238823e-10  Varinance:  1.5507740520891247e-10 \n",
      "\n",
      "Epoch:  18462  Learning Rate:  9.604827502721166e-10  Varinance:  1.5486076775808984e-10 \n",
      "\n",
      "Epoch:  18463  Learning Rate:  9.59522747603178e-10  Varinance:  1.546444329418453e-10 \n",
      "\n",
      "Epoch:  18464  Learning Rate:  9.58563704457067e-10  Varinance:  1.544284003374094e-10 \n",
      "\n",
      "Epoch:  18465  Learning Rate:  9.576056198747439e-10  Varinance:  1.5421266952260396e-10 \n",
      "\n",
      "Epoch:  18466  Learning Rate:  9.56648492898117e-10  Varinance:  1.5399724007583878e-10 \n",
      "\n",
      "Epoch:  18467  Learning Rate:  9.556923225700624e-10  Varinance:  1.5378211157611433e-10 \n",
      "\n",
      "Epoch:  18468  Learning Rate:  9.547371079344137e-10  Varinance:  1.5356728360301863e-10 \n",
      "\n",
      "Epoch:  18469  Learning Rate:  9.537828480359488e-10  Varinance:  1.5335275573672693e-10 \n",
      "\n",
      "Epoch:  18470  Learning Rate:  9.528295419204118e-10  Varinance:  1.5313852755800108e-10 \n",
      "\n",
      "Epoch:  18471  Learning Rate:  9.518771886344993e-10  Varinance:  1.5292459864818853e-10 \n",
      "\n",
      "Epoch:  18472  Learning Rate:  9.509257872258513e-10  Varinance:  1.5271096858922213e-10 \n",
      "\n",
      "Epoch:  18473  Learning Rate:  9.4997533674307e-10  Varinance:  1.524976369636171e-10 \n",
      "\n",
      "Epoch:  18474  Learning Rate:  9.490258362357077e-10  Varinance:  1.522846033544735e-10 \n",
      "\n",
      "Epoch:  18475  Learning Rate:  9.480772847542576e-10  Varinance:  1.520718673454733e-10 \n",
      "\n",
      "Epoch:  18476  Learning Rate:  9.471296813501712e-10  Varinance:  1.5185942852087996e-10 \n",
      "\n",
      "Epoch:  18477  Learning Rate:  9.461830250758484e-10  Varinance:  1.516472864655378e-10 \n",
      "\n",
      "Epoch:  18478  Learning Rate:  9.452373149846263e-10  Varinance:  1.51435440764871e-10 \n",
      "\n",
      "Epoch:  18479  Learning Rate:  9.442925501307976e-10  Varinance:  1.5122389100488354e-10 \n",
      "\n",
      "Epoch:  18480  Learning Rate:  9.433487295696016e-10  Varinance:  1.5101263677215604e-10 \n",
      "\n",
      "Epoch:  18481  Learning Rate:  9.4240585235721e-10  Varinance:  1.5080167765384827e-10 \n",
      "\n",
      "Epoch:  18482  Learning Rate:  9.414639175507495e-10  Varinance:  1.5059101323769622e-10 \n",
      "\n",
      "Epoch:  18483  Learning Rate:  9.405229242082884e-10  Varinance:  1.503806431120118e-10 \n",
      "\n",
      "Epoch:  18484  Learning Rate:  9.395828713888263e-10  Varinance:  1.5017056686568195e-10 \n",
      "\n",
      "Epoch:  18485  Learning Rate:  9.386437581523138e-10  Varinance:  1.4996078408816805e-10 \n",
      "\n",
      "Epoch:  18486  Learning Rate:  9.377055835596415e-10  Varinance:  1.4975129436950542e-10 \n",
      "\n",
      "Epoch:  18487  Learning Rate:  9.36768346672627e-10  Varinance:  1.4954209730030053e-10 \n",
      "\n",
      "Epoch:  18488  Learning Rate:  9.35832046554041e-10  Varinance:  1.4933319247173335e-10 \n",
      "\n",
      "Epoch:  18489  Learning Rate:  9.348966822675761e-10  Varinance:  1.4912457947555442e-10 \n",
      "\n",
      "Epoch:  18490  Learning Rate:  9.339622528778714e-10  Varinance:  1.4891625790408457e-10 \n",
      "\n",
      "Epoch:  18491  Learning Rate:  9.330287574505006e-10  Varinance:  1.487082273502142e-10 \n",
      "\n",
      "Epoch:  18492  Learning Rate:  9.320961950519616e-10  Varinance:  1.4850048740740236e-10 \n",
      "\n",
      "Epoch:  18493  Learning Rate:  9.311645647496956e-10  Varinance:  1.482930376696766e-10 \n",
      "\n",
      "Epoch:  18494  Learning Rate:  9.302338656120752e-10  Varinance:  1.4808587773163e-10 \n",
      "\n",
      "Epoch:  18495  Learning Rate:  9.293040967083946e-10  Varinance:  1.4787900718842354e-10 \n",
      "\n",
      "Epoch:  18496  Learning Rate:  9.283752571088881e-10  Varinance:  1.4767242563578324e-10 \n",
      "\n",
      "Epoch:  18497  Learning Rate:  9.274473458847194e-10  Varinance:  1.4746613266999985e-10 \n",
      "\n",
      "Epoch:  18498  Learning Rate:  9.265203621079705e-10  Varinance:  1.4726012788792818e-10 \n",
      "\n",
      "Epoch:  18499  Learning Rate:  9.25594304851661e-10  Varinance:  1.4705441088698608e-10 \n",
      "\n",
      "Epoch:  18500  Learning Rate:  9.246691731897368e-10  Varinance:  1.4684898126515441e-10 \n",
      "\n",
      "Epoch:  18501  Learning Rate:  9.237449661970594e-10  Varinance:  1.4664383862097405e-10 \n",
      "\n",
      "Epoch:  18502  Learning Rate:  9.228216829494254e-10  Varinance:  1.4643898255354821e-10 \n",
      "\n",
      "Epoch:  18503  Learning Rate:  9.218993225235545e-10  Varinance:  1.4623441266253976e-10 \n",
      "\n",
      "Epoch:  18504  Learning Rate:  9.209778839970795e-10  Varinance:  1.4603012854817064e-10 \n",
      "\n",
      "Epoch:  18505  Learning Rate:  9.200573664485655e-10  Varinance:  1.4582612981122141e-10 \n",
      "\n",
      "Epoch:  18506  Learning Rate:  9.191377689574976e-10  Varinance:  1.4562241605303027e-10 \n",
      "\n",
      "Epoch:  18507  Learning Rate:  9.182190906042722e-10  Varinance:  1.4541898687549287e-10 \n",
      "\n",
      "Epoch:  18508  Learning Rate:  9.173013304702138e-10  Varinance:  1.452158418810594e-10 \n",
      "\n",
      "Epoch:  18509  Learning Rate:  9.163844876375656e-10  Varinance:  1.45012980672737e-10 \n",
      "\n",
      "Epoch:  18510  Learning Rate:  9.154685611894782e-10  Varinance:  1.4481040285408694e-10 \n",
      "\n",
      "Epoch:  18511  Learning Rate:  9.145535502100282e-10  Varinance:  1.4460810802922413e-10 \n",
      "\n",
      "Epoch:  18512  Learning Rate:  9.136394537842079e-10  Varinance:  1.4440609580281673e-10 \n",
      "\n",
      "Epoch:  18513  Learning Rate:  9.127262709979142e-10  Varinance:  1.44204365780085e-10 \n",
      "\n",
      "Epoch:  18514  Learning Rate:  9.118140009379677e-10  Varinance:  1.440029175668012e-10 \n",
      "\n",
      "Epoch:  18515  Learning Rate:  9.109026426921015e-10  Varinance:  1.4380175076928696e-10 \n",
      "\n",
      "Epoch:  18516  Learning Rate:  9.099921953489503e-10  Varinance:  1.4360086499441521e-10 \n",
      "\n",
      "Epoch:  18517  Learning Rate:  9.090826579980737e-10  Varinance:  1.4340025984960762e-10 \n",
      "\n",
      "Epoch:  18518  Learning Rate:  9.081740297299277e-10  Varinance:  1.4319993494283427e-10 \n",
      "\n",
      "Epoch:  18519  Learning Rate:  9.07266309635887e-10  Varinance:  1.4299988988261294e-10 \n",
      "\n",
      "Epoch:  18520  Learning Rate:  9.063594968082347e-10  Varinance:  1.4280012427800823e-10 \n",
      "\n",
      "Epoch:  18521  Learning Rate:  9.054535903401516e-10  Varinance:  1.4260063773863134e-10 \n",
      "\n",
      "Epoch:  18522  Learning Rate:  9.045485893257343e-10  Varinance:  1.4240142987463747e-10 \n",
      "\n",
      "Epoch:  18523  Learning Rate:  9.03644492859985e-10  Varinance:  1.4220250029672776e-10 \n",
      "\n",
      "Epoch:  18524  Learning Rate:  9.027413000388006e-10  Varinance:  1.4200384861614675e-10 \n",
      "\n",
      "Epoch:  18525  Learning Rate:  9.018390099589915e-10  Varinance:  1.4180547444468207e-10 \n",
      "\n",
      "Epoch:  18526  Learning Rate:  9.009376217182706e-10  Varinance:  1.4160737739466368e-10 \n",
      "\n",
      "Epoch:  18527  Learning Rate:  9.000371344152434e-10  Varinance:  1.4140955707896303e-10 \n",
      "\n",
      "Epoch:  18528  Learning Rate:  8.991375471494255e-10  Varinance:  1.4121201311099297e-10 \n",
      "\n",
      "Epoch:  18529  Learning Rate:  8.982388590212328e-10  Varinance:  1.410147451047048e-10 \n",
      "\n",
      "Epoch:  18530  Learning Rate:  8.97341069131971e-10  Varinance:  1.4081775267459074e-10 \n",
      "\n",
      "Epoch:  18531  Learning Rate:  8.96444176583853e-10  Varinance:  1.406210354356809e-10 \n",
      "\n",
      "Epoch:  18532  Learning Rate:  8.955481804799896e-10  Varinance:  1.404245930035433e-10 \n",
      "\n",
      "Epoch:  18533  Learning Rate:  8.94653079924378e-10  Varinance:  1.4022842499428292e-10 \n",
      "\n",
      "Epoch:  18534  Learning Rate:  8.937588740219209e-10  Varinance:  1.4003253102454157e-10 \n",
      "\n",
      "Epoch:  18535  Learning Rate:  8.928655618784155e-10  Varinance:  1.398369107114951e-10 \n",
      "\n",
      "Epoch:  18536  Learning Rate:  8.919731426005432e-10  Varinance:  1.3964156367285562e-10 \n",
      "\n",
      "Epoch:  18537  Learning Rate:  8.910816152958878e-10  Varinance:  1.3944648952686884e-10 \n",
      "\n",
      "Epoch:  18538  Learning Rate:  8.901909790729252e-10  Varinance:  1.3925168789231365e-10 \n",
      "\n",
      "Epoch:  18539  Learning Rate:  8.893012330410127e-10  Varinance:  1.3905715838850164e-10 \n",
      "\n",
      "Epoch:  18540  Learning Rate:  8.884123763104073e-10  Varinance:  1.3886290063527613e-10 \n",
      "\n",
      "Epoch:  18541  Learning Rate:  8.875244079922554e-10  Varinance:  1.3866891425301199e-10 \n",
      "\n",
      "Epoch:  18542  Learning Rate:  8.866373271985823e-10  Varinance:  1.384751988626129e-10 \n",
      "\n",
      "Epoch:  18543  Learning Rate:  8.857511330423102e-10  Varinance:  1.3828175408551373e-10 \n",
      "\n",
      "Epoch:  18544  Learning Rate:  8.848658246372482e-10  Varinance:  1.3808857954367756e-10 \n",
      "\n",
      "Epoch:  18545  Learning Rate:  8.839814010980813e-10  Varinance:  1.3789567485959568e-10 \n",
      "\n",
      "Epoch:  18546  Learning Rate:  8.830978615403894e-10  Varinance:  1.3770303965628665e-10 \n",
      "\n",
      "Epoch:  18547  Learning Rate:  8.822152050806357e-10  Varinance:  1.3751067355729577e-10 \n",
      "\n",
      "Epoch:  18548  Learning Rate:  8.813334308361574e-10  Varinance:  1.3731857618669462e-10 \n",
      "\n",
      "Epoch:  18549  Learning Rate:  8.804525379251865e-10  Varinance:  1.3712674716907847e-10 \n",
      "\n",
      "Epoch:  18550  Learning Rate:  8.795725254668238e-10  Varinance:  1.369351861295686e-10 \n",
      "\n",
      "Epoch:  18551  Learning Rate:  8.786933925810598e-10  Varinance:  1.367438926938094e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18552  Learning Rate:  8.778151383887648e-10  Varinance:  1.365528664879682e-10 \n",
      "\n",
      "Epoch:  18553  Learning Rate:  8.769377620116781e-10  Varinance:  1.3636210713873464e-10 \n",
      "\n",
      "Epoch:  18554  Learning Rate:  8.760612625724268e-10  Varinance:  1.3617161427331983e-10 \n",
      "\n",
      "Epoch:  18555  Learning Rate:  8.751856391945139e-10  Varinance:  1.3598138751945607e-10 \n",
      "\n",
      "Epoch:  18556  Learning Rate:  8.743108910023101e-10  Varinance:  1.357914265053944e-10 \n",
      "\n",
      "Epoch:  18557  Learning Rate:  8.734370171210701e-10  Varinance:  1.3560173085990642e-10 \n",
      "\n",
      "Epoch:  18558  Learning Rate:  8.725640166769233e-10  Varinance:  1.3541230021228206e-10 \n",
      "\n",
      "Epoch:  18559  Learning Rate:  8.716918887968626e-10  Varinance:  1.3522313419232897e-10 \n",
      "\n",
      "Epoch:  18560  Learning Rate:  8.708206326087635e-10  Varinance:  1.35034232430372e-10 \n",
      "\n",
      "Epoch:  18561  Learning Rate:  8.699502472413725e-10  Varinance:  1.3484559455725243e-10 \n",
      "\n",
      "Epoch:  18562  Learning Rate:  8.690807318242983e-10  Varinance:  1.3465722020432773e-10 \n",
      "\n",
      "Epoch:  18563  Learning Rate:  8.682120854880281e-10  Varinance:  1.3446910900346883e-10 \n",
      "\n",
      "Epoch:  18564  Learning Rate:  8.67344307363919e-10  Varinance:  1.3428126058706244e-10 \n",
      "\n",
      "Epoch:  18565  Learning Rate:  8.664773965841866e-10  Varinance:  1.340936745880083e-10 \n",
      "\n",
      "Epoch:  18566  Learning Rate:  8.656113522819228e-10  Varinance:  1.3390635063971905e-10 \n",
      "\n",
      "Epoch:  18567  Learning Rate:  8.647461735910865e-10  Varinance:  1.337192883761193e-10 \n",
      "\n",
      "Epoch:  18568  Learning Rate:  8.638818596464929e-10  Varinance:  1.3353248743164515e-10 \n",
      "\n",
      "Epoch:  18569  Learning Rate:  8.630184095838307e-10  Varinance:  1.333459474412439e-10 \n",
      "\n",
      "Epoch:  18570  Learning Rate:  8.621558225396532e-10  Varinance:  1.331596680403712e-10 \n",
      "\n",
      "Epoch:  18571  Learning Rate:  8.612940976513672e-10  Varinance:  1.3297364886499362e-10 \n",
      "\n",
      "Epoch:  18572  Learning Rate:  8.604332340572504e-10  Varinance:  1.3278788955158562e-10 \n",
      "\n",
      "Epoch:  18573  Learning Rate:  8.595732308964425e-10  Varinance:  1.326023897371296e-10 \n",
      "\n",
      "Epoch:  18574  Learning Rate:  8.58714087308934e-10  Varinance:  1.32417149059115e-10 \n",
      "\n",
      "Epoch:  18575  Learning Rate:  8.578558024355844e-10  Varinance:  1.322321671555377e-10 \n",
      "\n",
      "Epoch:  18576  Learning Rate:  8.569983754181119e-10  Varinance:  1.320474436648998e-10 \n",
      "\n",
      "Epoch:  18577  Learning Rate:  8.561418053990831e-10  Varinance:  1.3186297822620696e-10 \n",
      "\n",
      "Epoch:  18578  Learning Rate:  8.552860915219311e-10  Varinance:  1.3167877047897049e-10 \n",
      "\n",
      "Epoch:  18579  Learning Rate:  8.544312329309447e-10  Varinance:  1.3149482006320492e-10 \n",
      "\n",
      "Epoch:  18580  Learning Rate:  8.535772287712596e-10  Varinance:  1.3131112661942758e-10 \n",
      "\n",
      "Epoch:  18581  Learning Rate:  8.527240781888775e-10  Varinance:  1.3112768978865806e-10 \n",
      "\n",
      "Epoch:  18582  Learning Rate:  8.518717803306414e-10  Varinance:  1.3094450921241737e-10 \n",
      "\n",
      "Epoch:  18583  Learning Rate:  8.510203343442569e-10  Varinance:  1.307615845327278e-10 \n",
      "\n",
      "Epoch:  18584  Learning Rate:  8.501697393782804e-10  Varinance:  1.305789153921103e-10 \n",
      "\n",
      "Epoch:  18585  Learning Rate:  8.493199945821112e-10  Varinance:  1.3039650143358665e-10 \n",
      "\n",
      "Epoch:  18586  Learning Rate:  8.484710991060074e-10  Varinance:  1.302143423006768e-10 \n",
      "\n",
      "Epoch:  18587  Learning Rate:  8.476230521010766e-10  Varinance:  1.300324376373987e-10 \n",
      "\n",
      "Epoch:  18588  Learning Rate:  8.467758527192652e-10  Varinance:  1.2985078708826763e-10 \n",
      "\n",
      "Epoch:  18589  Learning Rate:  8.459295001133771e-10  Varinance:  1.296693902982954e-10 \n",
      "\n",
      "Epoch:  18590  Learning Rate:  8.450839934370628e-10  Varinance:  1.2948824691299025e-10 \n",
      "\n",
      "Epoch:  18591  Learning Rate:  8.442393318448093e-10  Varinance:  1.2930735657835423e-10 \n",
      "\n",
      "Epoch:  18592  Learning Rate:  8.433955144919581e-10  Varinance:  1.291267189408853e-10 \n",
      "\n",
      "Epoch:  18593  Learning Rate:  8.425525405346945e-10  Varinance:  1.2894633364757476e-10 \n",
      "\n",
      "Epoch:  18594  Learning Rate:  8.417104091300387e-10  Varinance:  1.2876620034590707e-10 \n",
      "\n",
      "Epoch:  18595  Learning Rate:  8.408691194358622e-10  Varinance:  1.2858631868385916e-10 \n",
      "\n",
      "Epoch:  18596  Learning Rate:  8.400286706108782e-10  Varinance:  1.2840668830989968e-10 \n",
      "\n",
      "Epoch:  18597  Learning Rate:  8.391890618146318e-10  Varinance:  1.2822730887298887e-10 \n",
      "\n",
      "Epoch:  18598  Learning Rate:  8.383502922075172e-10  Varinance:  1.2804818002257593e-10 \n",
      "\n",
      "Epoch:  18599  Learning Rate:  8.375123609507676e-10  Varinance:  1.2786930140860116e-10 \n",
      "\n",
      "Epoch:  18600  Learning Rate:  8.366752672064459e-10  Varinance:  1.2769067268149345e-10 \n",
      "\n",
      "Epoch:  18601  Learning Rate:  8.358390101374609e-10  Varinance:  1.2751229349217002e-10 \n",
      "\n",
      "Epoch:  18602  Learning Rate:  8.350035889075589e-10  Varinance:  1.273341634920357e-10 \n",
      "\n",
      "Epoch:  18603  Learning Rate:  8.341690026813121e-10  Varinance:  1.2715628233298232e-10 \n",
      "\n",
      "Epoch:  18604  Learning Rate:  8.333352506241378e-10  Varinance:  1.269786496673885e-10 \n",
      "\n",
      "Epoch:  18605  Learning Rate:  8.325023319022864e-10  Varinance:  1.2680126514811705e-10 \n",
      "\n",
      "Epoch:  18606  Learning Rate:  8.316702456828333e-10  Varinance:  1.266241284285171e-10 \n",
      "\n",
      "Epoch:  18607  Learning Rate:  8.308389911336953e-10  Varinance:  1.2644723916242162e-10 \n",
      "\n",
      "Epoch:  18608  Learning Rate:  8.300085674236205e-10  Varinance:  1.2627059700414714e-10 \n",
      "\n",
      "Epoch:  18609  Learning Rate:  8.291789737221795e-10  Varinance:  1.2609420160849305e-10 \n",
      "\n",
      "Epoch:  18610  Learning Rate:  8.283502091997811e-10  Varinance:  1.2591805263074107e-10 \n",
      "\n",
      "Epoch:  18611  Learning Rate:  8.27522273027664e-10  Varinance:  1.2574214972665482e-10 \n",
      "\n",
      "Epoch:  18612  Learning Rate:  8.266951643778861e-10  Varinance:  1.2556649255247753e-10 \n",
      "\n",
      "Epoch:  18613  Learning Rate:  8.25868882423344e-10  Varinance:  1.2539108076493398e-10 \n",
      "\n",
      "Epoch:  18614  Learning Rate:  8.250434263377505e-10  Varinance:  1.25215914021228e-10 \n",
      "\n",
      "Epoch:  18615  Learning Rate:  8.242187952956521e-10  Varinance:  1.2504099197904242e-10 \n",
      "\n",
      "Epoch:  18616  Learning Rate:  8.233949884724204e-10  Varinance:  1.2486631429653808e-10 \n",
      "\n",
      "Epoch:  18617  Learning Rate:  8.225720050442431e-10  Varinance:  1.2469188063235353e-10 \n",
      "\n",
      "Epoch:  18618  Learning Rate:  8.217498441881394e-10  Varinance:  1.2451769064560454e-10 \n",
      "\n",
      "Epoch:  18619  Learning Rate:  8.209285050819511e-10  Varinance:  1.243437439958818e-10 \n",
      "\n",
      "Epoch:  18620  Learning Rate:  8.201079869043335e-10  Varinance:  1.241700403432528e-10 \n",
      "\n",
      "Epoch:  18621  Learning Rate:  8.19288288834771e-10  Varinance:  1.2399657934825954e-10 \n",
      "\n",
      "Epoch:  18622  Learning Rate:  8.184694100535688e-10  Varinance:  1.238233606719182e-10 \n",
      "\n",
      "Epoch:  18623  Learning Rate:  8.176513497418418e-10  Varinance:  1.2365038397571852e-10 \n",
      "\n",
      "Epoch:  18624  Learning Rate:  8.168341070815326e-10  Varinance:  1.234776489216231e-10 \n",
      "\n",
      "Epoch:  18625  Learning Rate:  8.160176812554015e-10  Varinance:  1.2330515517206715e-10 \n",
      "\n",
      "Epoch:  18626  Learning Rate:  8.152020714470168e-10  Varinance:  1.231329023899563e-10 \n",
      "\n",
      "Epoch:  18627  Learning Rate:  8.143872768407715e-10  Varinance:  1.2296089023866822e-10 \n",
      "\n",
      "Epoch:  18628  Learning Rate:  8.135732966218737e-10  Varinance:  1.2278911838205055e-10 \n",
      "\n",
      "Epoch:  18629  Learning Rate:  8.127601299763376e-10  Varinance:  1.2261758648442036e-10 \n",
      "\n",
      "Epoch:  18630  Learning Rate:  8.119477760909991e-10  Varinance:  1.2244629421056378e-10 \n",
      "\n",
      "Epoch:  18631  Learning Rate:  8.111362341535072e-10  Varinance:  1.222752412257352e-10 \n",
      "\n",
      "Epoch:  18632  Learning Rate:  8.103255033523141e-10  Varinance:  1.2210442719565699e-10 \n",
      "\n",
      "Epoch:  18633  Learning Rate:  8.095155828766921e-10  Varinance:  1.2193385178651734e-10 \n",
      "\n",
      "Epoch:  18634  Learning Rate:  8.087064719167231e-10  Varinance:  1.2176351466497192e-10 \n",
      "\n",
      "Epoch:  18635  Learning Rate:  8.078981696632907e-10  Varinance:  1.215934154981417e-10 \n",
      "\n",
      "Epoch:  18636  Learning Rate:  8.070906753080952e-10  Varinance:  1.2142355395361267e-10 \n",
      "\n",
      "Epoch:  18637  Learning Rate:  8.062839880436451e-10  Varinance:  1.2125392969943521e-10 \n",
      "\n",
      "Epoch:  18638  Learning Rate:  8.054781070632475e-10  Varinance:  1.210845424041234e-10 \n",
      "\n",
      "Epoch:  18639  Learning Rate:  8.04673031561024e-10  Varinance:  1.2091539173665476e-10 \n",
      "\n",
      "Epoch:  18640  Learning Rate:  8.038687607319018e-10  Varinance:  1.2074647736646805e-10 \n",
      "\n",
      "Epoch:  18641  Learning Rate:  8.030652937716048e-10  Varinance:  1.2057779896346505e-10 \n",
      "\n",
      "Epoch:  18642  Learning Rate:  8.022626298766711e-10  Varinance:  1.2040935619800826e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18643  Learning Rate:  8.014607682444314e-10  Varinance:  1.2024114874092065e-10 \n",
      "\n",
      "Epoch:  18644  Learning Rate:  8.006597080730268e-10  Varinance:  1.200731762634851e-10 \n",
      "\n",
      "Epoch:  18645  Learning Rate:  7.998594485613997e-10  Varinance:  1.1990543843744358e-10 \n",
      "\n",
      "Epoch:  18646  Learning Rate:  7.99059988909285e-10  Varinance:  1.1973793493499717e-10 \n",
      "\n",
      "Epoch:  18647  Learning Rate:  7.982613283172259e-10  Varinance:  1.195706654288035e-10 \n",
      "\n",
      "Epoch:  18648  Learning Rate:  7.974634659865643e-10  Varinance:  1.1940362959197885e-10 \n",
      "\n",
      "Epoch:  18649  Learning Rate:  7.966664011194323e-10  Varinance:  1.192368270980957e-10 \n",
      "\n",
      "Epoch:  18650  Learning Rate:  7.95870132918768e-10  Varinance:  1.1907025762118242e-10 \n",
      "\n",
      "Epoch:  18651  Learning Rate:  7.950746605883056e-10  Varinance:  1.1890392083572294e-10 \n",
      "\n",
      "Epoch:  18652  Learning Rate:  7.942799833325674e-10  Varinance:  1.1873781641665576e-10 \n",
      "\n",
      "Epoch:  18653  Learning Rate:  7.934861003568785e-10  Varinance:  1.18571944039374e-10 \n",
      "\n",
      "Epoch:  18654  Learning Rate:  7.926930108673591e-10  Varinance:  1.1840630337972297e-10 \n",
      "\n",
      "Epoch:  18655  Learning Rate:  7.919007140709137e-10  Varinance:  1.18240894114002e-10 \n",
      "\n",
      "Epoch:  18656  Learning Rate:  7.911092091752484e-10  Varinance:  1.1807571591896228e-10 \n",
      "\n",
      "Epoch:  18657  Learning Rate:  7.90318495388861e-10  Varinance:  1.1791076847180657e-10 \n",
      "\n",
      "Epoch:  18658  Learning Rate:  7.895285719210322e-10  Varinance:  1.1774605145018848e-10 \n",
      "\n",
      "Epoch:  18659  Learning Rate:  7.887394379818408e-10  Varinance:  1.17581564532212e-10 \n",
      "\n",
      "Epoch:  18660  Learning Rate:  7.879510927821562e-10  Varinance:  1.1741730739643122e-10 \n",
      "\n",
      "Epoch:  18661  Learning Rate:  7.87163535533627e-10  Varinance:  1.1725327972184797e-10 \n",
      "\n",
      "Epoch:  18662  Learning Rate:  7.86376765448699e-10  Varinance:  1.1708948118791378e-10 \n",
      "\n",
      "Epoch:  18663  Learning Rate:  7.855907817406049e-10  Varinance:  1.1692591147452757e-10 \n",
      "\n",
      "Epoch:  18664  Learning Rate:  7.848055836233551e-10  Varinance:  1.1676257026203543e-10 \n",
      "\n",
      "Epoch:  18665  Learning Rate:  7.840211703117544e-10  Varinance:  1.1659945723123e-10 \n",
      "\n",
      "Epoch:  18666  Learning Rate:  7.832375410213922e-10  Varinance:  1.1643657206335022e-10 \n",
      "\n",
      "Epoch:  18667  Learning Rate:  7.824546949686333e-10  Varinance:  1.1627391444007909e-10 \n",
      "\n",
      "Epoch:  18668  Learning Rate:  7.816726313706347e-10  Varinance:  1.1611148404354558e-10 \n",
      "\n",
      "Epoch:  18669  Learning Rate:  7.808913494453354e-10  Varinance:  1.1594928055632227e-10 \n",
      "\n",
      "Epoch:  18670  Learning Rate:  7.801108484114478e-10  Varinance:  1.1578730366142514e-10 \n",
      "\n",
      "Epoch:  18671  Learning Rate:  7.793311274884735e-10  Varinance:  1.1562555304231305e-10 \n",
      "\n",
      "Epoch:  18672  Learning Rate:  7.785521858966946e-10  Varinance:  1.1546402838288698e-10 \n",
      "\n",
      "Epoch:  18673  Learning Rate:  7.777740228571636e-10  Varinance:  1.1530272936748996e-10 \n",
      "\n",
      "Epoch:  18674  Learning Rate:  7.769966375917231e-10  Varinance:  1.1514165568090474e-10 \n",
      "\n",
      "Epoch:  18675  Learning Rate:  7.762200293229821e-10  Varinance:  1.1498080700835566e-10 \n",
      "\n",
      "Epoch:  18676  Learning Rate:  7.754441972743352e-10  Varinance:  1.1482018303550632e-10 \n",
      "\n",
      "Epoch:  18677  Learning Rate:  7.746691406699529e-10  Varinance:  1.1465978344845948e-10 \n",
      "\n",
      "Epoch:  18678  Learning Rate:  7.738948587347731e-10  Varinance:  1.1449960793375643e-10 \n",
      "\n",
      "Epoch:  18679  Learning Rate:  7.731213506945165e-10  Varinance:  1.143396561783763e-10 \n",
      "\n",
      "Epoch:  18680  Learning Rate:  7.723486157756778e-10  Varinance:  1.141799278697359e-10 \n",
      "\n",
      "Epoch:  18681  Learning Rate:  7.715766532055165e-10  Varinance:  1.1402042269568754e-10 \n",
      "\n",
      "Epoch:  18682  Learning Rate:  7.708054622120726e-10  Varinance:  1.1386114034452078e-10 \n",
      "\n",
      "Epoch:  18683  Learning Rate:  7.70035042024158e-10  Varinance:  1.1370208050496022e-10 \n",
      "\n",
      "Epoch:  18684  Learning Rate:  7.692653918713468e-10  Varinance:  1.135432428661653e-10 \n",
      "\n",
      "Epoch:  18685  Learning Rate:  7.684965109839916e-10  Varinance:  1.1338462711772969e-10 \n",
      "\n",
      "Epoch:  18686  Learning Rate:  7.677283985932142e-10  Varinance:  1.1322623294968069e-10 \n",
      "\n",
      "Epoch:  18687  Learning Rate:  7.669610539308965e-10  Varinance:  1.1306806005247903e-10 \n",
      "\n",
      "Epoch:  18688  Learning Rate:  7.661944762296967e-10  Varinance:  1.1291010811701665e-10 \n",
      "\n",
      "Epoch:  18689  Learning Rate:  7.654286647230397e-10  Varinance:  1.1275237683461848e-10 \n",
      "\n",
      "Epoch:  18690  Learning Rate:  7.646636186451087e-10  Varinance:  1.1259486589704031e-10 \n",
      "\n",
      "Epoch:  18691  Learning Rate:  7.638993372308598e-10  Varinance:  1.1243757499646846e-10 \n",
      "\n",
      "Epoch:  18692  Learning Rate:  7.631358197160146e-10  Varinance:  1.1228050382551932e-10 \n",
      "\n",
      "Epoch:  18693  Learning Rate:  7.623730653370501e-10  Varinance:  1.1212365207723867e-10 \n",
      "\n",
      "Epoch:  18694  Learning Rate:  7.616110733312145e-10  Varinance:  1.1196701944510149e-10 \n",
      "\n",
      "Epoch:  18695  Learning Rate:  7.608498429365181e-10  Varinance:  1.1181060562300973e-10 \n",
      "\n",
      "Epoch:  18696  Learning Rate:  7.600893733917255e-10  Varinance:  1.116544103052942e-10 \n",
      "\n",
      "Epoch:  18697  Learning Rate:  7.593296639363697e-10  Varinance:  1.1149843318671228e-10 \n",
      "\n",
      "Epoch:  18698  Learning Rate:  7.585707138107437e-10  Varinance:  1.1134267396244779e-10 \n",
      "\n",
      "Epoch:  18699  Learning Rate:  7.578125222558922e-10  Varinance:  1.1118713232811035e-10 \n",
      "\n",
      "Epoch:  18700  Learning Rate:  7.570550885136259e-10  Varinance:  1.1103180797973482e-10 \n",
      "\n",
      "Epoch:  18701  Learning Rate:  7.562984118265141e-10  Varinance:  1.1087670061378105e-10 \n",
      "\n",
      "Epoch:  18702  Learning Rate:  7.555424914378742e-10  Varinance:  1.1072180992713176e-10 \n",
      "\n",
      "Epoch:  18703  Learning Rate:  7.54787326591789e-10  Varinance:  1.1056713561709433e-10 \n",
      "\n",
      "Epoch:  18704  Learning Rate:  7.540329165330957e-10  Varinance:  1.1041267738139854e-10 \n",
      "\n",
      "Epoch:  18705  Learning Rate:  7.532792605073793e-10  Varinance:  1.1025843491819645e-10 \n",
      "\n",
      "Epoch:  18706  Learning Rate:  7.525263577609887e-10  Varinance:  1.101044079260618e-10 \n",
      "\n",
      "Epoch:  18707  Learning Rate:  7.517742075410159e-10  Varinance:  1.0995059610398941e-10 \n",
      "\n",
      "Epoch:  18708  Learning Rate:  7.510228090953134e-10  Varinance:  1.0979699915139498e-10 \n",
      "\n",
      "Epoch:  18709  Learning Rate:  7.502721616724851e-10  Varinance:  1.0964361676811297e-10 \n",
      "\n",
      "Epoch:  18710  Learning Rate:  7.495222645218785e-10  Varinance:  1.0949044865439825e-10 \n",
      "\n",
      "Epoch:  18711  Learning Rate:  7.487731168935988e-10  Varinance:  1.0933749451092414e-10 \n",
      "\n",
      "Epoch:  18712  Learning Rate:  7.480247180385011e-10  Varinance:  1.0918475403878202e-10 \n",
      "\n",
      "Epoch:  18713  Learning Rate:  7.472770672081811e-10  Varinance:  1.0903222693948089e-10 \n",
      "\n",
      "Epoch:  18714  Learning Rate:  7.465301636549905e-10  Varinance:  1.0887991291494672e-10 \n",
      "\n",
      "Epoch:  18715  Learning Rate:  7.457840066320284e-10  Varinance:  1.0872781166752224e-10 \n",
      "\n",
      "Epoch:  18716  Learning Rate:  7.450385953931327e-10  Varinance:  1.0857592289996484e-10 \n",
      "\n",
      "Epoch:  18717  Learning Rate:  7.442939291928943e-10  Varinance:  1.0842424631544837e-10 \n",
      "\n",
      "Epoch:  18718  Learning Rate:  7.435500072866497e-10  Varinance:  1.0827278161756087e-10 \n",
      "\n",
      "Epoch:  18719  Learning Rate:  7.428068289304717e-10  Varinance:  1.0812152851030447e-10 \n",
      "\n",
      "Epoch:  18720  Learning Rate:  7.420643933811846e-10  Varinance:  1.0797048669809484e-10 \n",
      "\n",
      "Epoch:  18721  Learning Rate:  7.413226998963554e-10  Varinance:  1.0781965588576053e-10 \n",
      "\n",
      "Epoch:  18722  Learning Rate:  7.405817477342852e-10  Varinance:  1.0766903577854283e-10 \n",
      "\n",
      "Epoch:  18723  Learning Rate:  7.398415361540244e-10  Varinance:  1.0751862608209365e-10 \n",
      "\n",
      "Epoch:  18724  Learning Rate:  7.391020644153641e-10  Varinance:  1.0736842650247727e-10 \n",
      "\n",
      "Epoch:  18725  Learning Rate:  7.383633317788272e-10  Varinance:  1.0721843674616814e-10 \n",
      "\n",
      "Epoch:  18726  Learning Rate:  7.376253375056835e-10  Varinance:  1.070686565200508e-10 \n",
      "\n",
      "Epoch:  18727  Learning Rate:  7.368880808579414e-10  Varinance:  1.0691908553141926e-10 \n",
      "\n",
      "Epoch:  18728  Learning Rate:  7.361515610983391e-10  Varinance:  1.0676972348797642e-10 \n",
      "\n",
      "Epoch:  18729  Learning Rate:  7.354157774903591e-10  Varinance:  1.0662057009783389e-10 \n",
      "\n",
      "Epoch:  18730  Learning Rate:  7.346807292982205e-10  Varinance:  1.0647162506950988e-10 \n",
      "\n",
      "Epoch:  18731  Learning Rate:  7.339464157868699e-10  Varinance:  1.0632288811193098e-10 \n",
      "\n",
      "Epoch:  18732  Learning Rate:  7.332128362219962e-10  Varinance:  1.0617435893442997e-10 \n",
      "\n",
      "Epoch:  18733  Learning Rate:  7.324799898700224e-10  Varinance:  1.0602603724674569e-10 \n",
      "\n",
      "Epoch:  18734  Learning Rate:  7.31747875998097e-10  Varinance:  1.0587792275902248e-10 \n",
      "\n",
      "Epoch:  18735  Learning Rate:  7.310164938741085e-10  Varinance:  1.0573001518180958e-10 \n",
      "\n",
      "Epoch:  18736  Learning Rate:  7.302858427666773e-10  Varinance:  1.0558231422606097e-10 \n",
      "\n",
      "Epoch:  18737  Learning Rate:  7.295559219451473e-10  Varinance:  1.054348196031333e-10 \n",
      "\n",
      "Epoch:  18738  Learning Rate:  7.288267306796026e-10  Varinance:  1.0528753102478754e-10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18739  Learning Rate:  7.280982682408466e-10  Varinance:  1.0514044820318698e-10 \n",
      "\n",
      "Epoch:  18740  Learning Rate:  7.273705339004197e-10  Varinance:  1.0499357085089695e-10 \n",
      "\n",
      "Epoch:  18741  Learning Rate:  7.266435269305897e-10  Varinance:  1.048468986808844e-10 \n",
      "\n",
      "Epoch:  18742  Learning Rate:  7.259172466043448e-10  Varinance:  1.0470043140651718e-10 \n",
      "\n",
      "Epoch:  18743  Learning Rate:  7.251916921954069e-10  Varinance:  1.0455416874156399e-10 \n",
      "\n",
      "Epoch:  18744  Learning Rate:  7.244668629782242e-10  Varinance:  1.044081104001922e-10 \n",
      "\n",
      "Epoch:  18745  Learning Rate:  7.237427582279622e-10  Varinance:  1.0426225609696964e-10 \n",
      "\n",
      "Epoch:  18746  Learning Rate:  7.230193772205189e-10  Varinance:  1.0411660554686249e-10 \n",
      "\n",
      "Epoch:  18747  Learning Rate:  7.222967192325155e-10  Varinance:  1.0397115846523513e-10 \n",
      "\n",
      "Epoch:  18748  Learning Rate:  7.21574783541289e-10  Varinance:  1.0382591456784954e-10 \n",
      "\n",
      "Epoch:  18749  Learning Rate:  7.208535694249063e-10  Varinance:  1.0368087357086478e-10 \n",
      "\n",
      "Epoch:  18750  Learning Rate:  7.201330761621555e-10  Varinance:  1.035360351908368e-10 \n",
      "\n",
      "Epoch:  18751  Learning Rate:  7.194133030325383e-10  Varinance:  1.033913991447164e-10 \n",
      "\n",
      "Epoch:  18752  Learning Rate:  7.186942493162842e-10  Varinance:  1.032469651498509e-10 \n",
      "\n",
      "Epoch:  18753  Learning Rate:  7.179759142943419e-10  Varinance:  1.0310273292398211e-10 \n",
      "\n",
      "Epoch:  18754  Learning Rate:  7.17258297248371e-10  Varinance:  1.029587021852461e-10 \n",
      "\n",
      "Epoch:  18755  Learning Rate:  7.165413974607573e-10  Varinance:  1.0281487265217277e-10 \n",
      "\n",
      "Epoch:  18756  Learning Rate:  7.158252142146032e-10  Varinance:  1.0267124404368518e-10 \n",
      "\n",
      "Epoch:  18757  Learning Rate:  7.151097467937205e-10  Varinance:  1.0252781607909939e-10 \n",
      "\n",
      "Epoch:  18758  Learning Rate:  7.143949944826441e-10  Varinance:  1.0238458847812253e-10 \n",
      "\n",
      "Epoch:  18759  Learning Rate:  7.136809565666242e-10  Varinance:  1.022415609608543e-10 \n",
      "\n",
      "Epoch:  18760  Learning Rate:  7.12967632331618e-10  Varinance:  1.0209873324778515e-10 \n",
      "\n",
      "Epoch:  18761  Learning Rate:  7.122550210643034e-10  Varinance:  1.0195610505979589e-10 \n",
      "\n",
      "Epoch:  18762  Learning Rate:  7.115431220520718e-10  Varinance:  1.0181367611815732e-10 \n",
      "\n",
      "Epoch:  18763  Learning Rate:  7.108319345830189e-10  Varinance:  1.0167144614452958e-10 \n",
      "\n",
      "Epoch:  18764  Learning Rate:  7.1012145794596e-10  Varinance:  1.0152941486096202e-10 \n",
      "\n",
      "Epoch:  18765  Learning Rate:  7.094116914304207e-10  Varinance:  1.0138758198989118e-10 \n",
      "\n",
      "Epoch:  18766  Learning Rate:  7.087026343266294e-10  Varinance:  1.0124594725414246e-10 \n",
      "\n",
      "Epoch:  18767  Learning Rate:  7.07994285925534e-10  Varinance:  1.0110451037692803e-10 \n",
      "\n",
      "Epoch:  18768  Learning Rate:  7.07286645518781e-10  Varinance:  1.0096327108184682e-10 \n",
      "\n",
      "Epoch:  18769  Learning Rate:  7.065797123987324e-10  Varinance:  1.0082222909288381e-10 \n",
      "\n",
      "Epoch:  18770  Learning Rate:  7.058734858584577e-10  Varinance:  1.0068138413440958e-10 \n",
      "\n",
      "Epoch:  18771  Learning Rate:  7.051679651917252e-10  Varinance:  1.005407359311801e-10 \n",
      "\n",
      "Epoch:  18772  Learning Rate:  7.044631496930166e-10  Varinance:  1.0040028420833487e-10 \n",
      "\n",
      "Epoch:  18773  Learning Rate:  7.037590386575189e-10  Varinance:  1.0026002869139824e-10 \n",
      "\n",
      "Epoch:  18774  Learning Rate:  7.030556313811159e-10  Varinance:  1.0011996910627782e-10 \n",
      "\n",
      "Epoch:  18775  Learning Rate:  7.02352927160403e-10  Varinance:  9.998010517926404e-11 \n",
      "\n",
      "Epoch:  18776  Learning Rate:  7.016509252926783e-10  Varinance:  9.984043663702967e-11 \n",
      "\n",
      "Epoch:  18777  Learning Rate:  7.009496250759348e-10  Varinance:  9.970096320662934e-11 \n",
      "\n",
      "Epoch:  18778  Learning Rate:  7.002490258088747e-10  Varinance:  9.956168461549935e-11 \n",
      "\n",
      "Epoch:  18779  Learning Rate:  6.995491267909014e-10  Varinance:  9.942260059145565e-11 \n",
      "\n",
      "Epoch:  18780  Learning Rate:  6.988499273221107e-10  Varinance:  9.92837108626955e-11 \n",
      "\n",
      "Epoch:  18781  Learning Rate:  6.981514267033056e-10  Varinance:  9.914501515779553e-11 \n",
      "\n",
      "Epoch:  18782  Learning Rate:  6.974536242359877e-10  Varinance:  9.900651320571151e-11 \n",
      "\n",
      "Epoch:  18783  Learning Rate:  6.967565192223498e-10  Varinance:  9.886820473577787e-11 \n",
      "\n",
      "Epoch:  18784  Learning Rate:  6.960601109652891e-10  Varinance:  9.873008947770712e-11 \n",
      "\n",
      "Epoch:  18785  Learning Rate:  6.953643987684e-10  Varinance:  9.859216716158972e-11 \n",
      "\n",
      "Epoch:  18786  Learning Rate:  6.94669381935965e-10  Varinance:  9.845443751789212e-11 \n",
      "\n",
      "Epoch:  18787  Learning Rate:  6.939750597729699e-10  Varinance:  9.831690027745842e-11 \n",
      "\n",
      "Epoch:  18788  Learning Rate:  6.932814315850947e-10  Varinance:  9.817955517150826e-11 \n",
      "\n",
      "Epoch:  18789  Learning Rate:  6.925884966787066e-10  Varinance:  9.804240193163681e-11 \n",
      "\n",
      "Epoch:  18790  Learning Rate:  6.918962543608727e-10  Varinance:  9.790544028981422e-11 \n",
      "\n",
      "Epoch:  18791  Learning Rate:  6.912047039393535e-10  Varinance:  9.776866997838502e-11 \n",
      "\n",
      "Epoch:  18792  Learning Rate:  6.905138447225933e-10  Varinance:  9.763209073006801e-11 \n",
      "\n",
      "Epoch:  18793  Learning Rate:  6.898236760197354e-10  Varinance:  9.749570227795433e-11 \n",
      "\n",
      "Epoch:  18794  Learning Rate:  6.891341971406134e-10  Varinance:  9.735950435550906e-11 \n",
      "\n",
      "Epoch:  18795  Learning Rate:  6.884454073957435e-10  Varinance:  9.722349669656917e-11 \n",
      "\n",
      "Epoch:  18796  Learning Rate:  6.877573060963384e-10  Varinance:  9.708767903534356e-11 \n",
      "\n",
      "Epoch:  18797  Learning Rate:  6.870698925542991e-10  Varinance:  9.695205110641239e-11 \n",
      "\n",
      "Epoch:  18798  Learning Rate:  6.863831660822072e-10  Varinance:  9.681661264472693e-11 \n",
      "\n",
      "Epoch:  18799  Learning Rate:  6.856971259933411e-10  Varinance:  9.668136338560768e-11 \n",
      "\n",
      "Epoch:  18800  Learning Rate:  6.850117716016556e-10  Varinance:  9.654630306474598e-11 \n",
      "\n",
      "Epoch:  18801  Learning Rate:  6.843271022217988e-10  Varinance:  9.641143141820198e-11 \n",
      "\n",
      "Epoch:  18802  Learning Rate:  6.836431171691037e-10  Varinance:  9.627674818240454e-11 \n",
      "\n",
      "Epoch:  18803  Learning Rate:  6.829598157595803e-10  Varinance:  9.614225309415083e-11 \n",
      "\n",
      "Epoch:  18804  Learning Rate:  6.822771973099295e-10  Varinance:  9.600794589060557e-11 \n",
      "\n",
      "Epoch:  18805  Learning Rate:  6.815952611375355e-10  Varinance:  9.587382630930108e-11 \n",
      "\n",
      "Epoch:  18806  Learning Rate:  6.809140065604568e-10  Varinance:  9.573989408813525e-11 \n",
      "\n",
      "Epoch:  18807  Learning Rate:  6.802334328974415e-10  Varinance:  9.560614896537321e-11 \n",
      "\n",
      "Epoch:  18808  Learning Rate:  6.795535394679182e-10  Varinance:  9.547259067964533e-11 \n",
      "\n",
      "Epoch:  18809  Learning Rate:  6.788743255919886e-10  Varinance:  9.533921896994713e-11 \n",
      "\n",
      "Epoch:  18810  Learning Rate:  6.781957905904412e-10  Varinance:  9.520603357563874e-11 \n",
      "\n",
      "Epoch:  18811  Learning Rate:  6.775179337847431e-10  Varinance:  9.507303423644435e-11 \n",
      "\n",
      "Epoch:  18812  Learning Rate:  6.76840754497033e-10  Varinance:  9.494022069245218e-11 \n",
      "\n",
      "Epoch:  18813  Learning Rate:  6.761642520501338e-10  Varinance:  9.480759268411244e-11 \n",
      "\n",
      "Epoch:  18814  Learning Rate:  6.754884257675455e-10  Varinance:  9.467514995223895e-11 \n",
      "\n",
      "Epoch:  18815  Learning Rate:  6.748132749734366e-10  Varinance:  9.45428922380073e-11 \n",
      "\n",
      "Epoch:  18816  Learning Rate:  6.741387989926592e-10  Varinance:  9.441081928295462e-11 \n",
      "\n",
      "Epoch:  18817  Learning Rate:  6.734649971507392e-10  Varinance:  9.427893082897911e-11 \n",
      "\n",
      "Epoch:  18818  Learning Rate:  6.727918687738701e-10  Varinance:  9.414722661833953e-11 \n",
      "\n",
      "Epoch:  18819  Learning Rate:  6.721194131889258e-10  Varinance:  9.4015706393655e-11 \n",
      "\n",
      "Epoch:  18820  Learning Rate:  6.714476297234531e-10  Varinance:  9.388436989790323e-11 \n",
      "\n",
      "Epoch:  18821  Learning Rate:  6.707765177056638e-10  Varinance:  9.3753216874422e-11 \n",
      "\n",
      "Epoch:  18822  Learning Rate:  6.70106076464448e-10  Varinance:  9.362224706690726e-11 \n",
      "\n",
      "Epoch:  18823  Learning Rate:  6.694363053293669e-10  Varinance:  9.349146021941299e-11 \n",
      "\n",
      "Epoch:  18824  Learning Rate:  6.687672036306445e-10  Varinance:  9.336085607635077e-11 \n",
      "\n",
      "Epoch:  18825  Learning Rate:  6.680987706991815e-10  Varinance:  9.323043438248922e-11 \n",
      "\n",
      "Epoch:  18826  Learning Rate:  6.674310058665473e-10  Varinance:  9.310019488295378e-11 \n",
      "\n",
      "Epoch:  18827  Learning Rate:  6.667639084649722e-10  Varinance:  9.297013732322506e-11 \n",
      "\n",
      "Epoch:  18828  Learning Rate:  6.660974778273612e-10  Varinance:  9.284026144914009e-11 \n",
      "\n",
      "Epoch:  18829  Learning Rate:  6.654317132872858e-10  Varinance:  9.271056700689069e-11 \n",
      "\n",
      "Epoch:  18830  Learning Rate:  6.647666141789767e-10  Varinance:  9.258105374302328e-11 \n",
      "\n",
      "Epoch:  18831  Learning Rate:  6.641021798373396e-10  Varinance:  9.245172140443827e-11 \n",
      "\n",
      "Epoch:  18832  Learning Rate:  6.634384095979353e-10  Varinance:  9.232256973838968e-11 \n",
      "\n",
      "Epoch:  18833  Learning Rate:  6.62775302796996e-10  Varinance:  9.219359849248491e-11 \n",
      "\n",
      "Epoch:  18834  Learning Rate:  6.62112858771417e-10  Varinance:  9.2064807414683e-11 \n",
      "\n",
      "Epoch:  18835  Learning Rate:  6.614510768587497e-10  Varinance:  9.193619625329604e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18836  Learning Rate:  6.607899563972142e-10  Varinance:  9.180776475698738e-11 \n",
      "\n",
      "Epoch:  18837  Learning Rate:  6.601294967256925e-10  Varinance:  9.167951267477148e-11 \n",
      "\n",
      "Epoch:  18838  Learning Rate:  6.594696971837204e-10  Varinance:  9.155143975601346e-11 \n",
      "\n",
      "Epoch:  18839  Learning Rate:  6.588105571115003e-10  Varinance:  9.14235457504285e-11 \n",
      "\n",
      "Epoch:  18840  Learning Rate:  6.581520758498946e-10  Varinance:  9.129583040808182e-11 \n",
      "\n",
      "Epoch:  18841  Learning Rate:  6.574942527404172e-10  Varinance:  9.116829347938676e-11 \n",
      "\n",
      "Epoch:  18842  Learning Rate:  6.568370871252474e-10  Varinance:  9.104093471510631e-11 \n",
      "\n",
      "Epoch:  18843  Learning Rate:  6.561805783472217e-10  Varinance:  9.091375386635131e-11 \n",
      "\n",
      "Epoch:  18844  Learning Rate:  6.555247257498268e-10  Varinance:  9.078675068458029e-11 \n",
      "\n",
      "Epoch:  18845  Learning Rate:  6.548695286772122e-10  Varinance:  9.0659924921599e-11 \n",
      "\n",
      "Epoch:  18846  Learning Rate:  6.542149864741832e-10  Varinance:  9.053327632955987e-11 \n",
      "\n",
      "Epoch:  18847  Learning Rate:  6.535610984861928e-10  Varinance:  9.040680466096194e-11 \n",
      "\n",
      "Epoch:  18848  Learning Rate:  6.529078640593555e-10  Varinance:  9.028050966864898e-11 \n",
      "\n",
      "Epoch:  18849  Learning Rate:  6.522552825404389e-10  Varinance:  9.015439110581103e-11 \n",
      "\n",
      "Epoch:  18850  Learning Rate:  6.516033532768569e-10  Varinance:  9.002844872598258e-11 \n",
      "\n",
      "Epoch:  18851  Learning Rate:  6.509520756166825e-10  Varinance:  8.990268228304242e-11 \n",
      "\n",
      "Epoch:  18852  Learning Rate:  6.503014489086402e-10  Varinance:  8.977709153121315e-11 \n",
      "\n",
      "Epoch:  18853  Learning Rate:  6.496514725020988e-10  Varinance:  8.965167622506077e-11 \n",
      "\n",
      "Epoch:  18854  Learning Rate:  6.49002145747084e-10  Varinance:  8.952643611949437e-11 \n",
      "\n",
      "Epoch:  18855  Learning Rate:  6.483534679942713e-10  Varinance:  8.940137096976454e-11 \n",
      "\n",
      "Epoch:  18856  Learning Rate:  6.477054385949783e-10  Varinance:  8.927648053146472e-11 \n",
      "\n",
      "Epoch:  18857  Learning Rate:  6.470580569011779e-10  Varinance:  8.915176456052942e-11 \n",
      "\n",
      "Epoch:  18858  Learning Rate:  6.464113222654907e-10  Varinance:  8.902722281323415e-11 \n",
      "\n",
      "Epoch:  18859  Learning Rate:  6.457652340411772e-10  Varinance:  8.890285504619487e-11 \n",
      "\n",
      "Epoch:  18860  Learning Rate:  6.451197915821516e-10  Varinance:  8.877866101636751e-11 \n",
      "\n",
      "Epoch:  18861  Learning Rate:  6.444749942429737e-10  Varinance:  8.86546404810479e-11 \n",
      "\n",
      "Epoch:  18862  Learning Rate:  6.438308413788413e-10  Varinance:  8.853079319786992e-11 \n",
      "\n",
      "Epoch:  18863  Learning Rate:  6.431873323456063e-10  Varinance:  8.840711892480701e-11 \n",
      "\n",
      "Epoch:  18864  Learning Rate:  6.42544466499755e-10  Varinance:  8.828361742017038e-11 \n",
      "\n",
      "Epoch:  18865  Learning Rate:  6.419022431984238e-10  Varinance:  8.816028844260886e-11 \n",
      "\n",
      "Epoch:  18866  Learning Rate:  6.412606617993916e-10  Varinance:  8.803713175110845e-11 \n",
      "\n",
      "Epoch:  18867  Learning Rate:  6.406197216610721e-10  Varinance:  8.791414710499186e-11 \n",
      "\n",
      "Epoch:  18868  Learning Rate:  6.399794221425279e-10  Varinance:  8.779133426391828e-11 \n",
      "\n",
      "Epoch:  18869  Learning Rate:  6.393397626034614e-10  Varinance:  8.766869298788173e-11 \n",
      "\n",
      "Epoch:  18870  Learning Rate:  6.387007424042084e-10  Varinance:  8.75462230372125e-11 \n",
      "\n",
      "Epoch:  18871  Learning Rate:  6.380623609057511e-10  Varinance:  8.74239241725753e-11 \n",
      "\n",
      "Epoch:  18872  Learning Rate:  6.374246174697102e-10  Varinance:  8.73017961549692e-11 \n",
      "\n",
      "Epoch:  18873  Learning Rate:  6.367875114583375e-10  Varinance:  8.717983874572721e-11 \n",
      "\n",
      "Epoch:  18874  Learning Rate:  6.361510422345294e-10  Varinance:  8.705805170651564e-11 \n",
      "\n",
      "Epoch:  18875  Learning Rate:  6.355152091618188e-10  Varinance:  8.693643479933414e-11 \n",
      "\n",
      "Epoch:  18876  Learning Rate:  6.348800116043681e-10  Varinance:  8.681498778651389e-11 \n",
      "\n",
      "Epoch:  18877  Learning Rate:  6.342454489269819e-10  Varinance:  8.669371043071899e-11 \n",
      "\n",
      "Epoch:  18878  Learning Rate:  6.336115204950996e-10  Varinance:  8.65726024949448e-11 \n",
      "\n",
      "Epoch:  18879  Learning Rate:  6.329782256747885e-10  Varinance:  8.645166374251778e-11 \n",
      "\n",
      "Epoch:  18880  Learning Rate:  6.323455638327557e-10  Varinance:  8.633089393709494e-11 \n",
      "\n",
      "Epoch:  18881  Learning Rate:  6.317135343363417e-10  Varinance:  8.621029284266355e-11 \n",
      "\n",
      "Epoch:  18882  Learning Rate:  6.310821365535126e-10  Varinance:  8.608986022354085e-11 \n",
      "\n",
      "Epoch:  18883  Learning Rate:  6.304513698528725e-10  Varinance:  8.596959584437239e-11 \n",
      "\n",
      "Epoch:  18884  Learning Rate:  6.298212336036571e-10  Varinance:  8.584949947013341e-11 \n",
      "\n",
      "Epoch:  18885  Learning Rate:  6.291917271757254e-10  Varinance:  8.572957086612719e-11 \n",
      "\n",
      "Epoch:  18886  Learning Rate:  6.285628499395735e-10  Varinance:  8.560980979798489e-11 \n",
      "\n",
      "Epoch:  18887  Learning Rate:  6.27934601266326e-10  Varinance:  8.549021603166499e-11 \n",
      "\n",
      "Epoch:  18888  Learning Rate:  6.2730698052773e-10  Varinance:  8.537078933345303e-11 \n",
      "\n",
      "Epoch:  18889  Learning Rate:  6.266799870961666e-10  Varinance:  8.525152946996128e-11 \n",
      "\n",
      "Epoch:  18890  Learning Rate:  6.26053620344645e-10  Varinance:  8.513243620812714e-11 \n",
      "\n",
      "Epoch:  18891  Learning Rate:  6.254278796467935e-10  Varinance:  8.501350931521448e-11 \n",
      "\n",
      "Epoch:  18892  Learning Rate:  6.248027643768761e-10  Varinance:  8.489474855881205e-11 \n",
      "\n",
      "Epoch:  18893  Learning Rate:  6.241782739097729e-10  Varinance:  8.477615370683323e-11 \n",
      "\n",
      "Epoch:  18894  Learning Rate:  6.235544076209957e-10  Varinance:  8.46577245275156e-11 \n",
      "\n",
      "Epoch:  18895  Learning Rate:  6.229311648866801e-10  Varinance:  8.453946078942055e-11 \n",
      "\n",
      "Epoch:  18896  Learning Rate:  6.223085450835793e-10  Varinance:  8.442136226143303e-11 \n",
      "\n",
      "Epoch:  18897  Learning Rate:  6.216865475890753e-10  Varinance:  8.430342871276e-11 \n",
      "\n",
      "Epoch:  18898  Learning Rate:  6.21065171781173e-10  Varinance:  8.418565991293169e-11 \n",
      "\n",
      "Epoch:  18899  Learning Rate:  6.204444170384918e-10  Varinance:  8.40680556318e-11 \n",
      "\n",
      "Epoch:  18900  Learning Rate:  6.198242827402796e-10  Varinance:  8.395061563953835e-11 \n",
      "\n",
      "Epoch:  18901  Learning Rate:  6.192047682664039e-10  Varinance:  8.383333970664122e-11 \n",
      "\n",
      "Epoch:  18902  Learning Rate:  6.185858729973459e-10  Varinance:  8.371622760392368e-11 \n",
      "\n",
      "Epoch:  18903  Learning Rate:  6.179675963142124e-10  Varinance:  8.359927910252126e-11 \n",
      "\n",
      "Epoch:  18904  Learning Rate:  6.173499375987289e-10  Varinance:  8.348249397388834e-11 \n",
      "\n",
      "Epoch:  18905  Learning Rate:  6.167328962332324e-10  Varinance:  8.336587198979944e-11 \n",
      "\n",
      "Epoch:  18906  Learning Rate:  6.161164716006833e-10  Varinance:  8.324941292234762e-11 \n",
      "\n",
      "Epoch:  18907  Learning Rate:  6.155006630846594e-10  Varinance:  8.31331165439443e-11 \n",
      "\n",
      "Epoch:  18908  Learning Rate:  6.148854700693478e-10  Varinance:  8.301698262731885e-11 \n",
      "\n",
      "Epoch:  18909  Learning Rate:  6.142708919395574e-10  Varinance:  8.290101094551811e-11 \n",
      "\n",
      "Epoch:  18910  Learning Rate:  6.136569280807124e-10  Varinance:  8.27852012719063e-11 \n",
      "\n",
      "Epoch:  18911  Learning Rate:  6.130435778788443e-10  Varinance:  8.266955338016328e-11 \n",
      "\n",
      "Epoch:  18912  Learning Rate:  6.124308407206053e-10  Varinance:  8.255406704428605e-11 \n",
      "\n",
      "Epoch:  18913  Learning Rate:  6.118187159932602e-10  Varinance:  8.243874203858697e-11 \n",
      "\n",
      "Epoch:  18914  Learning Rate:  6.112072030846799e-10  Varinance:  8.232357813769368e-11 \n",
      "\n",
      "Epoch:  18915  Learning Rate:  6.105963013833536e-10  Varinance:  8.22085751165487e-11 \n",
      "\n",
      "Epoch:  18916  Learning Rate:  6.099860102783817e-10  Varinance:  8.20937327504089e-11 \n",
      "\n",
      "Epoch:  18917  Learning Rate:  6.093763291594689e-10  Varinance:  8.197905081484542e-11 \n",
      "\n",
      "Epoch:  18918  Learning Rate:  6.087672574169359e-10  Varinance:  8.186452908574204e-11 \n",
      "\n",
      "Epoch:  18919  Learning Rate:  6.081587944417133e-10  Varinance:  8.175016733929651e-11 \n",
      "\n",
      "Epoch:  18920  Learning Rate:  6.075509396253336e-10  Varinance:  8.163596535201892e-11 \n",
      "\n",
      "Epoch:  18921  Learning Rate:  6.069436923599441e-10  Varinance:  8.152192290073154e-11 \n",
      "\n",
      "Epoch:  18922  Learning Rate:  6.063370520382997e-10  Varinance:  8.140803976256848e-11 \n",
      "\n",
      "Epoch:  18923  Learning Rate:  6.057310180537558e-10  Varinance:  8.129431571497513e-11 \n",
      "\n",
      "Epoch:  18924  Learning Rate:  6.051255898002826e-10  Varinance:  8.118075053570808e-11 \n",
      "\n",
      "Epoch:  18925  Learning Rate:  6.045207666724474e-10  Varinance:  8.106734400283351e-11 \n",
      "\n",
      "Epoch:  18926  Learning Rate:  6.039165480654292e-10  Varinance:  8.095409589472854e-11 \n",
      "\n",
      "Epoch:  18927  Learning Rate:  6.033129333750117e-10  Varinance:  8.084100599007956e-11 \n",
      "\n",
      "Epoch:  18928  Learning Rate:  6.027099219975756e-10  Varinance:  8.072807406788215e-11 \n",
      "\n",
      "Epoch:  18929  Learning Rate:  6.021075133301117e-10  Varinance:  8.06152999074406e-11 \n",
      "\n",
      "Epoch:  18930  Learning Rate:  6.015057067702135e-10  Varinance:  8.050268328836781e-11 \n",
      "\n",
      "Epoch:  18931  Learning Rate:  6.0090450171607e-10  Varinance:  8.03902239905837e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18932  Learning Rate:  6.003038975664784e-10  Varinance:  8.027792179431648e-11 \n",
      "\n",
      "Epoch:  18933  Learning Rate:  5.997038937208364e-10  Varinance:  8.016577648010108e-11 \n",
      "\n",
      "Epoch:  18934  Learning Rate:  5.991044895791361e-10  Varinance:  8.005378782877904e-11 \n",
      "\n",
      "Epoch:  18935  Learning Rate:  5.985056845419751e-10  Varinance:  7.994195562149803e-11 \n",
      "\n",
      "Epoch:  18936  Learning Rate:  5.979074780105508e-10  Varinance:  7.983027963971145e-11 \n",
      "\n",
      "Epoch:  18937  Learning Rate:  5.973098693866522e-10  Varinance:  7.971875966517832e-11 \n",
      "\n",
      "Epoch:  18938  Learning Rate:  5.967128580726728e-10  Varinance:  7.960739547996162e-11 \n",
      "\n",
      "Epoch:  18939  Learning Rate:  5.961164434716033e-10  Varinance:  7.94961868664297e-11 \n",
      "\n",
      "Epoch:  18940  Learning Rate:  5.955206249870248e-10  Varinance:  7.938513360725462e-11 \n",
      "\n",
      "Epoch:  18941  Learning Rate:  5.949254020231208e-10  Varinance:  7.927423548541203e-11 \n",
      "\n",
      "Epoch:  18942  Learning Rate:  5.943307739846707e-10  Varinance:  7.916349228418074e-11 \n",
      "\n",
      "Epoch:  18943  Learning Rate:  5.937367402770419e-10  Varinance:  7.905290378714239e-11 \n",
      "\n",
      "Epoch:  18944  Learning Rate:  5.931433003062029e-10  Varinance:  7.89424697781811e-11 \n",
      "\n",
      "Epoch:  18945  Learning Rate:  5.925504534787157e-10  Varinance:  7.883219004148219e-11 \n",
      "\n",
      "Epoch:  18946  Learning Rate:  5.919581992017293e-10  Varinance:  7.872206436153321e-11 \n",
      "\n",
      "Epoch:  18947  Learning Rate:  5.913665368829914e-10  Varinance:  7.861209252312254e-11 \n",
      "\n",
      "Epoch:  18948  Learning Rate:  5.907754659308418e-10  Varinance:  7.85022743113392e-11 \n",
      "\n",
      "Epoch:  18949  Learning Rate:  5.901849857542052e-10  Varinance:  7.83926095115724e-11 \n",
      "\n",
      "Epoch:  18950  Learning Rate:  5.895950957626036e-10  Varinance:  7.82830979095112e-11 \n",
      "\n",
      "Epoch:  18951  Learning Rate:  5.890057953661489e-10  Varinance:  7.817373929114427e-11 \n",
      "\n",
      "Epoch:  18952  Learning Rate:  5.884170839755367e-10  Varinance:  7.806453344275846e-11 \n",
      "\n",
      "Epoch:  18953  Learning Rate:  5.878289610020574e-10  Varinance:  7.795548015094e-11 \n",
      "\n",
      "Epoch:  18954  Learning Rate:  5.872414258575902e-10  Varinance:  7.78465792025729e-11 \n",
      "\n",
      "Epoch:  18955  Learning Rate:  5.866544779545957e-10  Varinance:  7.773783038483902e-11 \n",
      "\n",
      "Epoch:  18956  Learning Rate:  5.860681167061301e-10  Varinance:  7.762923348521739e-11 \n",
      "\n",
      "Epoch:  18957  Learning Rate:  5.854823415258281e-10  Varinance:  7.752078829148397e-11 \n",
      "\n",
      "Epoch:  18958  Learning Rate:  5.848971518279163e-10  Varinance:  7.74124945917115e-11 \n",
      "\n",
      "Epoch:  18959  Learning Rate:  5.843125470272072e-10  Varinance:  7.73043521742679e-11 \n",
      "\n",
      "Epoch:  18960  Learning Rate:  5.837285265390917e-10  Varinance:  7.71963608278176e-11 \n",
      "\n",
      "Epoch:  18961  Learning Rate:  5.831450897795513e-10  Varinance:  7.708852034131994e-11 \n",
      "\n",
      "Epoch:  18962  Learning Rate:  5.825622361651515e-10  Varinance:  7.698083050402909e-11 \n",
      "\n",
      "Epoch:  18963  Learning Rate:  5.819799651130343e-10  Varinance:  7.687329110549365e-11 \n",
      "\n",
      "Epoch:  18964  Learning Rate:  5.813982760409306e-10  Varinance:  7.676590193555618e-11 \n",
      "\n",
      "Epoch:  18965  Learning Rate:  5.808171683671536e-10  Varinance:  7.665866278435309e-11 \n",
      "\n",
      "Epoch:  18966  Learning Rate:  5.802366415105913e-10  Varinance:  7.65515734423132e-11 \n",
      "\n",
      "Epoch:  18967  Learning Rate:  5.796566948907188e-10  Varinance:  7.644463370015886e-11 \n",
      "\n",
      "Epoch:  18968  Learning Rate:  5.790773279275915e-10  Varinance:  7.633784334890449e-11 \n",
      "\n",
      "Epoch:  18969  Learning Rate:  5.784985400418385e-10  Varinance:  7.623120217985652e-11 \n",
      "\n",
      "Epoch:  18970  Learning Rate:  5.779203306546737e-10  Varinance:  7.612470998461283e-11 \n",
      "\n",
      "Epoch:  18971  Learning Rate:  5.773426991878896e-10  Varinance:  7.601836655506252e-11 \n",
      "\n",
      "Epoch:  18972  Learning Rate:  5.767656450638509e-10  Varinance:  7.59121716833856e-11 \n",
      "\n",
      "Epoch:  18973  Learning Rate:  5.761891677055053e-10  Varinance:  7.580612516205164e-11 \n",
      "\n",
      "Epoch:  18974  Learning Rate:  5.756132665363775e-10  Varinance:  7.570022678382094e-11 \n",
      "\n",
      "Epoch:  18975  Learning Rate:  5.750379409805621e-10  Varinance:  7.559447634174298e-11 \n",
      "\n",
      "Epoch:  18976  Learning Rate:  5.744631904627356e-10  Varinance:  7.548887362915641e-11 \n",
      "\n",
      "Epoch:  18977  Learning Rate:  5.738890144081495e-10  Varinance:  7.538341843968851e-11 \n",
      "\n",
      "Epoch:  18978  Learning Rate:  5.733154122426236e-10  Varinance:  7.527811056725493e-11 \n",
      "\n",
      "Epoch:  18979  Learning Rate:  5.727423833925577e-10  Varinance:  7.517294980605942e-11 \n",
      "\n",
      "Epoch:  18980  Learning Rate:  5.72169927284925e-10  Varinance:  7.506793595059241e-11 \n",
      "\n",
      "Epoch:  18981  Learning Rate:  5.715980433472653e-10  Varinance:  7.496306879563231e-11 \n",
      "\n",
      "Epoch:  18982  Learning Rate:  5.710267310076964e-10  Varinance:  7.485834813624384e-11 \n",
      "\n",
      "Epoch:  18983  Learning Rate:  5.704559896949082e-10  Varinance:  7.475377376777809e-11 \n",
      "\n",
      "Epoch:  18984  Learning Rate:  5.698858188381552e-10  Varinance:  7.464934548587196e-11 \n",
      "\n",
      "Epoch:  18985  Learning Rate:  5.693162178672685e-10  Varinance:  7.454506308644793e-11 \n",
      "\n",
      "Epoch:  18986  Learning Rate:  5.687471862126492e-10  Varinance:  7.444092636571375e-11 \n",
      "\n",
      "Epoch:  18987  Learning Rate:  5.681787233052615e-10  Varinance:  7.433693512016111e-11 \n",
      "\n",
      "Epoch:  18988  Learning Rate:  5.676108285766464e-10  Varinance:  7.423308914656679e-11 \n",
      "\n",
      "Epoch:  18989  Learning Rate:  5.670435014589052e-10  Varinance:  7.412938824199117e-11 \n",
      "\n",
      "Epoch:  18990  Learning Rate:  5.664767413847127e-10  Varinance:  7.402583220377817e-11 \n",
      "\n",
      "Epoch:  18991  Learning Rate:  5.659105477873108e-10  Varinance:  7.392242082955477e-11 \n",
      "\n",
      "Epoch:  18992  Learning Rate:  5.653449201005019e-10  Varinance:  7.381915391723068e-11 \n",
      "\n",
      "Epoch:  18993  Learning Rate:  5.647798577586601e-10  Varinance:  7.371603126499821e-11 \n",
      "\n",
      "Epoch:  18994  Learning Rate:  5.642153601967252e-10  Varinance:  7.361305267133075e-11 \n",
      "\n",
      "Epoch:  18995  Learning Rate:  5.636514268501955e-10  Varinance:  7.351021793498404e-11 \n",
      "\n",
      "Epoch:  18996  Learning Rate:  5.630880571551396e-10  Varinance:  7.340752685499465e-11 \n",
      "\n",
      "Epoch:  18997  Learning Rate:  5.625252505481898e-10  Varinance:  7.330497923067996e-11 \n",
      "\n",
      "Epoch:  18998  Learning Rate:  5.619630064665354e-10  Varinance:  7.320257486163761e-11 \n",
      "\n",
      "Epoch:  18999  Learning Rate:  5.614013243479343e-10  Varinance:  7.310031354774527e-11 \n",
      "\n",
      "Epoch:  19000  Learning Rate:  5.608402036307064e-10  Varinance:  7.299819508916036e-11 \n",
      "\n",
      "Epoch:  19001  Learning Rate:  5.602796437537268e-10  Varinance:  7.289621928631876e-11 \n",
      "\n",
      "Epoch:  19002  Learning Rate:  5.597196441564377e-10  Varinance:  7.279438593993587e-11 \n",
      "\n",
      "Epoch:  19003  Learning Rate:  5.591602042788413e-10  Varinance:  7.269269485100526e-11 \n",
      "\n",
      "Epoch:  19004  Learning Rate:  5.586013235614939e-10  Varinance:  7.259114582079847e-11 \n",
      "\n",
      "Epoch:  19005  Learning Rate:  5.580430014455165e-10  Varinance:  7.248973865086466e-11 \n",
      "\n",
      "Epoch:  19006  Learning Rate:  5.574852373725891e-10  Varinance:  7.238847314303024e-11 \n",
      "\n",
      "Epoch:  19007  Learning Rate:  5.569280307849435e-10  Varinance:  7.22873490993987e-11 \n",
      "\n",
      "Epoch:  19008  Learning Rate:  5.563713811253751e-10  Varinance:  7.218636632234924e-11 \n",
      "\n",
      "Epoch:  19009  Learning Rate:  5.558152878372363e-10  Varinance:  7.208552461453787e-11 \n",
      "\n",
      "Epoch:  19010  Learning Rate:  5.552597503644295e-10  Varinance:  7.1984823778896e-11 \n",
      "\n",
      "Epoch:  19011  Learning Rate:  5.547047681514195e-10  Varinance:  7.188426361863043e-11 \n",
      "\n",
      "Epoch:  19012  Learning Rate:  5.541503406432256e-10  Varinance:  7.178384393722277e-11 \n",
      "\n",
      "Epoch:  19013  Learning Rate:  5.535964672854168e-10  Varinance:  7.168356453842924e-11 \n",
      "\n",
      "Epoch:  19014  Learning Rate:  5.530431475241213e-10  Varinance:  7.15834252262804e-11 \n",
      "\n",
      "Epoch:  19015  Learning Rate:  5.524903808060214e-10  Varinance:  7.148342580507983e-11 \n",
      "\n",
      "Epoch:  19016  Learning Rate:  5.519381665783465e-10  Varinance:  7.138356607940529e-11 \n",
      "\n",
      "Epoch:  19017  Learning Rate:  5.51386504288886e-10  Varinance:  7.128384585410722e-11 \n",
      "\n",
      "Epoch:  19018  Learning Rate:  5.508353933859737e-10  Varinance:  7.118426493430871e-11 \n",
      "\n",
      "Epoch:  19019  Learning Rate:  5.502848333185009e-10  Varinance:  7.108482312540509e-11 \n",
      "\n",
      "Epoch:  19020  Learning Rate:  5.49734823535909e-10  Varinance:  7.098552023306354e-11 \n",
      "\n",
      "Epoch:  19021  Learning Rate:  5.491853634881847e-10  Varinance:  7.088635606322297e-11 \n",
      "\n",
      "Epoch:  19022  Learning Rate:  5.486364526258695e-10  Varinance:  7.078733042209262e-11 \n",
      "\n",
      "Epoch:  19023  Learning Rate:  5.480880904000547e-10  Varinance:  7.068844311615321e-11 \n",
      "\n",
      "Epoch:  19024  Learning Rate:  5.47540276262374e-10  Varinance:  7.058969395215555e-11 \n",
      "\n",
      "Epoch:  19025  Learning Rate:  5.469930096650153e-10  Varinance:  7.049108273712042e-11 \n",
      "\n",
      "Epoch:  19026  Learning Rate:  5.464462900607136e-10  Varinance:  7.039260927833816e-11 \n",
      "\n",
      "Epoch:  19027  Learning Rate:  5.459001169027456e-10  Varinance:  7.029427338336835e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19028  Learning Rate:  5.4535448964494e-10  Varinance:  7.019607486003962e-11 \n",
      "\n",
      "Epoch:  19029  Learning Rate:  5.448094077416715e-10  Varinance:  7.009801351644833e-11 \n",
      "\n",
      "Epoch:  19030  Learning Rate:  5.442648706478542e-10  Varinance:  7.000008916095969e-11 \n",
      "\n",
      "Epoch:  19031  Learning Rate:  5.437208778189529e-10  Varinance:  6.99023016022063e-11 \n",
      "\n",
      "Epoch:  19032  Learning Rate:  5.431774287109765e-10  Varinance:  6.980465064908815e-11 \n",
      "\n",
      "Epoch:  19033  Learning Rate:  5.426345227804724e-10  Varinance:  6.970713611077219e-11 \n",
      "\n",
      "Epoch:  19034  Learning Rate:  5.420921594845361e-10  Varinance:  6.960975779669193e-11 \n",
      "\n",
      "Epoch:  19035  Learning Rate:  5.415503382808064e-10  Varinance:  6.951251551654734e-11 \n",
      "\n",
      "Epoch:  19036  Learning Rate:  5.410090586274583e-10  Varinance:  6.941540908030353e-11 \n",
      "\n",
      "Epoch:  19037  Learning Rate:  5.404683199832139e-10  Varinance:  6.931843829819179e-11 \n",
      "\n",
      "Epoch:  19038  Learning Rate:  5.399281218073363e-10  Varinance:  6.922160298070824e-11 \n",
      "\n",
      "Epoch:  19039  Learning Rate:  5.393884635596238e-10  Varinance:  6.912490293861381e-11 \n",
      "\n",
      "Epoch:  19040  Learning Rate:  5.388493447004196e-10  Varinance:  6.902833798293369e-11 \n",
      "\n",
      "Epoch:  19041  Learning Rate:  5.38310764690607e-10  Varinance:  6.893190792495715e-11 \n",
      "\n",
      "Epoch:  19042  Learning Rate:  5.377727229916021e-10  Varinance:  6.883561257623726e-11 \n",
      "\n",
      "Epoch:  19043  Learning Rate:  5.37235219065365e-10  Varinance:  6.873945174858965e-11 \n",
      "\n",
      "Epoch:  19044  Learning Rate:  5.366982523743936e-10  Varinance:  6.864342525409356e-11 \n",
      "\n",
      "Epoch:  19045  Learning Rate:  5.361618223817173e-10  Varinance:  6.85475329050905e-11 \n",
      "\n",
      "Epoch:  19046  Learning Rate:  5.356259285509082e-10  Varinance:  6.845177451418413e-11 \n",
      "\n",
      "Epoch:  19047  Learning Rate:  5.350905703460741e-10  Varinance:  6.835614989423989e-11 \n",
      "\n",
      "Epoch:  19048  Learning Rate:  5.345557472318531e-10  Varinance:  6.826065885838467e-11 \n",
      "\n",
      "Epoch:  19049  Learning Rate:  5.340214586734257e-10  Varinance:  6.816530122000661e-11 \n",
      "\n",
      "Epoch:  19050  Learning Rate:  5.334877041364996e-10  Varinance:  6.807007679275387e-11 \n",
      "\n",
      "Epoch:  19051  Learning Rate:  5.329544830873222e-10  Varinance:  6.797498539053561e-11 \n",
      "\n",
      "Epoch:  19052  Learning Rate:  5.324217949926741e-10  Varinance:  6.788002682752073e-11 \n",
      "\n",
      "Epoch:  19053  Learning Rate:  5.318896393198635e-10  Varinance:  6.778520091813776e-11 \n",
      "\n",
      "Epoch:  19054  Learning Rate:  5.313580155367366e-10  Varinance:  6.76905074770744e-11 \n",
      "\n",
      "Epoch:  19055  Learning Rate:  5.308269231116712e-10  Varinance:  6.75959463192773e-11 \n",
      "\n",
      "Epoch:  19056  Learning Rate:  5.302963615135715e-10  Varinance:  6.750151725995181e-11 \n",
      "\n",
      "Epoch:  19057  Learning Rate:  5.297663302118774e-10  Varinance:  6.740722011456072e-11 \n",
      "\n",
      "Epoch:  19058  Learning Rate:  5.292368286765596e-10  Varinance:  6.731305469882533e-11 \n",
      "\n",
      "Epoch:  19059  Learning Rate:  5.287078563781126e-10  Varinance:  6.721902082872417e-11 \n",
      "\n",
      "Epoch:  19060  Learning Rate:  5.281794127875661e-10  Varinance:  6.71251183204928e-11 \n",
      "\n",
      "Epoch:  19061  Learning Rate:  5.276514973764782e-10  Varinance:  6.70313469906235e-11 \n",
      "\n",
      "Epoch:  19062  Learning Rate:  5.271241096169298e-10  Varinance:  6.693770665586513e-11 \n",
      "\n",
      "Epoch:  19063  Learning Rate:  5.26597248981535e-10  Varinance:  6.684419713322187e-11 \n",
      "\n",
      "Epoch:  19064  Learning Rate:  5.260709149434349e-10  Varinance:  6.675081823995421e-11 \n",
      "\n",
      "Epoch:  19065  Learning Rate:  5.255451069762917e-10  Varinance:  6.665756979357771e-11 \n",
      "\n",
      "Epoch:  19066  Learning Rate:  5.250198245542993e-10  Varinance:  6.656445161186284e-11 \n",
      "\n",
      "Epoch:  19067  Learning Rate:  5.244950671521771e-10  Varinance:  6.647146351283464e-11 \n",
      "\n",
      "Epoch:  19068  Learning Rate:  5.239708342451639e-10  Varinance:  6.637860531477237e-11 \n",
      "\n",
      "Epoch:  19069  Learning Rate:  5.234471253090286e-10  Varinance:  6.628587683620937e-11 \n",
      "\n",
      "Epoch:  19070  Learning Rate:  5.22923939820064e-10  Varinance:  6.619327789593181e-11 \n",
      "\n",
      "Epoch:  19071  Learning Rate:  5.22401277255081e-10  Varinance:  6.610080831297964e-11 \n",
      "\n",
      "Epoch:  19072  Learning Rate:  5.218791370914188e-10  Varinance:  6.600846790664546e-11 \n",
      "\n",
      "Epoch:  19073  Learning Rate:  5.213575188069391e-10  Varinance:  6.591625649647426e-11 \n",
      "\n",
      "Epoch:  19074  Learning Rate:  5.208364218800196e-10  Varinance:  6.582417390226312e-11 \n",
      "\n",
      "Epoch:  19075  Learning Rate:  5.203158457895655e-10  Varinance:  6.573221994406087e-11 \n",
      "\n",
      "Epoch:  19076  Learning Rate:  5.197957900150024e-10  Varinance:  6.564039444216796e-11 \n",
      "\n",
      "Epoch:  19077  Learning Rate:  5.192762540362708e-10  Varinance:  6.554869721713516e-11 \n",
      "\n",
      "Epoch:  19078  Learning Rate:  5.187572373338366e-10  Varinance:  6.545712808976461e-11 \n",
      "\n",
      "Epoch:  19079  Learning Rate:  5.182387393886847e-10  Varinance:  6.536568688110862e-11 \n",
      "\n",
      "Epoch:  19080  Learning Rate:  5.177207596823136e-10  Varinance:  6.52743734124694e-11 \n",
      "\n",
      "Epoch:  19081  Learning Rate:  5.17203297696747e-10  Varinance:  6.518318750539884e-11 \n",
      "\n",
      "Epoch:  19082  Learning Rate:  5.166863529145195e-10  Varinance:  6.50921289816981e-11 \n",
      "\n",
      "Epoch:  19083  Learning Rate:  5.16169924818688e-10  Varinance:  6.500119766341752e-11 \n",
      "\n",
      "Epoch:  19084  Learning Rate:  5.156540128928261e-10  Varinance:  6.49103933728553e-11 \n",
      "\n",
      "Epoch:  19085  Learning Rate:  5.151386166210182e-10  Varinance:  6.481971593255865e-11 \n",
      "\n",
      "Epoch:  19086  Learning Rate:  5.146237354878699e-10  Varinance:  6.472916516532238e-11 \n",
      "\n",
      "Epoch:  19087  Learning Rate:  5.141093689785017e-10  Varinance:  6.463874089418885e-11 \n",
      "\n",
      "Epoch:  19088  Learning Rate:  5.135955165785437e-10  Varinance:  6.454844294244765e-11 \n",
      "\n",
      "Epoch:  19089  Learning Rate:  5.130821777741449e-10  Varinance:  6.445827113363523e-11 \n",
      "\n",
      "Epoch:  19090  Learning Rate:  5.125693520519685e-10  Varinance:  6.436822529153478e-11 \n",
      "\n",
      "Epoch:  19091  Learning Rate:  5.120570388991851e-10  Varinance:  6.427830524017494e-11 \n",
      "\n",
      "Epoch:  19092  Learning Rate:  5.115452378034832e-10  Varinance:  6.418851080383089e-11 \n",
      "\n",
      "Epoch:  19093  Learning Rate:  5.110339482530636e-10  Varinance:  6.409884180702305e-11 \n",
      "\n",
      "Epoch:  19094  Learning Rate:  5.105231697366329e-10  Varinance:  6.400929807451701e-11 \n",
      "\n",
      "Epoch:  19095  Learning Rate:  5.100129017434146e-10  Varinance:  6.39198794313231e-11 \n",
      "\n",
      "Epoch:  19096  Learning Rate:  5.095031437631424e-10  Varinance:  6.383058570269618e-11 \n",
      "\n",
      "Epoch:  19097  Learning Rate:  5.089938952860546e-10  Varinance:  6.374141671413536e-11 \n",
      "\n",
      "Epoch:  19098  Learning Rate:  5.084851558029044e-10  Varinance:  6.365237229138291e-11 \n",
      "\n",
      "Epoch:  19099  Learning Rate:  5.079769248049542e-10  Varinance:  6.356345226042519e-11 \n",
      "\n",
      "Epoch:  19100  Learning Rate:  5.074692017839694e-10  Varinance:  6.34746564474914e-11 \n",
      "\n",
      "Epoch:  19101  Learning Rate:  5.069619862322287e-10  Varinance:  6.338598467905355e-11 \n",
      "\n",
      "Epoch:  19102  Learning Rate:  5.064552776425182e-10  Varinance:  6.3297436781826e-11 \n",
      "\n",
      "Epoch:  19103  Learning Rate:  5.059490755081258e-10  Varinance:  6.320901258276521e-11 \n",
      "\n",
      "Epoch:  19104  Learning Rate:  5.05443379322851e-10  Varinance:  6.312071190906961e-11 \n",
      "\n",
      "Epoch:  19105  Learning Rate:  5.049381885809995e-10  Varinance:  6.303253458817833e-11 \n",
      "\n",
      "Epoch:  19106  Learning Rate:  5.044335027773768e-10  Varinance:  6.294448044777227e-11 \n",
      "\n",
      "Epoch:  19107  Learning Rate:  5.03929321407299e-10  Varinance:  6.285654931577278e-11 \n",
      "\n",
      "Epoch:  19108  Learning Rate:  5.034256439665863e-10  Varinance:  6.276874102034165e-11 \n",
      "\n",
      "Epoch:  19109  Learning Rate:  5.029224699515578e-10  Varinance:  6.268105538988068e-11 \n",
      "\n",
      "Epoch:  19110  Learning Rate:  5.024197988590411e-10  Varinance:  6.259349225303141e-11 \n",
      "\n",
      "Epoch:  19111  Learning Rate:  5.01917630186367e-10  Varinance:  6.250605143867497e-11 \n",
      "\n",
      "Epoch:  19112  Learning Rate:  5.01415963431363e-10  Varinance:  6.241873277593089e-11 \n",
      "\n",
      "Epoch:  19113  Learning Rate:  5.009147980923662e-10  Varinance:  6.233153609415806e-11 \n",
      "\n",
      "Epoch:  19114  Learning Rate:  5.004141336682073e-10  Varinance:  6.224446122295353e-11 \n",
      "\n",
      "Epoch:  19115  Learning Rate:  4.999139696582238e-10  Varinance:  6.21575079921524e-11 \n",
      "\n",
      "Epoch:  19116  Learning Rate:  4.994143055622534e-10  Varinance:  6.207067623182749e-11 \n",
      "\n",
      "Epoch:  19117  Learning Rate:  4.989151408806284e-10  Varinance:  6.198396577228901e-11 \n",
      "\n",
      "Epoch:  19118  Learning Rate:  4.984164751141859e-10  Varinance:  6.18973764440844e-11 \n",
      "\n",
      "Epoch:  19119  Learning Rate:  4.979183077642618e-10  Varinance:  6.18109080779972e-11 \n",
      "\n",
      "Epoch:  19120  Learning Rate:  4.974206383326851e-10  Varinance:  6.172456050504797e-11 \n",
      "\n",
      "Epoch:  19121  Learning Rate:  4.969234663217883e-10  Varinance:  6.163833355649317e-11 \n",
      "\n",
      "Epoch:  19122  Learning Rate:  4.964267912344009e-10  Varinance:  6.155222706382489e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19123  Learning Rate:  4.959306125738444e-10  Varinance:  6.146624085877071e-11 \n",
      "\n",
      "Epoch:  19124  Learning Rate:  4.954349298439419e-10  Varinance:  6.138037477329322e-11 \n",
      "\n",
      "Epoch:  19125  Learning Rate:  4.949397425490121e-10  Varinance:  6.129462863959e-11 \n",
      "\n",
      "Epoch:  19126  Learning Rate:  4.944450501938644e-10  Varinance:  6.120900229009238e-11 \n",
      "\n",
      "Epoch:  19127  Learning Rate:  4.939508522838081e-10  Varinance:  6.112349555746643e-11 \n",
      "\n",
      "Epoch:  19128  Learning Rate:  4.934571483246471e-10  Varinance:  6.103810827461179e-11 \n",
      "\n",
      "Epoch:  19129  Learning Rate:  4.929639378226737e-10  Varinance:  6.095284027466146e-11 \n",
      "\n",
      "Epoch:  19130  Learning Rate:  4.924712202846792e-10  Varinance:  6.086769139098164e-11 \n",
      "\n",
      "Epoch:  19131  Learning Rate:  4.919789952179478e-10  Varinance:  6.078266145717122e-11 \n",
      "\n",
      "Epoch:  19132  Learning Rate:  4.914872621302509e-10  Varinance:  6.069775030706187e-11 \n",
      "\n",
      "Epoch:  19133  Learning Rate:  4.90996020529857e-10  Varinance:  6.061295777471662e-11 \n",
      "\n",
      "Epoch:  19134  Learning Rate:  4.905052699255264e-10  Varinance:  6.052828369443106e-11 \n",
      "\n",
      "Epoch:  19135  Learning Rate:  4.900150098265048e-10  Varinance:  6.044372790073199e-11 \n",
      "\n",
      "Epoch:  19136  Learning Rate:  4.895252397425338e-10  Varinance:  6.035929022837738e-11 \n",
      "\n",
      "Epoch:  19137  Learning Rate:  4.890359591838452e-10  Varinance:  6.027497051235604e-11 \n",
      "\n",
      "Epoch:  19138  Learning Rate:  4.885471676611546e-10  Varinance:  6.019076858788731e-11 \n",
      "\n",
      "Epoch:  19139  Learning Rate:  4.880588646856725e-10  Varinance:  6.010668429042091e-11 \n",
      "\n",
      "Epoch:  19140  Learning Rate:  4.875710497690975e-10  Varinance:  6.002271745563581e-11 \n",
      "\n",
      "Epoch:  19141  Learning Rate:  4.870837224236112e-10  Varinance:  5.993886791944116e-11 \n",
      "\n",
      "Epoch:  19142  Learning Rate:  4.865968821618896e-10  Varinance:  5.985513551797511e-11 \n",
      "\n",
      "Epoch:  19143  Learning Rate:  4.86110528497089e-10  Varinance:  5.977152008760479e-11 \n",
      "\n",
      "Epoch:  19144  Learning Rate:  4.856246609428574e-10  Varinance:  5.96880214649258e-11 \n",
      "\n",
      "Epoch:  19145  Learning Rate:  4.851392790133289e-10  Varinance:  5.960463948676212e-11 \n",
      "\n",
      "Epoch:  19146  Learning Rate:  4.846543822231182e-10  Varinance:  5.952137399016584e-11 \n",
      "\n",
      "Epoch:  19147  Learning Rate:  4.8416997008733e-10  Varinance:  5.943822481241605e-11 \n",
      "\n",
      "Epoch:  19148  Learning Rate:  4.83686042121554e-10  Varinance:  5.93551917910198e-11 \n",
      "\n",
      "Epoch:  19149  Learning Rate:  4.832025978418587e-10  Varinance:  5.927227476371093e-11 \n",
      "\n",
      "Epoch:  19150  Learning Rate:  4.827196367648016e-10  Varinance:  5.918947356844995e-11 \n",
      "\n",
      "Epoch:  19151  Learning Rate:  4.822371584074232e-10  Varinance:  5.910678804342372e-11 \n",
      "\n",
      "Epoch:  19152  Learning Rate:  4.817551622872416e-10  Varinance:  5.902421802704516e-11 \n",
      "\n",
      "Epoch:  19153  Learning Rate:  4.812736479222624e-10  Varinance:  5.894176335795316e-11 \n",
      "\n",
      "Epoch:  19154  Learning Rate:  4.80792614830973e-10  Varinance:  5.885942387501135e-11 \n",
      "\n",
      "Epoch:  19155  Learning Rate:  4.803120625323369e-10  Varinance:  5.877719941730911e-11 \n",
      "\n",
      "Epoch:  19156  Learning Rate:  4.798319905458031e-10  Varinance:  5.869508982416041e-11 \n",
      "\n",
      "Epoch:  19157  Learning Rate:  4.793523983913017e-10  Varinance:  5.861309493510366e-11 \n",
      "\n",
      "Epoch:  19158  Learning Rate:  4.788732855892369e-10  Varinance:  5.853121458990146e-11 \n",
      "\n",
      "Epoch:  19159  Learning Rate:  4.783946516604976e-10  Varinance:  5.844944862854024e-11 \n",
      "\n",
      "Epoch:  19160  Learning Rate:  4.779164961264516e-10  Varinance:  5.836779689123017e-11 \n",
      "\n",
      "Epoch:  19161  Learning Rate:  4.774388185089398e-10  Varinance:  5.828625921840403e-11 \n",
      "\n",
      "Epoch:  19162  Learning Rate:  4.769616183302863e-10  Varinance:  5.8204835450718123e-11 \n",
      "\n",
      "Epoch:  19163  Learning Rate:  4.764848951132925e-10  Varinance:  5.8123525429051147e-11 \n",
      "\n",
      "Epoch:  19164  Learning Rate:  4.760086483812318e-10  Varinance:  5.8042328994504064e-11 \n",
      "\n",
      "Epoch:  19165  Learning Rate:  4.755328776578593e-10  Varinance:  5.7961245988399847e-11 \n",
      "\n",
      "Epoch:  19166  Learning Rate:  4.750575824674057e-10  Varinance:  5.788027625228313e-11 \n",
      "\n",
      "Epoch:  19167  Learning Rate:  4.745827623345725e-10  Varinance:  5.779941962792008e-11 \n",
      "\n",
      "Epoch:  19168  Learning Rate:  4.741084167845412e-10  Varinance:  5.7718675957297305e-11 \n",
      "\n",
      "Epoch:  19169  Learning Rate:  4.736345453429678e-10  Varinance:  5.7638045082622796e-11 \n",
      "\n",
      "Epoch:  19170  Learning Rate:  4.731611475359775e-10  Varinance:  5.755752684632474e-11 \n",
      "\n",
      "Epoch:  19171  Learning Rate:  4.726882228901742e-10  Varinance:  5.7477121091051444e-11 \n",
      "\n",
      "Epoch:  19172  Learning Rate:  4.72215770932635e-10  Varinance:  5.739682765967105e-11 \n",
      "\n",
      "Epoch:  19173  Learning Rate:  4.717437911909042e-10  Varinance:  5.731664639527121e-11 \n",
      "\n",
      "Epoch:  19174  Learning Rate:  4.712722831930058e-10  Varinance:  5.723657714115894e-11 \n",
      "\n",
      "Epoch:  19175  Learning Rate:  4.70801246467428e-10  Varinance:  5.715661974085958e-11 \n",
      "\n",
      "Epoch:  19176  Learning Rate:  4.70330680543136e-10  Varinance:  5.707677403811765e-11 \n",
      "\n",
      "Epoch:  19177  Learning Rate:  4.698605849495653e-10  Varinance:  5.699703987689577e-11 \n",
      "\n",
      "Epoch:  19178  Learning Rate:  4.693909592166172e-10  Varinance:  5.691741710137451e-11 \n",
      "\n",
      "Epoch:  19179  Learning Rate:  4.689218028746673e-10  Varinance:  5.683790555595214e-11 \n",
      "\n",
      "Epoch:  19180  Learning Rate:  4.684531154545611e-10  Varinance:  5.675850508524428e-11 \n",
      "\n",
      "Epoch:  19181  Learning Rate:  4.679848964876077e-10  Varinance:  5.667921553408384e-11 \n",
      "\n",
      "Epoch:  19182  Learning Rate:  4.675171455055897e-10  Varinance:  5.660003674751986e-11 \n",
      "\n",
      "Epoch:  19183  Learning Rate:  4.67049862040758e-10  Varinance:  5.652096857081847e-11 \n",
      "\n",
      "Epoch:  19184  Learning Rate:  4.665830456258254e-10  Varinance:  5.644201084946174e-11 \n",
      "\n",
      "Epoch:  19185  Learning Rate:  4.661166957939775e-10  Varinance:  5.6363163429147606e-11 \n",
      "\n",
      "Epoch:  19186  Learning Rate:  4.656508120788658e-10  Varinance:  5.628442615578956e-11 \n",
      "\n",
      "Epoch:  19187  Learning Rate:  4.651853940146033e-10  Varinance:  5.6205798875516327e-11 \n",
      "\n",
      "Epoch:  19188  Learning Rate:  4.647204411357736e-10  Varinance:  5.612728143467181e-11 \n",
      "\n",
      "Epoch:  19189  Learning Rate:  4.6425595297742546e-10  Varinance:  5.604887367981395e-11 \n",
      "\n",
      "Epoch:  19190  Learning Rate:  4.6379192907506734e-10  Varinance:  5.5970575457715646e-11 \n",
      "\n",
      "Epoch:  19191  Learning Rate:  4.633283689646769e-10  Varinance:  5.589238661536365e-11 \n",
      "\n",
      "Epoch:  19192  Learning Rate:  4.6286527218269564e-10  Varinance:  5.581430699995845e-11 \n",
      "\n",
      "Epoch:  19193  Learning Rate:  4.6240263826602357e-10  Varinance:  5.5736336458914024e-11 \n",
      "\n",
      "Epoch:  19194  Learning Rate:  4.6194046675202826e-10  Varinance:  5.565847483985769e-11 \n",
      "\n",
      "Epoch:  19195  Learning Rate:  4.6147875717853987e-10  Varinance:  5.5580721990629e-11 \n",
      "\n",
      "Epoch:  19196  Learning Rate:  4.6101750908384546e-10  Varinance:  5.550307775928072e-11 \n",
      "\n",
      "Epoch:  19197  Learning Rate:  4.6055672200669853e-10  Varinance:  5.542554199407763e-11 \n",
      "\n",
      "Epoch:  19198  Learning Rate:  4.6009639548631366e-10  Varinance:  5.534811454349651e-11 \n",
      "\n",
      "Epoch:  19199  Learning Rate:  4.596365290623609e-10  Varinance:  5.52707952562258e-11 \n",
      "\n",
      "Epoch:  19200  Learning Rate:  4.5917712227497566e-10  Varinance:  5.5193583981165323e-11 \n",
      "\n",
      "Epoch:  19201  Learning Rate:  4.587181746647524e-10  Varinance:  5.511648056742618e-11 \n",
      "\n",
      "Epoch:  19202  Learning Rate:  4.5825968577274057e-10  Varinance:  5.5039484864329666e-11 \n",
      "\n",
      "Epoch:  19203  Learning Rate:  4.578016551404526e-10  Varinance:  5.4962596721408145e-11 \n",
      "\n",
      "Epoch:  19204  Learning Rate:  4.573440823098596e-10  Varinance:  5.488581598840401e-11 \n",
      "\n",
      "Epoch:  19205  Learning Rate:  4.5688696682338533e-10  Varinance:  5.4809142515269536e-11 \n",
      "\n",
      "Epoch:  19206  Learning Rate:  4.564303082239177e-10  Varinance:  5.473257615216662e-11 \n",
      "\n",
      "Epoch:  19207  Learning Rate:  4.559741060547946e-10  Varinance:  5.4656116749466466e-11 \n",
      "\n",
      "Epoch:  19208  Learning Rate:  4.5551835985981556e-10  Varinance:  5.457976415774953e-11 \n",
      "\n",
      "Epoch:  19209  Learning Rate:  4.5506306918323603e-10  Varinance:  5.450351822780435e-11 \n",
      "\n",
      "Epoch:  19210  Learning Rate:  4.5460823356976196e-10  Varinance:  5.4427378810628585e-11 \n",
      "\n",
      "Epoch:  19211  Learning Rate:  4.541538525645593e-10  Varinance:  5.4351345757427767e-11 \n",
      "\n",
      "Epoch:  19212  Learning Rate:  4.5369992571324865e-10  Varinance:  5.427541891961533e-11 \n",
      "\n",
      "Epoch:  19213  Learning Rate:  4.532464525618999e-10  Varinance:  5.419959814881227e-11 \n",
      "\n",
      "Epoch:  19214  Learning Rate:  4.527934326570416e-10  Varinance:  5.412388329684688e-11 \n",
      "\n",
      "Epoch:  19215  Learning Rate:  4.523408655456552e-10  Varinance:  5.404827421575459e-11 \n",
      "\n",
      "Epoch:  19216  Learning Rate:  4.5188875077517054e-10  Varinance:  5.397277075777699e-11 \n",
      "\n",
      "Epoch:  19217  Learning Rate:  4.5143708789347425e-10  Varinance:  5.389737277536267e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19218  Learning Rate:  4.5098587644890506e-10  Varinance:  5.382208012116612e-11 \n",
      "\n",
      "Epoch:  19219  Learning Rate:  4.505351159902483e-10  Varinance:  5.374689264804768e-11 \n",
      "\n",
      "Epoch:  19220  Learning Rate:  4.50084806066745e-10  Varinance:  5.367181020907324e-11 \n",
      "\n",
      "Epoch:  19221  Learning Rate:  4.49634946228087e-10  Varinance:  5.359683265751396e-11 \n",
      "\n",
      "Epoch:  19222  Learning Rate:  4.49185536024411e-10  Varinance:  5.3521959846846146e-11 \n",
      "\n",
      "Epoch:  19223  Learning Rate:  4.487365750063086e-10  Varinance:  5.344719163075023e-11 \n",
      "\n",
      "Epoch:  19224  Learning Rate:  4.482880627248201e-10  Varinance:  5.337252786311163e-11 \n",
      "\n",
      "Epoch:  19225  Learning Rate:  4.4783999873143e-10  Varinance:  5.329796839801966e-11 \n",
      "\n",
      "Epoch:  19226  Learning Rate:  4.4739238257807614e-10  Varinance:  5.3223513089767505e-11 \n",
      "\n",
      "Epoch:  19227  Learning Rate:  4.469452138171436e-10  Varinance:  5.314916179285188e-11 \n",
      "\n",
      "Epoch:  19228  Learning Rate:  4.4649849200146053e-10  Varinance:  5.307491436197274e-11 \n",
      "\n",
      "Epoch:  19229  Learning Rate:  4.460522166843068e-10  Varinance:  5.3000770652033244e-11 \n",
      "\n",
      "Epoch:  19230  Learning Rate:  4.4560638741940837e-10  Varinance:  5.292673051813865e-11 \n",
      "\n",
      "Epoch:  19231  Learning Rate:  4.45161003760933e-10  Varinance:  5.285279381559723e-11 \n",
      "\n",
      "Epoch:  19232  Learning Rate:  4.447160652634984e-10  Varinance:  5.2778960399919155e-11 \n",
      "\n",
      "Epoch:  19233  Learning Rate:  4.4427157148216773e-10  Varinance:  5.270523012681648e-11 \n",
      "\n",
      "Epoch:  19234  Learning Rate:  4.4382752197244404e-10  Varinance:  5.2631602852202785e-11 \n",
      "\n",
      "Epoch:  19235  Learning Rate:  4.4338391629027927e-10  Varinance:  5.255807843219297e-11 \n",
      "\n",
      "Epoch:  19236  Learning Rate:  4.4294075399206933e-10  Varinance:  5.248465672310311e-11 \n",
      "\n",
      "Epoch:  19237  Learning Rate:  4.424980346346487e-10  Varinance:  5.241133758144944e-11 \n",
      "\n",
      "Epoch:  19238  Learning Rate:  4.4205575777530116e-10  Varinance:  5.233812086394921e-11 \n",
      "\n",
      "Epoch:  19239  Learning Rate:  4.416139229717466e-10  Varinance:  5.2265006427519605e-11 \n",
      "\n",
      "Epoch:  19240  Learning Rate:  4.4117252978215194e-10  Varinance:  5.219199412927774e-11 \n",
      "\n",
      "Epoch:  19241  Learning Rate:  4.4073157776512533e-10  Varinance:  5.211908382654031e-11 \n",
      "\n",
      "Epoch:  19242  Learning Rate:  4.402910664797116e-10  Varinance:  5.2046275376823316e-11 \n",
      "\n",
      "Epoch:  19243  Learning Rate:  4.3985099548540114e-10  Varinance:  5.197356863784202e-11 \n",
      "\n",
      "Epoch:  19244  Learning Rate:  4.3941136434212425e-10  Varinance:  5.1900963467509884e-11 \n",
      "\n",
      "Epoch:  19245  Learning Rate:  4.3897217261024694e-10  Varinance:  5.182845972393942e-11 \n",
      "\n",
      "Epoch:  19246  Learning Rate:  4.385334198505787e-10  Varinance:  5.175605726544113e-11 \n",
      "\n",
      "Epoch:  19247  Learning Rate:  4.380951056243684e-10  Varinance:  5.1683755950523516e-11 \n",
      "\n",
      "Epoch:  19248  Learning Rate:  4.376572294932987e-10  Varinance:  5.161155563789269e-11 \n",
      "\n",
      "Epoch:  19249  Learning Rate:  4.3721979101949495e-10  Varinance:  5.153945618645214e-11 \n",
      "\n",
      "Epoch:  19250  Learning Rate:  4.3678278976552023e-10  Varinance:  5.146745745530269e-11 \n",
      "\n",
      "Epoch:  19251  Learning Rate:  4.363462252943702e-10  Varinance:  5.139555930374141e-11 \n",
      "\n",
      "Epoch:  19252  Learning Rate:  4.359100971694817e-10  Varinance:  5.132376159126249e-11 \n",
      "\n",
      "Epoch:  19253  Learning Rate:  4.354744049547283e-10  Varinance:  5.12520641775562e-11 \n",
      "\n",
      "Epoch:  19254  Learning Rate:  4.350391482144146e-10  Varinance:  5.1180466922508844e-11 \n",
      "\n",
      "Epoch:  19255  Learning Rate:  4.346043265132854e-10  Varinance:  5.1108969686202435e-11 \n",
      "\n",
      "Epoch:  19256  Learning Rate:  4.341699394165204e-10  Varinance:  5.103757232891447e-11 \n",
      "\n",
      "Epoch:  19257  Learning Rate:  4.337359864897295e-10  Varinance:  5.0966274711117796e-11 \n",
      "\n",
      "Epoch:  19258  Learning Rate:  4.333024672989612e-10  Varinance:  5.089507669347964e-11 \n",
      "\n",
      "Epoch:  19259  Learning Rate:  4.3286938141069786e-10  Varinance:  5.0823978136862404e-11 \n",
      "\n",
      "Epoch:  19260  Learning Rate:  4.324367283918505e-10  Varinance:  5.07529789023227e-11 \n",
      "\n",
      "Epoch:  19261  Learning Rate:  4.320045078097676e-10  Varinance:  5.0682078851111216e-11 \n",
      "\n",
      "Epoch:  19262  Learning Rate:  4.3157271923222994e-10  Varinance:  5.0611277844672485e-11 \n",
      "\n",
      "Epoch:  19263  Learning Rate:  4.3114136222744594e-10  Varinance:  5.054057574464459e-11 \n",
      "\n",
      "Epoch:  19264  Learning Rate:  4.3071043636406015e-10  Varinance:  5.0469972412859077e-11 \n",
      "\n",
      "Epoch:  19265  Learning Rate:  4.3027994121114816e-10  Varinance:  5.0399467711339976e-11 \n",
      "\n",
      "Epoch:  19266  Learning Rate:  4.2984987633821174e-10  Varinance:  5.032906150230459e-11 \n",
      "\n",
      "Epoch:  19267  Learning Rate:  4.2942024131518893e-10  Varinance:  5.0258753648162534e-11 \n",
      "\n",
      "Epoch:  19268  Learning Rate:  4.289910357124417e-10  Varinance:  5.018854401151563e-11 \n",
      "\n",
      "Epoch:  19269  Learning Rate:  4.28562259100766e-10  Varinance:  5.011843245515764e-11 \n",
      "\n",
      "Epoch:  19270  Learning Rate:  4.2813391105138657e-10  Varinance:  5.0048418842073995e-11 \n",
      "\n",
      "Epoch:  19271  Learning Rate:  4.2770599113595234e-10  Varinance:  4.9978503035441704e-11 \n",
      "\n",
      "Epoch:  19272  Learning Rate:  4.2727849892654496e-10  Varinance:  4.990868489862837e-11 \n",
      "\n",
      "Epoch:  19273  Learning Rate:  4.268514339956736e-10  Varinance:  4.983896429519305e-11 \n",
      "\n",
      "Epoch:  19274  Learning Rate:  4.2642479591627026e-10  Varinance:  4.976934108888516e-11 \n",
      "\n",
      "Epoch:  19275  Learning Rate:  4.259985842616984e-10  Varinance:  4.969981514364449e-11 \n",
      "\n",
      "Epoch:  19276  Learning Rate:  4.2557279860574786e-10  Varinance:  4.9630386323600905e-11 \n",
      "\n",
      "Epoch:  19277  Learning Rate:  4.2514743852262973e-10  Varinance:  4.956105449307405e-11 \n",
      "\n",
      "Epoch:  19278  Learning Rate:  4.247225035869857e-10  Varinance:  4.949181951657331e-11 \n",
      "\n",
      "Epoch:  19279  Learning Rate:  4.242979933738821e-10  Varinance:  4.94226812587968e-11 \n",
      "\n",
      "Epoch:  19280  Learning Rate:  4.238739074588058e-10  Varinance:  4.935363958463219e-11 \n",
      "\n",
      "Epoch:  19281  Learning Rate:  4.234502454176721e-10  Varinance:  4.9284694359155714e-11 \n",
      "\n",
      "Epoch:  19282  Learning Rate:  4.2302700682682077e-10  Varinance:  4.921584544763209e-11 \n",
      "\n",
      "Epoch:  19283  Learning Rate:  4.2260419126300995e-10  Varinance:  4.914709271551426e-11 \n",
      "\n",
      "Epoch:  19284  Learning Rate:  4.2218179830342567e-10  Varinance:  4.907843602844311e-11 \n",
      "\n",
      "Epoch:  19285  Learning Rate:  4.2175982752567626e-10  Varinance:  4.9009875252247415e-11 \n",
      "\n",
      "Epoch:  19286  Learning Rate:  4.213382785077881e-10  Varinance:  4.894141025294284e-11 \n",
      "\n",
      "Epoch:  19287  Learning Rate:  4.209171508282136e-10  Varinance:  4.887304089673276e-11 \n",
      "\n",
      "Epoch:  19288  Learning Rate:  4.2049644406582645e-10  Varinance:  4.880476705000726e-11 \n",
      "\n",
      "Epoch:  19289  Learning Rate:  4.200761577999169e-10  Varinance:  4.873658857934312e-11 \n",
      "\n",
      "Epoch:  19290  Learning Rate:  4.196562916102002e-10  Varinance:  4.866850535150346e-11 \n",
      "\n",
      "Epoch:  19291  Learning Rate:  4.192368450768116e-10  Varinance:  4.8600517233437555e-11 \n",
      "\n",
      "Epoch:  19292  Learning Rate:  4.188178177803014e-10  Varinance:  4.853262409228069e-11 \n",
      "\n",
      "Epoch:  19293  Learning Rate:  4.18399209301644e-10  Varinance:  4.846482579535329e-11 \n",
      "\n",
      "Epoch:  19294  Learning Rate:  4.179810192222322e-10  Varinance:  4.839712221016158e-11 \n",
      "\n",
      "Epoch:  19295  Learning Rate:  4.175632471238729e-10  Varinance:  4.832951320439677e-11 \n",
      "\n",
      "Epoch:  19296  Learning Rate:  4.171458925887956e-10  Varinance:  4.8261998645934845e-11 \n",
      "\n",
      "Epoch:  19297  Learning Rate:  4.1672895519964714e-10  Varinance:  4.819457840283639e-11 \n",
      "\n",
      "Epoch:  19298  Learning Rate:  4.163124345394872e-10  Varinance:  4.8127252343346306e-11 \n",
      "\n",
      "Epoch:  19299  Learning Rate:  4.158963301917978e-10  Varinance:  4.8060020335893715e-11 \n",
      "\n",
      "Epoch:  19300  Learning Rate:  4.154806417404719e-10  Varinance:  4.799288224909101e-11 \n",
      "\n",
      "Epoch:  19301  Learning Rate:  4.1506536876982237e-10  Varinance:  4.792583795173467e-11 \n",
      "\n",
      "Epoch:  19302  Learning Rate:  4.1465051086457756e-10  Varinance:  4.785888731280429e-11 \n",
      "\n",
      "Epoch:  19303  Learning Rate:  4.142360676098768e-10  Varinance:  4.7792030201462455e-11 \n",
      "\n",
      "Epoch:  19304  Learning Rate:  4.1382203859127814e-10  Varinance:  4.7725266487054556e-11 \n",
      "\n",
      "Epoch:  19305  Learning Rate:  4.13408423394754e-10  Varinance:  4.765859603910852e-11 \n",
      "\n",
      "Epoch:  19306  Learning Rate:  4.129952216066863e-10  Varinance:  4.7592018727334674e-11 \n",
      "\n",
      "Epoch:  19307  Learning Rate:  4.1258243281387453e-10  Varinance:  4.7525534421624863e-11 \n",
      "\n",
      "Epoch:  19308  Learning Rate:  4.121700566035315e-10  Varinance:  4.7459142992053185e-11 \n",
      "\n",
      "Epoch:  19309  Learning Rate:  4.1175809256327794e-10  Varinance:  4.739284430887509e-11 \n",
      "\n",
      "Epoch:  19310  Learning Rate:  4.113465402811512e-10  Varinance:  4.7326638242527266e-11 \n",
      "\n",
      "Epoch:  19311  Learning Rate:  4.109353993456006e-10  Varinance:  4.726052466362739e-11 \n",
      "\n",
      "Epoch:  19312  Learning Rate:  4.10524669345482e-10  Varinance:  4.719450344297389e-11 \n",
      "\n",
      "Epoch:  19313  Learning Rate:  4.1011434987006697e-10  Varinance:  4.712857445154585e-11 \n",
      "\n",
      "Epoch:  19314  Learning Rate:  4.0970444050903756e-10  Varinance:  4.7062737560502094e-11 \n",
      "\n",
      "Epoch:  19315  Learning Rate:  4.092949408524812e-10  Varinance:  4.699699264118191e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19316  Learning Rate:  4.0888585049089994e-10  Varinance:  4.6931339565104175e-11 \n",
      "\n",
      "Epoch:  19317  Learning Rate:  4.084771690152046e-10  Varinance:  4.686577820396727e-11 \n",
      "\n",
      "Epoch:  19318  Learning Rate:  4.080688960167109e-10  Varinance:  4.680030842964876e-11 \n",
      "\n",
      "Epoch:  19319  Learning Rate:  4.0766103108714725e-10  Varinance:  4.673493011420522e-11 \n",
      "\n",
      "Epoch:  19320  Learning Rate:  4.072535738186501e-10  Varinance:  4.666964312987213e-11 \n",
      "\n",
      "Epoch:  19321  Learning Rate:  4.068465238037592e-10  Varinance:  4.660444734906294e-11 \n",
      "\n",
      "Epoch:  19322  Learning Rate:  4.06439880635426e-10  Varinance:  4.653934264436983e-11 \n",
      "\n",
      "Epoch:  19323  Learning Rate:  4.060336439070088e-10  Varinance:  4.647432888856281e-11 \n",
      "\n",
      "Epoch:  19324  Learning Rate:  4.0562781321226796e-10  Varinance:  4.640940595458961e-11 \n",
      "\n",
      "Epoch:  19325  Learning Rate:  4.05222388145374e-10  Varinance:  4.634457371557548e-11 \n",
      "\n",
      "Epoch:  19326  Learning Rate:  4.048173683009035e-10  Varinance:  4.627983204482303e-11 \n",
      "\n",
      "Epoch:  19327  Learning Rate:  4.044127532738335e-10  Varinance:  4.6215180815811396e-11 \n",
      "\n",
      "Epoch:  19328  Learning Rate:  4.0400854265955053e-10  Varinance:  4.615061990219694e-11 \n",
      "\n",
      "Epoch:  19329  Learning Rate:  4.036047360538453e-10  Varinance:  4.608614917781238e-11 \n",
      "\n",
      "Epoch:  19330  Learning Rate:  4.0320133305290836e-10  Varinance:  4.6021768516666645e-11 \n",
      "\n",
      "Epoch:  19331  Learning Rate:  4.027983332533395e-10  Varinance:  4.5957477792944705e-11 \n",
      "\n",
      "Epoch:  19332  Learning Rate:  4.0239573625213607e-10  Varinance:  4.589327688100728e-11 \n",
      "\n",
      "Epoch:  19333  Learning Rate:  4.019935416467024e-10  Varinance:  4.582916565539075e-11 \n",
      "\n",
      "Epoch:  19334  Learning Rate:  4.0159174903484524e-10  Varinance:  4.576514399080629e-11 \n",
      "\n",
      "Epoch:  19335  Learning Rate:  4.0119035801476915e-10  Varinance:  4.5701211762140586e-11 \n",
      "\n",
      "Epoch:  19336  Learning Rate:  4.0078936818508457e-10  Varinance:  4.563736884445494e-11 \n",
      "\n",
      "Epoch:  19337  Learning Rate:  4.0038877914480303e-10  Varinance:  4.557361511298518e-11 \n",
      "\n",
      "Epoch:  19338  Learning Rate:  3.999885904933325e-10  Varinance:  4.5509950443141434e-11 \n",
      "\n",
      "Epoch:  19339  Learning Rate:  3.9958880183048586e-10  Varinance:  4.5446374710507875e-11 \n",
      "\n",
      "Epoch:  19340  Learning Rate:  3.9918941275647563e-10  Varinance:  4.538288779084264e-11 \n",
      "\n",
      "Epoch:  19341  Learning Rate:  3.987904228719101e-10  Varinance:  4.531948956007695e-11 \n",
      "\n",
      "Epoch:  19342  Learning Rate:  3.9839183177780076e-10  Varinance:  4.5256179894315824e-11 \n",
      "\n",
      "Epoch:  19343  Learning Rate:  3.979936390755577e-10  Varinance:  4.5192958669837195e-11 \n",
      "\n",
      "Epoch:  19344  Learning Rate:  3.975958443669855e-10  Varinance:  4.5129825763091834e-11 \n",
      "\n",
      "Epoch:  19345  Learning Rate:  3.9719844725429085e-10  Varinance:  4.506678105070309e-11 \n",
      "\n",
      "Epoch:  19346  Learning Rate:  3.9680144734007786e-10  Varinance:  4.500382440946671e-11 \n",
      "\n",
      "Epoch:  19347  Learning Rate:  3.96404844227344e-10  Varinance:  4.494095571635066e-11 \n",
      "\n",
      "Epoch:  19348  Learning Rate:  3.960086375194873e-10  Varinance:  4.487817484849432e-11 \n",
      "\n",
      "Epoch:  19349  Learning Rate:  3.9561282682030256e-10  Varinance:  4.481548168320919e-11 \n",
      "\n",
      "Epoch:  19350  Learning Rate:  3.9521741173397617e-10  Varinance:  4.4752876097977986e-11 \n",
      "\n",
      "Epoch:  19351  Learning Rate:  3.9482239186509447e-10  Varinance:  4.469035797045459e-11 \n",
      "\n",
      "Epoch:  19352  Learning Rate:  3.9442776681863896e-10  Varinance:  4.462792717846379e-11 \n",
      "\n",
      "Epoch:  19353  Learning Rate:  3.9403353619998167e-10  Varinance:  4.456558360000104e-11 \n",
      "\n",
      "Epoch:  19354  Learning Rate:  3.9363969961489346e-10  Varinance:  4.4503327113232395e-11 \n",
      "\n",
      "Epoch:  19355  Learning Rate:  3.9324625666953914e-10  Varinance:  4.4441157596493646e-11 \n",
      "\n",
      "Epoch:  19356  Learning Rate:  3.9285320697047274e-10  Varinance:  4.437907492829099e-11 \n",
      "\n",
      "Epoch:  19357  Learning Rate:  3.9246055012464606e-10  Varinance:  4.4317078987300234e-11 \n",
      "\n",
      "Epoch:  19358  Learning Rate:  3.920682857394037e-10  Varinance:  4.425516965236661e-11 \n",
      "\n",
      "Epoch:  19359  Learning Rate:  3.916764134224783e-10  Varinance:  4.419334680250465e-11 \n",
      "\n",
      "Epoch:  19360  Learning Rate:  3.91284932781999e-10  Varinance:  4.413161031689787e-11 \n",
      "\n",
      "Epoch:  19361  Learning Rate:  3.908938434264864e-10  Varinance:  4.4069960074898735e-11 \n",
      "\n",
      "Epoch:  19362  Learning Rate:  3.905031449648485e-10  Varinance:  4.4008395956027767e-11 \n",
      "\n",
      "Epoch:  19363  Learning Rate:  3.9011283700638947e-10  Varinance:  4.394691783997428e-11 \n",
      "\n",
      "Epoch:  19364  Learning Rate:  3.8972291916079855e-10  Varinance:  4.388552560659547e-11 \n",
      "\n",
      "Epoch:  19365  Learning Rate:  3.893333910381593e-10  Varinance:  4.382421913591641e-11 \n",
      "\n",
      "Epoch:  19366  Learning Rate:  3.8894425224894487e-10  Varinance:  4.376299830812974e-11 \n",
      "\n",
      "Epoch:  19367  Learning Rate:  3.8855550240401373e-10  Varinance:  4.370186300359548e-11 \n",
      "\n",
      "Epoch:  19368  Learning Rate:  3.881671411146174e-10  Varinance:  4.364081310284094e-11 \n",
      "\n",
      "Epoch:  19369  Learning Rate:  3.8777916799239585e-10  Varinance:  4.357984848655985e-11 \n",
      "\n",
      "Epoch:  19370  Learning Rate:  3.8739158264937327e-10  Varinance:  4.35189690356131e-11 \n",
      "\n",
      "Epoch:  19371  Learning Rate:  3.870043846979656e-10  Varinance:  4.3458174631027827e-11 \n",
      "\n",
      "Epoch:  19372  Learning Rate:  3.8661757375097626e-10  Varinance:  4.3397465153997375e-11 \n",
      "\n",
      "Epoch:  19373  Learning Rate:  3.862311494215916e-10  Varinance:  4.333684048588107e-11 \n",
      "\n",
      "Epoch:  19374  Learning Rate:  3.858451113233885e-10  Varinance:  4.327630050820398e-11 \n",
      "\n",
      "Epoch:  19375  Learning Rate:  3.8545945907033016e-10  Varinance:  4.3215845102656803e-11 \n",
      "\n",
      "Epoch:  19376  Learning Rate:  3.850741922767617e-10  Varinance:  4.315547415109507e-11 \n",
      "\n",
      "Epoch:  19377  Learning Rate:  3.8468931055741765e-10  Varinance:  4.309518753553981e-11 \n",
      "\n",
      "Epoch:  19378  Learning Rate:  3.843048135274175e-10  Varinance:  4.3034985138176716e-11 \n",
      "\n",
      "Epoch:  19379  Learning Rate:  3.839207008022616e-10  Varinance:  4.297486684135607e-11 \n",
      "\n",
      "Epoch:  19380  Learning Rate:  3.835369719978385e-10  Varinance:  4.291483252759249e-11 \n",
      "\n",
      "Epoch:  19381  Learning Rate:  3.8315362673042067e-10  Varinance:  4.2854882079564707e-11 \n",
      "\n",
      "Epoch:  19382  Learning Rate:  3.827706646166602e-10  Varinance:  4.279501538011554e-11 \n",
      "\n",
      "Epoch:  19383  Learning Rate:  3.823880852735962e-10  Varinance:  4.2735232312250975e-11 \n",
      "\n",
      "Epoch:  19384  Learning Rate:  3.820058883186507e-10  Varinance:  4.267553275914092e-11 \n",
      "\n",
      "Epoch:  19385  Learning Rate:  3.8162407336962404e-10  Varinance:  4.2615916604118305e-11 \n",
      "\n",
      "Epoch:  19386  Learning Rate:  3.812426400447025e-10  Varinance:  4.2556383730679065e-11 \n",
      "\n",
      "Epoch:  19387  Learning Rate:  3.808615879624542e-10  Varinance:  4.2496934022481886e-11 \n",
      "\n",
      "Epoch:  19388  Learning Rate:  3.8048091674182416e-10  Varinance:  4.243756736334797e-11 \n",
      "\n",
      "Epoch:  19389  Learning Rate:  3.8010062600214265e-10  Varinance:  4.2378283637260975e-11 \n",
      "\n",
      "Epoch:  19390  Learning Rate:  3.797207153631201e-10  Varinance:  4.231908272836617e-11 \n",
      "\n",
      "Epoch:  19391  Learning Rate:  3.793411844448432e-10  Varinance:  4.225996452097112e-11 \n",
      "\n",
      "Epoch:  19392  Learning Rate:  3.7896203286778374e-10  Varinance:  4.2200928899544867e-11 \n",
      "\n",
      "Epoch:  19393  Learning Rate:  3.7858326025278737e-10  Varinance:  4.214197574871783e-11 \n",
      "\n",
      "Epoch:  19394  Learning Rate:  3.7820486622108284e-10  Varinance:  4.208310495328162e-11 \n",
      "\n",
      "Epoch:  19395  Learning Rate:  3.7782685039427735e-10  Varinance:  4.202431639818876e-11 \n",
      "\n",
      "Epoch:  19396  Learning Rate:  3.774492123943524e-10  Varinance:  4.196560996855267e-11 \n",
      "\n",
      "Epoch:  19397  Learning Rate:  3.770719518436713e-10  Varinance:  4.190698554964678e-11 \n",
      "\n",
      "Epoch:  19398  Learning Rate:  3.766950683649748e-10  Varinance:  4.184844302690527e-11 \n",
      "\n",
      "Epoch:  19399  Learning Rate:  3.763185615813767e-10  Varinance:  4.178998228592219e-11 \n",
      "\n",
      "Epoch:  19400  Learning Rate:  3.759424311163716e-10  Varinance:  4.173160321245143e-11 \n",
      "\n",
      "Epoch:  19401  Learning Rate:  3.7556667659383027e-10  Varinance:  4.1673305692406464e-11 \n",
      "\n",
      "Epoch:  19402  Learning Rate:  3.7519129763799545e-10  Varinance:  4.161508961186014e-11 \n",
      "\n",
      "Epoch:  19403  Learning Rate:  3.748162938734896e-10  Varinance:  4.155695485704462e-11 \n",
      "\n",
      "Epoch:  19404  Learning Rate:  3.7444166492531017e-10  Varinance:  4.1498901314350544e-11 \n",
      "\n",
      "Epoch:  19405  Learning Rate:  3.7406741041882545e-10  Varinance:  4.14409288703277e-11 \n",
      "\n",
      "Epoch:  19406  Learning Rate:  3.736935299797824e-10  Varinance:  4.138303741168421e-11 \n",
      "\n",
      "Epoch:  19407  Learning Rate:  3.7332002323430183e-10  Varinance:  4.132522682528648e-11 \n",
      "\n",
      "Epoch:  19408  Learning Rate:  3.7294688980887417e-10  Varinance:  4.1267496998158935e-11 \n",
      "\n",
      "Epoch:  19409  Learning Rate:  3.7257412933036755e-10  Varinance:  4.120984781748385e-11 \n",
      "\n",
      "Epoch:  19410  Learning Rate:  3.722017414260225e-10  Varinance:  4.115227917060121e-11 \n",
      "\n",
      "Epoch:  19411  Learning Rate:  3.7182972572344866e-10  Varinance:  4.109479094500799e-11 \n",
      "\n",
      "Epoch:  19412  Learning Rate:  3.7145808185063144e-10  Varinance:  4.1037383028358733e-11 \n",
      "\n",
      "Epoch:  19413  Learning Rate:  3.7108680943592844e-10  Varinance:  4.0980055308464785e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19414  Learning Rate:  3.707159081080644e-10  Varinance:  4.0922807673294217e-11 \n",
      "\n",
      "Epoch:  19415  Learning Rate:  3.703453774961394e-10  Varinance:  4.086564001097162e-11 \n",
      "\n",
      "Epoch:  19416  Learning Rate:  3.6997521722962407e-10  Varinance:  4.080855220977784e-11 \n",
      "\n",
      "Epoch:  19417  Learning Rate:  3.6960542693835546e-10  Varinance:  4.075154415814996e-11 \n",
      "\n",
      "Epoch:  19418  Learning Rate:  3.6923600625254464e-10  Varinance:  4.069461574468047e-11 \n",
      "\n",
      "Epoch:  19419  Learning Rate:  3.6886695480277214e-10  Varinance:  4.063776685811793e-11 \n",
      "\n",
      "Epoch:  19420  Learning Rate:  3.684982722199839e-10  Varinance:  4.058099738736615e-11 \n",
      "\n",
      "Epoch:  19421  Learning Rate:  3.681299581354985e-10  Varinance:  4.052430722148417e-11 \n",
      "\n",
      "Epoch:  19422  Learning Rate:  3.677620121810033e-10  Varinance:  4.046769624968599e-11 \n",
      "\n",
      "Epoch:  19423  Learning Rate:  3.673944339885495e-10  Varinance:  4.041116436134038e-11 \n",
      "\n",
      "Epoch:  19424  Learning Rate:  3.6702722319056176e-10  Varinance:  4.0354711445970796e-11 \n",
      "\n",
      "Epoch:  19425  Learning Rate:  3.666603794198264e-10  Varinance:  4.029833739325459e-11 \n",
      "\n",
      "Epoch:  19426  Learning Rate:  3.662939023095011e-10  Varinance:  4.024204209302368e-11 \n",
      "\n",
      "Epoch:  19427  Learning Rate:  3.659277914931099e-10  Varinance:  4.018582543526373e-11 \n",
      "\n",
      "Epoch:  19428  Learning Rate:  3.6556204660453935e-10  Varinance:  4.0129687310114084e-11 \n",
      "\n",
      "Epoch:  19429  Learning Rate:  3.6519666727804587e-10  Varinance:  4.007362760786757e-11 \n",
      "\n",
      "Epoch:  19430  Learning Rate:  3.648316531482514e-10  Varinance:  4.001764621897026e-11 \n",
      "\n",
      "Epoch:  19431  Learning Rate:  3.644670038501392e-10  Varinance:  3.996174303402143e-11 \n",
      "\n",
      "Epoch:  19432  Learning Rate:  3.6410271901906123e-10  Varinance:  3.990591794377274e-11 \n",
      "\n",
      "Epoch:  19433  Learning Rate:  3.637387982907339e-10  Varinance:  3.98501708391289e-11 \n",
      "\n",
      "Epoch:  19434  Learning Rate:  3.633752413012339e-10  Varinance:  3.9794501611146866e-11 \n",
      "\n",
      "Epoch:  19435  Learning Rate:  3.6301204768700547e-10  Varinance:  3.9738910151035814e-11 \n",
      "\n",
      "Epoch:  19436  Learning Rate:  3.6264921708485626e-10  Varinance:  3.9683396350156864e-11 \n",
      "\n",
      "Epoch:  19437  Learning Rate:  3.622867491319531e-10  Varinance:  3.962796010002293e-11 \n",
      "\n",
      "Epoch:  19438  Learning Rate:  3.619246434658292e-10  Varinance:  3.95726012922986e-11 \n",
      "\n",
      "Epoch:  19439  Learning Rate:  3.6156289972438025e-10  Varinance:  3.9517319818799366e-11 \n",
      "\n",
      "Epoch:  19440  Learning Rate:  3.6120151754585987e-10  Varinance:  3.946211557149232e-11 \n",
      "\n",
      "Epoch:  19441  Learning Rate:  3.6084049656888715e-10  Varinance:  3.940698844249528e-11 \n",
      "\n",
      "Epoch:  19442  Learning Rate:  3.6047983643244234e-10  Varinance:  3.935193832407681e-11 \n",
      "\n",
      "Epoch:  19443  Learning Rate:  3.601195367758627e-10  Varinance:  3.9296965108655946e-11 \n",
      "\n",
      "Epoch:  19444  Learning Rate:  3.597595972388499e-10  Varinance:  3.924206868880202e-11 \n",
      "\n",
      "Epoch:  19445  Learning Rate:  3.5940001746146553e-10  Varinance:  3.918724895723457e-11 \n",
      "\n",
      "Epoch:  19446  Learning Rate:  3.5904079708412737e-10  Varinance:  3.913250580682261e-11 \n",
      "\n",
      "Epoch:  19447  Learning Rate:  3.5868193574761616e-10  Varinance:  3.90778391305852e-11 \n",
      "\n",
      "Epoch:  19448  Learning Rate:  3.5832343309307185e-10  Varinance:  3.9023248821690724e-11 \n",
      "\n",
      "Epoch:  19449  Learning Rate:  3.5796528876198924e-10  Varinance:  3.896873477345681e-11 \n",
      "\n",
      "Epoch:  19450  Learning Rate:  3.5760750239622524e-10  Varinance:  3.89142968793501e-11 \n",
      "\n",
      "Epoch:  19451  Learning Rate:  3.5725007363799465e-10  Varinance:  3.885993503298607e-11 \n",
      "\n",
      "Epoch:  19452  Learning Rate:  3.568930021298663e-10  Varinance:  3.880564912812896e-11 \n",
      "\n",
      "Epoch:  19453  Learning Rate:  3.565362875147697e-10  Varinance:  3.8751439058690984e-11 \n",
      "\n",
      "Epoch:  19454  Learning Rate:  3.5617992943599166e-10  Varinance:  3.869730471873298e-11 \n",
      "\n",
      "Epoch:  19455  Learning Rate:  3.558239275371715e-10  Varinance:  3.864324600246364e-11 \n",
      "\n",
      "Epoch:  19456  Learning Rate:  3.5546828146230974e-10  Varinance:  3.858926280423944e-11 \n",
      "\n",
      "Epoch:  19457  Learning Rate:  3.5511299085575785e-10  Varinance:  3.853535501856446e-11 \n",
      "\n",
      "Epoch:  19458  Learning Rate:  3.5475805536222636e-10  Varinance:  3.848152254009009e-11 \n",
      "\n",
      "Epoch:  19459  Learning Rate:  3.544034746267811e-10  Varinance:  3.842776526361511e-11 \n",
      "\n",
      "Epoch:  19460  Learning Rate:  3.540492482948387e-10  Varinance:  3.837408308408479e-11 \n",
      "\n",
      "Epoch:  19461  Learning Rate:  3.5369537601217413e-10  Varinance:  3.832047589659159e-11 \n",
      "\n",
      "Epoch:  19462  Learning Rate:  3.533418574249163e-10  Varinance:  3.8266943596374384e-11 \n",
      "\n",
      "Epoch:  19463  Learning Rate:  3.529886921795441e-10  Varinance:  3.8213486078818395e-11 \n",
      "\n",
      "Epoch:  19464  Learning Rate:  3.5263587992289346e-10  Varinance:  3.816010323945498e-11 \n",
      "\n",
      "Epoch:  19465  Learning Rate:  3.522834203021534e-10  Varinance:  3.810679497396158e-11 \n",
      "\n",
      "Epoch:  19466  Learning Rate:  3.519313129648617e-10  Varinance:  3.8053561178160966e-11 \n",
      "\n",
      "Epoch:  19467  Learning Rate:  3.515795575589123e-10  Varinance:  3.8000401748021835e-11 \n",
      "\n",
      "Epoch:  19468  Learning Rate:  3.5122815373255105e-10  Varinance:  3.794731657965808e-11 \n",
      "\n",
      "Epoch:  19469  Learning Rate:  3.5087710113437154e-10  Varinance:  3.789430556932874e-11 \n",
      "\n",
      "Epoch:  19470  Learning Rate:  3.505263994133225e-10  Varinance:  3.784136861343774e-11 \n",
      "\n",
      "Epoch:  19471  Learning Rate:  3.5017604821870316e-10  Varinance:  3.778850560853376e-11 \n",
      "\n",
      "Epoch:  19472  Learning Rate:  3.498260472001601e-10  Varinance:  3.7735716451310103e-11 \n",
      "\n",
      "Epoch:  19473  Learning Rate:  3.4947639600769333e-10  Varinance:  3.768300103860401e-11 \n",
      "\n",
      "Epoch:  19474  Learning Rate:  3.4912709429165293e-10  Varinance:  3.763035926739721e-11 \n",
      "\n",
      "Epoch:  19475  Learning Rate:  3.487781417027347e-10  Varinance:  3.757779103481524e-11 \n",
      "\n",
      "Epoch:  19476  Learning Rate:  3.484295378919872e-10  Varinance:  3.7525296238127343e-11 \n",
      "\n",
      "Epoch:  19477  Learning Rate:  3.480812825108079e-10  Varinance:  3.747287477474626e-11 \n",
      "\n",
      "Epoch:  19478  Learning Rate:  3.477333752109389e-10  Varinance:  3.7420526542228044e-11 \n",
      "\n",
      "Epoch:  19479  Learning Rate:  3.4738581564447404e-10  Varinance:  3.7368251438272e-11 \n",
      "\n",
      "Epoch:  19480  Learning Rate:  3.4703860346385505e-10  Varinance:  3.731604936071994e-11 \n",
      "\n",
      "Epoch:  19481  Learning Rate:  3.4669173832186717e-10  Varinance:  3.726392020755679e-11 \n",
      "\n",
      "Epoch:  19482  Learning Rate:  3.4634521987164654e-10  Varinance:  3.721186387690986e-11 \n",
      "\n",
      "Epoch:  19483  Learning Rate:  3.459990477666759e-10  Varinance:  3.7159880267048746e-11 \n",
      "\n",
      "Epoch:  19484  Learning Rate:  3.4565322166078057e-10  Varinance:  3.7107969276385185e-11 \n",
      "\n",
      "Epoch:  19485  Learning Rate:  3.4530774120813573e-10  Varinance:  3.705613080347282e-11 \n",
      "\n",
      "Epoch:  19486  Learning Rate:  3.449626060632621e-10  Varinance:  3.7004364747007147e-11 \n",
      "\n",
      "Epoch:  19487  Learning Rate:  3.4461781588102204e-10  Varinance:  3.6952671005824776e-11 \n",
      "\n",
      "Epoch:  19488  Learning Rate:  3.442733703166278e-10  Varinance:  3.6901049478904036e-11 \n",
      "\n",
      "Epoch:  19489  Learning Rate:  3.4392926902563136e-10  Varinance:  3.6849500065364254e-11 \n",
      "\n",
      "Epoch:  19490  Learning Rate:  3.4358551166393264e-10  Varinance:  3.679802266446568e-11 \n",
      "\n",
      "Epoch:  19491  Learning Rate:  3.432420978877754e-10  Varinance:  3.674661717560929e-11 \n",
      "\n",
      "Epoch:  19492  Learning Rate:  3.428990273537434e-10  Varinance:  3.6695283498336606e-11 \n",
      "\n",
      "Epoch:  19493  Learning Rate:  3.425562997187674e-10  Varinance:  3.664402153232959e-11 \n",
      "\n",
      "Epoch:  19494  Learning Rate:  3.4221391464012077e-10  Varinance:  3.6592831177409994e-11 \n",
      "\n",
      "Epoch:  19495  Learning Rate:  3.418718717754162e-10  Varinance:  3.6541712333539874e-11 \n",
      "\n",
      "Epoch:  19496  Learning Rate:  3.4153017078261185e-10  Varinance:  3.649066490082091e-11 \n",
      "\n",
      "Epoch:  19497  Learning Rate:  3.4118881132000796e-10  Varinance:  3.6439688779494354e-11 \n",
      "\n",
      "Epoch:  19498  Learning Rate:  3.408477930462426e-10  Varinance:  3.638878386994079e-11 \n",
      "\n",
      "Epoch:  19499  Learning Rate:  3.405071156202987e-10  Varinance:  3.633795007267999e-11 \n",
      "\n",
      "Epoch:  19500  Learning Rate:  3.401667787015e-10  Varinance:  3.6287187288370816e-11 \n",
      "\n",
      "Epoch:  19501  Learning Rate:  3.398267819495071e-10  Varinance:  3.623649541781052e-11 \n",
      "\n",
      "Epoch:  19502  Learning Rate:  3.3948712502432454e-10  Varinance:  3.6185874361935316e-11 \n",
      "\n",
      "Epoch:  19503  Learning Rate:  3.3914780758629646e-10  Varinance:  3.613532402181969e-11 \n",
      "\n",
      "Epoch:  19504  Learning Rate:  3.3880882929610306e-10  Varinance:  3.608484429867633e-11 \n",
      "\n",
      "Epoch:  19505  Learning Rate:  3.3847018981476716e-10  Varinance:  3.603443509385589e-11 \n",
      "\n",
      "Epoch:  19506  Learning Rate:  3.381318888036505e-10  Varinance:  3.5984096308846875e-11 \n",
      "\n",
      "Epoch:  19507  Learning Rate:  3.377939259244496e-10  Varinance:  3.593382784527551e-11 \n",
      "\n",
      "Epoch:  19508  Learning Rate:  3.374563008392028e-10  Varinance:  3.588362960490507e-11 \n",
      "\n",
      "Epoch:  19509  Learning Rate:  3.3711901321028615e-10  Varinance:  3.583350148963645e-11 \n",
      "\n",
      "Epoch:  19510  Learning Rate:  3.3678206270040956e-10  Varinance:  3.578344340150745e-11 \n",
      "\n",
      "Epoch:  19511  Learning Rate:  3.364454489726238e-10  Varinance:  3.573345524269272e-11 \n",
      "\n",
      "Epoch:  19512  Learning Rate:  3.3610917169031625e-10  Varinance:  3.568353691550357e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19513  Learning Rate:  3.3577323051720717e-10  Varinance:  3.563368832238778e-11 \n",
      "\n",
      "Epoch:  19514  Learning Rate:  3.3543762511735656e-10  Varinance:  3.558390936592954e-11 \n",
      "\n",
      "Epoch:  19515  Learning Rate:  3.3510235515516027e-10  Varinance:  3.5534199948848734e-11 \n",
      "\n",
      "Epoch:  19516  Learning Rate:  3.3476742029534583e-10  Varinance:  3.548455997400152e-11 \n",
      "\n",
      "Epoch:  19517  Learning Rate:  3.344328202029808e-10  Varinance:  3.543498934437965e-11 \n",
      "\n",
      "Epoch:  19518  Learning Rate:  3.3409855454346267e-10  Varinance:  3.538548796311039e-11 \n",
      "\n",
      "Epoch:  19519  Learning Rate:  3.337646229825269e-10  Varinance:  3.5336055733456314e-11 \n",
      "\n",
      "Epoch:  19520  Learning Rate:  3.334310251862431e-10  Varinance:  3.528669255881517e-11 \n",
      "\n",
      "Epoch:  19521  Learning Rate:  3.3309776082101105e-10  Varinance:  3.523739834271975e-11 \n",
      "\n",
      "Epoch:  19522  Learning Rate:  3.3276482955356763e-10  Varinance:  3.5188172988837245e-11 \n",
      "\n",
      "Epoch:  19523  Learning Rate:  3.3243223105098267e-10  Varinance:  3.513901640096979e-11 \n",
      "\n",
      "Epoch:  19524  Learning Rate:  3.3209996498065526e-10  Varinance:  3.508992848305379e-11 \n",
      "\n",
      "Epoch:  19525  Learning Rate:  3.3176803101032057e-10  Varinance:  3.504090913915983e-11 \n",
      "\n",
      "Epoch:  19526  Learning Rate:  3.314364288080457e-10  Varinance:  3.499195827349253e-11 \n",
      "\n",
      "Epoch:  19527  Learning Rate:  3.3110515804222604e-10  Varinance:  3.4943075790390296e-11 \n",
      "\n",
      "Epoch:  19528  Learning Rate:  3.3077421838159204e-10  Varinance:  3.489426159432533e-11 \n",
      "\n",
      "Epoch:  19529  Learning Rate:  3.304436094952051e-10  Varinance:  3.4845515589902894e-11 \n",
      "\n",
      "Epoch:  19530  Learning Rate:  3.301133310524541e-10  Varinance:  3.4796837681861886e-11 \n",
      "\n",
      "Epoch:  19531  Learning Rate:  3.2978338272306166e-10  Varinance:  3.4748227775074154e-11 \n",
      "\n",
      "Epoch:  19532  Learning Rate:  3.2945376417708056e-10  Varinance:  3.469968577454444e-11 \n",
      "\n",
      "Epoch:  19533  Learning Rate:  3.291244750848899e-10  Varinance:  3.46512115854102e-11 \n",
      "\n",
      "Epoch:  19534  Learning Rate:  3.287955151172018e-10  Varinance:  3.4602805112941385e-11 \n",
      "\n",
      "Epoch:  19535  Learning Rate:  3.2846688394505736e-10  Varinance:  3.455446626254043e-11 \n",
      "\n",
      "Epoch:  19536  Learning Rate:  3.2813858123982307e-10  Varinance:  3.4506194939741533e-11 \n",
      "\n",
      "Epoch:  19537  Learning Rate:  3.278106066731974e-10  Varinance:  3.445799105021124e-11 \n",
      "\n",
      "Epoch:  19538  Learning Rate:  3.274829599172068e-10  Varinance:  3.440985449974773e-11 \n",
      "\n",
      "Epoch:  19539  Learning Rate:  3.2715564064420237e-10  Varinance:  3.436178519428082e-11 \n",
      "\n",
      "Epoch:  19540  Learning Rate:  3.2682864852686576e-10  Varinance:  3.43137830398717e-11 \n",
      "\n",
      "Epoch:  19541  Learning Rate:  3.265019832382061e-10  Varinance:  3.4265847942712795e-11 \n",
      "\n",
      "Epoch:  19542  Learning Rate:  3.2617564445155567e-10  Varinance:  3.4217979809127715e-11 \n",
      "\n",
      "Epoch:  19543  Learning Rate:  3.25849631840577e-10  Varinance:  3.417017854557056e-11 \n",
      "\n",
      "Epoch:  19544  Learning Rate:  3.2552394507925835e-10  Varinance:  3.4122444058626474e-11 \n",
      "\n",
      "Epoch:  19545  Learning Rate:  3.2519858384191083e-10  Varinance:  3.407477625501098e-11 \n",
      "\n",
      "Epoch:  19546  Learning Rate:  3.248735478031742e-10  Varinance:  3.40271750415699e-11 \n",
      "\n",
      "Epoch:  19547  Learning Rate:  3.245488366380136e-10  Varinance:  3.3979640325279206e-11 \n",
      "\n",
      "Epoch:  19548  Learning Rate:  3.242244500217156e-10  Varinance:  3.393217201324482e-11 \n",
      "\n",
      "Epoch:  19549  Learning Rate:  3.2390038762989575e-10  Varinance:  3.388477001270255e-11 \n",
      "\n",
      "Epoch:  19550  Learning Rate:  3.235766491384893e-10  Varinance:  3.383743423101743e-11 \n",
      "\n",
      "Epoch:  19551  Learning Rate:  3.2325323422375906e-10  Varinance:  3.379016457568426e-11 \n",
      "\n",
      "Epoch:  19552  Learning Rate:  3.229301425622911e-10  Varinance:  3.3742960954326965e-11 \n",
      "\n",
      "Epoch:  19553  Learning Rate:  3.2260737383099147e-10  Varinance:  3.3695823274698484e-11 \n",
      "\n",
      "Epoch:  19554  Learning Rate:  3.2228492770709255e-10  Varinance:  3.3648751444680643e-11 \n",
      "\n",
      "Epoch:  19555  Learning Rate:  3.219628038681493e-10  Varinance:  3.3601745372283955e-11 \n",
      "\n",
      "Epoch:  19556  Learning Rate:  3.2164100199203563e-10  Varinance:  3.355480496564755e-11 \n",
      "\n",
      "Epoch:  19557  Learning Rate:  3.213195217569508e-10  Varinance:  3.3507930133038524e-11 \n",
      "\n",
      "Epoch:  19558  Learning Rate:  3.209983628414156e-10  Varinance:  3.3461120782852495e-11 \n",
      "\n",
      "Epoch:  19559  Learning Rate:  3.206775249242689e-10  Varinance:  3.34143768236129e-11 \n",
      "\n",
      "Epoch:  19560  Learning Rate:  3.2035700768467373e-10  Varinance:  3.336769816397101e-11 \n",
      "\n",
      "Epoch:  19561  Learning Rate:  3.200368108021142e-10  Varinance:  3.332108471270566e-11 \n",
      "\n",
      "Epoch:  19562  Learning Rate:  3.1971693395639094e-10  Varinance:  3.3274536378723144e-11 \n",
      "\n",
      "Epoch:  19563  Learning Rate:  3.193973768276283e-10  Varinance:  3.322805307105712e-11 \n",
      "\n",
      "Epoch:  19564  Learning Rate:  3.1907813909627027e-10  Varinance:  3.3181634698867976e-11 \n",
      "\n",
      "Epoch:  19565  Learning Rate:  3.1875922044307674e-10  Varinance:  3.313528117144335e-11 \n",
      "\n",
      "Epoch:  19566  Learning Rate:  3.1844062054913027e-10  Varinance:  3.3088992398197475e-11 \n",
      "\n",
      "Epoch:  19567  Learning Rate:  3.1812233909583195e-10  Varinance:  3.304276828867114e-11 \n",
      "\n",
      "Epoch:  19568  Learning Rate:  3.1780437576489813e-10  Varinance:  3.299660875253151e-11 \n",
      "\n",
      "Epoch:  19569  Learning Rate:  3.174867302383666e-10  Varinance:  3.2950513699571924e-11 \n",
      "\n",
      "Epoch:  19570  Learning Rate:  3.1716940219859284e-10  Varinance:  3.290448303971185e-11 \n",
      "\n",
      "Epoch:  19571  Learning Rate:  3.168523913282466e-10  Varinance:  3.285851668299626e-11 \n",
      "\n",
      "Epoch:  19572  Learning Rate:  3.165356973103181e-10  Varinance:  3.2812614539596136e-11 \n",
      "\n",
      "Epoch:  19573  Learning Rate:  3.162193198281144e-10  Varinance:  3.276677651980783e-11 \n",
      "\n",
      "Epoch:  19574  Learning Rate:  3.159032585652558e-10  Varinance:  3.2721002534053014e-11 \n",
      "\n",
      "Epoch:  19575  Learning Rate:  3.15587513205682e-10  Varinance:  3.267529249287848e-11 \n",
      "\n",
      "Epoch:  19576  Learning Rate:  3.152720834336489e-10  Varinance:  3.2629646306956e-11 \n",
      "\n",
      "Epoch:  19577  Learning Rate:  3.149569689337244e-10  Varinance:  3.258406388708224e-11 \n",
      "\n",
      "Epoch:  19578  Learning Rate:  3.1464216939079503e-10  Varinance:  3.2538545144178135e-11 \n",
      "\n",
      "Epoch:  19579  Learning Rate:  3.143276844900624e-10  Varinance:  3.249308998928941e-11 \n",
      "\n",
      "Epoch:  19580  Learning Rate:  3.1401351391703935e-10  Varinance:  3.244769833358594e-11 \n",
      "\n",
      "Epoch:  19581  Learning Rate:  3.136996573575575e-10  Varinance:  3.2402370088361686e-11 \n",
      "\n",
      "Epoch:  19582  Learning Rate:  3.1338611449775806e-10  Varinance:  3.235710516503453e-11 \n",
      "\n",
      "Epoch:  19583  Learning Rate:  3.130728850240992e-10  Varinance:  3.231190347514611e-11 \n",
      "\n",
      "Epoch:  19584  Learning Rate:  3.127599686233526e-10  Varinance:  3.226676493036175e-11 \n",
      "\n",
      "Epoch:  19585  Learning Rate:  3.1244736498259947e-10  Varinance:  3.2221689442469804e-11 \n",
      "\n",
      "Epoch:  19586  Learning Rate:  3.121350737892375e-10  Varinance:  3.217667692338225e-11 \n",
      "\n",
      "Epoch:  19587  Learning Rate:  3.1182309473097636e-10  Varinance:  3.2131727285133944e-11 \n",
      "\n",
      "Epoch:  19588  Learning Rate:  3.1151142749583483e-10  Varinance:  3.2086840439882676e-11 \n",
      "\n",
      "Epoch:  19589  Learning Rate:  3.112000717721468e-10  Varinance:  3.204201629990893e-11 \n",
      "\n",
      "Epoch:  19590  Learning Rate:  3.108890272485575e-10  Varinance:  3.199725477761573e-11 \n",
      "\n",
      "Epoch:  19591  Learning Rate:  3.1057829361402035e-10  Varinance:  3.1952555785528584e-11 \n",
      "\n",
      "Epoch:  19592  Learning Rate:  3.1026787055780266e-10  Varinance:  3.190791923629486e-11 \n",
      "\n",
      "Epoch:  19593  Learning Rate:  3.0995775776948244e-10  Varinance:  3.1863345042684296e-11 \n",
      "\n",
      "Epoch:  19594  Learning Rate:  3.096479549389447e-10  Varinance:  3.181883311758837e-11 \n",
      "\n",
      "Epoch:  19595  Learning Rate:  3.093384617563878e-10  Varinance:  3.177438337402025e-11 \n",
      "\n",
      "Epoch:  19596  Learning Rate:  3.0902927791231945e-10  Varinance:  3.172999572511461e-11 \n",
      "\n",
      "Epoch:  19597  Learning Rate:  3.0872040309755375e-10  Varinance:  3.168567008412761e-11 \n",
      "\n",
      "Epoch:  19598  Learning Rate:  3.084118370032168e-10  Varinance:  3.164140636443624e-11 \n",
      "\n",
      "Epoch:  19599  Learning Rate:  3.081035793207437e-10  Varinance:  3.159720447953881e-11 \n",
      "\n",
      "Epoch:  19600  Learning Rate:  3.0779562974187445e-10  Varinance:  3.155306434305441e-11 \n",
      "\n",
      "Epoch:  19601  Learning Rate:  3.074879879586606e-10  Varinance:  3.150898586872274e-11 \n",
      "\n",
      "Epoch:  19602  Learning Rate:  3.071806536634615e-10  Varinance:  3.1464968970404046e-11 \n",
      "\n",
      "Epoch:  19603  Learning Rate:  3.068736265489405e-10  Varinance:  3.14210135620789e-11 \n",
      "\n",
      "Epoch:  19604  Learning Rate:  3.0656690630807165e-10  Varinance:  3.1377119557848133e-11 \n",
      "\n",
      "Epoch:  19605  Learning Rate:  3.062604926341357e-10  Varinance:  3.1333286871932266e-11 \n",
      "\n",
      "Epoch:  19606  Learning Rate:  3.0595438522071687e-10  Varinance:  3.128951541867197e-11 \n",
      "\n",
      "Epoch:  19607  Learning Rate:  3.0564858376170874e-10  Varinance:  3.124580511252746e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19608  Learning Rate:  3.0534308795131094e-10  Varinance:  3.120215586807847e-11 \n",
      "\n",
      "Epoch:  19609  Learning Rate:  3.0503789748402544e-10  Varinance:  3.115856760002403e-11 \n",
      "\n",
      "Epoch:  19610  Learning Rate:  3.0473301205466283e-10  Varinance:  3.1115040223182374e-11 \n",
      "\n",
      "Epoch:  19611  Learning Rate:  3.0442843135833876e-10  Varinance:  3.107157365249081e-11 \n",
      "\n",
      "Epoch:  19612  Learning Rate:  3.041241550904704e-10  Varinance:  3.102816780300515e-11 \n",
      "\n",
      "Epoch:  19613  Learning Rate:  3.0382018294678344e-10  Varinance:  3.098482258990022e-11 \n",
      "\n",
      "Epoch:  19614  Learning Rate:  3.035165146233037e-10  Varinance:  3.09415379284692e-11 \n",
      "\n",
      "Epoch:  19615  Learning Rate:  3.032131498163639e-10  Varinance:  3.0898313734123634e-11 \n",
      "\n",
      "Epoch:  19616  Learning Rate:  3.029100882226003e-10  Varinance:  3.085514992239322e-11 \n",
      "\n",
      "Epoch:  19617  Learning Rate:  3.02607329538949e-10  Varinance:  3.081204640892565e-11 \n",
      "\n",
      "Epoch:  19618  Learning Rate:  3.023048734626525e-10  Varinance:  3.076900310948657e-11 \n",
      "\n",
      "Epoch:  19619  Learning Rate:  3.020027196912558e-10  Varinance:  3.0726019939958976e-11 \n",
      "\n",
      "Epoch:  19620  Learning Rate:  3.0170086792260274e-10  Varinance:  3.0683096816343695e-11 \n",
      "\n",
      "Epoch:  19621  Learning Rate:  3.013993178548428e-10  Varinance:  3.064023365475879e-11 \n",
      "\n",
      "Epoch:  19622  Learning Rate:  3.0109806918642693e-10  Varinance:  3.05974303714395e-11 \n",
      "\n",
      "Epoch:  19623  Learning Rate:  3.007971216161043e-10  Varinance:  3.055468688273808e-11 \n",
      "\n",
      "Epoch:  19624  Learning Rate:  3.004964748429283e-10  Varinance:  3.051200310512365e-11 \n",
      "\n",
      "Epoch:  19625  Learning Rate:  3.0019612856625326e-10  Varinance:  3.0469378955182103e-11 \n",
      "\n",
      "Epoch:  19626  Learning Rate:  2.9989608248573076e-10  Varinance:  3.042681434961556e-11 \n",
      "\n",
      "Epoch:  19627  Learning Rate:  2.995963363013157e-10  Varinance:  3.038430920524282e-11 \n",
      "\n",
      "Epoch:  19628  Learning Rate:  2.99296889713263e-10  Varinance:  3.0341863438998766e-11 \n",
      "\n",
      "Epoch:  19629  Learning Rate:  2.9899774242212387e-10  Varinance:  3.029947696793434e-11 \n",
      "\n",
      "Epoch:  19630  Learning Rate:  2.986988941287521e-10  Varinance:  3.025714970921634e-11 \n",
      "\n",
      "Epoch:  19631  Learning Rate:  2.984003445343004e-10  Varinance:  3.02148815801273e-11 \n",
      "\n",
      "Epoch:  19632  Learning Rate:  2.9810209334021703e-10  Varinance:  3.017267249806539e-11 \n",
      "\n",
      "Epoch:  19633  Learning Rate:  2.978041402482519e-10  Varinance:  3.013052238054387e-11 \n",
      "\n",
      "Epoch:  19634  Learning Rate:  2.975064849604528e-10  Varinance:  3.008843114519155e-11 \n",
      "\n",
      "Epoch:  19635  Learning Rate:  2.9720912717916243e-10  Varinance:  3.004639870975219e-11 \n",
      "\n",
      "Epoch:  19636  Learning Rate:  2.9691206660702407e-10  Varinance:  3.0004424992084465e-11 \n",
      "\n",
      "Epoch:  19637  Learning Rate:  2.9661530294697807e-10  Varinance:  2.996250991016181e-11 \n",
      "\n",
      "Epoch:  19638  Learning Rate:  2.963188359022587e-10  Varinance:  2.9920653382072225e-11 \n",
      "\n",
      "Epoch:  19639  Learning Rate:  2.9602266517639984e-10  Varinance:  2.987885532601827e-11 \n",
      "\n",
      "Epoch:  19640  Learning Rate:  2.95726790473232e-10  Varinance:  2.983711566031643e-11 \n",
      "\n",
      "Epoch:  19641  Learning Rate:  2.9543121149687813e-10  Varinance:  2.979543430339765e-11 \n",
      "\n",
      "Epoch:  19642  Learning Rate:  2.9513592795176147e-10  Varinance:  2.975381117380668e-11 \n",
      "\n",
      "Epoch:  19643  Learning Rate:  2.9484093954259625e-10  Varinance:  2.971224619020208e-11 \n",
      "\n",
      "Epoch:  19644  Learning Rate:  2.9454624597439524e-10  Varinance:  2.9670739271356044e-11 \n",
      "\n",
      "Epoch:  19645  Learning Rate:  2.9425184695246573e-10  Varinance:  2.962929033615423e-11 \n",
      "\n",
      "Epoch:  19646  Learning Rate:  2.9395774218240665e-10  Varinance:  2.9587899303595706e-11 \n",
      "\n",
      "Epoch:  19647  Learning Rate:  2.936639313701143e-10  Varinance:  2.954656609279241e-11 \n",
      "\n",
      "Epoch:  19648  Learning Rate:  2.933704142217788e-10  Varinance:  2.9505290622969564e-11 \n",
      "\n",
      "Epoch:  19649  Learning Rate:  2.930771904438809e-10  Varinance:  2.946407281346513e-11 \n",
      "\n",
      "Epoch:  19650  Learning Rate:  2.927842597431979e-10  Varinance:  2.942291258372977e-11 \n",
      "\n",
      "Epoch:  19651  Learning Rate:  2.9249162182680007e-10  Varinance:  2.938180985332665e-11 \n",
      "\n",
      "Epoch:  19652  Learning Rate:  2.921992764020474e-10  Varinance:  2.9340764541931314e-11 \n",
      "\n",
      "Epoch:  19653  Learning Rate:  2.919072231765955e-10  Varinance:  2.929977656933161e-11 \n",
      "\n",
      "Epoch:  19654  Learning Rate:  2.916154618583921e-10  Varinance:  2.925884585542714e-11 \n",
      "\n",
      "Epoch:  19655  Learning Rate:  2.9132399215567385e-10  Varinance:  2.9217972320229716e-11 \n",
      "\n",
      "Epoch:  19656  Learning Rate:  2.9103281377697206e-10  Varinance:  2.917715588386276e-11 \n",
      "\n",
      "Epoch:  19657  Learning Rate:  2.907419264311093e-10  Varinance:  2.9136396466561314e-11 \n",
      "\n",
      "Epoch:  19658  Learning Rate:  2.904513298271962e-10  Varinance:  2.9095693988671833e-11 \n",
      "\n",
      "Epoch:  19659  Learning Rate:  2.901610236746371e-10  Varinance:  2.9055048370652038e-11 \n",
      "\n",
      "Epoch:  19660  Learning Rate:  2.8987100768312687e-10  Varinance:  2.9014459533070885e-11 \n",
      "\n",
      "Epoch:  19661  Learning Rate:  2.895812815626475e-10  Varinance:  2.8973927396607975e-11 \n",
      "\n",
      "Epoch:  19662  Learning Rate:  2.8929184502347377e-10  Varinance:  2.8933451882054025e-11 \n",
      "\n",
      "Epoch:  19663  Learning Rate:  2.8900269777617023e-10  Varinance:  2.8893032910310305e-11 \n",
      "\n",
      "Epoch:  19664  Learning Rate:  2.8871383953158755e-10  Varinance:  2.8852670402388587e-11 \n",
      "\n",
      "Epoch:  19665  Learning Rate:  2.884252700008684e-10  Varinance:  2.8812364279410974e-11 \n",
      "\n",
      "Epoch:  19666  Learning Rate:  2.8813698889544436e-10  Varinance:  2.8772114462609763e-11 \n",
      "\n",
      "Epoch:  19667  Learning Rate:  2.878489959270322e-10  Varinance:  2.8731920873327398e-11 \n",
      "\n",
      "Epoch:  19668  Learning Rate:  2.875612908076399e-10  Varinance:  2.8691783433015895e-11 \n",
      "\n",
      "Epoch:  19669  Learning Rate:  2.8727387324956343e-10  Varinance:  2.865170206323729e-11 \n",
      "\n",
      "Epoch:  19670  Learning Rate:  2.8698674296538316e-10  Varinance:  2.8611676685663112e-11 \n",
      "\n",
      "Epoch:  19671  Learning Rate:  2.866998996679697e-10  Varinance:  2.857170722207431e-11 \n",
      "\n",
      "Epoch:  19672  Learning Rate:  2.864133430704809e-10  Varinance:  2.8531793594361083e-11 \n",
      "\n",
      "Epoch:  19673  Learning Rate:  2.8612707288635793e-10  Varinance:  2.8491935724522767e-11 \n",
      "\n",
      "Epoch:  19674  Learning Rate:  2.858410888293328e-10  Varinance:  2.845213353466775e-11 \n",
      "\n",
      "Epoch:  19675  Learning Rate:  2.855553906134193e-10  Varinance:  2.8412386947012946e-11 \n",
      "\n",
      "Epoch:  19676  Learning Rate:  2.852699779529201e-10  Varinance:  2.8372695883884208e-11 \n",
      "\n",
      "Epoch:  19677  Learning Rate:  2.8498485056242375e-10  Varinance:  2.833306026771581e-11 \n",
      "\n",
      "Epoch:  19678  Learning Rate:  2.8470000815680063e-10  Varinance:  2.8293480021050385e-11 \n",
      "\n",
      "Epoch:  19679  Learning Rate:  2.8441545045120943e-10  Varinance:  2.8253955066538765e-11 \n",
      "\n",
      "Epoch:  19680  Learning Rate:  2.841311771610934e-10  Varinance:  2.821448532693984e-11 \n",
      "\n",
      "Epoch:  19681  Learning Rate:  2.8384718800217715e-10  Varinance:  2.8175070725120497e-11 \n",
      "\n",
      "Epoch:  19682  Learning Rate:  2.8356348269047256e-10  Varinance:  2.8135711184055084e-11 \n",
      "\n",
      "Epoch:  19683  Learning Rate:  2.8328006094227534e-10  Varinance:  2.809640662682584e-11 \n",
      "\n",
      "Epoch:  19684  Learning Rate:  2.8299692247416167e-10  Varinance:  2.8057156976622362e-11 \n",
      "\n",
      "Epoch:  19685  Learning Rate:  2.8271406700299403e-10  Varinance:  2.8017962156741553e-11 \n",
      "\n",
      "Epoch:  19686  Learning Rate:  2.824314942459179e-10  Varinance:  2.7978822090587462e-11 \n",
      "\n",
      "Epoch:  19687  Learning Rate:  2.8214920392035867e-10  Varinance:  2.7939736701671127e-11 \n",
      "\n",
      "Epoch:  19688  Learning Rate:  2.8186719574402683e-10  Varinance:  2.7900705913610568e-11 \n",
      "\n",
      "Epoch:  19689  Learning Rate:  2.815854694349152e-10  Varinance:  2.786172965013018e-11 \n",
      "\n",
      "Epoch:  19690  Learning Rate:  2.8130402471129545e-10  Varinance:  2.7822807835061226e-11 \n",
      "\n",
      "Epoch:  19691  Learning Rate:  2.810228612917239e-10  Varinance:  2.7783940392341274e-11 \n",
      "\n",
      "Epoch:  19692  Learning Rate:  2.80741978895038e-10  Varinance:  2.774512724601414e-11 \n",
      "\n",
      "Epoch:  19693  Learning Rate:  2.8046137724035345e-10  Varinance:  2.770636832022976e-11 \n",
      "\n",
      "Epoch:  19694  Learning Rate:  2.801810560470695e-10  Varinance:  2.7667663539244015e-11 \n",
      "\n",
      "Epoch:  19695  Learning Rate:  2.79901015034866e-10  Varinance:  2.762901282741871e-11 \n",
      "\n",
      "Epoch:  19696  Learning Rate:  2.7962125392369975e-10  Varinance:  2.7590416109221012e-11 \n",
      "\n",
      "Epoch:  19697  Learning Rate:  2.793417724338108e-10  Varinance:  2.75518733092239e-11 \n",
      "\n",
      "Epoch:  19698  Learning Rate:  2.790625702857185e-10  Varinance:  2.7513384352105628e-11 \n",
      "\n",
      "Epoch:  19699  Learning Rate:  2.787836472002188e-10  Varinance:  2.747494916264966e-11 \n",
      "\n",
      "Epoch:  19700  Learning Rate:  2.785050028983895e-10  Varinance:  2.7436567665744548e-11 \n",
      "\n",
      "Epoch:  19701  Learning Rate:  2.782266371015873e-10  Varinance:  2.7398239786383766e-11 \n",
      "\n",
      "Epoch:  19702  Learning Rate:  2.7794854953144443e-10  Varinance:  2.7359965449665668e-11 \n",
      "\n",
      "Epoch:  19703  Learning Rate:  2.776707399098742e-10  Varinance:  2.7321744580792945e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19704  Learning Rate:  2.7739320795906805e-10  Varinance:  2.7283577105073084e-11 \n",
      "\n",
      "Epoch:  19705  Learning Rate:  2.7711595340149197e-10  Varinance:  2.7245462947917798e-11 \n",
      "\n",
      "Epoch:  19706  Learning Rate:  2.7683897595989335e-10  Varinance:  2.7207402034843008e-11 \n",
      "\n",
      "Epoch:  19707  Learning Rate:  2.7656227535729283e-10  Varinance:  2.7169394291468686e-11 \n",
      "\n",
      "Epoch:  19708  Learning Rate:  2.762858513169907e-10  Varinance:  2.7131439643518714e-11 \n",
      "\n",
      "Epoch:  19709  Learning Rate:  2.760097035625639e-10  Varinance:  2.7093538016820822e-11 \n",
      "\n",
      "Epoch:  19710  Learning Rate:  2.7573383181786266e-10  Varinance:  2.7055689337306073e-11 \n",
      "\n",
      "Epoch:  19711  Learning Rate:  2.754582358070162e-10  Varinance:  2.701789353100929e-11 \n",
      "\n",
      "Epoch:  19712  Learning Rate:  2.7518291525442947e-10  Varinance:  2.6980150524068526e-11 \n",
      "\n",
      "Epoch:  19713  Learning Rate:  2.7490786988478e-10  Varinance:  2.6942460242725014e-11 \n",
      "\n",
      "Epoch:  19714  Learning Rate:  2.746330994230233e-10  Varinance:  2.6904822613323024e-11 \n",
      "\n",
      "Epoch:  19715  Learning Rate:  2.743586035943899e-10  Varinance:  2.6867237562309733e-11 \n",
      "\n",
      "Epoch:  19716  Learning Rate:  2.7408438212438197e-10  Varinance:  2.6829705016235145e-11 \n",
      "\n",
      "Epoch:  19717  Learning Rate:  2.73810434738779e-10  Varinance:  2.6792224901751604e-11 \n",
      "\n",
      "Epoch:  19718  Learning Rate:  2.7353676116363453e-10  Varinance:  2.675479714561418e-11 \n",
      "\n",
      "Epoch:  19719  Learning Rate:  2.732633611252731e-10  Varinance:  2.67174216746802e-11 \n",
      "\n",
      "Epoch:  19720  Learning Rate:  2.729902343502955e-10  Varinance:  2.6680098415909143e-11 \n",
      "\n",
      "Epoch:  19721  Learning Rate:  2.7271738056557604e-10  Varinance:  2.6642827296362523e-11 \n",
      "\n",
      "Epoch:  19722  Learning Rate:  2.724447994982589e-10  Varinance:  2.660560824320377e-11 \n",
      "\n",
      "Epoch:  19723  Learning Rate:  2.7217249087576396e-10  Varinance:  2.656844118369813e-11 \n",
      "\n",
      "Epoch:  19724  Learning Rate:  2.719004544257835e-10  Varinance:  2.6531326045212197e-11 \n",
      "\n",
      "Epoch:  19725  Learning Rate:  2.7162868987627914e-10  Varinance:  2.64942627552143e-11 \n",
      "\n",
      "Epoch:  19726  Learning Rate:  2.713571969554874e-10  Varinance:  2.6457251241274e-11 \n",
      "\n",
      "Epoch:  19727  Learning Rate:  2.710859753919161e-10  Varinance:  2.642029143106204e-11 \n",
      "\n",
      "Epoch:  19728  Learning Rate:  2.7081502491434185e-10  Varinance:  2.6383383252350214e-11 \n",
      "\n",
      "Epoch:  19729  Learning Rate:  2.705443452518151e-10  Varinance:  2.6346526633011295e-11 \n",
      "\n",
      "Epoch:  19730  Learning Rate:  2.7027393613365707e-10  Varinance:  2.6309721501018545e-11 \n",
      "\n",
      "Epoch:  19731  Learning Rate:  2.7000379728945675e-10  Varinance:  2.6272967784446126e-11 \n",
      "\n",
      "Epoch:  19732  Learning Rate:  2.697339284490762e-10  Varinance:  2.623626541146858e-11 \n",
      "\n",
      "Epoch:  19733  Learning Rate:  2.694643293426476e-10  Varinance:  2.6199614310360777e-11 \n",
      "\n",
      "Epoch:  19734  Learning Rate:  2.691949997005698e-10  Varinance:  2.6163014409497803e-11 \n",
      "\n",
      "Epoch:  19735  Learning Rate:  2.689259392535141e-10  Varinance:  2.6126465637354788e-11 \n",
      "\n",
      "Epoch:  19736  Learning Rate:  2.686571477324211e-10  Varinance:  2.608996792250688e-11 \n",
      "\n",
      "Epoch:  19737  Learning Rate:  2.6838862486849717e-10  Varinance:  2.605352119362873e-11 \n",
      "\n",
      "Epoch:  19738  Learning Rate:  2.681203703932215e-10  Varinance:  2.601712537949489e-11 \n",
      "\n",
      "Epoch:  19739  Learning Rate:  2.678523840383376e-10  Varinance:  2.5980780408979332e-11 \n",
      "\n",
      "Epoch:  19740  Learning Rate:  2.6758466553586003e-10  Varinance:  2.5944486211055383e-11 \n",
      "\n",
      "Epoch:  19741  Learning Rate:  2.6731721461807124e-10  Varinance:  2.5908242714795593e-11 \n",
      "\n",
      "Epoch:  19742  Learning Rate:  2.6705003101751844e-10  Varinance:  2.5872049849371604e-11 \n",
      "\n",
      "Epoch:  19743  Learning Rate:  2.667831144670189e-10  Varinance:  2.5835907544054076e-11 \n",
      "\n",
      "Epoch:  19744  Learning Rate:  2.6651646469965697e-10  Varinance:  2.5799815728212214e-11 \n",
      "\n",
      "Epoch:  19745  Learning Rate:  2.662500814487811e-10  Varinance:  2.576377433131416e-11 \n",
      "\n",
      "Epoch:  19746  Learning Rate:  2.6598396444800876e-10  Varinance:  2.5727783282926505e-11 \n",
      "\n",
      "Epoch:  19747  Learning Rate:  2.6571811343122397e-10  Varinance:  2.569184251271422e-11 \n",
      "\n",
      "Epoch:  19748  Learning Rate:  2.6545252813257385e-10  Varinance:  2.5655951950440538e-11 \n",
      "\n",
      "Epoch:  19749  Learning Rate:  2.65187208286474e-10  Varinance:  2.562011152596681e-11 \n",
      "\n",
      "Epoch:  19750  Learning Rate:  2.649221536276055e-10  Varinance:  2.5584321169252453e-11 \n",
      "\n",
      "Epoch:  19751  Learning Rate:  2.6465736389091173e-10  Varinance:  2.5548580810354466e-11 \n",
      "\n",
      "Epoch:  19752  Learning Rate:  2.6439283881160387e-10  Varinance:  2.551289037942782e-11 \n",
      "\n",
      "Epoch:  19753  Learning Rate:  2.6412857812515785e-10  Varinance:  2.5477249806724976e-11 \n",
      "\n",
      "Epoch:  19754  Learning Rate:  2.63864581567311e-10  Varinance:  2.544165902259581e-11 \n",
      "\n",
      "Epoch:  19755  Learning Rate:  2.636008488740677e-10  Varinance:  2.5406117957487522e-11 \n",
      "\n",
      "Epoch:  19756  Learning Rate:  2.633373797816962e-10  Varinance:  2.5370626541944457e-11 \n",
      "\n",
      "Epoch:  19757  Learning Rate:  2.630741740267255e-10  Varinance:  2.5335184706608083e-11 \n",
      "\n",
      "Epoch:  19758  Learning Rate:  2.628112313459507e-10  Varinance:  2.5299792382216482e-11 \n",
      "\n",
      "Epoch:  19759  Learning Rate:  2.625485514764301e-10  Varinance:  2.526444949960478e-11 \n",
      "\n",
      "Epoch:  19760  Learning Rate:  2.6228613415548197e-10  Varinance:  2.522915598970462e-11 \n",
      "\n",
      "Epoch:  19761  Learning Rate:  2.6202397912068983e-10  Varinance:  2.519391178354413e-11 \n",
      "\n",
      "Epoch:  19762  Learning Rate:  2.6176208610989954e-10  Varinance:  2.5158716812247795e-11 \n",
      "\n",
      "Epoch:  19763  Learning Rate:  2.6150045486121624e-10  Varinance:  2.5123571007036314e-11 \n",
      "\n",
      "Epoch:  19764  Learning Rate:  2.6123908511300965e-10  Varinance:  2.5088474299226556e-11 \n",
      "\n",
      "Epoch:  19765  Learning Rate:  2.6097797660391085e-10  Varinance:  2.505342662023107e-11 \n",
      "\n",
      "Epoch:  19766  Learning Rate:  2.607171290728094e-10  Varinance:  2.5018427901558497e-11 \n",
      "\n",
      "Epoch:  19767  Learning Rate:  2.604565422588598e-10  Varinance:  2.4983478074813057e-11 \n",
      "\n",
      "Epoch:  19768  Learning Rate:  2.6019621590147315e-10  Varinance:  2.4948577071694517e-11 \n",
      "\n",
      "Epoch:  19769  Learning Rate:  2.599361497403241e-10  Varinance:  2.491372482399806e-11 \n",
      "\n",
      "Epoch:  19770  Learning Rate:  2.596763435153474e-10  Varinance:  2.4878921263614154e-11 \n",
      "\n",
      "Epoch:  19771  Learning Rate:  2.5941679696673493e-10  Varinance:  2.4844166322528486e-11 \n",
      "\n",
      "Epoch:  19772  Learning Rate:  2.5915750983494106e-10  Varinance:  2.4809459932821516e-11 \n",
      "\n",
      "Epoch:  19773  Learning Rate:  2.588984818606795e-10  Varinance:  2.4774802026668825e-11 \n",
      "\n",
      "Epoch:  19774  Learning Rate:  2.5863971278492046e-10  Varinance:  2.4740192536340666e-11 \n",
      "\n",
      "Epoch:  19775  Learning Rate:  2.583812023488958e-10  Varinance:  2.470563139420191e-11 \n",
      "\n",
      "Epoch:  19776  Learning Rate:  2.581229502940959e-10  Varinance:  2.467111853271191e-11 \n",
      "\n",
      "Epoch:  19777  Learning Rate:  2.578649563622669e-10  Varinance:  2.4636653884424363e-11 \n",
      "\n",
      "Epoch:  19778  Learning Rate:  2.5760722029541574e-10  Varinance:  2.4602237381987283e-11 \n",
      "\n",
      "Epoch:  19779  Learning Rate:  2.5734974183580725e-10  Varinance:  2.456786895814251e-11 \n",
      "\n",
      "Epoch:  19780  Learning Rate:  2.5709252072596115e-10  Varinance:  2.45335485457261e-11 \n",
      "\n",
      "Epoch:  19781  Learning Rate:  2.5683555670865717e-10  Varinance:  2.4499276077667838e-11 \n",
      "\n",
      "Epoch:  19782  Learning Rate:  2.5657884952693226e-10  Varinance:  2.4465051486991225e-11 \n",
      "\n",
      "Epoch:  19783  Learning Rate:  2.563223989240774e-10  Varinance:  2.4430874706813307e-11 \n",
      "\n",
      "Epoch:  19784  Learning Rate:  2.560662046436427e-10  Varinance:  2.439674567034457e-11 \n",
      "\n",
      "Epoch:  19785  Learning Rate:  2.558102664294349e-10  Varinance:  2.436266431088889e-11 \n",
      "\n",
      "Epoch:  19786  Learning Rate:  2.5555458402551405e-10  Varinance:  2.432863056184306e-11 \n",
      "\n",
      "Epoch:  19787  Learning Rate:  2.552991571761984e-10  Varinance:  2.429464435669716e-11 \n",
      "\n",
      "Epoch:  19788  Learning Rate:  2.550439856260622e-10  Varinance:  2.426070562903411e-11 \n",
      "\n",
      "Epoch:  19789  Learning Rate:  2.547890691199319e-10  Varinance:  2.42268143125296e-11 \n",
      "\n",
      "Epoch:  19790  Learning Rate:  2.54534407402892e-10  Varinance:  2.4192970340951988e-11 \n",
      "\n",
      "Epoch:  19791  Learning Rate:  2.542800002202816e-10  Varinance:  2.415917364816214e-11 \n",
      "\n",
      "Epoch:  19792  Learning Rate:  2.5402584731769173e-10  Varinance:  2.4125424168113412e-11 \n",
      "\n",
      "Epoch:  19793  Learning Rate:  2.537719484409703e-10  Varinance:  2.4091721834851167e-11 \n",
      "\n",
      "Epoch:  19794  Learning Rate:  2.535183033362194e-10  Varinance:  2.405806658251315e-11 \n",
      "\n",
      "Epoch:  19795  Learning Rate:  2.5326491174979203e-10  Varinance:  2.402445834532904e-11 \n",
      "\n",
      "Epoch:  19796  Learning Rate:  2.530117734282975e-10  Varinance:  2.399089705762039e-11 \n",
      "\n",
      "Epoch:  19797  Learning Rate:  2.5275888811859846e-10  Varinance:  2.39573826538005e-11 \n",
      "\n",
      "Epoch:  19798  Learning Rate:  2.525062555678077e-10  Varinance:  2.3923915068374305e-11 \n",
      "\n",
      "Epoch:  19799  Learning Rate:  2.522538755232944e-10  Varinance:  2.38904942359383e-11 \n",
      "\n",
      "Epoch:  19800  Learning Rate:  2.520017477326767e-10  Varinance:  2.3857120091180106e-11 \n",
      "\n",
      "Epoch:  19801  Learning Rate:  2.517498719438278e-10  Varinance:  2.382379256887884e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19802  Learning Rate:  2.514982479048727e-10  Varinance:  2.3790511603904626e-11 \n",
      "\n",
      "Epoch:  19803  Learning Rate:  2.512468753641856e-10  Varinance:  2.375727713121859e-11 \n",
      "\n",
      "Epoch:  19804  Learning Rate:  2.509957540703948e-10  Varinance:  2.372408908587271e-11 \n",
      "\n",
      "Epoch:  19805  Learning Rate:  2.507448837723798e-10  Varinance:  2.3690947403009695e-11 \n",
      "\n",
      "Epoch:  19806  Learning Rate:  2.504942642192687e-10  Varinance:  2.3657852017862936e-11 \n",
      "\n",
      "Epoch:  19807  Learning Rate:  2.5024389516044263e-10  Varinance:  2.362480286575606e-11 \n",
      "\n",
      "Epoch:  19808  Learning Rate:  2.499937763455334e-10  Varinance:  2.359179988210328e-11 \n",
      "\n",
      "Epoch:  19809  Learning Rate:  2.4974390752442054e-10  Varinance:  2.3558843002408955e-11 \n",
      "\n",
      "Epoch:  19810  Learning Rate:  2.4949428844723596e-10  Varinance:  2.3525932162267558e-11 \n",
      "\n",
      "Epoch:  19811  Learning Rate:  2.4924491886436157e-10  Varinance:  2.3493067297363518e-11 \n",
      "\n",
      "Epoch:  19812  Learning Rate:  2.489957985264259e-10  Varinance:  2.3460248343471107e-11 \n",
      "\n",
      "Epoch:  19813  Learning Rate:  2.487469271843095e-10  Varinance:  2.3427475236454412e-11 \n",
      "\n",
      "Epoch:  19814  Learning Rate:  2.4849830458914194e-10  Varinance:  2.3394747912266863e-11 \n",
      "\n",
      "Epoch:  19815  Learning Rate:  2.482499304922987e-10  Varinance:  2.33620663069516e-11 \n",
      "\n",
      "Epoch:  19816  Learning Rate:  2.480018046454067e-10  Varinance:  2.3329430356641043e-11 \n",
      "\n",
      "Epoch:  19817  Learning Rate:  2.477539268003409e-10  Varinance:  2.3296839997556823e-11 \n",
      "\n",
      "Epoch:  19818  Learning Rate:  2.475062967092217e-10  Varinance:  2.326429516600967e-11 \n",
      "\n",
      "Epoch:  19819  Learning Rate:  2.4725891412441977e-10  Varinance:  2.3231795798399276e-11 \n",
      "\n",
      "Epoch:  19820  Learning Rate:  2.4701177879855343e-10  Varinance:  2.3199341831214277e-11 \n",
      "\n",
      "Epoch:  19821  Learning Rate:  2.467648904844857e-10  Varinance:  2.3166933201031775e-11 \n",
      "\n",
      "Epoch:  19822  Learning Rate:  2.465182489353289e-10  Varinance:  2.3134569844517717e-11 \n",
      "\n",
      "Epoch:  19823  Learning Rate:  2.4627185390444256e-10  Varinance:  2.3102251698426458e-11 \n",
      "\n",
      "Epoch:  19824  Learning Rate:  2.460257051454297e-10  Varinance:  2.306997869960069e-11 \n",
      "\n",
      "Epoch:  19825  Learning Rate:  2.4577980241214246e-10  Varinance:  2.303775078497134e-11 \n",
      "\n",
      "Epoch:  19826  Learning Rate:  2.455341454586791e-10  Varinance:  2.300556789155743e-11 \n",
      "\n",
      "Epoch:  19827  Learning Rate:  2.452887340393807e-10  Varinance:  2.2973429956466058e-11 \n",
      "\n",
      "Epoch:  19828  Learning Rate:  2.450435679088368e-10  Varinance:  2.294133691689194e-11 \n",
      "\n",
      "Epoch:  19829  Learning Rate:  2.447986468218821e-10  Varinance:  2.290928871011775e-11 \n",
      "\n",
      "Epoch:  19830  Learning Rate:  2.4455397053359374e-10  Varinance:  2.2877285273513725e-11 \n",
      "\n",
      "Epoch:  19831  Learning Rate:  2.443095387992972e-10  Varinance:  2.284532654453757e-11 \n",
      "\n",
      "Epoch:  19832  Learning Rate:  2.4406535137455894e-10  Varinance:  2.2813412460734372e-11 \n",
      "\n",
      "Epoch:  19833  Learning Rate:  2.4382140801519234e-10  Varinance:  2.278154295973646e-11 \n",
      "\n",
      "Epoch:  19834  Learning Rate:  2.43577708477255e-10  Varinance:  2.2749717979263365e-11 \n",
      "\n",
      "Epoch:  19835  Learning Rate:  2.4333425251704555e-10  Varinance:  2.271793745712139e-11 \n",
      "\n",
      "Epoch:  19836  Learning Rate:  2.430910398911089e-10  Varinance:  2.2686201331203953e-11 \n",
      "\n",
      "Epoch:  19837  Learning Rate:  2.4284807035623327e-10  Varinance:  2.265450953949116e-11 \n",
      "\n",
      "Epoch:  19838  Learning Rate:  2.4260534366944736e-10  Varinance:  2.2622862020049746e-11 \n",
      "\n",
      "Epoch:  19839  Learning Rate:  2.423628595880253e-10  Varinance:  2.2591258711032973e-11 \n",
      "\n",
      "Epoch:  19840  Learning Rate:  2.421206178694839e-10  Varinance:  2.25596995506805e-11 \n",
      "\n",
      "Epoch:  19841  Learning Rate:  2.418786182715797e-10  Varinance:  2.2528184477318343e-11 \n",
      "\n",
      "Epoch:  19842  Learning Rate:  2.4163686055231395e-10  Varinance:  2.2496713429358426e-11 \n",
      "\n",
      "Epoch:  19843  Learning Rate:  2.4139534446992975e-10  Varinance:  2.246528634529896e-11 \n",
      "\n",
      "Epoch:  19844  Learning Rate:  2.4115406978290927e-10  Varinance:  2.243390316372399e-11 \n",
      "\n",
      "Epoch:  19845  Learning Rate:  2.4091303624997865e-10  Varinance:  2.2402563823303353e-11 \n",
      "\n",
      "Epoch:  19846  Learning Rate:  2.4067224363010524e-10  Varinance:  2.2371268262792558e-11 \n",
      "\n",
      "Epoch:  19847  Learning Rate:  2.404316916824946e-10  Varinance:  2.234001642103268e-11 \n",
      "\n",
      "Epoch:  19848  Learning Rate:  2.401913801665957e-10  Varinance:  2.230880823695031e-11 \n",
      "\n",
      "Epoch:  19849  Learning Rate:  2.3995130884209787e-10  Varinance:  2.22776436495571e-11 \n",
      "\n",
      "Epoch:  19850  Learning Rate:  2.39711477468928e-10  Varinance:  2.2246522597950172e-11 \n",
      "\n",
      "Epoch:  19851  Learning Rate:  2.394718858072556e-10  Varinance:  2.2215445021311613e-11 \n",
      "\n",
      "Epoch:  19852  Learning Rate:  2.392325336174898e-10  Varinance:  2.2184410858908493e-11 \n",
      "\n",
      "Epoch:  19853  Learning Rate:  2.389934206602767e-10  Varinance:  2.2153420050092716e-11 \n",
      "\n",
      "Epoch:  19854  Learning Rate:  2.387545466965042e-10  Varinance:  2.2122472534300905e-11 \n",
      "\n",
      "Epoch:  19855  Learning Rate:  2.385159114872991e-10  Varinance:  2.2091568251054375e-11 \n",
      "\n",
      "Epoch:  19856  Learning Rate:  2.3827751479402455e-10  Varinance:  2.2060707139958685e-11 \n",
      "\n",
      "Epoch:  19857  Learning Rate:  2.3803935637828464e-10  Varinance:  2.2029889140704008e-11 \n",
      "\n",
      "Epoch:  19858  Learning Rate:  2.378014360019218e-10  Varinance:  2.1999114193064673e-11 \n",
      "\n",
      "Epoch:  19859  Learning Rate:  2.375637534270139e-10  Varinance:  2.1968382236899157e-11 \n",
      "\n",
      "Epoch:  19860  Learning Rate:  2.373263084158793e-10  Varinance:  2.193769321214995e-11 \n",
      "\n",
      "Epoch:  19861  Learning Rate:  2.3708910073107363e-10  Varinance:  2.1907047058843506e-11 \n",
      "\n",
      "Epoch:  19862  Learning Rate:  2.368521301353877e-10  Varinance:  2.187644371708984e-11 \n",
      "\n",
      "Epoch:  19863  Learning Rate:  2.366153963918524e-10  Varinance:  2.1845883127082862e-11 \n",
      "\n",
      "Epoch:  19864  Learning Rate:  2.3637889926373246e-10  Varinance:  2.1815365229099948e-11 \n",
      "\n",
      "Epoch:  19865  Learning Rate:  2.3614263851453137e-10  Varinance:  2.17848899635019e-11 \n",
      "\n",
      "Epoch:  19866  Learning Rate:  2.359066139079894e-10  Varinance:  2.175445727073284e-11 \n",
      "\n",
      "Epoch:  19867  Learning Rate:  2.3567082520808016e-10  Varinance:  2.1724067091320096e-11 \n",
      "\n",
      "Epoch:  19868  Learning Rate:  2.3543527217901577e-10  Varinance:  2.1693719365874134e-11 \n",
      "\n",
      "Epoch:  19869  Learning Rate:  2.3519995458524397e-10  Varinance:  2.166341403508817e-11 \n",
      "\n",
      "Epoch:  19870  Learning Rate:  2.3496487219144553e-10  Varinance:  2.1633151039738494e-11 \n",
      "\n",
      "Epoch:  19871  Learning Rate:  2.3473002476253887e-10  Varinance:  2.1602930320684052e-11 \n",
      "\n",
      "Epoch:  19872  Learning Rate:  2.344954120636774e-10  Varinance:  2.1572751818866408e-11 \n",
      "\n",
      "Epoch:  19873  Learning Rate:  2.3426103386024664e-10  Varinance:  2.1542615475309633e-11 \n",
      "\n",
      "Epoch:  19874  Learning Rate:  2.3402688991786926e-10  Varinance:  2.151252123112018e-11 \n",
      "\n",
      "Epoch:  19875  Learning Rate:  2.337929800024022e-10  Varinance:  2.148246902748685e-11 \n",
      "\n",
      "Epoch:  19876  Learning Rate:  2.3355930387993373e-10  Varinance:  2.1452458805680377e-11 \n",
      "\n",
      "Epoch:  19877  Learning Rate:  2.3332586131678863e-10  Varinance:  2.1422490507053765e-11 \n",
      "\n",
      "Epoch:  19878  Learning Rate:  2.330926520795251e-10  Varinance:  2.1392564073041867e-11 \n",
      "\n",
      "Epoch:  19879  Learning Rate:  2.3285967593493227e-10  Varinance:  2.1362679445161357e-11 \n",
      "\n",
      "Epoch:  19880  Learning Rate:  2.326269326500348e-10  Varinance:  2.1332836565010588e-11 \n",
      "\n",
      "Epoch:  19881  Learning Rate:  2.3239442199209018e-10  Varinance:  2.1303035374269524e-11 \n",
      "\n",
      "Epoch:  19882  Learning Rate:  2.3216214372858607e-10  Varinance:  2.1273275814699654e-11 \n",
      "\n",
      "Epoch:  19883  Learning Rate:  2.3193009762724503e-10  Varinance:  2.1243557828143616e-11 \n",
      "\n",
      "Epoch:  19884  Learning Rate:  2.3169828345602179e-10  Varinance:  2.1213881356525505e-11 \n",
      "\n",
      "Epoch:  19885  Learning Rate:  2.314667009831005e-10  Varinance:  2.1184246341850473e-11 \n",
      "\n",
      "Epoch:  19886  Learning Rate:  2.3123534997689947e-10  Varinance:  2.115465272620469e-11 \n",
      "\n",
      "Epoch:  19887  Learning Rate:  2.3100423020606848e-10  Varinance:  2.1125100451755235e-11 \n",
      "\n",
      "Epoch:  19888  Learning Rate:  2.3077334143948616e-10  Varinance:  2.1095589460749964e-11 \n",
      "\n",
      "Epoch:  19889  Learning Rate:  2.305426834462645e-10  Varinance:  2.1066119695517496e-11 \n",
      "\n",
      "Epoch:  19890  Learning Rate:  2.3031225599574632e-10  Varinance:  2.103669109846679e-11 \n",
      "\n",
      "Epoch:  19891  Learning Rate:  2.300820588575025e-10  Varinance:  2.1007303612087482e-11 \n",
      "\n",
      "Epoch:  19892  Learning Rate:  2.2985209180133754e-10  Varinance:  2.0977957178949467e-11 \n",
      "\n",
      "Epoch:  19893  Learning Rate:  2.296223545972827e-10  Varinance:  2.0948651741702873e-11 \n",
      "\n",
      "Epoch:  19894  Learning Rate:  2.2939284701560165e-10  Varinance:  2.091938724307794e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19895  Learning Rate:  2.291635688267875e-10  Varinance:  2.0890163625884917e-11 \n",
      "\n",
      "Epoch:  19896  Learning Rate:  2.2893451980156043e-10  Varinance:  2.0860980833014016e-11 \n",
      "\n",
      "Epoch:  19897  Learning Rate:  2.287056997108723e-10  Varinance:  2.083183880743501e-11 \n",
      "\n",
      "Epoch:  19898  Learning Rate:  2.2847710832590371e-10  Varinance:  2.080273749219755e-11 \n",
      "\n",
      "Epoch:  19899  Learning Rate:  2.282487454180617e-10  Varinance:  2.0773676830430795e-11 \n",
      "\n",
      "Epoch:  19900  Learning Rate:  2.2802061075898413e-10  Varinance:  2.0744656765343324e-11 \n",
      "\n",
      "Epoch:  19901  Learning Rate:  2.2779270412053713e-10  Varinance:  2.0715677240223072e-11 \n",
      "\n",
      "Epoch:  19902  Learning Rate:  2.275650252748124e-10  Varinance:  2.068673819843719e-11 \n",
      "\n",
      "Epoch:  19903  Learning Rate:  2.273375739941319e-10  Varinance:  2.0657839583432018e-11 \n",
      "\n",
      "Epoch:  19904  Learning Rate:  2.2711035005104517e-10  Varinance:  2.062898133873267e-11 \n",
      "\n",
      "Epoch:  19905  Learning Rate:  2.268833532183266e-10  Varinance:  2.0600163407943395e-11 \n",
      "\n",
      "Epoch:  19906  Learning Rate:  2.2665658326898018e-10  Varinance:  2.057138573474713e-11 \n",
      "\n",
      "Epoch:  19907  Learning Rate:  2.2643003997623671e-10  Varinance:  2.054264826290549e-11 \n",
      "\n",
      "Epoch:  19908  Learning Rate:  2.2620372311355129e-10  Varinance:  2.0513950936258666e-11 \n",
      "\n",
      "Epoch:  19909  Learning Rate:  2.259776324546078e-10  Varinance:  2.0485293698725285e-11 \n",
      "\n",
      "Epoch:  19910  Learning Rate:  2.2575176777331643e-10  Varinance:  2.0456676494302397e-11 \n",
      "\n",
      "Epoch:  19911  Learning Rate:  2.2552612884381083e-10  Varinance:  2.0428099267065076e-11 \n",
      "\n",
      "Epoch:  19912  Learning Rate:  2.253007154404529e-10  Varinance:  2.0399561961166724e-11 \n",
      "\n",
      "Epoch:  19913  Learning Rate:  2.2507552733782992e-10  Varinance:  2.03710645208387e-11 \n",
      "\n",
      "Epoch:  19914  Learning Rate:  2.248505643107523e-10  Varinance:  2.034260689039026e-11 \n",
      "\n",
      "Epoch:  19915  Learning Rate:  2.2462582613425768e-10  Varinance:  2.031418901420846e-11 \n",
      "\n",
      "Epoch:  19916  Learning Rate:  2.2440131258360875e-10  Varinance:  2.0285810836758046e-11 \n",
      "\n",
      "Epoch:  19917  Learning Rate:  2.2417702343429028e-10  Varinance:  2.0257472302581425e-11 \n",
      "\n",
      "Epoch:  19918  Learning Rate:  2.2395295846201395e-10  Varinance:  2.0229173356298244e-11 \n",
      "\n",
      "Epoch:  19919  Learning Rate:  2.237291174427155e-10  Varinance:  2.0200913942605743e-11 \n",
      "\n",
      "Epoch:  19920  Learning Rate:  2.2350550015255237e-10  Varinance:  2.0172694006278344e-11 \n",
      "\n",
      "Epoch:  19921  Learning Rate:  2.23282106367908e-10  Varinance:  2.014451349216761e-11 \n",
      "\n",
      "Epoch:  19922  Learning Rate:  2.2305893586538942e-10  Varinance:  2.0116372345202154e-11 \n",
      "\n",
      "Epoch:  19923  Learning Rate:  2.2283598842182451e-10  Varinance:  2.0088270510387513e-11 \n",
      "\n",
      "Epoch:  19924  Learning Rate:  2.226132638142674e-10  Varinance:  2.0060207932806122e-11 \n",
      "\n",
      "Epoch:  19925  Learning Rate:  2.223907618199918e-10  Varinance:  2.003218455761693e-11 \n",
      "\n",
      "Epoch:  19926  Learning Rate:  2.221684822164966e-10  Varinance:  2.0004200330055705e-11 \n",
      "\n",
      "Epoch:  19927  Learning Rate:  2.2194642478150287e-10  Varinance:  1.9976255195434633e-11 \n",
      "\n",
      "Epoch:  19928  Learning Rate:  2.217245892929517e-10  Varinance:  1.994834909914232e-11 \n",
      "\n",
      "Epoch:  19929  Learning Rate:  2.2150297552900827e-10  Varinance:  1.992048198664365e-11 \n",
      "\n",
      "Epoch:  19930  Learning Rate:  2.212815832680596e-10  Varinance:  1.9892653803479693e-11 \n",
      "\n",
      "Epoch:  19931  Learning Rate:  2.2106041228871185e-10  Varinance:  1.986486449526766e-11 \n",
      "\n",
      "Epoch:  19932  Learning Rate:  2.208394623697948e-10  Varinance:  1.9837114007700532e-11 \n",
      "\n",
      "Epoch:  19933  Learning Rate:  2.206187332903593e-10  Varinance:  1.980940228654736e-11 \n",
      "\n",
      "Epoch:  19934  Learning Rate:  2.2039822482967476e-10  Varinance:  1.9781729277652887e-11 \n",
      "\n",
      "Epoch:  19935  Learning Rate:  2.201779367672334e-10  Varinance:  1.9754094926937503e-11 \n",
      "\n",
      "Epoch:  19936  Learning Rate:  2.1995786888274788e-10  Varinance:  1.9726499180397152e-11 \n",
      "\n",
      "Epoch:  19937  Learning Rate:  2.197380209561488e-10  Varinance:  1.9698941984103214e-11 \n",
      "\n",
      "Epoch:  19938  Learning Rate:  2.1951839276758903e-10  Varinance:  1.9671423284202478e-11 \n",
      "\n",
      "Epoch:  19939  Learning Rate:  2.192989840974411e-10  Varinance:  1.9643943026916755e-11 \n",
      "\n",
      "Epoch:  19940  Learning Rate:  2.1907979472629472e-10  Varinance:  1.961650115854319e-11 \n",
      "\n",
      "Epoch:  19941  Learning Rate:  2.1886082443496136e-10  Varinance:  1.9589097625453885e-11 \n",
      "\n",
      "Epoch:  19942  Learning Rate:  2.1864207300447144e-10  Varinance:  1.9561732374095844e-11 \n",
      "\n",
      "Epoch:  19943  Learning Rate:  2.1842354021607197e-10  Varinance:  1.9534405350990896e-11 \n",
      "\n",
      "Epoch:  19944  Learning Rate:  2.1820522585123093e-10  Varinance:  1.9507116502735572e-11 \n",
      "\n",
      "Epoch:  19945  Learning Rate:  2.1798712969163467e-10  Varinance:  1.9479865776001077e-11 \n",
      "\n",
      "Epoch:  19946  Learning Rate:  2.177692515191855e-10  Varinance:  1.945265311753291e-11 \n",
      "\n",
      "Epoch:  19947  Learning Rate:  2.1755159111600601e-10  Varinance:  1.9425478474151154e-11 \n",
      "\n",
      "Epoch:  19948  Learning Rate:  2.1733414826443651e-10  Varinance:  1.9398341792750135e-11 \n",
      "\n",
      "Epoch:  19949  Learning Rate:  2.1711692274703264e-10  Varinance:  1.9371243020298363e-11 \n",
      "\n",
      "Epoch:  19950  Learning Rate:  2.1689991434656964e-10  Varinance:  1.9344182103838417e-11 \n",
      "\n",
      "Epoch:  19951  Learning Rate:  2.1668312284603976e-10  Varinance:  1.9317158990486866e-11 \n",
      "\n",
      "Epoch:  19952  Learning Rate:  2.1646654802865005e-10  Varinance:  1.929017362743422e-11 \n",
      "\n",
      "Epoch:  19953  Learning Rate:  2.1625018967782646e-10  Varinance:  1.9263225961944562e-11 \n",
      "\n",
      "Epoch:  19954  Learning Rate:  2.1603404757721125e-10  Varinance:  1.9236315941355847e-11 \n",
      "\n",
      "Epoch:  19955  Learning Rate:  2.158181215106609e-10  Varinance:  1.9209443513079525e-11 \n",
      "\n",
      "Epoch:  19956  Learning Rate:  2.156024112622508e-10  Varinance:  1.9182608624600515e-11 \n",
      "\n",
      "Epoch:  19957  Learning Rate:  2.1538691661626917e-10  Varinance:  1.9155811223477096e-11 \n",
      "\n",
      "Epoch:  19958  Learning Rate:  2.1517163735722208e-10  Varinance:  1.9129051257340804e-11 \n",
      "\n",
      "Epoch:  19959  Learning Rate:  2.1495657326983108e-10  Varinance:  1.9102328673896407e-11 \n",
      "\n",
      "Epoch:  19960  Learning Rate:  2.1474172413903046e-10  Varinance:  1.9075643420921517e-11 \n",
      "\n",
      "Epoch:  19961  Learning Rate:  2.145270897499719e-10  Varinance:  1.9048995446266903e-11 \n",
      "\n",
      "Epoch:  19962  Learning Rate:  2.1431266988802173e-10  Varinance:  1.902238469785612e-11 \n",
      "\n",
      "Epoch:  19963  Learning Rate:  2.1409846433875854e-10  Varinance:  1.8995811123685465e-11 \n",
      "\n",
      "Epoch:  19964  Learning Rate:  2.1388447288797752e-10  Varinance:  1.896927467182389e-11 \n",
      "\n",
      "Epoch:  19965  Learning Rate:  2.13670695321688e-10  Varinance:  1.8942775290412893e-11 \n",
      "\n",
      "Epoch:  19966  Learning Rate:  2.1345713142611083e-10  Varinance:  1.891631292766648e-11 \n",
      "\n",
      "Epoch:  19967  Learning Rate:  2.1324378098768288e-10  Varinance:  1.8889887531870793e-11 \n",
      "\n",
      "Epoch:  19968  Learning Rate:  2.1303064379305445e-10  Varinance:  1.886349905138443e-11 \n",
      "\n",
      "Epoch:  19969  Learning Rate:  2.128177196290868e-10  Varinance:  1.8837147434638053e-11 \n",
      "\n",
      "Epoch:  19970  Learning Rate:  2.126050082828565e-10  Varinance:  1.8810832630134374e-11 \n",
      "\n",
      "Epoch:  19971  Learning Rate:  2.1239250954165296e-10  Varinance:  1.8784554586448033e-11 \n",
      "\n",
      "Epoch:  19972  Learning Rate:  2.1218022319297593e-10  Varinance:  1.8758313252225522e-11 \n",
      "\n",
      "Epoch:  19973  Learning Rate:  2.1196814902453974e-10  Varinance:  1.8732108576185123e-11 \n",
      "\n",
      "Epoch:  19974  Learning Rate:  2.1175628682427105e-10  Varinance:  1.8705940507116576e-11 \n",
      "\n",
      "Epoch:  19975  Learning Rate:  2.1154463638030602e-10  Varinance:  1.8679808993881345e-11 \n",
      "\n",
      "Epoch:  19976  Learning Rate:  2.11333197480995e-10  Varinance:  1.865371398541227e-11 \n",
      "\n",
      "Epoch:  19977  Learning Rate:  2.1112196991489987e-10  Varinance:  1.862765543071353e-11 \n",
      "\n",
      "Epoch:  19978  Learning Rate:  2.1091095347079145e-10  Varinance:  1.860163327886055e-11 \n",
      "\n",
      "Epoch:  19979  Learning Rate:  2.1070014793765412e-10  Varinance:  1.8575647478999884e-11 \n",
      "\n",
      "Epoch:  19980  Learning Rate:  2.10489553104683e-10  Varinance:  1.8549697980349197e-11 \n",
      "\n",
      "Epoch:  19981  Learning Rate:  2.1027916876128178e-10  Varinance:  1.85237847321969e-11 \n",
      "\n",
      "Epoch:  19982  Learning Rate:  2.1006899469706684e-10  Varinance:  1.8497907683902437e-11 \n",
      "\n",
      "Epoch:  19983  Learning Rate:  2.0985903070186488e-10  Varinance:  1.847206678489594e-11 \n",
      "\n",
      "Epoch:  19984  Learning Rate:  2.0964927656571036e-10  Varinance:  1.8446261984678168e-11 \n",
      "\n",
      "Epoch:  19985  Learning Rate:  2.0943973207884986e-10  Varinance:  1.842049323282045e-11 \n",
      "\n",
      "Epoch:  19986  Learning Rate:  2.0923039703173964e-10  Varinance:  1.8394760478964537e-11 \n",
      "\n",
      "Epoch:  19987  Learning Rate:  2.0902127121504314e-10  Varinance:  1.8369063672822615e-11 \n",
      "\n",
      "Epoch:  19988  Learning Rate:  2.0881235441963602e-10  Varinance:  1.8343402764176906e-11 \n",
      "\n",
      "Epoch:  19989  Learning Rate:  2.086036464366e-10  Varinance:  1.8317777702879992e-11 \n",
      "\n",
      "Epoch:  19990  Learning Rate:  2.0839514705722774e-10  Varinance:  1.8292188438854443e-11 \n",
      "\n",
      "Epoch:  19991  Learning Rate:  2.081868560730207e-10  Varinance:  1.826663492209278e-11 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19992  Learning Rate:  2.0797877327568632e-10  Varinance:  1.8241117102657387e-11 \n",
      "\n",
      "Epoch:  19993  Learning Rate:  2.0777089845714257e-10  Varinance:  1.821563493068047e-11 \n",
      "\n",
      "Epoch:  19994  Learning Rate:  2.075632314095153e-10  Varinance:  1.819018835636371e-11 \n",
      "\n",
      "Epoch:  19995  Learning Rate:  2.07355771925136e-10  Varinance:  1.8164777329978544e-11 \n",
      "\n",
      "Epoch:  19996  Learning Rate:  2.071485197965459e-10  Varinance:  1.8139401801865814e-11 \n",
      "\n",
      "Epoch:  19997  Learning Rate:  2.0694147481649364e-10  Varinance:  1.811406172243574e-11 \n",
      "\n",
      "Epoch:  19998  Learning Rate:  2.0673463677793266e-10  Varinance:  1.8088757042167802e-11 \n",
      "\n",
      "Epoch:  19999  Learning Rate:  2.0652800547402571e-10  Varinance:  1.8063487711610676e-11 \n",
      "\n",
      "Epoch:  20000  Learning Rate:  2.0632158069814217e-10  Varinance:  1.8038253681382168e-11 \n",
      "\n",
      "Optimal Weights Reached!!!\n"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "W_new = deepcopy(W_Initial_Guess_Norm)\n",
    "eta = deepcopy(eta_0)\n",
    "sigma = deepcopy(sigma_0)\n",
    "while Epoch <= Max_Epoch:\n",
    "    # Update Weights\n",
    "    i = np.random.randint(0, Raw_Data_Shape[0])\n",
    "    W_new = update_weights(eta, sigma, X_train_norm[i], W_new, Index)\n",
    "    # Print\n",
    "    print('Epoch: ', Epoch, ' Learning Rate: ', eta, ' Varinance: ', sigma, '\\n')\n",
    "    # Next...\n",
    "    eta = decay_learning_rate(eta_0, Epoch, eta_time_const)\n",
    "    sigma = decay_variance(sigma_0, Epoch, sigma_time_const)\n",
    "    Epoch += 1\n",
    "print('Optimal Weights Reached!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_final = deepcopy(W_new)\n",
    "Colour = np.zeros((SOM_Network_Shape[0]*SOM_Network_Shape[1], 3))\n",
    "for i in range(0, Raw_Data_Shape[0]):\n",
    "    bmu = winning_neuron(X_train_norm[i], W_final)\n",
    "    Colour[bmu] = Saturation_Norm[ y_train[i]]\n",
    "\n",
    "Zero_Pos = np.where(~Colour.any(axis=1))[0] # numpy.where(Colour[:, 0] == 0)[0]\n",
    "for i in range(0, Zero_Pos.size):\n",
    "    temp = np.array([])\n",
    "    for j in range(0, Raw_Data_Shape[0]):\n",
    "        a = np.linalg.norm(X_train_norm[j] - W_final[Zero_Pos[i]])\n",
    "        temp = np.concatenate((temp, [a]), axis=0)\n",
    "    bmu = np.argmin(temp)\n",
    "    Colour[Zero_Pos[i]] = Saturation_Norm[ y_train[bmu]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "# setup axes\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "ax.set_xlim((0, SOM_Network_Shape[0]))\n",
    "ax.set_ylim((0, SOM_Network_Shape[1]))\n",
    "ax.set_title('Self-Organising Map after %d iterations' % Max_Epoch)\n",
    "\n",
    "# plot the rectangles\n",
    "i = 0\n",
    "for x in range(0, SOM_Network_Shape[0]):\n",
    "    for y in range(0, SOM_Network_Shape[1]):\n",
    "        ax.add_patch(patches.Rectangle((x, y), 1, 1, facecolor=Colour[i], edgecolor='none'))\n",
    "        i += 1\n",
    "# Add legends\n",
    "legend_elements = [Line2D([0], [0], color=Saturation_Norm[0], lw=4, label='0'), \n",
    "                  Line2D([0], [0], color=Saturation_Norm[1], lw=4, label='1'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[2], lw=4, label='2'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[3], lw=4, label='3'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[4], lw=4, label='4'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[5], lw=4, label='5'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[6], lw=4, label='6'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[7], lw=4, label='7'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[8], lw=4, label='8'),\n",
    "                  Line2D([0], [0], color=Saturation_Norm[9], lw=4, label='9')]\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig('Self-Organising Map.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
